{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdf = glob.glob(r\"PDF2\\*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_document = r\"PDF2\\Как я стал core-разработчиком Python в 19 лет _ Хабр.pdf\"\n",
    "# doc = pymupdf.open(pdf_document)\n",
    "# print(\"Исходный документ: \", doc.name)\n",
    "# print(\"\\nКоличество страниц: %i\\n\\n------------------\\n\\n\" % doc.page_count)\n",
    "# print(doc.metadata)\n",
    "# text = []\n",
    "# for current_page in range(len(doc)):\n",
    "#     page = doc.load_page(current_page)\n",
    "#     page_text = page.get_text(\"text\")\n",
    "#     print(\"Стр. \", current_page + 1, \"\\n\")\n",
    "#     text.append(page_text)\n",
    "#     print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[КАК СТАТЬ АВТОРОМ](https://habr.com/ru/sandbox/start/)  **[vstorozhilov](https://habr.com/ru/users/vstorozhilov/)**  7 часов назад # Три необсуждаемых вопроса о параллельной распределённой обработке данных — чтобы жить стало легче  **Средний** **12 мин** **328** ##### Python*,  Data Engineering*,  Распределённые системы*  Туториал ##### Привет, Хабр! На связи Владимир, техлид в команде разработки ИИ-инструментов в департаменте технологической надёжности одной из крупных компаний. Наша команда помогает делать корпоративные и клиентские сервисы надежнее помощью Data Science. ## Мы помогаем мониторить тысячи подсистем ##### Система, в разработке которой участвует моя команда, предназначена для централизованного мониторинга тысяч отдельных систем, в том числе распределённых географически. Наша группа отвечает за online-модель, подсвечивающую явно аномальные отклонения в показателях мониторинга. С её помощью инженер дежурной смены однозначно определяет, есть ли сейчас проблемы, требующие немедленного реагирования. Данные мониторинга собираются из множества различных источников. И поскольку их поступает много в единицу времени, возникла острая необходимость в распараллеливании. Про потоковую обработку данных, обработку событий и выполнение задач по расписанию в распределённой системе написано немало. Среди множества публикаций, на мой взгляд, можно выделить очень достойные: вот пример с глубоким анализом точек отказа. Но после прочтения многих статей мне не хватило явных ответов на вопросы, неизбежно возникающих у ещё недостаточно опытного разработчика. В большинстве случаев содержание выглядит примерно так: сначала схема с общей шиной (брокер, кеш, реляционная база) и несколькими воркерами (либо потоками) для параллельного выполнения, затем вставки с кодом, где используется какая-то библиотека (или фреймворк). В экосистеме Python это либо Celery, либо же что-то из оркестраторов (Nifi, Airflow, Spark), если речь идёт о задачах data-инжиниринга. Но это, как я уже сказал, не отвечает на все вопросы.   -----  ## Три архитектурных вопроса, которые считаются решёнными по умолчанию  Примерно так выглядели этапы проектирования и первого развёртывания на тестовом стенде :-) ##### Итак, у неопытных разработчиков обязательно возникнут такие вопросы: Как корректно распределять задачи между воркерами, чтобы не возникало различных аномалий конкурентности? Например, когда воркеры дублируют друг друга,   -----  ##### обрабатывая одни и те же данные, или, наоборот, блокируют один другого? Как архитектурно грамотно гарантировать «довыполнение» задачи в случае ошибки воркера? Как гарантировать выполнение задачи не позднее требуемой задержки (актуально для online-систем)? Тут для полноты картины стоит привести примеры нескольких сценариев отказа, которые мне предстояло решить: Воркер взял задачу в работу и получил ошибку из-за сетевой недоступности, без последующего перезапуска контейнера с приложением. Воркер взял задачу в работу и завершился аварийно, с перезапуском контейнера с приложением, например из-за проблем с выделением памяти. Воркер взял задачу в работу, находился в работе больше допустимого времени и завершился успешно или с ошибкой слишком поздно (актуально при наличии требования по максимальной задержке).  А так выглядели первые попытки учесть все возможные точки отказа   -----  ## Почему мой пример может предложить ответы на эти вопросы ##### Предсказательную онлайн-модель, над которой работала наша команда, можно назвать ETL-сервисом, если не углубляться в детали алгоритма обработки данных. Необходимость в подобных перекладчиках, либо же событийных обработчиках, возникает регулярно, когда проектируешь микросервисную систему или потоки данных внутри оркестраторов (data- инженеры, это я про вас). Кроме того, система, компонентом которой является сервис онлайн-модели, высококритичная, из-за чего к нашему сервису предъявляются повышенные нефункциональные требования: Отсутствие недоступности: обработанные данные по каждому показателю мониторинга должны появиться минута в минуту, потери в обработанных данных (пропуски точек на графиках показателей) не допускаются. Высокая пропускная способность (скорость обработки) : в течение 60 секунд нужно успеть загрузить из базы-источника значение каждого показателя за конкретную минуту, и дополнительно 179 предыдущих (то есть суммарно за 180-минутное окно), обработать и положить результат в базу-приёмник (для последующей отрисовки графиков на фронтенде). Всего более 100 тысяч регулярно обновляемых временных рядов с данными по различным системам. Георезервирование : надо уметь пережить отказ целого ЦОДа, на котором запущен отдельный k8s-кластер, где, в свою очередь, развёрнут сервис. Проектирование архитектуры системы с подобными требованиями и первые попытки тестирования ее прототипа привели меня, тогда ещё малоопытного, к перечисленным выше вопросам. Я сделаю упор на эти аспекты далее, при погружении в подробности архитектуры сервиса онлайн-модели. ## Архитектура сервиса онлайн-модели с акцентом на три поставленных вопроса ### В чём помогает механизм групп консьюмеров Kafka ##### Прежде чем перейдём к рассмотрению архитектуры, поговорим немного о том, почему в качестве шины данных использовали брокер Kafka, а также подробнее рассмотрим механизм групп консьюмеров. Про то, что такое Kafka и связанные с ней понятия — продьюсер, консьюмер, топик, партиция, оффсет — можно узнать из видео. Я рекомендую посмотреть его тем, кто пока не знаком с Kafka, так как рассчитываю на понимание при дальнейшем прочтении.   -----  ##### Во многом мне помог механизм групп консьюмеров, с помощью которого уже частично можно реализовать распределённое отказоустойчивое распараллеливание, а именно: Каждый консьюмер закрепляется только за своим набором партиций внутри группы, чтение из других партиций группы невозможно. Это позволяет избегать различных аномалий конкурентности при распределении задач между воркерами. Внутри группы консьюмеров для каждой партиции отслеживается свой собственный оффсет (номер позиции последнего сообщения в очереди, для которого сделан коммит). Например, если консьюмер будет делать коммиты прочитанных позиций только после успешного выполнения и записи результатов, а не сразу после прочтения сообщения из партиции, то в случае ошибки можно будет «довыполнить» задачу при повторной попытке. Консьюмеры периодически отправляют «пульс» (heartbeat) для обозначения себя «живыми». В случае, если какой-то консьюмер перестаёт подавать признаки жизни, или, наоборот, в группе появляется новый, то запускается процедура ребалансировки партиций (partition reassignment) между «живыми» консьюмерами по выбранной стратегии (round robin, range, sticky). Новый консьюмер может продолжить читать из партиции с того оффсета, на котором остановился «умерший», это тоже помогает гарантировать согласованное «довыполнение» при падениях воркеров.  Механизм групп консьюмеров Kafka ##### Краткий итог про группы консьюмеров: на первый и часть второго вопроса отвечает функциональность самой Kafka, что существенно упростило мне жизнь при разработке.   -----  Верхнеуровневая архитектура сервиса онлайн-модели. Оба компонента написаны на Python, чтобы не  нарушать общий стек с коллегами-аналитиками и data scientist-ами. ### Постановщик задач: бизнес-логика, точки отказа, георезервирование ##### Сервис постановки задач (task manager) является вторым ключевым инструментом распараллеливания. Первый — это группы консьюмеров Kafka, его цель — поставить задачу на выполнение онлайн-корректировки по фиксированному набору показателей за конкретный минутный таймстемп: например, 2025-01-01 10:01:00, 2025-01-01 10:02:00 и т. д. Его цикл бизнес-логики повторяется постоянно в реальном времени, с фиксированной величиной паузы между итерациями. Цикл содержит следующий набор действий: 1. Проверить, есть ли в топике уже поставленные на выполнение задачи за текущий и два следующих минутных таймстемпа (то есть в нормальном режиме работы в момент времени 12:05 будет поставлена задача на 12:07; при сбоях, во избежание пропусков в обработанных данных, — дополнительно за 12:06 и 12:05 по необходимости) и завершить цикл работы, а если нет — то переходить на следующий этап. 2. Определить актуальный набор показателей, по которым поступают новые данные (сделать select из таблицы с метаинформацией). 3. Разбить набор уникальных показателей на пакеты. 4. Сформировать задачи на выполнение для каждого пакета за недостающие минутные таймстемпы: в содержании указать таймстемп и набор идентификаторов показателей (metric_id), сгенерировать UUID с использованием порядкового номера батча и таймстемпа. 5. Отправить задачи в топик Kafka, распределяя по партициям: каждая задача в отдельную партицию.   -----  Цикл бизнес-логики постановщика задач ##### Цифры рядом с блоками соответствуют этапам, перечисленным выше. Ключевым шагом для обеспечения отказоустойчивости является предварительная проверка наличия в топике поставленных на выполнение задач за текущий и два следующих минутных таймстемпа. Такой «запас» необходим для «выживания» в случаях аварийного падения контейнера с приложением или ошибок сетевого взаимодействия. Рассмотрим возможные точки отказа (наглядная иллюстрация на рисунке ниже), не связанные с падением контейнера с приложением: 1. Сетевая ошибка чтения из топика Kafka последних задач для определения недостающих таймстемпов. 2. Сетевая ошибка запроса к базе-хранилищу агрегированных данных на получение полного списка актуальных показателей мониторинга. 3. Сетевая ошибка записи недостающих задач в топик Kafka.   -----  Цикл бизнес-логики постановщика задач с точками отказа при сетевом взаимодействии в виде блок-схемы ##### Цифры рядом с блоками соответствуют точкам отказа, перечисленным выше. Во всех случаях преодолением отказа будет повтор цикла на следующей итерации. Аварийное падение контейнера с приложением (на любом из этапов цикла бизнес-логики) преодолевается с помощью контроллера кластера виртуализации с одной стороны, и георезервирования с другой. Достичь георезервирования, а вернее согласованной работы экземпляров приложения «постановщик задач» из разных геокластеров, помогает всё тот же механизм групп консьюмеров Kafka.   -----  Архитектура сервиса постановщика задач c учётом георезервирования ##### В данном случае внутри группы консьюмеров используется собственная стратегия распределения партиций: все партиции только одному консьюмеру, второй при этом выступает в качестве резервной standby (неактивной) копии на случай сбоя первого. В коде постановщика на этапе определения недостающих задач предусмотрена дополнительная проверка на «пустые» данные (их получение возможно тогда, когда за консьюмером нет закреплённых партиций внутри группы, так как ему их не хватило) и, в случае положительного результата, переход сразу на этап паузы до следующей попытки   -----  ##### (итерации цикла бизнес-логики). То есть второй консьюмер будет постоянно получать «пустые» данные, выполняя «холостой» цикл. При отказе в работе активного консьюмера (вызванного, например, падением целого ЦОДа, на котором развёрнут кластер) брокер Kafka перестанет получать от первого heartbeat-сигналы и переназначит все партиции второму консьюмеру. Тем самым, второй консьюмер продолжит ставить задачи данных на выполнение без пропусков, а первый перезапустится контроллером геокластера виртуализации и уже сам станет stanby-копией. ### Воркер: бизнес-логика, точки отказа, георезервирование ##### Обработкой данных по показателям занимается компонент «воркер». Его цикл бизнес- логики повторяется постоянно в реальном времени, с фиксированной величиной паузы между итерациями. Цикл содержит следующий набор действий: 1. Получить задачу на выполнение из топика Kafka. 2. Проверить, выполнена ли она, то есть посмотреть, присутствует ли её UUID в таблице успешно выполненных, если нет — переходить на следующий этап, если есть — завершить цикл работы. 3. Загрузить необходимые данные по набору показателей: получить по каждому показателю значения за указанный таймстемп и предыдущие 179 минут. 4. Выполнить процедуру онлайн-корректировки по данным всего набора показателей из полученной задачи. 5. Записать результаты онлайн-корректировки в базу с обработанными данными с помощью SQL-запроса вида INSERT ... ON CONFLICT ..., чтобы не допустить повторную перезапись, если результирующие данные за таймстемп выполняемой задачи уже были сохранены ранее. 6. Отметить задачу как успешно выполненную: записать UUID в таблицу выполненных задач. 7. Закоммитить оффсет в соответствующую партицию топика (откуда была прочитана задача) в рамках используемой группы консьюмеров.   -----  Цикл бизнес-логики воркерa ##### Цифры рядом с блоками соответствуют этапам, перечисленным выше. Ключевыми шагами для обеспечения отказоустойчивости являются: Предварительная проверка на то, выполнена ли уже задача. Фиксация факта успешного выполнения задачи в отдельной таблице. Сохранение результатов корректировки данных по показателям через INSERT ... ON CONFLICT ... для исключения дублирования или перезаписи. Всё перечисленное гарантирует согласованное «довыполнение» задачи при ошибке воркера на любом из этапов цикла бизнес-логики и исключает повторное выполнение «тяжёлого» расчёта другим воркером в случае, если ошибка произошла на этапе 7.   -----  Автор слишком часто видел, как воркер с первой попытки не хотел брать задачу в работу ##### Рассмотрим возможные точки отказа, не связанные с аварийным падением контейнера с приложением (иллюстрация на рисунке ниже): 1. Сетевая ошибка чтения из топика Kafka задачи на выполнение. 2. Сетевая ошибка запроса к базе для предварительной проверки, была ли данная задача уже выполнена ранее. 3. Сетевая ошибка запроса к базе с целью получения необходимых для выполнения задачи данных. 4. Сетевая ошибка запроса к базе для записи результатов корректировки и возможных метаданных по новым метрикам.   -----  ##### 5. Сетевая ошибка запроса к базе с целью добавления UUID задачи в таблицу успешно выполненных. 6. Сетевая ошибка выполнения коммита в топик Kafka.  Цикл бизнес-логики воркера с точками отказа при сетевом взаимодействии ##### Цифры рядом с блоками соответствуют точкам отказа, перечисленным выше. Во всех случаях преодолением отказа будет повтор цикла на следующей итерации. Аварийное падение контейнера с приложением на любом из этапов цикла бизнес-логики преодолевается с помощью контроллера кластера виртуализации с одной стороны, и георезервирования с другой.   -----  Архитектура сервиса-воркера c учётом георезервирования ##### Вспомним, что в нефункциональных требованиях к системе присутствует пункт про максимальную задержку. По итогам тестов производительности воркера со всеми возможными оптимизациями (многопоточность, подбор оптимального размера пакета показателей, переход на самописные реализации некоторых библиотечных функций) в условиях доступного корпоративного железа максимальное время выполнения составило 30 секунд. С такими результатами очень высока вероятность того, что суммарная задержка в случае сбоя составит больше минуты.   -----  ##### Для выполнения требования по максимальной задержке георезервирование было реализовано так, что каждому геокластеру соответствует своя группа консьюмеров на уровне брокера Kafka. Обе группы настроены на одни и те же партиции. По итогу, в штатном режиме задача за конкретный таймстемп будет выполняться параллельно (дублироваться) двумя воркерами из разных геокластеров, а результат, возможные метаданные по новым метрикам и UUID выполненной задачи будут записаны только один раз (благодаря INSERT ... ON CONFLICT ...). Предполагается, что одновременное «падение» двух воркеров, выполняющих одну и ту же задачу, но находящихся в разных геокластерах (то есть физически в разных ЦОДах), маловероятно. Ещё одна вещь, обеспечивающая согласованность работы консьюмеров из разных геокластеров, — общая таблица с UUID успешно выполненных задач. Она исключает повторное выполнение в случае сбоя только одного из дублирующих воркеров. ## Ещё раз о ключевых моментах обеспечения нефункциональных требований ##### Мы подробно рассмотрели как архитектуру планировщика задач, так и архитектуру воркера (в обоих случаях с обеспечением георезервирования). Все поставленные перед системой нефункциональные требования обеспечены, а именно: Отсутствие недоступности (появление обработанных данных минута в минуту, отсутствие пропусков (потерь) в данных) гарантируется: со стороны планировщика задач — общей группой консьюмеров Kafka между единичными экземплярами контейнеров в разных геокластерах и «запасом» публикуемых в топик задач сразу на текущий и два последующих минутных таймстемпа; со стороны воркера — «дублирующими» группами консьюмеров Kafka для множественных экземпляров контейнеров в разных геокластерах. Высокая пропускная способность гарантируется распараллеливанием потока обработки через множество независимых друг от друга воркеров, а также некоторыми оптимизациями на уровне кода приложения. Георезервирование достигается размещением экземпляров приложений в k8s- кластерах, развёрнутых на географически разнесенных ЦОДах. Согласованная работа между контейнерами из разных геокластеров становится возможной: в случае планировщика задач — благодаря общей группе консьюмеров Kafka между единичными экземплярами контейнеров из разных геокластеров; в случае воркера — благодаря фиксации UUID успешно выполненных задач в одну общую SQL-таблицу контейнерами из разных геокластеров.   -----  Когда всё-таки предусмотрел все возможные сценарии отказа ## Почему архитектурные решения в нашем примере можно переиспользовать много где ещё ##### Рассмотренный пример может повторять типичную реализацию распределённых систем, связанных с обработкой событий, где в качестве события может быть что угодно: сообщение в чате, электронное письмо, транзакция платёжной операции, клиентская активность в CRM. Во всех перечисленных случаях крайне важно гарантировать обязательную доставку данных без потерь и с требованием по максимальной задержке, чтобы не допустить ухудшения клиентского опыта. Разобранный пример актуален и для потоковой обработки с помощью оркестраторов в задачах data-инжиниринга. Используемый оркестраторами directed acyclic graph (DAG) служит для преодоления различных точек отказа примерно такими же способами, как описано в примере. Поэтому в качестве ответов на вопросы, поставленные в начале, можно привести несколько обобщающих выводов, полезных при архитектурном проектировании подобных решений: Для достижения согласованной обработки между распределёнными контейнерами с приложением необходимо реализовать (или использовать возможности готовой технологии) кооперативный механизм распределения задач (событий), чтобы избежать различных аномалий конкурентности вроде нежелательных дублирований или блокировок. Тут, на самом деле, всё будет зависеть от решаемых системой задач и требований. В каких-то случаях может быть достаточно функциональности SQL с транзакциями и   -----  ##### блокировками. В моём случае помогли группы консьюмеров Kafka. Нечто аналогичное можно увидеть и в RabbitMQ, где сообщения из одной и той же очереди распределяются между консьюмерами через round robin. Даже Redis имеет функциональность каких-никаких транзакций. Но если всё-таки обойтись готовыми инструментами никак не получается, то придётся реализовывать ещё один компонент — менеджер задач (событий). Пример можно посмотреть всё в той же статье, упомянутой в предисловии. Чтобы однозначно гарантировать исполнение события (задачи), может быть необходима фиксация факта успешного выполнения в общее хранилище с помощью сохранения туда уникального идентификатора. С другой стороны, это позволит избежать повторного запуска «тяжёлого» алгоритма обработки данных и пустой траты вычислительных ресурсов, если воркер упал на одном из последних этапов. В моём примере это была отдельная SQL-таблица. Из готовых инструментов можно снова вспомнить пример библиотеки Celery в экосистеме Python, которая тоже сохраняет идентификаторы завершённых задач в отдельную очередь, таблицу или массив (в зависимости от того, что используется в качестве шины и хранилища). **Если к системе есть требование по максимально допустимой задержке, то** хорошим способом этого достичь будет дублирование потока обработки, то есть обработка одних и тех же событий (задач) через параллельно запущенные воркеры (возможно, из разных кластеров виртуализации, если есть требование по георезервированию). Теги: конкурентность, распределенные системы, распределённая обработка данных, георезервирование, таск-менеджер, таск-раннеры, дата-инжиниринг, параллелизм Хабы: Python, Data Engineering, Распределённые системы  **+1** **11** **0** ### **Редакторский дайджест**  Присылаем лучшие статьи раз в месяц  Электропочта  #### **3**  Карма  ##### **1**  Рейтинг  ##### Владимир Сторожилов @vstorozhilov  backend, devops и когда нужно data scientist ;-)   -----  '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''.join(text).replace('\\n',' ')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_markdown_text(document) -> str:\n",
    "    return pymupdf4llm.to_markdown(document)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_markdown(document: str):\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(document):\n",
    "    doc = pymupdf.open(document)\n",
    "    md = extract_markdown_text(document)\n",
    "    lines = md.splitlines()\n",
    "    \n",
    "    # 1. Author\n",
    "    author = None\n",
    "    for line in lines:\n",
    "        m = re.match(r\"\\*\\*\\[(.+?)\\]\\(\", line)\n",
    "        if m:\n",
    "            author = m.group(1)\n",
    "            break\n",
    "\n",
    "    # 2. Company (if any)\n",
    "    company = None\n",
    "    for line in lines:\n",
    "        if re.search(r\"\\d{2}\\.\\d{2}\\.\\d{4}\", line) and \"/\" in line:\n",
    "            company = line.split(\"/\")[-1].strip()\n",
    "            break\n",
    "\n",
    "    # 3. Publish date (absolute or relative)\n",
    "    publish_date = None\n",
    "    for line in lines:\n",
    "        m = re.match(r\"(\\d{2}\\.\\d{2}\\.\\d{4},\\s*\\d{2}:\\d{2})\", line)\n",
    "        if m:\n",
    "            publish_date = m.group(1); break\n",
    "        m = re.match(r\"(\\d{1,2}\\s+[А-Яа-я]{3}\\s+в\\s+\\d{2}:\\d{2})\", line)\n",
    "        if m:\n",
    "            publish_date = m.group(1); break\n",
    "        m = re.search(r\"(\\d+\\s+(?:минут[аи]?|часов?|дней?)\\s+назад)\", line)\n",
    "        if m:\n",
    "            publish_date = doc.metadata['creationDate']; break\n",
    "        \n",
    "        if publish_date is None:\n",
    "            publish_date = doc.metadata['creationDate']\n",
    "\n",
    "    # 4. Description and Rating\n",
    "    description = None\n",
    "    rating = None\n",
    "    for line in lines:\n",
    "        parts = re.findall(r\"\\*\\*(.+?)\\*\\*\", line)\n",
    "        # with company: expect 3+ bold parts: desc, time, rating\n",
    "        if company and len(parts) >= 3 and re.match(r\"^\\d+K?$\", parts[-1]):\n",
    "            description = parts[0]\n",
    "            raw = parts[-1]\n",
    "            rating = int(raw[:-1])*1000 if raw.endswith(\"K\") else int(raw)\n",
    "            break\n",
    "        # without company: expect 2 bold parts: time, rating\n",
    "        if not company and len(parts) >= 2 and re.match(r\"^\\d+K?$\", parts[-1]):\n",
    "            raw = parts[-1]\n",
    "            rating = int(raw[:-1])*1000 if raw.endswith(\"K\") else int(raw)\n",
    "            break\n",
    "\n",
    "    # 5. Find the tags line (#### or #####) with comma-separated categories\n",
    "    tags_idx = next(\n",
    "        (i for i, line in enumerate(lines)\n",
    "         if re.match(r\"^#{4,5}\\s*[^#].*\\*,\", line)),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    # 6. Capture article_text: all non-blank lines after tags_idx until the first '-----'\n",
    "    article_text = \"\"\n",
    "    if tags_idx is not None:\n",
    "        content_lines = []\n",
    "        for line in lines[tags_idx + 1:]:\n",
    "            if line.strip().startswith(\"-----\"):\n",
    "                break\n",
    "            if line.strip():\n",
    "                content_lines.append(line.strip())\n",
    "        article_text = \"\\n\\n\".join(content_lines)\n",
    "\n",
    "    # 7. Build result dict\n",
    "    result = {\n",
    "        \"author\": author,\n",
    "        \"publish_date\": publish_date,\n",
    "        \"rating\": rating,\n",
    "        \"article_text\": article_text\n",
    "    }\n",
    "    if company:\n",
    "        result[\"company_name\"] = company\n",
    "        result[\"description\"] = description\n",
    "    else:\n",
    "        result[\"company_name\"] = None\n",
    "        result[\"description\"] = None\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brak = []\n",
    "Author = []\n",
    "PublishDate = []\n",
    "Rating = []\n",
    "ArticleText = []\n",
    "CompanyName = []\n",
    "CompanyDescription = []\n",
    "for pdf in all_pdf:\n",
    "    # try:  \n",
    "        text = extract_markdown_text(pdf)\n",
    "\n",
    "\n",
    "        result = parse_article(pdf)\n",
    "        author = result[\"author\"]\n",
    "        publish_date = result[\"publish_date\"]\n",
    "        rating = result[\"rating\"]\n",
    "        article_text = result[\"article_text\"]\n",
    "        company_name = result[\"company_name\"]\n",
    "        company_description = result[\"description\"]\n",
    "\n",
    "        Author.append(author)\n",
    "        PublishDate.append(publish_date)\n",
    "        Rating.append(rating)\n",
    "        ArticleText.append(article_text)\n",
    "        CompanyName.append(company_name)\n",
    "        CompanyDescription.append(company_description)\n",
    "    # except:\n",
    "        # Brak.append(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Brak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import float32\n",
    "\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "# 'Author': pd.Series(dtype=str),\n",
    "# 'PublishDate': pd.Series(dtype=float32),\n",
    "# 'DataPublish': pd.Series(dtype=str),\n",
    "# 'Activity': pd.Series(dtype=str),\n",
    "# 'TextArticle': pd.Series(dtype=str)\n",
    "# })\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Author': pd.Series(dtype=str),\n",
    "    'PublishDate': pd.Series(dtype=str),\n",
    "    'Rating': pd.Series(dtype=float32),\n",
    "    'ArticleText': pd.Series(dtype=str),\n",
    "    'CompanyName': pd.Series(dtype=str),\n",
    "    'CompanyDescription': pd.Series(dtype=str)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TraPhro', None, 'technokratiya', None, 'LizzieSimpson', None, 'BabayMazay', 'El_Gato_Grande', 'AnnieBronson', 'manul', 'Guitariz', 'AlfaTeam', None, 'Exosphere', 'empenoso', 'Systems_Education', 'artyomsoft', 'Eclips4', 'erbanovanastasia', None, 'Arnak', 'TourmalineCore', 'Bright_Translate', 'vstorozhilov']\n"
     ]
    }
   ],
   "source": [
    "print(Author)\n",
    "df['Author'] = Author\n",
    "df['PublishDate'] = PublishDate\n",
    "df['Rating'] = Rating\n",
    "df['ArticleText'] = ArticleText\n",
    "df['CompanyName'] = CompanyName\n",
    "df['CompanyDescription'] = CompanyDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "PublishDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ArticleText",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CompanyName",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "CompanyDescription",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f63b698c-50e0-4aa1-af92-9ee9ae328b02",
       "rows": [
        [
         "0",
         "TraPhro",
         "D:20250318034441+00'00'",
         null,
         "## От аутсайдера-задиры до лидера литографии\n\n#### В начале 1980-х годов небольшой городок Вельдховен тихо пожинал плоды своего соседа Эйндховена — процветающего промышленного центра благодаря присутствию голландского электронного гиганта Philips. Поскольку Philips продвигал технологические инновации в регионе, его влияние распространялось за пределы Эйндховена, способствуя волновому эффекту промышленной активности и технического опыта, которые достигли Вельдховена. С самого начала ASML был подразделением, которое многие в материнской компании считали скорее обузой, чем возможностью. Фактически, это был проблемный ребенок Philips — проект, который потратил более десятилетия на попытки проникнуть в фотолитографический бизнес без особого успеха. В Philips шутили, что создание ASML было по сути отложенным увольнением для переведенных в него сотрудников. Несмотря на внутренний скептицизм, Philips увидела спасение в партнерстве с ASM International, другой голландской компанией по производству оборудования для полупроводников, которая сегодня занимает лидирующие позиции на рынке оборудования для так называемого атомно-слоевого осаждения (ALD) и эпитаксии, используемых в передовом производстве микросхем. В 1984 году ASML была создана как совместное предприятие Philips и ASM со штаб-квартирой в Вельдховене. Поначалу компанию в значительной степени считали неудачной, не имея ни коммерческой жизнеспособности, ни даже надлежащего офиса. Ее первым рабочим пространством был набор деревянных бараков в кампусе Philips в Эйндховене. Моральный дух был низким, многие сотрудники",
         null,
         null
        ],
        [
         "1",
         null,
         "D:20230320172822+00'00'",
         null,
         "",
         null,
         null
        ],
        [
         "2",
         "technokratiya",
         "D:20250303175201+00'00'",
         "419.0",
         "",
         null,
         null
        ],
        [
         "3",
         null,
         "D:20230320160606+00'00'",
         null,
         "",
         null,
         null
        ],
        [
         "4",
         "LizzieSimpson",
         "D:20240208175839+00'00'",
         "13000.0",
         "",
         null,
         null
        ],
        [
         "5",
         null,
         "D:20230320171750+00'00'",
         null,
         "",
         null,
         null
        ],
        [
         "6",
         "BabayMazay",
         "11 апр в 12:01",
         "51.0",
         "",
         null,
         null
        ],
        [
         "7",
         "El_Gato_Grande",
         "8 апр в 15:08",
         "5000.0",
         "",
         null,
         null
        ],
        [
         "8",
         "AnnieBronson",
         "D:20240208175310+00'00'",
         "107.0",
         "",
         null,
         null
        ],
        [
         "9",
         "manul",
         "07.03.2025, 08:20",
         "466.0",
         "",
         "Хабр",
         "Простой"
        ],
        [
         "10",
         "Guitariz",
         "D:20250303175101+00'00'",
         "85.0",
         "",
         null,
         null
        ],
        [
         "11",
         "AlfaTeam",
         "D:20250303175240+00'00'",
         "717.0",
         "",
         null,
         null
        ],
        [
         "12",
         null,
         "D:20230320172509+00'00'",
         null,
         "",
         null,
         null
        ],
        [
         "13",
         "Exosphere",
         "D:20240208175908+00'00'",
         "8.0",
         "",
         null,
         null
        ],
        [
         "14",
         "empenoso",
         "D:20250318034227+00'00'",
         "360.0",
         "Кейс\n\n#### Подбор бытовой техники для своего умного дома может быть сложной задачей. Даже если техника имеет \"умную часть\", не факт что она подойдёт именно к вашей экосистеме. Поэтому вопрос совместимости лучше выяснить ещё до покупки устройства. Недавно захотел подобрать себе робот-пылесос, а на сайте Home Assistant как обычно указаны только обобщенные интеграции. При этом на сайте Алисы приведены тысячи конкретных совместимых устройств, правда без цены и даже без указания выпускается ли до сих пор эта модель или нет. И тут мне пришла в голову идея: поместить эти совместимые устройства в Google Таблицу, а затем написать Google Apps Script, который через поиск найти цены на все эти устройства, а затем я уже выберу конкретное устройство исходя из моего бюджета.\n\n## Проблема совместимости устройств\n\n#### Разные платформы: от Home Assistant до Яндекс Алисы - используют собственные методы подключения. Одни устройства работают через Wi-Fi, другие через Zigbee или Bluetooth, а",
         null,
         null
        ],
        [
         "15",
         "Systems_Education",
         "20 мар в 14:20",
         "22.0",
         "Туториал\n\nlook, use the source!\n\n## 1. Указывайте количественно и качественно выраженные достижения\n\n##### Это самый главный и мощный пункт. Большинство людей пишут какие-то беспомощные аморфные функции и фразы про обязанности и участие — «состоял, привлекался, принимал участие». Это выглядит, как свидетель из Фрязино, а не мощный проектный специалист, который будет двигать проект вперёд.\n\n**0** **27** **22**",
         null,
         null
        ],
        [
         "16",
         "artyomsoft",
         "20 мар в 14:00",
         "0.0",
         "",
         null,
         null
        ],
        [
         "17",
         "Eclips4",
         "10 апр в 18:17",
         "33000.0",
         "Кейс [✏✏️ Технотекст 7](https://habr.com/ru/technotext/2024/) [Сезон Open source](https://habr.com/ru/specials/898552/)\n\n#### 20 сентября 2024 года я стал одним из участников команды разработки CPython.\n\n## Что значит CPython Core developer?\n\n#### CPython Core Developer — это core-разработчик, имеющий официальные полномочия вносить изменения в исходный код интерпретатора CPython, который является самой распространённой реализацией языка Python. Коротко говоря, это те люди, которые могут мержить пул-реквесты в репозитории CPython.\n\n## Мои первые шаги в CPython\n\n#### Первым моим пул-реквестом был фикс для документации. Это может показаться странным, но на самом деле такие изменения имеют смысл. Именно такие небольшие изменения позволяют познакомиться с workflow проекта: как правильно оформлять коммиты, как устроена система CI, как проходит процесс ревью, как добавлять записи в раздел What's New, и в целом — как построено взаимодействие с другими участниками разработки. Этот первый шаг особенно важен, потому что он снижает порог входа и помогает понять не только технические, но и социальные аспекты участия в проекте: как сообщество относится к новичкам, насколько оно открыто к диалогу и как формируется культура совместной работы. В случае с CPython всё оказалось очень здорово — core-разработчики оказались дружелюбными, терпеливыми и охотно шли навстречу. Сейчас я понимаю, что если бы сообщество было менее дружелюбным, я бы скорее всего забросил этот путь. Ревью пул-реквестов в основном делают triager'ы (о них я расскажу позже) и core- разработчики. Тем не менее, комментировать и ревьювить может абсолютно любой человек — это приветствуется. Однако чтобы пул-реквест был принят и влит в основную ветку, требуется как минимум одно одобрение (approve) от core-разработчика.\n\n## Выбор направления",
         null,
         null
        ],
        [
         "18",
         "erbanovanastasia",
         "13 апр в 15:14",
         "14000.0",
         " \n\n##### За последние несколько недель сразу несколько производителей анонсировали и/или запустили продажи новых мини-ПК: от пассивного MeLE Quieter 4C до флагманского GMK EVO-X2 с дискретной графикой. Одни компании делают ставку на энергоэффективность и компактность, другие — на производительность и интерфейсы. Разбираемся, что изменилось и что предлагают новые устройства.\n\n## Asus NUC 15 Pro",
         null,
         null
        ],
        [
         "19",
         null,
         "D:20230320173002+00'00'",
         null,
         "",
         null,
         null
        ],
        [
         "20",
         "Arnak",
         "D:20250429044043+00'00'",
         "52000.0",
         "",
         null,
         null
        ],
        [
         "21",
         "TourmalineCore",
         "D:20250429044214+00'00'",
         "12000.0",
         "Мнение\n\n##### Data Science сейчас во многом благодаря активному маркетингу становится очень популярной темой. Быть датасаентистом – модно и, как говорят многие рекламки, которые часто попадаются на глаза, не так уж и сложно. Ходят слухи, что работодатели стоят в очереди за возможность взять человека с курсов. Получить оффер на работу крайне легко, ведь в ваши обязанности будет входить требование данных от заказчика (как обычно говорят, чем больше данных – тем лучше) и закидывать их в искусственный интеллект, который работает по принципу черного ящика. Кстати, еще и платят немереное количество денег за всё это.  Спойлер : это не так.",
         null,
         null
        ],
        [
         "22",
         "Bright_Translate",
         "11 апр в 16:01",
         "137.0",
         "Обзор Перевод\n\n[Автор оригинала: Antonio G. Di Benedetto](https://www.theverge.com/tech/640119/camera-raw-spec-format-explained-adobe-dng-canon-nikon-sony-fujifilm)",
         null,
         null
        ],
        [
         "23",
         "vstorozhilov",
         "D:20250318034354+00'00'",
         "328.0",
         "Туториал\n\n##### Привет, Хабр! На связи Владимир, техлид в команде разработки ИИ-инструментов в департаменте технологической надёжности одной из крупных компаний. Наша команда помогает делать корпоративные и клиентские сервисы надежнее помощью Data Science.\n\n## Мы помогаем мониторить тысячи подсистем\n\n##### Система, в разработке которой участвует моя команда, предназначена для централизованного мониторинга тысяч отдельных систем, в том числе распределённых географически. Наша группа отвечает за online-модель, подсвечивающую явно аномальные отклонения в показателях мониторинга. С её помощью инженер дежурной смены однозначно определяет, есть ли сейчас проблемы, требующие немедленного реагирования. Данные мониторинга собираются из множества различных источников. И поскольку их поступает много в единицу времени, возникла острая необходимость в распараллеливании. Про потоковую обработку данных, обработку событий и выполнение задач по расписанию в распределённой системе написано немало. Среди множества публикаций, на мой взгляд, можно выделить очень достойные: вот пример с глубоким анализом точек отказа. Но после прочтения многих статей мне не хватило явных ответов на вопросы, неизбежно возникающих у ещё недостаточно опытного разработчика. В большинстве случаев содержание выглядит примерно так: сначала схема с общей шиной (брокер, кеш, реляционная база) и несколькими воркерами (либо потоками) для параллельного выполнения, затем вставки с кодом, где используется какая-то библиотека (или фреймворк). В экосистеме Python это либо Celery, либо же что-то из оркестраторов (Nifi, Airflow, Spark), если речь идёт о задачах data-инжиниринга. Но это, как я уже сказал, не отвечает на все вопросы.",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 24
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>Rating</th>\n",
       "      <th>ArticleText</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>CompanyDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TraPhro</td>\n",
       "      <td>D:20250318034441+00'00'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>## От аутсайдера-задиры до лидера литографии\\n...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>D:20230320172822+00'00'</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technokratiya</td>\n",
       "      <td>D:20250303175201+00'00'</td>\n",
       "      <td>419.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>D:20230320160606+00'00'</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LizzieSimpson</td>\n",
       "      <td>D:20240208175839+00'00'</td>\n",
       "      <td>13000.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>D:20230320171750+00'00'</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BabayMazay</td>\n",
       "      <td>11 апр в 12:01</td>\n",
       "      <td>51.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>El_Gato_Grande</td>\n",
       "      <td>8 апр в 15:08</td>\n",
       "      <td>5000.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AnnieBronson</td>\n",
       "      <td>D:20240208175310+00'00'</td>\n",
       "      <td>107.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>manul</td>\n",
       "      <td>07.03.2025, 08:20</td>\n",
       "      <td>466.0</td>\n",
       "      <td></td>\n",
       "      <td>Хабр</td>\n",
       "      <td>Простой</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Guitariz</td>\n",
       "      <td>D:20250303175101+00'00'</td>\n",
       "      <td>85.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AlfaTeam</td>\n",
       "      <td>D:20250303175240+00'00'</td>\n",
       "      <td>717.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>D:20230320172509+00'00'</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Exosphere</td>\n",
       "      <td>D:20240208175908+00'00'</td>\n",
       "      <td>8.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>empenoso</td>\n",
       "      <td>D:20250318034227+00'00'</td>\n",
       "      <td>360.0</td>\n",
       "      <td>Кейс\\n\\n#### Подбор бытовой техники для своего...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Systems_Education</td>\n",
       "      <td>20 мар в 14:20</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Туториал\\n\\nlook, use the source!\\n\\n## 1. Ука...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>artyomsoft</td>\n",
       "      <td>20 мар в 14:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Eclips4</td>\n",
       "      <td>10 апр в 18:17</td>\n",
       "      <td>33000.0</td>\n",
       "      <td>Кейс [✏✏️ Технотекст 7](https://habr.com/ru/te...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>erbanovanastasia</td>\n",
       "      <td>13 апр в 15:14</td>\n",
       "      <td>14000.0</td>\n",
       "      <td> \\n\\n##### За последние несколько недель сра...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>None</td>\n",
       "      <td>D:20230320173002+00'00'</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Arnak</td>\n",
       "      <td>D:20250429044043+00'00'</td>\n",
       "      <td>52000.0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TourmalineCore</td>\n",
       "      <td>D:20250429044214+00'00'</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>Мнение\\n\\n##### Data Science сейчас во многом ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bright_Translate</td>\n",
       "      <td>11 апр в 16:01</td>\n",
       "      <td>137.0</td>\n",
       "      <td>Обзор Перевод\\n\\n[Автор оригинала: Antonio G. ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vstorozhilov</td>\n",
       "      <td>D:20250318034354+00'00'</td>\n",
       "      <td>328.0</td>\n",
       "      <td>Туториал\\n\\n##### Привет, Хабр! На связи Влади...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Author              PublishDate   Rating  \\\n",
       "0             TraPhro  D:20250318034441+00'00'      NaN   \n",
       "1                None  D:20230320172822+00'00'      NaN   \n",
       "2       technokratiya  D:20250303175201+00'00'    419.0   \n",
       "3                None  D:20230320160606+00'00'      NaN   \n",
       "4       LizzieSimpson  D:20240208175839+00'00'  13000.0   \n",
       "5                None  D:20230320171750+00'00'      NaN   \n",
       "6          BabayMazay           11 апр в 12:01     51.0   \n",
       "7      El_Gato_Grande            8 апр в 15:08   5000.0   \n",
       "8        AnnieBronson  D:20240208175310+00'00'    107.0   \n",
       "9               manul        07.03.2025, 08:20    466.0   \n",
       "10           Guitariz  D:20250303175101+00'00'     85.0   \n",
       "11           AlfaTeam  D:20250303175240+00'00'    717.0   \n",
       "12               None  D:20230320172509+00'00'      NaN   \n",
       "13          Exosphere  D:20240208175908+00'00'      8.0   \n",
       "14           empenoso  D:20250318034227+00'00'    360.0   \n",
       "15  Systems_Education           20 мар в 14:20     22.0   \n",
       "16         artyomsoft           20 мар в 14:00      0.0   \n",
       "17            Eclips4           10 апр в 18:17  33000.0   \n",
       "18   erbanovanastasia           13 апр в 15:14  14000.0   \n",
       "19               None  D:20230320173002+00'00'      NaN   \n",
       "20              Arnak  D:20250429044043+00'00'  52000.0   \n",
       "21     TourmalineCore  D:20250429044214+00'00'  12000.0   \n",
       "22   Bright_Translate           11 апр в 16:01    137.0   \n",
       "23       vstorozhilov  D:20250318034354+00'00'    328.0   \n",
       "\n",
       "                                          ArticleText CompanyName  \\\n",
       "0   ## От аутсайдера-задиры до лидера литографии\\n...        None   \n",
       "1                                                            None   \n",
       "2                                                            None   \n",
       "3                                                            None   \n",
       "4                                                            None   \n",
       "5                                                            None   \n",
       "6                                                            None   \n",
       "7                                                            None   \n",
       "8                                                            None   \n",
       "9                                                            Хабр   \n",
       "10                                                           None   \n",
       "11                                                           None   \n",
       "12                                                           None   \n",
       "13                                                           None   \n",
       "14  Кейс\\n\\n#### Подбор бытовой техники для своего...        None   \n",
       "15  Туториал\\n\\nlook, use the source!\\n\\n## 1. Ука...        None   \n",
       "16                                                           None   \n",
       "17  Кейс [✏✏️ Технотекст 7](https://habr.com/ru/te...        None   \n",
       "18   \\n\\n##### За последние несколько недель сра...        None   \n",
       "19                                                           None   \n",
       "20                                                           None   \n",
       "21  Мнение\\n\\n##### Data Science сейчас во многом ...        None   \n",
       "22  Обзор Перевод\\n\\n[Автор оригинала: Antonio G. ...        None   \n",
       "23  Туториал\\n\\n##### Привет, Хабр! На связи Влади...        None   \n",
       "\n",
       "   CompanyDescription  \n",
       "0                None  \n",
       "1                None  \n",
       "2                None  \n",
       "3                None  \n",
       "4                None  \n",
       "5                None  \n",
       "6                None  \n",
       "7                None  \n",
       "8                None  \n",
       "9             Простой  \n",
       "10               None  \n",
       "11               None  \n",
       "12               None  \n",
       "13               None  \n",
       "14               None  \n",
       "15               None  \n",
       "16               None  \n",
       "17               None  \n",
       "18               None  \n",
       "19               None  \n",
       "20               None  \n",
       "21               None  \n",
       "22               None  \n",
       "23               None  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = extract_text_from_pdf(r\"PDF2\\ASML_ Создание самых сложных машин на Земле _ Хабр.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pars_pdf(pdf[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
