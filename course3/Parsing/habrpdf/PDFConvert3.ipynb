{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03dea4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2445ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка в list пути всех нужных pdf файлов\n",
    "all_pdf = glob.glob(r\"PDF2\\*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597a4de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9d369d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PDF2\\\\ASML_ Создание самых сложных машин на Земле _ Хабр.pdf',\n",
       " 'PDF2\\\\Cocoapods, Carthage, SPM как выбрать менеджер зависимостей в iOS.pdf',\n",
       " 'PDF2\\\\Deutsche Telekom и Perplexity объявили о новом «AI Phone» стоимостью менее 1 000 долларов _ Хабр.pdf',\n",
       " 'PDF2\\\\OSINT & Hacking — как работает фишинг для нельзяграма _ Хабр3.pdf',\n",
       " 'PDF2\\\\Yandex N.V. запретили в\\xa0течение пяти лет создавать аналогичные «Яндексу» сервисы _ Хабр.pdf',\n",
       " 'PDF2\\\\Быстрое начало работы с Gitlab CICD.pdf',\n",
       " 'PDF2\\\\Газоразрядная лампа с символами, своими руками _ Хабр.pdf',\n",
       " 'PDF2\\\\Геймеры и сообщества, повлиявшие на геймдев _ Хабр.pdf',\n",
       " 'PDF2\\\\Генеральный директор Mozilla покинула свой пост _ Хабр.pdf',\n",
       " 'PDF2\\\\За что безопасники будут гореть в аду_ _ Хабр.pdf',\n",
       " 'PDF2\\\\Заезжаем в Kotlin Multiplatform. Но какой ценой_ _ Хабр.pdf',\n",
       " 'PDF2\\\\ИИ-агенты в Альфа-Банке_ нейросети создают автотесты без участия человека _ Хабр.pdf',\n",
       " 'PDF2\\\\Инструменты наблюдаемости, о которых нужно знать в 2023 году.pdf',\n",
       " 'PDF2\\\\История российской науки_ напишем вместе_ _ Хабр.pdf',\n",
       " 'PDF2\\\\Как быстро найти цены на совместимую с Home Assistant бытовую технику_ мой опыт автоматизации поиска цен _ Хабр.pdf',\n",
       " 'PDF2\\\\Как системному аналитику написать хорошее резюме —\\xa011 рекомендаций _ Хабр.pdf',\n",
       " 'PDF2\\\\Как создать аппаратный эмулятор CD-ROM без паяльника _ Хабр.pdf',\n",
       " 'PDF2\\\\Как я стал core-разработчиком Python в 19 лет _ Хабр.pdf',\n",
       " 'PDF2\\\\Мини-ПК на любой вкус_ обзор свежих моделей апреля 2025 года _ Хабр.pdf',\n",
       " 'PDF2\\\\Может ли chatGPT забронировать столик в ресторане через WhatsApp.pdf',\n",
       " 'PDF2\\\\Новые утечки. Что мы знаем о выходе Windows 12 _ Хабр.pdf',\n",
       " 'PDF2\\\\Почему Data Science не для вас_ _ Хабр.pdf',\n",
       " 'PDF2\\\\Почему в индустрии фотокамер такой беспорядок с форматами RAW, и что говорят производители_ _ Хабр.pdf',\n",
       " 'PDF2\\\\Три необсуждаемых вопроса о параллельной распределённой обработке данных — чтобы жить стало легче _ Хабр.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9846963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный документ:  Document('PDF2\\Быстрое начало работы с Gitlab CICD.pdf')\n",
      "\n",
      "Количество страниц: 14\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "{'format': 'PDF 1.7', 'title': 'Aspose', 'author': 'Aspose', 'subject': 'Aspose', 'keywords': '', 'creator': 'Aspose Pty Ltd.', 'producer': 'Aspose.PDF for .NET 23.2.0', 'creationDate': \"D:20230320171750+00'00'\", 'modDate': \"D:20230320171803+00'00'\", 'trapped': '', 'encryption': None}\n",
      "Стр.  1 \n",
      "\n",
      "4.29\n",
      "Оценка\n",
      "280.79\n",
      "Рейтинг\n",
      "Southbridge\n",
      "Обеспечиваем стабильную работу highload-проектов\n",
      "Автор оригинала: Seifeldin Mahjoub\n",
      "Перевели статью о создании пайплайна для развертывания\n",
      "статического веб-сайта на AWS S3 Bucket на примере Gitlab\n",
      "CI/CD, чтобы быстро вникнуть в основы технологии и начать\n",
      "применять ее в работе. В статье рассматриваются базовые\n",
      "концепции CI и CD, а также этапы CI/CD-пайплайна.\n",
      " 2 часа назад\n",
      "5 мин\n",
      "zubarek\n",
      "Быстрое начало работы с Gitlab CI/CD:\n",
      "пайплайн для веб-сайта на AWS S3 Bucket\n",
      "362\n",
      "Блог компании Southbridge\n",
      ", \n",
      "Тестирование IT-систем*\n",
      ", \n",
      "Системное администрирование*\n",
      ", \n",
      "Туториал\n",
      "Перевод\n",
      "КАК СТАТЬ АВТОРОМ\n",
      "\n",
      "Стр.  2 \n",
      "\n",
      "От автора\n",
      "Мне повезло быть частью некоторых профессиональных команд,\n",
      "каждая из которых применяла несколько DevOps практик. И меня\n",
      "поразило то, как качество кода, скорость разработки и позитивный\n",
      "настрой команды коррелируют с CI/CD-пайплайном.\n",
      "По моему мнению, зрелость пайплайна может служить\n",
      "прекрасным показателем опытности разработчика, качества\n",
      "кода и эффективности всей команды.\n",
      "Во многих случаях, которые я наблюдал, пайплайны были\n",
      "выстроены либо DevOps-инженером, либо отдельной DevOps-\n",
      "командой. Да и последний отчет State of CD 2022\n",
      "продемонстрировал, что только 22% разработчиков создают\n",
      "пайплайны.\n",
      "Моя цель — увеличить это число: помочь разработчикам взять на\n",
      "себя ответственность за пайплайны, выстраивать непрерывный\n",
      "процесс доставки и создавать качественный код.\n",
      "В статье рассматриваются фундаментальные концепции CI и CD.\n",
      "\n",
      "Стр.  3 \n",
      "\n",
      "Что такое CI/CD?\n",
      "Многие бизнесы применяют фреймворки Agile, так как они позволяют\n",
      "менять приоритеты и повышать скорость доставки. Кроме всего\n",
      "прочего, такой подход улучшает атмосферу в команде и помогает\n",
      "увеличить прибыль.\n",
      "Если ваша компания следует по пути Agile, то принятие культуры,\n",
      "философии и практик DevOps станет ее большим преимуществом.\n",
      "Модное словечко последних десятилетий, DevOps сегодня считается\n",
      "настоящим стандартом индустрии. CI/CD — это практика DevOps,\n",
      "которая помогает разработчикам ПО доставлять изменения в коде с\n",
      "высокой частотой и надежностью.\n",
      "«Быстрый билд, быстрый тест, быстрый фейл»\n",
      "При наличии автоматизированных тестов команды тяготеют к общей\n",
      "автоматизации задач и частым, надежным поставкам кода. Создание\n",
      "CI/CD-пайплайна в этом случае может привести к нескольким\n",
      "преимуществам.\n",
      "Бизнес выигрывает от снижения затрат и повышения\n",
      "производительности, ускорения Time to Market и адаптации к\n",
      "изменяющимся требованиям рынка.\n",
      "Команда выигрывает от быстрой обратной связи, улучшения\n",
      "эффективности разработки, уменьшения количества бутылочных\n",
      "горлышек и повышения уровня вовлеченности и удовлетворенности\n",
      "сотрудников.\n",
      "Фазы CI и CD\n",
      "\n",
      "Стр.  4 \n",
      "\n",
      "CI — непрерывная интеграция. Непрерывная интеграция\n",
      "позволяет по много раз в день коммитить изменения в основную\n",
      "ветку вашей кодовой базы.\n",
      "Учитывая ограниченные когнитивные способности человека, CI\n",
      "стимулирует разработчиков вносить в код небольшие изменения,\n",
      "которые легче рассмотреть, покрыть автоматическими тестами и\n",
      "часто релизить.\n",
      "Это позволяет избежать напряженных и переполненных merge\n",
      "conflict-ами дней подготовки к релизу с тоннами ручного\n",
      "тестирования.\n",
      "CD — непрерывная доставка. Следующий шаг после CI позволяет\n",
      "гарантировать, что кодовая база постоянно готова к деплою, а\n",
      "задеплоить ее можно одним нажатием кнопки.\n",
      "При этом неважно, с чем вы работаете: с масштабной\n",
      "распределенной системой, сложной производственной средой и т. д.\n",
      "Ключевой момент — автоматизация.\n",
      "CD — непрерывное развертывание. Последний этап зрелого\n",
      "CI/CD-пайплайна, где все изменения в коде автоматически\n",
      "развертываются в продакшн без ручного вмешательства.\n",
      "\n",
      "Стр.  5 \n",
      "\n",
      "Само собой, для этого требуется большое количество хорошо\n",
      "продуманных автоматических тестов. State of CD 2022 утверждает,\n",
      "что «47% разработчиков применяют CI или СD, но только один из\n",
      "пяти использует оба подхода для автоматизации сборки,\n",
      "тестирования и развертывания кода».\n",
      "Книга Accelerate подводит итоги многолетнего исследования с\n",
      "использованием отчетов State of DevOps, основанных на 23 000\n",
      "наборов данных компаний по всему миру. Как видите,\n",
      "высокопроизводительные команды могут деплоить по требованию\n",
      "(или несколько раз в день).\n",
      "Этапы CI/CD-пайплайна\n",
      "\n",
      "Стр.  6 \n",
      "\n",
      "Стадия исходного кода — здесь запускается пайплайн. Обычно это\n",
      "происходит после изменений в Git-репозитории, которые\n",
      "проявляются в открытии нового Pull Request-а или в пуше в ветку.\n",
      "Другой способ заключается в настройке инструментария CI/CD для\n",
      "запуска пайплайна через автоматическое расписание или после\n",
      "запуска другого пайплайна.\n",
      "Стадия сборки — этап, в процессе которого происходит проверка и\n",
      "сборка кода. Здесь особенно полезны такие инструменты, как\n",
      "Docker: они обеспечивают однородную среду.\n",
      "Стадия тестирования — CI/CD невозможно представить без\n",
      "автоматизированных тестов. В конце концов, все хотят быть\n",
      "уверены, что изменения в коде не сломают продакшн.\n",
      "Стадия развертывания — на последнем этапе (после успешного\n",
      "прохождения всех предыдущих стадий) код можно развернуть в\n",
      "выбранной среде.\n",
      "Пример с Gitlab\n",
      "В этом примере будет использован Gitlab CI/CD, однако концепции\n",
      "аналогичны и для остальных инструментов, поэтому их можно\n",
      "применить к другим сервисам хостинга репозиториев.\n",
      "Существует несколько инструментов CI/CD, например всемирно\n",
      "известный Jenkins. Этот инструмент требует некоторой настройки и\n",
      "конфигурации, в то время как другие поставляются сервисами\n",
      "хостинга репозиториев (такими как GitHub Actions и Bitbucket\n",
      "Pipelines) с предварительной настройкой.\n",
      "Поэтому если ваш код размещен на Gitlab, то легче всего\n",
      "использовать Gitlab CI/CD, поскольку код и управление CI/CD\n",
      "находятся на одной платформе.\n",
      "Как все это может работать без настроек?\n",
      "\n",
      "Стр.  7 \n",
      "\n",
      "Для ответа на этот вопрос стоит немного погрузиться в архитектуру\n",
      "Gitlab, а именно — в инстансы и раннеры. Инстансы хранят код\n",
      "приложения и конфигурации пайплайна. Раннеры выступают в\n",
      "качестве агентов, выполняющих операции в пайплайнах.\n",
      "В Gitlab каждый инстанс может быть подключен к одному или\n",
      "нескольким раннерам.\n",
      "Gitlab.com — это управляемый инстанс с несколькими раннерами,\n",
      "которые сам Gitlab и поддерживает. Следовательно, если вы\n",
      "используете этот инстанс, то получаете все необходимое из коробки.\n",
      "Приступим к работе\n",
      "Gitlab предлагает несколько шаблонов при создании нового проекта.\n",
      "Конфигурация пайплайна Gitlab CI/CD по умолчанию находится в\n",
      "файле .gitlab-ci.yml в корневом каталоге.\n",
      "\n",
      "Стр.  8 \n",
      "\n",
      "Предположим, мы хотим создать простой пайплайн, который\n",
      "проверяет: написан, протестирован и развернут ли код. Вот\n",
      "несколько концепций и терминов для ознакомления перед началом\n",
      "работы.\n",
      "Пайплайн (Pipeline)\n",
      "Пайплайн — это набор заданий, разделенных на этапы. Gitlab\n",
      "предлагает различные типы пайплайнов, например parent-child или\n",
      "multi-project. Полный список см. здесь.\n",
      "Этап (Stage)\n",
      "Этап — это шаг в пайплайне, предоставляющий информацию о том,\n",
      "какие задания запускать (сборка, тестирование и т. д.). Один этап\n",
      "может включать одно или несколько заданий.\n",
      "Задание (Job)\n",
      "Задание — основной блок пайплайна (компиляция, линтинг и т. д.).\n",
      "Для каждого задания должны быть определены name и script. После\n",
      "выполнения всех заданий на этапе пайплайн переходит к\n",
      "следующему.\n",
      "Теперь — к коду\n",
      "Выстраиваем пайплайн Gitlab CI/CD, который собирает, тестирует и\n",
      "разворачивает статический веб-сайт в AWS S3 Bucket.\n",
      "\n",
      "Стр.  9 \n",
      "\n",
      "Для начала создадим новый .gitlab-ci.yml\n",
      "1. Определим переменные\n",
      "2. Определим этапы\n",
      "3. Определим задания на каждом этапе\n",
      "variables: # variabiles definitions for easier reuse of valu\n",
      " CI_NODE_IMAGE: \"node:16.13.2\"\n",
      "# Pipeline stages\n",
      "stages:\n",
      " - install\n",
      " - build\n",
      " - test\n",
      " - deploy\n",
      "#install job definition\n",
      "install:\n",
      " stage: install\n",
      " image: \"$CI_NODE_IMAGE\" # variable reference\n",
      " script: # Shell script that is executed by the runner.\n",
      "   - npm ci\n",
      " cache: # List of files that should be cached between subseq\n",
      "   key:\n",
      "     files:\n",
      "       - package.json\n",
      "       - package-lock.json\n",
      "   paths: # directories to cache\n",
      "     - node_modules\n",
      "# Build Job definition\n",
      "\n",
      "Стр.  10 \n",
      "\n",
      "На этом все, спасибо за внимание.\n",
      "Научиться работать с пайплайнами, билдами и артефактами можно\n",
      "на курсе Gitlab CI/CD в Слёрм. Вы узнаете, из чего состоит Gitlab\n",
      "и какие у него возможности и настройки, а также разберете лучшие\n",
      "практики построения пайплайна, особенности шаблонизации\n",
      "и работы с переменными.\n",
      "build:\n",
      " stage: build\n",
      " image: $CI_NODE_IMAGE\n",
      " script:\n",
      "   - npm run build\n",
      " artifacts: # list of files and directories that are attache\n",
      "   paths:\n",
      "     - dist/\n",
      " cache:\n",
      "   key:\n",
      "     files:\n",
      "       - package.json\n",
      "       - package-lock.json\n",
      "   paths:\n",
      "     - node_modules\n",
      "   policy: pull\n",
      "# Test Job definition\n",
      "test:\n",
      " stage: test\n",
      " image: $CI_NODE_IMAGE\n",
      " script:\n",
      "   - npm run test\n",
      "# Deploy Job definition\n",
      "deploy:\n",
      " stage: deploy\n",
      " image: registry.gitlab.com/gitlab-org/cloud-deploy/aws-base\n",
      " script:\n",
      "   - aws s3 cp --recursive dist s3://bucket-name # copies th\n",
      "\n",
      "Стр.  11 \n",
      "\n",
      "Southbridge\n",
      "Обеспечиваем стабильную работу highload-проектов\n",
      "Сайт\n",
      "Сайт\n",
      "14\n",
      "Карма\n",
      "16.9\n",
      "Рейтинг\n",
      "Лиза Зубарькова @zubarek\n",
      "Пользователь\n",
      "Комментировать\n",
      "Публикации\n",
      "Видеокурс доступен всегда. Посмотреть программу:\n",
      "https://slurm.club/3JUKdzT\n",
      "Теги:  \n",
      "ci/cd\n",
      ",\n",
      "gitlab-ci\n",
      ",\n",
      "aws\n",
      ",\n",
      "gitlab\n",
      ",\n",
      "pipeline\n",
      "Хабы:  \n",
      "Блог компании Southbridge\n",
      ",\n",
      "Тестирование IT-систем\n",
      ",\n",
      "Системное администрирование\n",
      ",\n",
      "Программирование\n",
      ",\n",
      "DevOps\n",
      "+7\n",
      "14\n",
      "0\n",
      "ЛУЧШИЕ ЗА СУТКИ \n",
      "ПОХОЖИЕ\n",
      "\n",
      "Стр.  12 \n",
      "\n",
      "\n",
      "Стр.  13 \n",
      "\n",
      "Ваш\n",
      "аккаунт\n",
      "Разделы\n",
      "Информация\n",
      "Услуги\n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      "ИНФОРМАЦИЯ\n",
      "Сайт\n",
      "southbridge.io\n",
      "Дата регистрации\n",
      "15 ноября 2012\n",
      "Дата основания\n",
      "22 февраля 2008\n",
      "Численность\n",
      "51–100 человек\n",
      "Местоположение\n",
      "Россия\n",
      "Представитель\n",
      "Антон Скобин\n",
      "\n",
      "Стр.  14 \n",
      "\n",
      "Войти\n",
      "Регистрация\n",
      "Публикации\n",
      "Новости\n",
      "Хабы\n",
      "Компании\n",
      "Авторы\n",
      "Песочница\n",
      "Устройство сайта\n",
      "Для авторов\n",
      "Для компаний\n",
      "Документы\n",
      "Соглашение\n",
      "Конфиденциальность\n",
      "Корпоративный блог\n",
      "Медийная реклама\n",
      "Нативные проекты\n",
      "Образовательные\n",
      "программы\n",
      "Стартапам\n",
      "Мегапроекты\n",
      "Настройка языка\n",
      "Техническая поддержка\n",
      "Вернуться на старую версию\n",
      "© 2006–2023, Habr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_document = r\"PDF2\\Быстрое начало работы с Gitlab CICD.pdf\"\n",
    "doc = fitz.open(pdf_document)\n",
    "print(\"Исходный документ: \", doc)\n",
    "print(\"\\nКоличество страниц: %i\\n\\n------------------\\n\\n\" % doc.page_count)\n",
    "print(doc.metadata)\n",
    "for current_page in range(len(doc)):\n",
    "    page = doc.load_page(current_page)\n",
    "    page_text = page.get_text(\"text\")\n",
    "    print(\"Стр. \", current_page+1, \"\\n\")\n",
    "    print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff31e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный документ:  Document('PDF2\\Почему Data Science не для вас_ _ Хабр.pdf')\n",
      "Data Science сейчас во многом благодаря активному маркетингу становится очень\n",
      "популярной темой. Быть датасаентистом – модно и, как говорят многие рекламки, которые\n",
      "часто попадаются на глаза, не так уж и сложно. Ходят слухи, что работодатели стоят в\n",
      "очереди за возможность взять человека с курсов. Получить оффер на работу крайне легко,\n",
      "ведь в ваши обязанности будет входить требование данных от заказчика (как обычно\n",
      "говорят, чем больше данных – тем лучше) и закидывать их в искусственный интеллект,\n",
      "который работает по принципу черного ящика. Кстати, еще и платят немереное количество\n",
      "денег за всё это. \n",
      "Спойлер: это не так.\n",
      "TourmalineCore\n",
      "20 мар 2023 в 14:47\n",
      "Почему Data Science не для вас?\n",
      "Простой\n",
      "9 мин\n",
      "12K\n",
      "Big Data*, Data Engineering*\n",
      "Мнение\n",
      "КАК СТАТЬ АВТОРОМ\n",
      "\n",
      "В этой душераздирающей статье решили попробовать отговорить людей, которые готовы\n",
      "оставить кучу денег за курсы по Data Science, браться за это дело, а может быть и помочь\n",
      "определиться с тем, что на самом деле стоит сделать, чтобы встать на путь истинный.\n",
      "Обзор понятий\n",
      "И начнем, пожалуй, с того, что может быть вам и не очень-то нужно быть датасаентистом.\n",
      "Есть много направлений, поди попробуй разберись. Датасаентист, дата аналитик, дата\n",
      "инженер… Часто бывает, что всех этих людей гребут под одну гребенку, и совсем не\n",
      "разделяют их обязанностей. Разберемся, что это за звери такие и с чем их едят. \n",
      "Дата аналитики обычно заняты тем, что получают данные и пытаются найти в них\n",
      "закономерности. Эти люди отвечают на вопросы бизнеса: что, почему, как, как сделать\n",
      "лучше, а еще вот можете на график посмотреть. Аналитики чаще контактируют с другими\n",
      "людьми: чтобы выстроить правильную коммуникацию в команде и правильно описать и\n",
      "презентовать результаты своего анализа. Ваши друзья здесь: мат.статистика, python/R,\n",
      "SQL, платформы типа PowerBI/Tableau/Qlik и прочее необходимое для визуализации\n",
      "результатов. Также необходимо глубокое понимание области, в которой вы проводите\n",
      "анализ.\n",
      "Дата инженеры же заняты поставкой данных и их хранением. Эти люди настраивают\n",
      "хранилища, заставляют данные сопоставляться между собой и быть пригодными к\n",
      "использованию. Создание пайплайна обработки и архитектуры данных  – вот их задача. \n",
      "Датасаентисты – более расплывчатое понятие. Сначала они анализируют данные, на\n",
      "которых дальше строится предобработка. В предобработку входит очистка от выбросов,\n",
      "заполнение пропущенных значений, трансформация, нормализация и еще куча всякого\n",
      "разного. И после всего этого датасаентисты  обучают модели, которые будут давать какой-\n",
      "то предикт. От этих ребят нужен тот самый «искусственный интеллект» (кстати, стоит\n",
      "убрать из своего лексикона эти слова на собеседовании, расскажем дальше, почему).\n",
      "Здесь надо знать и уметь в машин и дип лёрнинг, опять таки уметь в матстат и обработку\n",
      "того, что вы видите, понимать предметную область, уметь продумать, какие данные нужны\n",
      "для обучения и составить ТЗ по их сбору. Будет большим плюсом понимание того, на каких\n",
      "машинах модели, которые вы собираетесь обучать, будут запускаться. \n",
      "Хороший доклад на тему того, чем заняты разные люди из разных ветвлений размытого\n",
      "понятия data scientist, есть по ссылке.\n",
      "У описанных направлений совершенно разные ключевые навыки и компетенции. Чем\n",
      "больше компания, тем вероятнее, что в ней есть разделение на эти направления. Но если\n",
      "вы с курсов решили устроиться в стартап, где есть биг дата, но особо нет людей на эту биг\n",
      "дату, тогда вашим ключевым навыком должно быть умение выживать. Именно здесь\n",
      "обычно нужен Data Scientist, который на все руки мастер. Готовы ли вы к этому?\n",
      "\n",
      "Почему всё-таки Data Science не для вас?\n",
      "1. Data Science – для усидчивых. Над одной и той же задачей придется сидеть очень\n",
      "долго. Пока найдешь все необходимое в данных, пока найдешь оптимальный способ\n",
      "обработки данных, пока погрузишься в область, пока переберешь все модельки,\n",
      "которые могут потенциально хорошо решить эту задачу… А еще надо дождаться, пока\n",
      "модель обучится - это ж целая вечность пройдет! Потом еще выясняется, что у модели\n",
      "метрики недостаточно хорошие, и весь этот путь нужно пройти заново, но с новыми\n",
      "идеями. \n",
      "Хватит ли вам умения занять себя другими делами во время обучения модели? Вряд\n",
      "ли проджект будет в восторге, если узнает, что несколько дней вы пили кофе, пока\n",
      "модель обучалась. Хватит ли вам терпения перебирать идеи до тех пор, пока точность\n",
      "модели не достигнет нужного бизнесу числа?\n",
      "2. Математика останавливается не на 2+2 и даже не на интегралах. Вам точно\n",
      "понадобятся знания по мат.статистике, линейной алгебре, теории вероятностей. Нет,\n",
      "вам не хватит встроенных в библиотечки функций. Может быть для какого-то\n",
      "поверхностного анализа и составления простецкой модели – да, но чтобы лучше\n",
      "анализировать огромные массивы данных, в математике действительно нужно\n",
      "разбираться.\n",
      "3. Вам нужно знать Python/R/Scala или еще что-то, на чем вы соберете ваше решение.\n",
      "Вы не бэкенд-разработчик, но вам нужно приготовить умную штуку, которую можно\n",
      "будет встроить в прод. Нужно будет составлять POC-шки, функции предобработки\n",
      "данных, которые затем будет имплементировать бэкенд-разраб. Необходимо уметь\n",
      "общаться с бэкендерами на их языке, если им понадобится помощь при внедрении\n",
      "вашего решения.\n",
      "4. Еще немного о встраивании модели в прод: вам нужно интересоваться тем, на каком\n",
      "железе будет запущена модель. Ну не получится запустить трансформер на сервачке\n",
      "заказчика с двумя ядрами и без малейшего признака GPU. А решение, которое\n",
      "работает на вашем компе и не работает на устройствах клиента не будет принято и,\n",
      "соответственно, оплачено.\n",
      "5. Нужно знать фреймворки, которые приняты в команде. Если вы до этого делали fit на\n",
      "керасе, а потом внезапно пришли в команду, которая работает на pytorch, уйдет много\n",
      "времени на обучение работе с новыми инструментами.\n",
      "Справедливости ради, вряд ли вас возьмут на работу со стеком, в котором у вас нет\n",
      "опыта, но на всякий случай лучше задавать этот вопрос на первых этапах\n",
      "собеседования.\n",
      "6. По ходу проектирования решения нужно будет использовать огромное количество,\n",
      "библиотек и лучше знать как ими пользоваться до начала работы над проектом. Или\n",
      "\n",
      "же уметь быстро ориентироваться в документации, а в процессе только добирать\n",
      "необходимое. Обучаться всему и сразу плохая идея.\n",
      "8. Вам нужно уметь использовать тулзины, которые тесно связаны с разработкой\n",
      "моделей. Модели и данные нуждаются в версионировании. В этом могут помочь DVC,\n",
      "MLflow или, например, Weights&Biases. А еще модели нужно сервить. В этом случае\n",
      "могут быть пригодны TorchServe или Tensorflow Serving. А может быть вам нужно будет\n",
      "написать свою обвязку вокруг модели, чтобы ее можно было быстро потестить. Этот\n",
      "зоопарк огромен. Нужно выбрать из него именно те вещи, которые вам помогут, и\n",
      "учиться ими пользоваться.\n",
      "9. Нужно уметь не только чистить данные, но и размечать их, когда вам не дали\n",
      "нормальной разметки, искать, какие данные можно добавить к выданному заказчиком\n",
      "датасету, если его явно не хватает.\n",
      "10. Важной частью вашей работы будет общение с бизнесом для того, чтобы точно\n",
      "определить цель того, что вы делаете, и не сделать того, что не нужно. Иногда\n",
      "заказчики дают расплывчатое ТЗ, внимательно вчитываясь в которое вы обнаружите,\n",
      "что заказчик хочет всё и сразу, но при этом непонятно зачем. Поэтому нужно будет\n",
      "суметь задать такие вопросы, которые прояснят, что действительно требуется от вас,\n",
      "или суметь сократить скоуп работ до реально реализуемого.\n",
      "\n",
      "11. Если проджект где-то там пообещал нереальные сроки для выполнения вашей задачи,\n",
      "нужно вовремя суметь сказать, что пятилетку за три года – не получится. Нельзя\n",
      "просто так взять и выстроить радужные мечты вокруг того, как быстро вы сможете\n",
      "сделать задачу. А с вас точно потребуют сроки, в которые будет необходимо\n",
      "уложиться.\n",
      "Вопрос оценки времени на реализацию – очень сложен. Здесь важно не только\n",
      "правильно декомпозировать задачу, но и добиться понимания от вышестоящих людей,\n",
      "почему эта задача не может быть реализована в очень сжатые сроки. Так что\n",
      "составляем план, и его придерживаемся. А если что-то выходит из-под контроля\n",
      "срочно трубим о том, что ничего не успевается и с этим надо что-то делать.\n",
      "12. А еще вам самим придется составлять ТЗ по данным, которые вы хотите получить. Из\n",
      "нашего опыта, чаще приходят заказчики, которые не понимают, что вам от них нужно и\n",
      "в каком объеме. Будьте готовы отвечать на вопрос сколько и каких данных вам надо,\n",
      "вникать в задачу на этапе обсуждения проекта и предполагать, какие кейсы могут\n",
      "возникать в вашей задаче. Также количество размеченных данных, следует\n",
      "запрашивать в таком объеме, который сможет удовлетворить заказчика. Если вы\n",
      "потребуете терабайты размеченных данных, а другой исполнитель скажет, что ему\n",
      "хватит пары гигов, то выберут скорее его, а не вас, просто потому что разметка данных\n",
      "стоит немалых денег. А ответ «чем данных больше – тем лучше» никого не устроит.\n",
      "13. Data Science – не только про генерацию красивых картинок и красивой музыки без\n",
      "помощи человека. Частый запрос бизнеса – проанализировать информацию о\n",
      "клиентах. Из этой бизнес-задачи может вытекать DS-задача, которая вам не\n",
      "понравится. Например, выстроить контекстную рекламу так, чтобы новые покупатели\n",
      "приобрели товар подороже, или завысить цену продукта в соответствии с\n",
      "предполагаемым уровнем дохода покупателя. Будет ли такая задача вписываться в\n",
      "ваши моральные рамки? Возможно, у вас будет опция отказаться от задач, которые не\n",
      "подходят вашему пониманию хорошо-плохо.Но бывает и так, что вас не спрашивают.\n",
      "Конечно, вопрос, стоит ли работать с компанией с которой у вас разное понимание\n",
      "границ не в плоскости data science, а скорее в общем. Но будьте готовы к тому, что в\n",
      "data science часто встает вопрос этики использования данных, и эта этика\n",
      "зафиксирована в достаточно размытом формате.\n",
      "Вы можете сказать, что на самом деле многие навыки из этого списка входят в программу\n",
      "популярных курсов. Да, это так. Но дадут ли вам необходимое количество разнородных\n",
      "задач, которые присутствуют в реальном мире? Насколько смешанные, странные и\n",
      "неполные данные будут присутствовать в этих задачах? Требования от заказчиков в\n",
      "реальном мире будут сильно сложнее, чем обычно определяют на курсах. На курсах вам\n",
      "создадут рафинированную среду, в которой все понятно, четко и ясно определено в\n",
      "требованиях и точно опирается на предыдущий опыт из лекций. К сожалению, на работе\n",
      "будет не так. Придётся ловить на лету и определять, как действовать, очень быстро. Так\n",
      "что наше мнение: пойти на курсы, чтобы стать датасаентистом, недостаточно. \n",
      "\n",
      "Иметь высшее образование, кстати, тоже. Знаем, бывали. Даже если ваша вышка с\n",
      "уклоном в дата саенс, по факту, она часто дает не сильно больше курсов. Не спорим, есть\n",
      "разные университеты, но, по нашему опыту, вам там дадут примерно такую же базу, как и\n",
      "на курсах, только чуть более разностороннюю (и то не факт). Но все-таки, это обучение\n",
      "длится подольше, и скорее всего, информации вы впитаете побольше. Еще и курсы по\n",
      "философии и экологии подкинут :)\n",
      "Важное качество в профессии датасаентиста – способность постоянно учиться, находить\n",
      "для себя новые задачи и расти на этом практическом опыте. А еще постоянно следить за\n",
      "событиями в сфере, потому что кажется, что она обновляется чаще и кардинальнее, чем\n",
      "другие сферы IT. Поэтому специализированные курсы – это хорошо, высшее образование\n",
      "– тоже, но важно понимать, что это ещё далеко не всё, это только начало длинного и\n",
      "тяжелого пути.\n",
      "Cекция \"вопрос-ответ\" в лучших традициях пиар-страниц курсов\n",
      "1. Датасаенс – это весело?\n",
      "Конечно! Но только если вы действительно упороты и любите искать что-то странное и\n",
      "копаться в куче данных, чтобы в итоге делать штуку, которая будет помогать бизнесу в\n",
      "\n",
      "реальных задачах с большой точностью. Но будьте готовы и к не самым веселым\n",
      "моментам: на этом самурайском пути много препятствий. В каком-то смысле, датасаенс –\n",
      "это образ жизни, при котором нужно всегда держать руку на пульсе и интересоваться\n",
      "происходящим в области. Став датасаентистом однажды, больше уже не сможешь\n",
      "смотреть на все эти умные штуки как на абсолютную магию.\n",
      "2. Я математик по образованию и по желанию и очень хочу в DS. Получится?\n",
      "Отличная база. Математика как основа – это очень хорошо. Но вам еще очень многое\n",
      "придется освоить. Удачи вам в этом нелегком пути!\n",
      "3. А я вот из программиста решил переквалифицироваться в датасаентиста, каковы\n",
      "шансы?\n",
      "Вам будет тяжело. Шанс есть всегда, но это будет очень сложный путь. На нашем опыте,\n",
      "понять программирование для человека с математическим бэкграундом проще, чем\n",
      "программисту математику. Мы все проходили курс по высшей математике в универе и\n",
      "помним матешу со школы, но это не тот уровень, который будет требоваться на месте\n",
      "работы.\n",
      "\n",
      "4. А почему вы говорите, что искусственного интеллекта сейчас не существует?\n",
      "Искусственный интеллект – интересное понятие. Этим термином внезапно стало принято\n",
      "называть всё, что может делать что-то за человека. Но до «интеллекта» там далеко. Да\n",
      "данный момент то, что мы называем ИИ, это алгоритмы, которые могут решать\n",
      "узконаправленную задачу. Распознавание лиц? Детекция объектов? Генерация\n",
      "контекстной рекламы? Интеллектуальный анализ данных? Все это узконаправленные\n",
      "сферы. Разве мы можем назвать калькулятор искусственным интеллектом? Ну, это вряд\n",
      "ли. Хотя он также решает узконаправленную задачу – решение математических уравнений.\n",
      "А пару столетий назад это казалось задачей искусственного интеллекта. В целом, можно\n",
      "сказать, что ИИ существует, но в очень примитивном виде. Его эволюцию можно сравнить\n",
      "в эволюцией человека. На ранних стадиях задачи человека также были\n",
      "узконаправленными и примитивными, но человек эволюционировал. Так же можно\n",
      "предположить и с ИИ, он может эволюционировать. И не обязательно базой для\n",
      "«прогрессивного ИИ» будут нейронные сети.\n",
      "Подведем итоги\n",
      "Data Science – это профессия с высоким порогом входа. Нужно многое знать, нужно\n",
      "многое уметь. Если вы хотите в IT, и вам кто-то сказал, что через дата саенс это\n",
      "\n",
      "20\n",
      "Карма\n",
      "0\n",
      "Рейтинг\n",
      "Tourmaline Core @TourmalineCore\n",
      "Пользователь\n",
      "сделать проще, чем через другие профессии, то не слушайте этого человека.\n",
      "Обратите внимание на другие возможные варианты входа в IT, например, верстку или\n",
      "ручное тестирование. \n",
      "Data Science – это не только про построить предсказывающую модель в jupyter-\n",
      "notebook и отдать ее дальше. Скорее всего, вам придется делать сильно больше этого\n",
      "функционала.\n",
      "В работе с данными есть разные ветвления профессий, например, инженер данных\n",
      "или аналитик данных. Для них требуются иные навыки. Может быть, если ваше\n",
      "желание именно работать с данными, вам стоит посмотреть на эти специализации.\n",
      "Data Science – про постоянное обучение и слежение за новыми разработками в этой\n",
      "сфере. Держать руку на пульсе – это один из ключевых навыков датасаентиста.\n",
      "Курсов, на которых вам за несколько месяцев дают всё необходимое для профессии –\n",
      "не существует. Вам могут дать базу, но стоит ли эта база тех денег, которые за нее\n",
      "просят, это спорный вопрос.\n",
      "Есть множество курсов по DS, которые находятся в открытом доступе абсолютно\n",
      "бесплатно, и дают примерно тот же материал, что и дорогостоящие. А практические\n",
      "задачи можно найти на kaggle. Кстати, там же есть и решения других людей, которые\n",
      "могут вам помочь обучиться и понять, как же все-таки делать лучше. Рассмотрите этот\n",
      "вариант. Возможно, в будущем мы сделаем статью-сборник с такими материалами, а\n",
      "пока посоветуем отличный ресурс для начала обучения.\n",
      "Теги: ML, data science, курсы, курсы data science, machine-learning\n",
      "Хабы: Big Data, Data Engineering\n",
      "Редакторский дайджест\n",
      "Присылаем лучшие статьи раз в месяц\n",
      "Электропочта\n",
      "+6\n",
      "42\n",
      "22\n",
      "Подписаться\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_document = r\"PDF2\\Почему Data Science не для вас_ _ Хабр.pdf\"\n",
    "doc = fitz.open(pdf_document)\n",
    "print(\"Исходный документ: \", doc)\n",
    "text = []\n",
    "for current_page in range(len(doc)):\n",
    "    page = doc.load_page(current_page)\n",
    "    page_text = page.get_text(\"text\")\n",
    "    text.append(page_text)\n",
    "    print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0169200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный документ:  Document('PDF2\\OSINT & Hacking — как работает фишинг для нельзяграма _ Хабр3.pdf')\n",
      "\n",
      "Количество страниц: 14\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "{'format': 'PDF 1.7', 'title': 'Aspose', 'author': 'Aspose', 'subject': 'Aspose', 'keywords': '', 'creator': 'Aspose Pty Ltd.', 'producer': 'Aspose.PDF for .NET 23.2.0', 'creationDate': \"D:20230320160606+00'00'\", 'modDate': \"D:20230320160621+00'00'\", 'trapped': '', 'encryption': None}\n",
      "Стр.  1 \n",
      "\n",
      "71.07\n",
      "Рейтинг\n",
      "Cloud4Y\n",
      "#1 Корпоративный облачный провайдер\n",
      "Автор оригинала: Yashwant Singh\n",
      "Взлом Instagram*аккаунта — популярный запрос в поисковиках.\n",
      "Поэтому есть смысл рассказать о том, как это обычно работает.\n",
      "Просто для того, чтобы вы знали, откуда может пойти атака.\n",
      " 5 часов назад\n",
      "Простой3 мин\n",
      "Cloud4Y\n",
      "OSINT & Hacking — как работает фишинг для\n",
      "нельзяграма\n",
      "1.1K\n",
      "Блог компании Cloud4Y\n",
      ", \n",
      "Информационная безопасность*\n",
      ", \n",
      "Социальные сети и сообщест\n",
      "Обзор\n",
      "Перевод\n",
      "КАК СТАТЬ АВТОРОМ\n",
      "\n",
      "Стр.  2 \n",
      "\n",
      "Чтобы начать попытки заполучить доступ к аккаунту, вы должны\n",
      "знать ник человека, которого вы пытаетесь взломать. Так\n",
      "что небольшая разведка будет очень кстати. Только не увлекайтесь.\n",
      "Существуют различные инструменты для разведки, в первую\n",
      "очередь, поиск пользователя в конкретной соцсети с целью узнать\n",
      "его ник. Я нашёл отличный инструмент под названием «Slash»,\n",
      "который можно использовать для поиска любых учётных записей\n",
      "пользователя, если он везде регистрируется под одним ником.\n",
      "Ставим Slash\n",
      "Я проверил Slash на себе, и посмотрите на эти результаты.\n",
      "Некоторые из учетных записей, перечисленных здесь, были созданы\n",
      "много лет назад.\n",
      "git clone https://github.com/theahmadov/slash \n",
      "cd slash \n",
      "pip install -r requirements.txt \n",
      "python slash.py help\n",
      "\n",
      "Стр.  3 \n",
      "\n",
      "Slash — это простой консольный инструмент. Но вы также можете\n",
      "использовать такие инструменты, как WhatsMyName Web, который\n",
      "совершенно бесплатен.\n",
      "Вот, посмотрите. Я проверил WhatsMyName на себе. Мой ник\n",
      "«earthtoyash».\n",
      "\n",
      "Стр.  4 \n",
      "\n",
      "\n",
      "Стр.  5 \n",
      "\n",
      "Теперь, когда мы знаем больше о нашем пользователе, можно\n",
      "использовать эти знания. Например, через отправку фишинговых\n",
      "ссылок. Для этого создадим полезную нагрузку с помощью Zphisher.\n",
      "Ставим Zphisher с GitHub\n",
      "Клонируем репозиторий:\n",
      "Запускаем файл zphisher.sh:\n",
      "При первом запуске он установит зависимости и на этом всё.\n",
      "Система скажет, что Zphisher установлен. После установки вам\n",
      "нужно будет снова запустить zphisher.sh в каталоге zphisher\n",
      "командой ./zphisher , и тогда вы получите что-то вроде этого:\n",
      "git clone --depth=1 https://github.com/htr-tech/zphisher.git\n",
      "cd zphisher && ./zphisher.sh\n",
      "\n",
      "Стр.  6 \n",
      "\n",
      "Как вы можете видеть, есть много вариантов и шаблонов, благодаря\n",
      "чему любой может заниматься фишингом. Мы сосредоточимся на\n",
      "нельзяграме. Итак, введите «2» и нажмите Enter.\n",
      "Следующий шаг полностью зависит от вас, выберите любой из них.\n",
      "Затем появится окно с выбором. Я выбрал третий вариант, так как он\n",
      "минималистичный и удобен для того, чтобы показать возможности\n",
      "инструмента.\n",
      "\n",
      "Стр.  7 \n",
      "\n",
      "Опять же, чтобы все было просто, я пропущу пользовательский порт,\n",
      "но если вы уже используете порт 8080, то можете изменить его на\n",
      "8000. Если нет, оставляйте всё как есть. Также важно маскировать\n",
      "URL, ну просто в целях безопасности. Можно использовать чтото\n",
      "вроде этого:\n",
      "Всё, Zphisher создал фишинговую ссылку, которую можно\n",
      "отправить жертве. Как только она нажмёт на ссылку, вы начнёте\n",
      "\n",
      "Стр.  8 \n",
      "\n",
      "получать информацию о ней. Например, IPадреса, имена\n",
      "пользователей, пароли и т. д. Ещё можно использовать обратный\n",
      "поиск IP, чтобы определить местоположение вашей цели и многое\n",
      "другое.\n",
      "Итак, вот эти фишинговые ссылки.\n",
      "При нажатии открывается страница, похожая на официальную\n",
      "страницу входа в запрещённую соцсеть.\n",
      "\n",
      "Стр.  9 \n",
      "\n",
      "Вот она, нехорошая\n",
      "После ввода учётных данных можно получить много информации на\n",
      "«хакерской» стороне терминала.\n",
      "Вот так, господа и дамы, можно без особого труда взломать учетную\n",
      "запись в нельзяграме. Поэтому в очередной раз напоминаем: нельзя\n",
      "\n",
      "Стр.  10 \n",
      "\n",
      "нажимать на ссылки, которым вы не доверяете.\n",
      "Само собой разумеется, не используйте информацию из этой статьи\n",
      "с намерением причинить кому-либо вред. OSINT законен, но фишинг\n",
      "и кража личных данных даже в запрещённой соцсети является\n",
      "уголовным преступлением. И да, * Организация Meta, а также её\n",
      "продукт Instagram, на которые мы ссылаемся в этой статье,\n",
      "признаны экстремистскими и запрещены на территории РФ.\n",
      "Спасибо за внимание!\n",
      "Что ещё интересного есть в блоге Cloud4Y\n",
      "→ Информационная безопасность и глупость: необычные примеры\n",
      "→ NAS за шапку сухарей\n",
      "→ Взлом Hyundai Tucson, часть 1, часть 2\n",
      "→ Столетний язык программирования — какой он\n",
      "→ 50 самых интересных клавиатур из частной коллекции\n",
      "Теги:  \n",
      "взлом\n",
      ",\n",
      "соцсети\n",
      ",\n",
      "osint\n",
      "Хабы:  \n",
      "Блог компании Cloud4Y\n",
      ",\n",
      "Информационная безопасность\n",
      ",\n",
      "Социальные сети и сообщества\n",
      "0\n",
      "19\n",
      "1\n",
      "\n",
      "Стр.  11 \n",
      "\n",
      "Cloud4Y\n",
      "#1 Корпоративный облачный провайдер\n",
      "Сайт\n",
      "Facebook\n",
      "Twitter\n",
      "ВКонтакте Telegram\n",
      "149\n",
      "Карма\n",
      "53.5\n",
      "Рейтинг\n",
      "Cloud4Y @Cloud4Y\n",
      "Корпоративный облачный провайдер\n",
      "Сайт\n",
      "ВКонтакте Telegram\n",
      "Комментарии 1\n",
      "Публикации\n",
      "ЛУЧШИЕ ЗА СУТКИ \n",
      "ПОХОЖИЕ\n",
      "\n",
      "Стр.  12 \n",
      "\n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      " · \n",
      "\n",
      "Стр.  13 \n",
      "\n",
      "Ваш\n",
      "аккаунт\n",
      "Войти\n",
      "Регистрация\n",
      "Разделы\n",
      "Публикации\n",
      "Новости\n",
      "Хабы\n",
      "Компании\n",
      "Авторы\n",
      "Песочница\n",
      "Информация\n",
      "Устройство сайта\n",
      "Для авторов\n",
      "Для компаний\n",
      "Документы\n",
      "Соглашение\n",
      "Конфиденциальность\n",
      "Услуги\n",
      "Корпоративный блог\n",
      "Медийная реклама\n",
      "Нативные проекты\n",
      "Образовательные\n",
      "программы\n",
      "Стартапам\n",
      "Мегапроекты\n",
      "ИНФОРМАЦИЯ\n",
      "Сайт\n",
      "www.cloud4y.ru\n",
      "Дата регистрации\n",
      "29 июля 2011\n",
      "Дата основания\n",
      "2009\n",
      "Численность\n",
      "51–100 человек\n",
      "Местоположение\n",
      "Россия\n",
      "Представитель\n",
      "\n",
      "Стр.  14 \n",
      "\n",
      "Настройка языка\n",
      "Техническая поддержка\n",
      "Вернуться на старую версию\n",
      "© 2006–2023, Habr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_document = r\"PDF2\\OSINT & Hacking — как работает фишинг для нельзяграма _ Хабр3.pdf\"\n",
    "doc = fitz.open(pdf_document)\n",
    "print(\"Исходный документ: \", doc)\n",
    "print(\"\\nКоличество страниц: %i\\n\\n------------------\\n\\n\" % doc.page_count)\n",
    "print(doc.metadata)\n",
    "for current_page in range(len(doc)):\n",
    "    page = doc.load_page(current_page)\n",
    "    page_text = page.get_text(\"text\")\n",
    "    print(\"Стр. \", current_page+1, \"\\n\")\n",
    "    print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c209602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный документ:  Document('PDF2\\Новые утечки. Что мы знаем о выходе Windows 12 _ Хабр.pdf')\n",
      "4.74\n",
      "Оценка\n",
      "50.53\n",
      "Рейтинг\n",
      "getmatch\n",
      "Рассказываем о том, как строить карьеру в IT\n",
      "Подписаться\n",
      "Весь интернет уже готовится к выходу Windows 12. Вероятно, следующая версия будет во\n",
      "многом сосредоточена на поддержке работы с ИИ. На это намекает в том числе недавняя\n",
      "утечка от Intel, а также последние действия AMD и направление развития Bing.\n",
      "Мы знаем, что и Intel, и Microsoft готовятся к поддержке работы Windows 12 на новых\n",
      "процессорах. Об этом нам говорит утечка от leaf_hobby, который известен тем, что\n",
      "раскрывает полные спецификации чипов Intel Xeon перед их запуском. На этот раз\n",
      "leaf_hobby опубликовали подробности аппаратных показателей десктопной платформы\n",
      "Intel Meteor Lake, которую планируется выпустить в этом или в следующем году. Сейчас их\n",
      "Arnak\n",
      "20 мар 2023 в 16:06\n",
      "Новые утечки. Что мы знаем о выходе Windows 12\n",
      "5 мин\n",
      "52K\n",
      "Блог компании getmatch, Искусственный интеллект, Разработка под Windows*, Софт\n",
      "КАК СТАТЬ АВТОРОМ\n",
      "\n",
      "твиты уже защищены, но интернет помнит™, а у журналистов отдельных изданий, вроде\n",
      "Insider и The Verge, к ним есть доступ.\n",
      "Сообщается, что Intel в имейле для своих сотрудников упоминает, что ее процессоры\n",
      "следующего поколения будут поддерживать Windows 12. Для этого Meteor Lake должен\n",
      "содержать 20 линий PCIe Gen5.\n",
      "Хотя Microsoft не объявляла о каких-либо планах в отношении Windows 12, уже есть\n",
      "признаки того, что компания планирует будущие версии Windows с расчетом на глубокую\n",
      "интеграцию функций ИИ. Даже последнее обновление Windows 11 уже интегрирует новую\n",
      "версию Bing с искусственным интеллектом прямо в панель задач. А ещё в этом\n",
      "обновлении бизнес-пользователи получают рекомендованный ИИ контент в меню «Пуск».\n",
      "Скоро искусственный интеллект придет в Edge, а потом — и во все остальные аспекты\n",
      "продуктов Microsoft. Компания очень воодушевлена успехом своей инвестиции в ChatGPT,\n",
      "и собирается сделать на него очень сильную ставку в надежде победить конкурентов, в\n",
      "том числе Google, у которой дело с инвестициями в ИИ пошло не так хорошо.\n",
      "Один из слайдов от Intel\n",
      "В начале января руководитель разработки Windows Паноса Паная на CES заявил, что «ИИ\n",
      "собирается заново переизобрести всё то, что вы делаете в Windows». Команда Microsoft\n",
      "\n",
      "Surface также сообщила, что «ИИ изменит то, как мы используем ПК». Всё однозначно\n",
      "указывает на то, что в следующей версии Windows компания собирается очень широко\n",
      "использовать ИИ. Это будет главная новая «фишка» платформы. А чтобы всё это могло\n",
      "работать, Microsoft приходится тесно сотрудничать с партнерами по аппаратному\n",
      "обеспечению, такими как Intel и AMD. Ей нужно, чтобы новые процессоры смогли\n",
      "справляться с высокими рабочими нагрузками ИИ, и без ошибок работали с новой\n",
      "технологией. Это объясняет то, почему Intel, как сообщается, уже упоминает об\n",
      "оптимизации под Windows 12 внутри компании.\n",
      "AMD недавно выпустила свои мобильные процессоры Ryzen 7000, и похвасталась тем, что\n",
      "они являются первыми процессорами x86, содержащими специальный механизм\n",
      "искусственного интеллекта, способный поддерживать эффекты Windows Studio. Эти\n",
      "функции, в том числе удаление фонового шума, размытие фона и автоматическое\n",
      "кадрирование, будут доступны только для техники с ускорителями на чипах ARM, которые\n",
      "работают с помощью специального нейронного процессора (NPU).\n",
      "То есть, AMD и Microsoft сейчас тоже активно совместно работают над внедрением\n",
      "расширенного числа функций на основе ИИ в свои продукты. И такие возможности, как\n",
      "заявляют компании, в дальнейшем будут более широко доступны в традиционных\n",
      "ноутбуках с Windows, работающих на AMD. Возможно, это тоже является подготовкой\n",
      "новых поколений процессоров к запуску с Windows 12.\n",
      "Все эти даты очень хорошо стыкуются. Новые процессоры от Intel и AMD ожидают в конце\n",
      "этого года или в 2024-м. А на разработку новой версии ОС у Microsoft раньше уходило 2-3\n",
      "года. Windows 11 была выпущена в октябре 2021-го. Так что, если новые утечки верны,\n",
      "следующую основную версию Windows можно ожидать в 2024 году — как раз под ряд\n",
      "новых процессоров.\n",
      "Как она будет выглядеть\n",
      "\n",
      "То, что Windows 12 точно находится в разработке, мы узнали и из утечек декабря 2022\n",
      "года. Тогда во время мероприятия Ignite Keynote 2022 компания Microsoft (вполне\n",
      "возможно, намеренно) «засветила» интерфейс следующего большого обновления\n",
      "Windows. Потом об этом зудел весь интернет, споря о том, что это могло быть. Что,\n",
      "согласитесь, совсем неплохо для одного скриншота.\n",
      "Изображение, «случайно» показанное на Ignite Keynote, демонстрирует ряд нехарактерных\n",
      "для Windows 11 особенностей: плавающая (а не доходящая до краев экрана) панель\n",
      "задач; плавающее меню поиска; системные значки в верхней (а не привычной нижней)\n",
      "части экрана справа. И интересный виджет погоды слева вверху.\n",
      "Видно, что версия будет заметно отличаться от всех предыдущих. Коллективный разум\n",
      "сейчас считает, что она выйдет, скорее всего, в 2024 году, и будет называться Windows 12\n",
      "как минимум из-за большого количества разных новшеств — так же, как это было с\n",
      "Windows 11.\n",
      "\n",
      "Скорее всего, так будет выглядеть следующая ОС\n",
      "Следующее крупное обновление Windows пока находится на стадии прототипа, но\n",
      "источники, знакомые с ситуацией, говорят, что показанный во время Ignite интерфейс\n",
      "отображает то, к чему стремится Microsoft в следующей версии своей операционной\n",
      "системы. На мероприятии показали только часть возможных новшеств — в реальности их\n",
      "будет больше. Например, ожидается, что в Windows 12 появится новый экран блокировки,\n",
      "новый центр уведомлений, новый поиск. Ну и ИИ везде, где только можно.\n",
      "В плане интерфейса глобальная цель — сделать его более удобным для сенсорного\n",
      "ввода. У Microsoft пока всё не очень хорошо с балансом по этой части: Windows 8 была\n",
      "чересчур сильно ориентированной именно на «тач-взаимодействие», а Windows 10,\n",
      "наоборот, заточена под работу с мышью и клавиатурой. В Windows 11 компания\n",
      "попыталась найти золотую середину, но не получилось: всё-таки удобство управления при\n",
      "помощи мыши однозначно осталось на первом месте.\n",
      "Туманная перспектива\n",
      "\n",
      "Облачная, точнее. Ходят не очень приятные слухи (впрочем, смотря с какой стороны\n",
      "посмотреть). Что, мол, Windows 12 может стать первой облачной ОС от Microsoft. В\n",
      "последние годы Сатья Наделла переносит всё в сеть, в том числе MS Office. И говорит, что\n",
      "за этим будущее, а «облако изменит всё». Microsoft 365 активно развивается, в том числе\n",
      "за счет Windows 365, облачного компьютера, который можно использовать на любом\n",
      "устройстве, способном поддерживать браузер.\n",
      "Сервис был запущен компанией в 2021 году, работать так можно как с Windows 10, так и с\n",
      "Windows 11. Компания позиционирует это как альтернативу облачным сервисам Google:\n",
      "теперь у неё в облаке тоже можно и хранить данные, и редактировать документы, и даже\n",
      "ходить по меню «Пуск» и папкам в привычном интерфейсе, даже если у вас стоит Ubuntu\n",
      "или вы вообще сидите с планшетом на Android. Если Windows 12 будет доступна только в\n",
      "виде такой облачной платформы (по крайней мере, изначально) — это может серьезно\n",
      "повысить видимость пакета 365 в глазах рядовых пользователей. А там, глядишь, многие и\n",
      "задумаются, не удобнее ли им так будет хранить данные, и продолжать работать в одной\n",
      "ОС с разных устройств.\n",
      "Система может быть во многом похожа на традиционные варианты виртуальных рабочих\n",
      "столов, которые уже много лет развертываются предприятиями для поддержки сценариев\n",
      "BYOD. Но с более простой структурой ценообразования, удобным запуском и ориентацией\n",
      "на более широкую аудиторию. Это также может стать более контролируемым способом\n",
      "для Microsoft проверить механизмы работы своих новых ИИ-помощников. При запуске\n",
      "\n",
      "getmatch\n",
      "Рассказываем о том, как строить карьеру в IT\n",
      "Telegram\n",
      "Сайт\n",
      "ВКонтакте\n",
      "Twitter\n",
      "Сайт\n",
      "167\n",
      "Карма\n",
      "0\n",
      "Рейтинг\n",
      "облачной ОС им не придется отпускать ИИ «гулять» на каждый из компьютеров\n",
      "пользователей, все процедуры будут мониториться на собственных серверах.\n",
      "Будем надеяться, что это один из тех наборов слухов, которые скоро будут опровергнуты.\n",
      "Возможно, команда поэкспериментировала с этой идеей, и поняла, что она не\n",
      "приживается. Но пока что — нужно быть морально готовыми к тому, что со всё большим\n",
      "числом сервисов, в том числе от Microsoft, мы будем взаимодействовать через окно\n",
      "браузера.\n",
      "P.S. Тысячи крутых вакансий в РФ и за рубежом — в телеграм-боте getmatch. Задаете\n",
      "нужную зарплату, и к вам приходят лучшие предложения, а наши эксперты помогают\n",
      "пройти интервью. Не нужно ни резюме, ни портфолио, настройка занимает меньше 30\n",
      "секунд. А ещё это отличный способ следить за текущим рынком труда. Откликаться не\n",
      "обязательно.\n",
      "Теги: windows, microsoft, windows 12, AI, intel, chatgpt\n",
      "Хабы: Блог компании getmatch, Искусственный интеллект, Разработка под Windows, Софт\n",
      "Редакторский дайджест\n",
      "Присылаем лучшие статьи раз в месяц\n",
      "Электропочта\n",
      "+19\n",
      "22\n",
      "123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_document = r\"PDF2\\Новые утечки. Что мы знаем о выходе Windows 12 _ Хабр.pdf\"\n",
    "doc = fitz.open(pdf_document)\n",
    "print(\"Исходный документ: \", doc)\n",
    "text = []\n",
    "for current_page in range(len(doc)):\n",
    "    page = doc.load_page(current_page)\n",
    "    page_text = page.get_text(\"text\")\n",
    "    text.append(page_text)\n",
    "    print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27ef027",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''.join(text).replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3ff273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.74 Оценка 50.53 Рейтинг getmatch Рассказываем о том, как строить карьеру в IT Подписаться Весь интернет уже готовится к выходу Windows 12. Вероятно, следующая версия будет во многом сосредоточена на поддержке работы с ИИ. На это намекает в том числе недавняя утечка от Intel, а также последние действия AMD и направление развития Bing. Мы знаем, что и Intel, и Microsoft готовятся к поддержке работы Windows 12 на новых процессорах. Об этом нам говорит утечка от leaf_hobby, который известен тем, что раскрывает полные спецификации чипов Intel Xeon перед их запуском. На этот раз leaf_hobby опубликовали подробности аппаратных показателей десктопной платформы Intel Meteor Lake, которую планируется выпустить в этом или в следующем году. Сейчас их Arnak 20 мар 2023 в 16:06 Новые утечки. Что мы знаем о выходе Windows 12 5 мин 52K Блог компании getmatch,\\xa0Искусственный интеллект,\\xa0Разработка под Windows*,\\xa0Софт КАК СТАТЬ АВТОРОМ твиты уже защищены, но интернет помнит™, а у журналистов отдельных изданий, вроде Insider и The Verge, к ним есть доступ. Сообщается, что Intel в имейле для своих сотрудников упоминает, что ее процессоры следующего поколения будут поддерживать Windows 12. Для этого Meteor Lake должен содержать 20 линий PCIe Gen5. Хотя Microsoft не объявляла о каких-либо планах в отношении Windows 12, уже есть признаки того, что компания планирует будущие версии Windows с расчетом на глубокую интеграцию функций ИИ. Даже последнее обновление Windows 11 уже интегрирует новую версию Bing с искусственным интеллектом прямо в панель задач. А ещё в этом обновлении бизнес-пользователи получают рекомендованный ИИ контент в меню «Пуск». Скоро искусственный интеллект придет в Edge, а потом — и во все остальные аспекты продуктов Microsoft. Компания очень воодушевлена успехом своей инвестиции в ChatGPT, и собирается сделать на него очень сильную ставку в надежде победить конкурентов, в том числе Google, у которой дело с инвестициями в ИИ пошло не так хорошо. Один из слайдов от Intel В начале января руководитель разработки Windows Паноса Паная на CES заявил, что «ИИ собирается заново переизобрести всё то, что вы делаете в Windows». Команда Microsoft Surface также сообщила, что «ИИ изменит то, как мы используем ПК». Всё однозначно указывает на то, что в следующей версии Windows компания собирается очень широко использовать ИИ. Это будет главная новая «фишка» платформы. А чтобы всё это могло работать, Microsoft приходится тесно сотрудничать с партнерами по аппаратному обеспечению, такими как Intel и AMD. Ей нужно, чтобы новые процессоры смогли справляться с высокими рабочими нагрузками ИИ, и без ошибок работали с новой технологией. Это объясняет то, почему Intel, как сообщается, уже упоминает об оптимизации под Windows 12 внутри компании. AMD недавно выпустила свои мобильные процессоры Ryzen 7000, и похвасталась тем, что они являются первыми процессорами x86, содержащими специальный механизм искусственного интеллекта, способный поддерживать эффекты Windows Studio. Эти функции, в том числе удаление фонового шума, размытие фона и автоматическое кадрирование, будут доступны только для техники с ускорителями на чипах ARM, которые работают с помощью специального нейронного процессора (NPU). То есть, AMD и Microsoft сейчас тоже активно совместно работают над внедрением расширенного числа функций на основе ИИ в свои продукты. И такие возможности, как заявляют компании, в дальнейшем будут более широко доступны в традиционных ноутбуках с Windows, работающих на AMD. Возможно, это тоже является подготовкой новых поколений процессоров к запуску с Windows 12. Все эти даты очень хорошо стыкуются. Новые процессоры от Intel и AMD ожидают в конце этого года или в 2024-м. А на разработку новой версии ОС у Microsoft раньше уходило 2-3 года. Windows 11 была выпущена в октябре 2021-го. Так что, если новые утечки верны, следующую основную версию Windows можно ожидать в 2024 году — как раз под ряд новых процессоров. Как она будет выглядеть То, что Windows 12 точно находится в разработке, мы узнали и из утечек декабря 2022 года. Тогда во время мероприятия Ignite Keynote 2022 компания Microsoft (вполне возможно, намеренно) «засветила» интерфейс следующего большого обновления Windows. Потом об этом зудел весь интернет, споря о том, что это могло быть. Что, согласитесь, совсем неплохо для одного скриншота. Изображение, «случайно» показанное на Ignite Keynote, демонстрирует ряд нехарактерных для Windows 11 особенностей: плавающая (а не доходящая до краев экрана) панель задач; плавающее меню поиска; системные значки в верхней (а не привычной нижней) части экрана справа. И интересный виджет погоды слева вверху. Видно, что версия будет заметно отличаться от всех предыдущих. Коллективный разум сейчас считает, что она выйдет, скорее всего, в 2024 году, и будет называться Windows 12 как минимум из-за большого количества разных новшеств — так же, как это было с Windows 11. Скорее всего, так будет выглядеть следующая ОС Следующее крупное обновление Windows пока находится на стадии прототипа, но источники, знакомые с ситуацией, говорят, что показанный во время Ignite интерфейс отображает то, к чему стремится Microsoft в следующей версии своей операционной системы. На мероприятии показали только часть возможных новшеств — в реальности их будет больше. Например, ожидается, что в Windows 12 появится новый экран блокировки, новый центр уведомлений, новый поиск. Ну и ИИ везде, где только можно. В плане интерфейса глобальная цель — сделать его более удобным для сенсорного ввода. У Microsoft пока всё не очень хорошо с балансом по этой части: Windows 8 была чересчур сильно ориентированной именно на «тач-взаимодействие», а Windows 10, наоборот, заточена под работу с мышью и клавиатурой. В Windows 11 компания попыталась найти золотую середину, но не получилось: всё-таки удобство управления при помощи мыши однозначно осталось на первом месте. Туманная перспектива Облачная, точнее. Ходят не очень приятные слухи (впрочем, смотря с какой стороны посмотреть). Что, мол, Windows 12 может стать первой облачной ОС от Microsoft. В последние годы Сатья Наделла переносит всё в сеть, в том числе MS Office. И говорит, что за этим будущее, а «облако изменит всё». Microsoft 365 активно развивается, в том числе за счет Windows 365, облачного компьютера, который можно использовать на любом устройстве, способном поддерживать браузер. Сервис был запущен компанией в 2021 году, работать так можно как с Windows 10, так и с Windows 11. Компания позиционирует это как альтернативу облачным сервисам Google: теперь у неё в облаке тоже можно и хранить данные, и редактировать документы, и даже ходить по меню «Пуск» и папкам в привычном интерфейсе, даже если у вас стоит Ubuntu или вы вообще сидите с планшетом на Android. Если Windows 12 будет доступна только в виде такой облачной платформы (по крайней мере, изначально) — это может серьезно повысить видимость пакета 365 в глазах рядовых пользователей. А там, глядишь, многие и задумаются, не удобнее ли им так будет хранить данные, и продолжать работать в одной ОС с разных устройств. Система может быть во многом похожа на традиционные варианты виртуальных рабочих столов, которые уже много лет развертываются предприятиями для поддержки сценариев BYOD. Но с более простой структурой ценообразования, удобным запуском и ориентацией на более широкую аудиторию. Это также может стать более контролируемым способом для Microsoft проверить механизмы работы своих новых ИИ-помощников. При запуске getmatch Рассказываем о том, как строить карьеру в IT Telegram Сайт ВКонтакте Twitter Сайт 167 Карма 0 Рейтинг облачной ОС им не придется отпускать ИИ «гулять» на каждый из компьютеров пользователей, все процедуры будут мониториться на собственных серверах. Будем надеяться, что это один из тех наборов слухов, которые скоро будут опровергнуты. Возможно, команда поэкспериментировала с этой идеей, и поняла, что она не приживается. Но пока что — нужно быть морально готовыми к тому, что со всё большим числом сервисов, в том числе от Microsoft, мы будем взаимодействовать через окно браузера. P.S. Тысячи крутых вакансий в РФ и за рубежом — в телеграм-боте getmatch. Задаете нужную зарплату, и к вам приходят лучшие предложения, а наши эксперты помогают пройти интервью. Не нужно ни резюме, ни портфолио, настройка занимает меньше 30 секунд. А ещё это отличный способ следить за текущим рынком труда. Откликаться не обязательно. Теги: windows, microsoft, windows 12, AI, intel, chatgpt Хабы: Блог компании getmatch, Искусственный интеллект, Разработка под Windows, Софт Редакторский дайджест Присылаем лучшие статьи раз\\xa0в\\xa0месяц Электропочта +19 22 123 '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a461c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf):\n",
    "    doc = fitz.open(pdf)\n",
    "    text = []\n",
    "    for current_page in range(len(doc)):\n",
    "        page = doc.load_page(current_page)\n",
    "        page_text = page.get_text(\"text\")\n",
    "        text.append(page_text)\n",
    "    print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40cdf8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pars_pdf(text):\n",
    "    text2 = text.split('\\n')\n",
    "    print(text2)\n",
    "    if 'Рейтинг' in text2:\n",
    "        num = text2.index('Рейтинг')\n",
    "        raiting = text2[num-1]\n",
    "        name = text2[num+1]\n",
    "        activity = text2[num+2]\n",
    "        date_publish = text2[num+3]\n",
    "        print(num, raiting, name, activity, date_publish)\n",
    "        \n",
    "    else:\n",
    "        raiting = 0\n",
    "        name = \"\"\n",
    "        activity = \"\"\n",
    "        if 'Простой' in text2:\n",
    "            num2 = text2.index('Простой')\n",
    "            date_publish = text2[num2-1]\n",
    "        else:\n",
    "            date_publish = \"\"\n",
    "        print(num, raiting, name, activity, date_publish)\n",
    "    \n",
    "    return  name, raiting, activity, date_publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31f7ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет, Хабр! На связи Владимир, техлид в команде разработки ИИ-инструментов в\\nдепартаменте технологической надёжности одной из крупных компаний. Наша команда\\nпомогает делать корпоративные и клиентские сервисы надежнее помощью Data Science.\\nМы помогаем мониторить тысячи подсистем\\nСистема, в разработке которой участвует моя команда, предназначена для\\nцентрализованного мониторинга тысяч отдельных систем, в том числе распределённых\\nгеографически. Наша группа отвечает за online-модель, подсвечивающую явно\\nаномальные отклонения в показателях мониторинга. С её помощью инженер дежурной\\nсмены однозначно определяет, есть ли сейчас проблемы, требующие немедленного\\nреагирования.\\nДанные мониторинга собираются из множества различных источников. И поскольку их\\nпоступает много в единицу времени, возникла острая необходимость в\\nраспараллеливании. Про потоковую обработку данных, обработку событий и выполнение\\nзадач по расписанию в распределённой системе написано немало. Среди множества\\nпубликаций, на мой взгляд, можно выделить очень достойные: вот пример с глубоким\\nанализом точек отказа.\\nНо после прочтения многих статей мне не хватило явных ответов на вопросы, неизбежно\\nвозникающих у ещё недостаточно опытного разработчика.\\nВ большинстве случаев содержание выглядит примерно так: сначала схема с общей\\nшиной (брокер, кеш, реляционная база) и несколькими воркерами (либо потоками) для\\nпараллельного выполнения, затем вставки с кодом, где используется какая-то библиотека\\n(или фреймворк). В экосистеме Python это либо Celery, либо же что-то из оркестраторов\\n(Nifi, Airflow, Spark), если речь идёт о задачах data-инжиниринга. Но это, как я уже сказал,\\nне отвечает на все вопросы.\\nvstorozhilov\\n7 часов назад\\nТри необсуждаемых вопроса о параллельной\\nраспределённой обработке данных — чтобы жить стало\\nлегче\\nСредний\\n12 мин\\n328\\nPython*,\\xa0Data Engineering*,\\xa0Распределённые системы*\\nТуториал\\nКАК СТАТЬ АВТОРОМ\\n', 'Три архитектурных вопроса, которые считаются решёнными по\\nумолчанию\\nПримерно так выглядели этапы проектирования и первого развёртывания на тестовом стенде :-)\\nИтак, у неопытных разработчиков обязательно возникнут такие вопросы:\\nКак корректно распределять задачи между воркерами, чтобы не возникало различных\\nаномалий конкурентности? Например, когда воркеры дублируют друг друга,\\n', 'обрабатывая одни и те же данные, или, наоборот, блокируют один другого?\\nКак архитектурно грамотно гарантировать «довыполнение» задачи в случае ошибки\\nворкера?\\nКак гарантировать выполнение задачи не позднее требуемой задержки (актуально для\\nonline-систем)?\\nТут для полноты картины стоит привести примеры нескольких сценариев отказа, которые\\nмне предстояло решить:\\nВоркер взял задачу в работу и получил ошибку из-за сетевой недоступности, без\\nпоследующего перезапуска контейнера с приложением.\\nВоркер взял задачу в работу и завершился аварийно, с перезапуском контейнера с\\nприложением, например из-за проблем с выделением памяти.\\nВоркер взял задачу в работу, находился в работе больше допустимого времени и\\nзавершился успешно или с ошибкой слишком поздно (актуально при наличии\\nтребования по максимальной задержке).\\nА так выглядели первые попытки учесть все возможные точки отказа\\n', 'Почему мой пример может предложить ответы на эти вопросы\\nПредсказательную онлайн-модель, над которой работала наша команда, можно назвать\\nETL-сервисом, если не углубляться в детали алгоритма обработки данных. Необходимость\\nв подобных перекладчиках, либо же событийных обработчиках, возникает регулярно, когда\\nпроектируешь микросервисную систему или потоки данных внутри оркестраторов (data-\\nинженеры, это я про вас).\\nКроме того, система, компонентом которой является сервис онлайн-модели,\\nвысококритичная, из-за чего к нашему сервису предъявляются повышенные\\nнефункциональные требования:\\nОтсутствие недоступности: обработанные данные по каждому показателю\\nмониторинга должны появиться минута в минуту, потери в обработанных данных\\n(пропуски точек на графиках показателей) не допускаются.\\nВысокая пропускная способность (скорость обработки): в течение 60 секунд\\nнужно успеть загрузить из базы-источника значение каждого показателя за конкретную\\nминуту, и дополнительно 179 предыдущих (то есть суммарно за 180-минутное окно),\\nобработать и положить результат в базу-приёмник (для последующей отрисовки\\nграфиков на фронтенде). Всего более 100 тысяч регулярно обновляемых временных\\nрядов с данными по различным системам.\\nГеорезервирование: надо уметь пережить отказ целого ЦОДа, на котором запущен\\nотдельный k8s-кластер, где, в свою очередь, развёрнут сервис.\\nПроектирование архитектуры системы с подобными требованиями и первые попытки\\nтестирования ее прототипа привели меня, тогда ещё малоопытного, к перечисленным\\nвыше вопросам. Я сделаю упор на эти аспекты далее, при погружении в подробности\\nархитектуры сервиса онлайн-модели.\\nАрхитектура сервиса онлайн-модели с акцентом на три поставленных\\nвопроса\\nВ чём помогает механизм групп консьюмеров Kafka\\nПрежде чем перейдём к рассмотрению архитектуры, поговорим немного о том, почему в\\nкачестве шины данных использовали брокер Kafka, а также подробнее рассмотрим\\nмеханизм групп консьюмеров.\\nПро то, что такое Kafka и связанные с ней понятия — продьюсер, консьюмер, топик,\\nпартиция, оффсет — можно узнать из видео. Я рекомендую посмотреть его тем, кто пока\\nне знаком с Kafka, так как рассчитываю на понимание при дальнейшем прочтении.\\n', 'Во многом мне помог механизм групп консьюмеров, с помощью которого уже частично\\nможно реализовать распределённое отказоустойчивое распараллеливание, а именно:\\nКаждый консьюмер закрепляется только за своим набором партиций внутри группы,\\nчтение из других партиций группы невозможно. Это позволяет избегать различных\\nаномалий конкурентности при распределении задач между воркерами.\\nВнутри группы консьюмеров для каждой партиции отслеживается свой собственный\\nоффсет (номер позиции последнего сообщения в очереди, для которого сделан\\nкоммит). Например, если консьюмер будет делать коммиты прочитанных позиций\\nтолько после успешного выполнения и записи результатов, а не сразу после прочтения\\nсообщения из партиции, то в случае ошибки можно будет «довыполнить» задачу при\\nповторной попытке.\\nКонсьюмеры периодически отправляют «пульс» (heartbeat) для обозначения себя\\n«живыми».\\nВ случае, если какой-то консьюмер перестаёт подавать признаки жизни, или, наоборот,\\nв группе появляется новый, то запускается процедура ребалансировки партиций\\n(partition reassignment) между «живыми» консьюмерами по выбранной стратегии (round\\nrobin, range, sticky). Новый консьюмер может продолжить читать из партиции с того\\nоффсета, на котором остановился «умерший», это тоже помогает гарантировать\\nсогласованное «довыполнение» при падениях воркеров.\\nМеханизм групп консьюмеров Kafka    \\nКраткий итог про группы консьюмеров: на первый и часть второго вопроса отвечает\\nфункциональность самой Kafka, что существенно упростило мне жизнь при разработке.\\n', 'Верхнеуровневая архитектура сервиса онлайн-модели. Оба компонента написаны на Python, чтобы не \\nнарушать общий стек с коллегами-аналитиками и data scientist-ами.\\nПостановщик задач: бизнес-логика, точки отказа, георезервирование\\nСервис постановки задач (task manager) является вторым ключевым инструментом\\nраспараллеливания. Первый — это группы консьюмеров Kafka, его цель — поставить\\nзадачу на выполнение онлайн-корректировки по фиксированному набору показателей за\\nконкретный минутный таймстемп: например, 2025-01-01 10:01:00, 2025-01-01 10:02:00 и т.\\nд. Его цикл бизнес-логики повторяется постоянно в реальном времени, с фиксированной\\nвеличиной паузы между итерациями. Цикл содержит следующий набор действий:\\n1. Проверить, есть ли в топике уже поставленные на выполнение задачи за текущий и\\nдва следующих минутных таймстемпа (то есть в нормальном режиме работы в момент\\nвремени 12:05 будет поставлена задача на 12:07; при сбоях, во избежание пропусков в\\nобработанных данных, — дополнительно за 12:06 и 12:05 по необходимости) и\\nзавершить цикл работы, а если нет — то переходить на следующий этап.\\n2. Определить актуальный набор показателей, по которым поступают новые данные\\n(сделать select из таблицы с метаинформацией).\\n3. Разбить набор уникальных показателей на пакеты.\\n4. Сформировать задачи на выполнение для каждого пакета за недостающие минутные\\nтаймстемпы: в содержании указать таймстемп и набор идентификаторов показателей\\n(metric_id), сгенерировать UUID с использованием порядкового номера батча и\\nтаймстемпа.\\n5. Отправить задачи в топик Kafka, распределяя по партициям: каждая задача в\\nотдельную партицию.\\n', 'Цикл бизнес-логики постановщика задач\\nЦифры рядом с блоками соответствуют этапам, перечисленным выше. Ключевым шагом\\nдля обеспечения отказоустойчивости является предварительная проверка наличия в\\nтопике поставленных на выполнение задач за текущий и два следующих минутных\\nтаймстемпа. Такой «запас» необходим для «выживания» в случаях аварийного падения\\nконтейнера с приложением или ошибок сетевого взаимодействия.\\nРассмотрим возможные точки отказа (наглядная иллюстрация на рисунке ниже), не\\nсвязанные с падением контейнера с приложением:\\n1. Сетевая ошибка чтения из топика Kafka последних задач для определения\\nнедостающих таймстемпов.\\n2. Сетевая ошибка запроса к базе-хранилищу агрегированных данных на получение\\nполного списка актуальных показателей мониторинга.\\n3. Сетевая ошибка записи недостающих задач в топик Kafka.\\n', 'Цикл бизнес-логики постановщика задач с точками отказа при сетевом взаимодействии в виде блок-схемы\\nЦифры рядом с блоками соответствуют точкам отказа, перечисленным выше. Во всех\\nслучаях преодолением отказа будет повтор цикла на следующей итерации.\\nАварийное падение контейнера с приложением (на любом из этапов цикла бизнес-логики)\\nпреодолевается с помощью контроллера кластера виртуализации с одной стороны, и\\nгеорезервирования с другой. Достичь георезервирования, а вернее согласованной работы\\nэкземпляров приложения «постановщик задач» из разных геокластеров, помогает всё тот\\nже механизм групп консьюмеров Kafka.\\n', 'Архитектура сервиса постановщика задач c учётом георезервирования\\nВ данном случае внутри группы консьюмеров используется собственная стратегия\\nраспределения партиций: все партиции только одному консьюмеру, второй при этом\\nвыступает в качестве резервной standby (неактивной) копии на случай сбоя первого.\\nВ коде постановщика на этапе определения недостающих задач предусмотрена\\nдополнительная проверка на «пустые» данные (их получение возможно тогда, когда за\\nконсьюмером нет закреплённых партиций внутри группы, так как ему их не хватило) и, в\\nслучае положительного результата, переход сразу на этап паузы до следующей попытки\\n', '(итерации цикла бизнес-логики). То есть второй консьюмер будет постоянно получать\\n«пустые» данные, выполняя «холостой» цикл.\\nПри отказе в работе активного консьюмера (вызванного, например, падением целого\\nЦОДа, на котором развёрнут кластер) брокер Kafka перестанет получать от первого\\nheartbeat-сигналы и переназначит все партиции второму консьюмеру. Тем самым, второй\\nконсьюмер продолжит ставить задачи данных на выполнение без пропусков, а первый\\nперезапустится контроллером геокластера виртуализации и уже сам станет stanby-копией.\\nВоркер: бизнес-логика, точки отказа, георезервирование\\nОбработкой данных по показателям занимается компонент «воркер». Его цикл бизнес-\\nлогики повторяется постоянно в реальном времени, с фиксированной величиной паузы\\nмежду итерациями. Цикл содержит следующий набор действий:\\n1. Получить задачу на выполнение из топика Kafka.\\n2. Проверить, выполнена ли она, то есть посмотреть, присутствует ли её UUID в таблице\\nуспешно выполненных, если нет — переходить на следующий этап, если есть —\\nзавершить цикл работы.\\n3. Загрузить необходимые данные по набору показателей: получить по каждому\\nпоказателю значения за указанный таймстемп и предыдущие 179 минут.\\n4. Выполнить процедуру онлайн-корректировки по данным всего набора показателей из\\nполученной задачи.\\n5. Записать результаты онлайн-корректировки в базу с обработанными данными с\\nпомощью SQL-запроса вида INSERT ... ON CONFLICT ..., чтобы не допустить\\nповторную перезапись, если результирующие данные за таймстемп выполняемой\\nзадачи уже были сохранены ранее.\\n6. Отметить задачу как успешно выполненную: записать UUID в таблицу выполненных\\nзадач.\\n7. Закоммитить оффсет в соответствующую партицию топика (откуда была прочитана\\nзадача) в рамках используемой группы консьюмеров.\\n', 'Цикл бизнес-логики воркерa\\nЦифры рядом с блоками соответствуют этапам, перечисленным выше. Ключевыми шагами\\nдля обеспечения отказоустойчивости являются:\\nПредварительная проверка на то, выполнена ли уже задача.\\nФиксация факта успешного выполнения задачи в отдельной таблице.\\nСохранение результатов корректировки данных по показателям через INSERT ... ON\\nCONFLICT ... для исключения дублирования или перезаписи.\\nВсё перечисленное гарантирует согласованное «довыполнение» задачи при ошибке\\nворкера на любом из этапов цикла бизнес-логики и исключает повторное выполнение\\n«тяжёлого» расчёта другим воркером в случае, если ошибка произошла на этапе 7.\\n', 'Автор слишком часто видел, как воркер с первой попытки не хотел брать задачу в работу\\nРассмотрим возможные точки отказа, не связанные с аварийным падением контейнера с\\nприложением (иллюстрация на рисунке ниже):\\n1. Сетевая ошибка чтения из топика Kafka задачи на выполнение.\\n2. Сетевая ошибка запроса к базе для предварительной проверки, была ли данная\\nзадача уже выполнена ранее.\\n3. Сетевая ошибка запроса к базе с целью получения необходимых для выполнения\\nзадачи данных.\\n4. Сетевая ошибка запроса к базе для записи результатов корректировки и возможных\\nметаданных по новым метрикам.\\n', '5. Сетевая ошибка запроса к базе с целью добавления UUID задачи в таблицу успешно\\nвыполненных.\\n6. Сетевая ошибка выполнения коммита в топик Kafka.\\nЦикл бизнес-логики воркера с точками отказа при сетевом взаимодействии\\nЦифры рядом с блоками соответствуют точкам отказа, перечисленным выше. Во всех\\nслучаях преодолением отказа будет повтор цикла на следующей итерации.\\nАварийное падение контейнера с приложением на любом из этапов цикла бизнес-логики\\nпреодолевается с помощью контроллера кластера виртуализации с одной стороны, и\\nгеорезервирования с другой.\\n', 'Архитектура сервиса-воркера c учётом георезервирования\\nВспомним, что в нефункциональных требованиях к системе присутствует пункт про\\nмаксимальную задержку. По итогам тестов производительности воркера со всеми\\nвозможными оптимизациями (многопоточность, подбор оптимального размера пакета\\nпоказателей, переход на самописные реализации некоторых библиотечных функций) в\\nусловиях доступного корпоративного железа максимальное время выполнения составило\\n30 секунд. С такими результатами очень высока вероятность того, что суммарная задержка\\nв случае сбоя составит больше минуты.\\n', 'Для выполнения требования по максимальной задержке георезервирование было\\nреализовано так, что каждому геокластеру соответствует своя группа консьюмеров на\\nуровне брокера Kafka. Обе группы настроены на одни и те же партиции.\\nПо итогу, в штатном режиме задача за конкретный таймстемп будет выполняться\\nпараллельно (дублироваться) двумя воркерами из разных геокластеров, а результат,\\nвозможные метаданные по новым метрикам и UUID выполненной задачи будут записаны\\nтолько один раз (благодаря INSERT ... ON CONFLICT ...). Предполагается, что\\nодновременное «падение» двух воркеров, выполняющих одну и ту же задачу, но\\nнаходящихся в разных геокластерах (то есть физически в разных ЦОДах), маловероятно.\\nЕщё одна вещь, обеспечивающая согласованность работы консьюмеров из разных\\nгеокластеров, — общая таблица с UUID успешно выполненных задач. Она исключает\\nповторное выполнение в случае сбоя только одного из дублирующих воркеров.\\nЕщё раз о ключевых моментах обеспечения нефункциональных\\nтребований\\nМы подробно рассмотрели как архитектуру планировщика задач, так и архитектуру\\nворкера (в обоих случаях с обеспечением георезервирования). Все поставленные перед\\nсистемой нефункциональные требования обеспечены, а именно:\\nОтсутствие недоступности (появление обработанных данных минута в минуту,\\nотсутствие пропусков (потерь) в данных) гарантируется: со стороны планировщика\\nзадач — общей группой консьюмеров Kafka между единичными экземплярами\\nконтейнеров в разных геокластерах и «запасом» публикуемых в топик задач сразу на\\nтекущий и два последующих минутных таймстемпа; со стороны воркера —\\n«дублирующими» группами консьюмеров Kafka для множественных экземпляров\\nконтейнеров в разных геокластерах.\\nВысокая пропускная способность гарантируется распараллеливанием потока\\nобработки через множество независимых друг от друга воркеров, а также некоторыми\\nоптимизациями на уровне кода приложения.\\nГеорезервирование достигается размещением экземпляров приложений в k8s-\\nкластерах, развёрнутых на географически разнесенных ЦОДах. Согласованная работа\\nмежду контейнерами из разных геокластеров становится возможной: в случае\\nпланировщика задач — благодаря общей группе консьюмеров Kafka между\\nединичными экземплярами контейнеров из разных геокластеров; в случае воркера —\\nблагодаря фиксации UUID успешно выполненных задач в одну общую SQL-таблицу\\nконтейнерами из разных геокластеров.\\n', 'Когда всё-таки предусмотрел все возможные сценарии отказа\\nПочему архитектурные решения в нашем примере можно\\nпереиспользовать много где ещё\\nРассмотренный пример может повторять типичную реализацию распределённых систем,\\nсвязанных с обработкой событий, где в качестве события может быть что угодно:\\nсообщение в чате, электронное письмо, транзакция платёжной операции, клиентская\\nактивность в CRM. Во всех перечисленных случаях крайне важно гарантировать\\nобязательную доставку данных без потерь и с требованием по максимальной задержке,\\nчтобы не допустить ухудшения клиентского опыта.\\nРазобранный пример актуален и для потоковой обработки с помощью оркестраторов в\\nзадачах data-инжиниринга. Используемый оркестраторами directed acyclic graph (DAG)\\nслужит для преодоления различных точек отказа примерно такими же способами, как\\nописано в примере.\\nПоэтому в качестве ответов на вопросы, поставленные в начале, можно привести\\nнесколько обобщающих выводов, полезных при архитектурном проектировании подобных\\nрешений:\\nДля достижения согласованной обработки между распределёнными контейнерами с\\nприложением необходимо реализовать (или использовать возможности готовой\\nтехнологии) кооперативный механизм распределения задач (событий), чтобы\\nизбежать различных аномалий конкурентности вроде нежелательных дублирований\\nили блокировок.\\nТут, на самом деле, всё будет зависеть от решаемых системой задач и требований. В\\nкаких-то случаях может быть достаточно функциональности SQL с транзакциями и\\n', '3\\nКарма\\n1\\nРейтинг\\nВладимир Сторожилов @vstorozhilov\\nbackend, devops и когда нужно data scientist ;-)\\nблокировками. В моём случае помогли группы консьюмеров Kafka. Нечто аналогичное\\nможно увидеть и в RabbitMQ, где сообщения из одной и той же очереди\\nраспределяются между консьюмерами через round robin. Даже Redis имеет\\nфункциональность каких-никаких транзакций. Но если всё-таки обойтись готовыми\\nинструментами никак не получается, то придётся реализовывать ещё один компонент\\n— менеджер задач (событий). Пример можно посмотреть всё в той же статье,\\nупомянутой в предисловии.\\nЧтобы однозначно гарантировать исполнение события (задачи), может быть\\nнеобходима фиксация факта успешного выполнения в общее хранилище с\\nпомощью сохранения туда уникального идентификатора.\\nС другой стороны, это позволит избежать повторного запуска «тяжёлого» алгоритма\\nобработки данных и пустой траты вычислительных ресурсов, если воркер упал на\\nодном из последних этапов. В моём примере это была отдельная SQL-таблица. Из\\nготовых инструментов можно снова вспомнить пример библиотеки Celery в экосистеме\\nPython, которая тоже сохраняет идентификаторы завершённых задач в отдельную\\nочередь, таблицу или массив (в зависимости от того, что используется в качестве\\nшины и хранилища).\\nЕсли к системе есть требование по максимально допустимой задержке, то\\nхорошим способом этого достичь будет дублирование потока обработки, то есть\\nобработка одних и тех же событий (задач) через параллельно запущенные воркеры\\n(возможно, из разных кластеров виртуализации, если есть требование по\\nгеорезервированию).\\nТеги: конкурентность, распределенные системы, распределённая обработка данных, \\nгеорезервирование, таск-менеджер, таск-раннеры, дата-инжиниринг, параллелизм\\nХабы: Python, Data Engineering, Распределённые системы\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\nЭлектропочта\\n+1\\n11\\n0\\n']\n"
     ]
    }
   ],
   "source": [
    "x = extract_text_from_pdf(r\"PDF2\\Три необсуждаемых вопроса о параллельной распределённой обработке данных — чтобы жить стало легче _ Хабр.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37796035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет, Хабр! На связи Владимир, техлид в команде разработки ИИ-инструментов в', 'департаменте технологической надёжности одной из крупных компаний. Наша команда', 'помогает делать корпоративные и клиентские сервисы надежнее помощью Data Science.', 'Мы помогаем мониторить тысячи подсистем', 'Система, в разработке которой участвует моя команда, предназначена для', 'централизованного мониторинга тысяч отдельных систем, в том числе распределённых', 'географически. Наша группа отвечает за online-модель, подсвечивающую явно', 'аномальные отклонения в показателях мониторинга. С её помощью инженер дежурной', 'смены однозначно определяет, есть ли сейчас проблемы, требующие немедленного', 'реагирования.', 'Данные мониторинга собираются из множества различных источников. И поскольку их', 'поступает много в единицу времени, возникла острая необходимость в', 'распараллеливании. Про потоковую обработку данных, обработку событий и выполнение', 'задач по расписанию в распределённой системе написано немало. Среди множества', 'публикаций, на мой взгляд, можно выделить очень достойные: вот пример с глубоким', 'анализом точек отказа.', 'Но после прочтения многих статей мне не хватило явных ответов на вопросы, неизбежно', 'возникающих у ещё недостаточно опытного разработчика.', 'В большинстве случаев содержание выглядит примерно так: сначала схема с общей', 'шиной (брокер, кеш, реляционная база) и несколькими воркерами (либо потоками) для', 'параллельного выполнения, затем вставки с кодом, где используется какая-то библиотека', '(или фреймворк). В экосистеме Python это либо Celery, либо же что-то из оркестраторов', '(Nifi, Airflow, Spark), если речь идёт о задачах data-инжиниринга. Но это, как я уже сказал,', 'не отвечает на все вопросы.', 'vstorozhilov', '7 часов назад', 'Три необсуждаемых вопроса о параллельной', 'распределённой обработке данных — чтобы жить стало', 'легче', 'Средний', '12 мин', '328', 'Python*,\\xa0Data Engineering*,\\xa0Распределённые системы*', 'Туториал', 'КАК СТАТЬ АВТОРОМ', '']\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'num' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpars_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mpars_pdf\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m         date_publish \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mnum\u001b[49m, raiting, name, activity, date_publish)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  name, raiting, activity, date_publish\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'num' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "pars_pdf(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ccf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASML возникла в 1984 году как совместное предприятие Philips и ASM, и которое с самого\\nначала рассматривалась как провальный проект.\\xa0Но сегодня ASML имеет рыночную\\nкапитализацию более 300 млрд евро и является одной из самых важных компаний для\\nтехнологического развития мира. Компании удалось создать, по сути, технологическую\\nмонополию в самом передовом оборудовании для производства чипов —литографии в\\nэкстремальном ультрафиолетовом диапазоне (EUV) - технологии, необходимой для\\nсоздания самых современных чипов, с которой не смогла справиться ни одна другая\\nкомпания в мире...\\nОт аутсайдера-задиры до лидера литографии\\nВ начале 1980-х годов небольшой городок Вельдховен тихо пожинал плоды своего соседа\\nЭйндховена — процветающего промышленного центра благодаря присутствию\\nголландского электронного гиганта Philips. Поскольку Philips продвигал технологические\\nинновации в регионе, его влияние распространялось за пределы Эйндховена, способствуя\\nволновому эффекту промышленной активности и технического опыта, которые достигли\\nВельдховена.\\nС самого начала ASML был подразделением, которое многие в материнской компании\\nсчитали скорее обузой, чем возможностью. Фактически, это был проблемный ребенок\\nPhilips — проект, который потратил более десятилетия на попытки проникнуть в\\nфотолитографический бизнес без особого успеха. В Philips шутили, что создание ASML\\nбыло по сути отложенным увольнением для переведенных в него сотрудников.\\nНесмотря на внутренний скептицизм, Philips увидела спасение в партнерстве с ASM\\nInternational , другой голландской компанией по производству оборудования для\\nполупроводников, которая сегодня занимает лидирующие позиции на рынке оборудования\\nдля так называемого атомно-слоевого осаждения (ALD) и эпитаксии, используемых в\\nпередовом производстве микросхем. В 1984 году ASML была создана как совместное\\nпредприятие Philips и ASM со штаб-квартирой в Вельдховене. Поначалу компанию в\\nзначительной степени считали неудачной, не имея ни коммерческой жизнеспособности, ни\\nдаже надлежащего офиса. Ее первым рабочим пространством был набор деревянных\\nбараков в кампусе Philips в Эйндховене. Моральный дух был низким, многие сотрудники\\nTraPhro\\n9 часов назад\\nASML: Создание самых сложных машин на Земле\\n17 мин\\n2.5K\\nЧитальный зал,\\xa0Бизнес-модели*,\\xa0Инфографика,\\xa0Развитие стартапа\\nКАК СТАТЬ АВТОРОМ\\n', 'скептически относились к шансам предприятия на успех, рассматривая его скорее как\\nпроект, обреченный на провал с самого начала.\\nНесмотря на столь неперспективное начало, у ASML было несколько важных активов.\\nКомпания унаследовала две ведущие в отрасли технологии, разработанные в Philips,\\nнебольшую группу упорных инженеров и уникальное окно возможностей в\\nлитографической отрасли, которая находилась на пороге значительного технологического\\nсдвига.\\nПервые годы были суровыми, ASML едва выживала и в значительной степени зависела от\\nфинансовой поддержки своих спонсоров, Philips и ASM, чтобы оставаться на плаву. PAS\\n(Philips Automatic Stepper) 2000 — первая система ASML (часто называемая «машинами»,\\nхотя ASML называет их «системами»), которую она унаследовала от Philips и изначально\\nпродавала под своим прежним названием ASM Lithography — была далека от успеха. Ее\\nгидравлика на масляной основе сделала невозможным ее продажу, поскольку\\nпотенциальные пары масла загрязняли строго контролируемые чистые помещения, где\\nизготавливаются чипы.\\nИстория ASML отмечена моментами счастливой случайности, один из которых был связан\\nс Моррисом Чангом, основателем TSMC, крупнейшего в мире литейного завода\\nполупроводников. В 1987 году, когда TSMC только начинала свой путь, Чанг разместил\\nкрупный заказ на литографические машины ASML. Однако вскоре после поставки на\\nпредприятии TSMC вспыхнул пожар, уничтоживший оборудование. То, что могло стать\\nкатастрофической неудачей, обернулось для ASML счастливой случайностью. Страховка\\nTSMC покрыла убытки, и литейный завод был вынужден повторно заказать те же машины,\\nфактически удвоив заказ ASML. Говорят, что это событие имело решающее значение не\\nтолько для укрепления отношений между TSMC и ASML, но и для обеспечения\\nфинансовой поддержки в одном из самых финансово сложных периодов для ASML.\\n', 'Набор дискет с конфигурацией программного обеспечения ASML PAS 5500.\\nНо только в начале 1990-х годов ASML совершила свой первый настоящий прорыв с PAS\\n5500, разработанным тогда молодым человеком по имени Мартин Ван ден Бринк.\\nБлагодаря упорству, как в финансовом плане, так и в научно-исследовательской\\nлаборатории, ASML все-таки переломила ситуацию. Вскоре после этого компания стала\\nодним из трех ведущих игроков в отрасли литографии, наряду с японскими Nikon и Canon.\\nА в 2002 году, почти через два десятилетия после того, как Philips и ASM решили дать тому,\\nчто должно было называться ASML, еще один шанс, компания обогнала Nikon и стала\\nномером один в отрасли. Но вместо того, чтобы почивать на лаврах, ASML продолжала\\nраздвигать границы, инвестируя в новаторские технологии, которые, как многие считали,\\nникогда не будут работать.\\nЭпоха литографии в экстремальном ультрафиолете\\nКорни технологии литографии уходят на несколько десятилетий назад. Изначально\\nпроизводители полупроводников использовали естественный свет для фотолитографии —\\nпроцесса травления микроскопических схем на кремниевых пластинах. Однако по мере\\n', 'роста спроса на транзисторы меньшего размера ограничения использования видимого\\nсвета стали очевидны. Световые волны, которые не распространяются по прямым линиям,\\nначали искажать рисунки, вытравленные на чипах, поскольку эти рисунки уменьшались до\\nвсе меньших размеров.\\nЧтобы решить эту проблему, отрасль развивалась через различные источники света,\\nпереходя от видимого света к ультрафиолетовому, а затем к глубокому ультрафиолетовому\\n(DUV) свету. К 1990-м и 2000-м годам DUV-свет стал стандартом, используя лазеры для\\nдостижения длины волны 193 нанометра. Однако, поскольку размеры чипа уменьшились\\nдо нескольких нанометров — где нанометр составляет одну миллиардную часть метра,\\nпримерно в 100 000 раз меньше ширины человеческого волоса — даже DUV-свет оказался\\nслишком громоздким, сродни попытке написать свое имя лопатой для уборки снега.\\nНеобходимость в решении для следующего поколения была очевидна, если отрасль\\nсобиралась идти в ногу с законом Мура — наблюдением одного из основателей Intel,\\nГордона Мура, сделанным в 1965 году, что количество транзисторов на чипе удваивается\\nпримерно каждые два года.\\nОсознавая ограничения DUV, полупроводниковая промышленность вместо этого начала\\nизучать экстремальный ультрафиолет (EUV) в 1990-х годах. Теоретическая концепция\\nбыла разработана десятилетием ранее, но считалась практически невозможной для\\nкоммерциализации.\\n«Обычно» используемый способ создания EUV-света — это процесс, называемый\\nлазерно-производимой плазмой. В этом процессе вы стреляете лазером по капле\\nрасплавленного олова примерно 50 000 раз в секунду, что создает плазму, которая\\nизлучает EUV-свет. Плазма, образующаяся в ходе этого процесса, может достигать\\nтемпературы в несколько сотен тысяч градусов по Цельсию, что во много раз горячее, чем\\nповерхность Солнца, температура которой составляет 5500 градусов.\\nЧтобы иметь возможность травить кремниевые пластины с помощью EUV для\\nпроизводства чипов, свет должен быть направлен через ряд прецизионных зеркал,\\nкоторые фокусируют и формируют луч, прежде чем он освещает фотошаблон,\\nсодержащий желаемые шаблоны схемы. EUV-свет проецирует эти шаблоны на\\nкремниевую пластину, покрытую светочувствительным материалом, что позволяет\\nизготавливать сложные полупроводниковые элементы.\\n', 'По словам Марка Хиджинка, в его биографической FOCUS: The ASML Way, ни один\\nчеловек не понимает полностью все аспекты того, как работают эти машины.\\n', 'Иллюстрация внутреннего устройства системы EUV-литографии.\\nБольшой прорыв. Гениальный M&A\\nИнтенсифицированные исследования в области EUV были инициированы американским\\nконсорциумом, финансируемым Министерством энергетики, движимым желанием вернуть\\nутраченные позиции у японских конкурентов. Однако в США не было отечественного\\nпроизводителя литографии, способного продвигать эту технологию вперед. Появляется\\nASML, которую пригласили присоединиться к консорциуму и взять на себя руководство\\nразработкой технологии EUV. Эта возможность стала решающим поворотным моментом\\nдля ASML.\\nНесколько лет спустя, в 2001 году, ASML сделала важнейший шаг, приобретя\\nамериканскую компанию Silicon Valley Group, также известную как SVG. Несмотря на\\nагрессивное лоббирование против голландцев — влиятельные люди опасались, что США\\nпотеряют свои позиции в гонке литографии — политики в Вашингтоне дали зеленый свет\\nсделке после месяцев колебаний. Во многом это произошло благодаря Intel, у которой\\nбыли прочные отношения с правительством США. Это приобретение было не просто\\nрасширением бизнеса ASML; это было рассчитанное и стратегическое решение,\\nсыгравшее решающую роль в формировании будущего полупроводниковой\\nпромышленности.\\nПриобретя SVG, ASML не только устранила ключевого конкурента, но и получила доступ к\\nценной интеллектуальной собственности SVG и лицензиям EUV, текущим исследованиям\\nи инженерному таланту. Этот интеллектуальный капитал имел решающее значение для\\n', 'ускорения усилий ASML по разработке EUV. В то время такие производители микросхем,\\nкак Intel, TSMC и Samsung, были клиентами SVG, поэтому ASML также укрепила свои\\nключевые отношения с клиентами. Это стало важной вехой на пути компании к успешной\\nкоммерциализации EUV.\\nПервоначально планировалось внедрить технологию EUV в середине 2000-х годов, но\\nпроцесс разработки оказался сложнее, чем предполагалось. К 2010 году технология все\\nеще не была готова, несмотря на несколько миллиардов долларов в НИОКР. В 2012 году\\nASML получила значительные инвестиции от Intel , Samsung и TSMC , которые в\\nсовокупности приобрели 23% акций компании и предоставили 1,4 млрд евро на\\nфинансирование НИОКР.\\nЭто сотрудничество, известное как The Musketeer Project, было не просто финансовой\\nподдержкой; это был стратегический альянс, который обеспечил ASML ресурсами и\\nтехнической экспертизой, необходимыми для преодоления проблем, связанных с\\nразработкой EUV. Сообщается, что эта инициатива была «последним большим толчком» в\\nвыводе ожидаемой технологии на рынок, что наконец произошло в 2019 году — почти на\\nдва десятилетия позже первоначально запланированного и примерно через 40 лет после\\nразработки первоначальной теоретической концепции.\\nХотя японские Nikon и Canon предприняли значительные попытки разработать EUV, в\\nконечном итоге им пришлось отказаться от своих усилий из-за огромной стоимости и\\nтехнических проблем. ASML, с другой стороны, имела ресурсы, поддержку, бизнес-модель\\nи решимость довести проект до конца. Явная сложность и сопутствующие расходы на\\nНИОКР по разработке коммерчески жизнеспособной системы литографии с\\nиспользованием EUV создают огромные барьеры для входа.\\nМашины EUV компании ASML сегодня необходимы для производства самых современных\\nчипов, которые, в свою очередь, стимулируют прогресс практически всех современных\\nвычислительных технологий. Таким образом, ASML играет ключевую роль, если не сказать\\nбольше, в расширении возможностей всего: от смартфонов и суперкомпьютеров до\\nбеспилотных автомобилей и медицинских приборов. Возможность втиснуть больше\\nтранзисторов на один чип позволяет создавать более мощные процессоры, более\\nэффективную память и разрабатывать сложные алгоритмы, которые улучшают ИИ и\\nдругие передовые технологии.\\nЭти машины, часто описываются как самые сложные машины, когда-либо созданные\\nлюдьми, размером примерно с двухэтажный автобус. Их транспортировка сама по себе\\nявляется инженерной задачей, требующей семи Boeing 747 для доставки каждой машины к\\nместу назначения. Каждая отдельная часть машины также перевозится в своем\\nсобственном специализированном контейнере, чтобы гарантировать поддержание нужной\\nтемпературы до тех пор, пока она не попадет на фабрику. Следующее поколение\\nоборудования ASML, которое, как ожидается, поступит в массовое производство к 2025\\n', 'или 2026 году, будет еще больше. Сообщается, что цена этой будущей машины High NA\\nEUV, которую Intel будет эксплуатировать первой, составляет 350 миллионов долларов.\\nБолее старые машины EUV продаются примерно за 150-250 миллионов долларов в\\nзависимости от модели.\\nВизуальное сравнение размеров TWINSCAN EXE:5000 от ASML с размером человека.\\nЖизненный цикл литографической машины\\nКогда производитель чипов, такой как TSMC или Samsung, инвестирует в одну из\\nлитографических машин ASML, это не просто покупка оборудования; это приобретение\\nдолгосрочного актива и многолетнего партнерства. Эти машины созданы, чтобы служить\\nдолго. Фактически, ASML недавно сообщила, что 90% всех литографических машин,\\nпроданных ею за последние 30 лет, все еще работают, что довольно примечательно,\\nучитывая их сложность. Некоторые остаются на своих первоначальных местах, в то время\\nкак другие, достигнув конца своего первоначального использования, восстанавливаются и\\nперепродаются другим производителям. Даже некоторые из их старых систем PAS 5500\\nвсе еще штампуют чипы.\\nТаким образом, после установки эти машины могут работать десятилетиями, что является\\nрешающим фактором в отрасли, где капитальные затраты на строительство завода\\nобычно превышают 15 миллиардов долларов. Кроме того, ASML овладела искусством\\nподдержания этих машин не просто в рабочем состоянии, но и в улучшении. Она\\nпредлагают услуги по модернизации и улучшению машин даже спустя десятилетия после\\nустановки, а также имеет обслуживающий персонал, доступный круглосуточно и без\\nвыходных на каждом объекте, где работает машина ASML. Это означает, что машина,\\nкупленная сегодня, может продолжать становиться более эффективной и\\nпроизводительной в течение следующих нескольких десятилетий. Эта способность к\\nмодернизации гарантирует, что клиенты продолжат пользоваться плодами своих\\nинвестиций, что еще больше мотивирует их покупать у ASML.\\n', 'Модель доходов ASML отражает эти долгосрочные отношения с клиентами. Продажи\\nновых машин составляют около 75% дохода ASML, а оставшиеся 25% поступают от услуг\\nи полевых опций, включая техническое обслуживание и модернизацию. Этот\\nдиверсифицированный поток доходов также помогает ASML поддерживать стабильный\\nдоход, даже несмотря на то, что полупроводниковая промышленность переживает свои\\nнеизбежные циклы. Кроме того, обучение обслуживающего персонала ASML — это\\nвысокоспециализированный процесс, который занимает около двух лет, что еще больше\\nусложняет их операции и укрепляет конкурентное преимущество компании.\\nПолупроводниковая промышленность исторически была очень цикличной, и опыт ASML не\\nявляется исключением. Однако в последние годы эти циклы стали менее суровыми для\\nASML. Одной из причин этого является, как уже упоминалось ранее, уникальное\\nположение компании как единственного производителя определенных типов\\nлитографического оборудования. Если производитель, такой как TSMC, хочет создать\\nследующее поколение чипов, ему придется обратиться в ASML. Производители не\\nрешаются отменять заказы в ASML, даже в сложных экономических условиях, потому что\\nони знают, что потеряют свое место в очереди, когда спрос снова вырастет.\\nЦепочка создания стоимости полупроводников\\nНиже представлена \\u200b\\u200bподробная инфографика, объясняющая сложную цепочку создания\\nстоимости в полупроводниковой отрасли и иллюстрирующая, как более 100 ключевых\\nигроков, таких как TSMC , NVIDIA , Broadcom и ASML, вписываются в эту экосистему.\\n', 'Инфографика, иллюстрирующая цепочку создания стоимости полупроводников, включающую более 100 \\nключевых игроков, разделенных по сегментам отрасли.\\nЦепочка снабжения\\nЕще одним важным аспектом деятельности компании ASML является ее модель поиска\\nпоставщиков и сборки. Несмотря на продажу самого сложного оборудования на Земле,\\nоколо 90% его деталей разрабатываются и поставляются третьими сторонами. По своей\\nсути ASML является не производителем, а архитектором, совместно разрабатывающим и\\nпоставляющим детали от высокоспециализированной сети из тысяч поставщиков по всему\\nмиру. Около 80% стоимости проданных товаров приходится на эти внешние компоненты и\\nматериалы, и только 20% - на оплату труда на предприятиях ASML.\\nЭта модель на самом деле родилась из необходимости. В первые дни у ASML не было\\nресурсов для принятия капиталоемкой вертикально интегрированной модели,\\nиспользуемой японскими конкурентами, такими как Nikon и Canon . Вместо этого у них не\\nбыло выбора, кроме как положиться на внешних поставщиков. Теперь это превратилось в\\nключевое конкурентное преимущество для ASML, поскольку ее уникальные отношения с\\nпоставщиками углубились по мере развития технологий. Можно также утверждать, что\\nрасходы ASML на НИОКР сильно занижены, поскольку ее сложная сеть высокоразвитых\\n', 'поставщиков ежегодно тратит дополнительные миллиарды на совершенствование своих\\nкомпонентов литографических машин.\\nТаким образом, самым важным элементом системы литографии EUV был и остается не\\nкакой-либо отдельный компонент, а экспертиза ASML в управлении цепочкой поставок. По\\nсловам бывшего исполнительного вице-президента и директора по стратегии Фрица ван\\nХаута, ASML спроектировала эту сеть деловых отношений «как машину», создав тонко\\nнастроенную систему из тысяч компаний, способных удовлетворять ее строгим\\nтребованиям.\\nЧтобы обеспечить стабильность в этой сложной цепочке поставок, ASML установила\\nнесколько ключевых критериев для своих поставщиков. Не более 40% дохода каждого\\nпоставщика может поступать от ASML, чтобы избежать их банкротства, когда наступит\\nнеизбежный спад в полупроводниковой отрасли. Поскольку большинство компонентов из-\\nза своей сложности поставляются одним поставщиком, это означает, что найти\\nпоставщика-заменителя чаще всего невозможно. Это привело к тому, что ASML время от\\nвремени как вливала капитал в проблемных поставщиков, так и, как в случае Cymer и\\nBerliner Glas, приобретала всю компанию. Хотя приобретение поставщиков\\nрассматривается как последнее средство.\\nРешающая роль оптики Zeiss SMT\\nОдним из важнейших поставщиков ASML является Zeiss SMT (Semiconductor Manufacturing\\nTechnology). Zeiss SMT, как и материнская компания, является частной компанией.\\nРасположенная всего в 6 часах езды на машине от Вельдховена, в Баден-Вюртемберге,\\nГермания, Zeiss SMT разрабатывает сложную оптическую систему, используемую в\\nмашинах EUV компании ASML.\\n', 'Два инженера Zeiss SMT осматривают новую оптическую систему, используемую в машинах EUV компании \\nASML\\nОптическая система сама по себе является невероятным технологическим достижением и\\nпродуктом, а вся система весит несколько тонн. Она использует гиперспециализированные\\nзеркала для отражения и направления EUV-света, поскольку традиционные линзы не могут\\nиспользоваться на этих длинах волн. Лазерный свет отражается от зеркал около 40 раз,\\nпрежде чем, наконец, достичь фотошаблона, а затем кремниевой пластины. Чтобы это\\nупражнение по отражению лазера вообще работало, зеркала Zeiss должны быть безумно\\nплоскими. Фактически, эти зеркала известны как одни из самых плоских\\nповерхностей, когда-либо созданных людьми, с отклонениями от идеальной\\nплоскостности, измеряемыми долями нанометра. Эти оптические системы поставляются\\nисключительно ASML.\\nУчитывая уникальное положение, которое заняла компания ASML, можно задаться\\nвопросом, как изменилось ценообразование в компании. Да, система EUV чрезвычайно\\nдорога в абсолютном долларовом выражении, но ASML действительно хочет быть\\nуверенной в том, что она обеспечивает ценность, которая оправдывает инвестиции\\nдля клиентов, поэтому она не завышает цены. То же самое касается и отношений с\\nпоставщиками, поскольку многие поставщики в значительной степени зависят от\\n', 'голландской компании. ASML стремится к долгосрочной оптимизации партнерских\\nотношений.\\nУспех ASML в цифрах\\nВ 2023 году ASML получила доход в размере 27,6 млрд евро — на 30% больше, чем в\\nпредыдущем году. Ее операционная прибыль выросла на 38% до 9 млрд евро. Довольно\\nвпечатляюще цифры для компании, которая начиналась как «проблемный ребенок».\\nРыночная капитализация компании сейчас превышает 300 млрд евро, что делает ее\\nтретьей по величине компанией в Европе после Novo Nordisk и LVMH, а значит, и\\nодной из крупнейших в мире.\\nВ 2023 году ASML продала в общей сложности 449 литографических машин. Это не\\nизделия массового производства в обычном смысле; каждая из них представляет собой\\nузкоспециализированное и невероятно дорогостоящее оборудование.\\nПо состоянию на 2024 год в ASML работает более 42 000 человек, как на зарплате, так и по\\nгибким контрактам, в более чем 60 филиалах по всему миру. В штаб-квартире компании,\\nкоторая по-прежнему находится в Велдховене, Нидерланды, размещается более половины\\nее сотрудников. Это одна из самых высокоспециализированных команд в мире, которая\\nсотрудничает для решения некоторых из самых сложных технологических задач в мире.\\nЛучшие инженеры, физики, разработчики программного обеспечения и эксперты по\\nцепочке поставок вносят свой вклад в сложную оркестровку, необходимую для создания\\nмашин ASML.\\nОдна машина ASML EUV содержит более 700 000 компонентов. Для сравнения,\\nтипичный автомобиль состоит примерно из 30 000 деталей. Сложность этих машин\\nтрудно осознать. Каждая деталь должна быть идеально откалибрована, чтобы\\nвыполнять свою роль в системе, требующей точности на атомном уровне.\\nНо чтобы по-настоящему оценить успех и масштаб ASML, можно сравнить ее с\\nматеринской компанией. За последние три десятилетия из Philips вышло три крупных\\nкомпании: ASML в 1984 году, NXP в 2006 году и Signify в 2016 году. Сегодня все три\\nкомпании стоят в 14 раз дороже самого Philips.\\nПри этом капитализация Philips 27 млрд долл, а ASML 305 млрд долл.\\nКонцентрированная клиентская база\\nКлиентская база ASML высококонцентрирована, и, по данным отраслевых экспертов, на\\nдвух крупнейших клиентов приходится около 60% ее выручки. Хотя ASML не\\nраскрывает точных цифр, она постоянно указывает на концентрацию клиентов как на\\nключевой риск в каждом отчете о доходах. Эта узкая клиентская база отражает\\nспециализированный характер полупроводниковой промышленности, где только\\n', 'несколько компаний имеют возможности и финансовые ресурсы для производства\\nведущих полупроводников – и, следовательно, для покупки машин EUV ASML.\\nЭти клиенты — не просто компании; это гиганты вроде TSMC, Samsung, Intel и UMC —\\nкомпании с глубокими карманами. Они представляют собой немногих игроков в мире,\\nкоторые действительно могут использовать мощь технологии ASML для производства\\nсамых передовых чипов в мире. Такая концентрация может показаться рискованной, но это\\nестественный результат структуры отрасли, и ее вряд ли можно было избежать.\\nДенежный поток и «скрытая» ценовая сила\\nASML — это генератор кеша с высокой рентабельностью. При этом клиенты часто вносят\\nденьги авансом за свое оборудование, что обеспечивает финансовую подушку\\nбезопасности, поскольку ASML продолжает активно инвестировать в НИОКР и расширение\\nмощностей. Это, возможно, можно рассматривать как форму ценовой власти, хотя и в\\nболее тонком виде, чем типичный подход простого повышения цен.\\nКомпания тратит около 15-16% своего годового дохода на НИОКР. Важно учитывать, что\\nэта цифра не учитывает усилия по НИОКР поставщиков, которые на самом деле несут\\nнаибольшую нагрузку.\\nВедущие фигуры, стоящие за успехом\\n', 'В основе стремительного взлета ASML лежит команда руководителей, которая превратила\\nкомпанию из забытого аутсайдера в титана отрасли. Среди них выделяется Питер Веннинк\\n— лидер, чей путь в ASML начался задолго до того, как он переступил ее порог.\\nВ 1995 году Веннинк работал в Deloitte, где он возглавлял команду, управляющую IPO\\nASML . Изучая потенциал компании, он был очарован ее потенциальным будущим и\\nстоящими перед ней вызовами. ASML была не просто еще одним клиентом; она стала\\nстрастью. Чем больше он узнавал, тем больше понимал, что хочет стать частью ее\\nистории.\\nЧетыре года спустя, в 1999 году, Веннинк совершил важный шаг, присоединившись к ASML\\nв качестве финансового директора. Его влияние было высоким. Он провел компанию через\\nнесколько взлетов и падений, в конечном итоге заняв пост генерального директора в 2013\\nгоду. Под его руководством ASML преодолела самые сложные испытания, закрепив за\\nсобой место неоспоримого лидера в своей нише. Веннинк ушел на пенсию в апреле 2024\\nгода после 25 лет преобразований в компании, передав эстафету Кристофу Фуке,\\nопытному руководителю, который провел 17 лет в ASML на различных ключевых\\nдолжностях.\\nНо если есть одно имя, синоним успеха ASML, то это Мартин Ван ден Бринк, о котором\\nупоминалось ранее. Ван ден Бринк, бывший технический директор и президент компании,\\nбыл частью ASML с самого начала. Он ушел на пенсию вместе с Питером Веннинком в\\nначале 2024 года, ознаменовав уход двух самых важных фигур в истории ASML —\\nсобытие, которое может означать сложный период для компании в будущем. Ван ден\\nБринк присоединился к Philips еще в 1983 году и оказался в ASML с момента ее основания.\\nВ то время и как уже упоминалось ранее, многие в отрасли считали, что подразделение, в\\nкоторое он перешел, обречено на провал. Несмотря на эти сомнения, Ван ден Бринк не\\nсвернул с пути.\\nЕго первые годы в ASML были трудными, так как компания находилась на грани\\nкатастрофы, пытаясь вывести на рынок свой первый успешный продукт. Всего через два\\nгода после основания компании Ван ден Бринк занял руководящую должность, возглавив\\nразработку продукта. Первоначально это рассматривалось как временная мера, но его\\nруководство оказалось настолько эффективным, что компания фактически так и не нашла\\nникого более подходящего для этой работы вплоть до его выхода на пенсию.\\nК 1995 году Ван ден Бринк вошел в состав правления и был назначен техническим\\nдиректором (все еще в основном сосредоточенным на разработке продуктов), должность,\\nкоторую он занимал до выхода на пенсию. Его считают гением, его вклад сыграл\\nрешающую роль не только в ASML, но и во всей полупроводниковой промышленности,\\nпостоянно раздвигая границы возможного. Параллельно с уходом на пенсию с поста\\nтехнического директора ASML Ван ден Бринк вошел в наблюдательный совет ASM.\\n', 'Мартин ван ден Бринк (слева) и Питер Веннинк (справа).\\nКультура инноваций\\nГоворя о ключевых фигурах, стоящих за успехом ASML, надо подчеркнуть ее уникальную\\nкорпоративную культуру. В отличие от многих организаций, где патенты подаются от имени\\nкомпании, ASML поощряет своих сотрудников подавать патенты от своего имени.\\nТакая практика способствовала формированию культуры гордости и собственности, где\\nинновации не только ожидаются, но и приветствуются (и фактически частично\\nпринадлежат самим людям).\\nОдним из самых символичных жестов этой культуры является традиция гравировки лиц\\nлучших новаторов года на кремниевой пластине, которая затем выставляется на видном\\nместе в штаб-квартире компании. Этот акт признания служит как мотивацией, так и\\nнапоминанием об индивидуальных вкладах, которые обеспечивают успех компании.\\nПриобретение, которое так и не произошло: как ASML осталась независимой\\nЕсли углубиться в историю, можно спросить, почему ASML до сих пор не куплена. Это\\nможет показаться глупым вопросом, как будто каждая великая компания в конечном итоге\\nдолжна быть приобретена. Однако, учитывая критическую роль ASML в бурно\\n', 'развивающейся отрасли, где доминируют несколько гигантов, на первый взгляд довольно\\nстранно, что ASML до сих пор остается независимой. Сегодня ответ, скорее всего, кажется\\nпростым и логичным, учитывая ее текущую рыночную стоимость - теперь это\\nсущественный барьер. Однако более интересной представляется совместная природа\\nсамой полупроводниковой промышленности.\\nУспех отрасли всегда зависел от разделения труда по различным нишам, в каждой\\nиз которых был свой набор экспертов. Эта специализация сделала замечательный\\nуспех ASML как независимой компании решающим для продвижения всех основных\\nпроизводителей полупроводников. Можно утверждать, что это своего рода базовое\\nсоглашение о том, чтобы «чесать друг другу спины» в полупроводниковом пространстве,\\nпоскольку многие из крупнейших игроков полагаются друг на друга для достижения\\nуспеха, а не конкурируют лицом к лицу.\\nПредставьте себе, если бы такая компания, как Samsung или TSMC, приобрела ASML\\nдесять лет назад. Совместные усилия, необходимые для финансирования и развития\\nтехнологии EUV-литографии ASML, возможно, никогда бы не были реализованы. Другие\\nлитейщики или разработчики чипов без фабрик, жизненно важные для развития отрасли,\\nтакже были бы менее склонны вносить свой вклад, если бы ASML принадлежала прямому\\nконкуренту. Интересно, что на рубеже тысячелетий американские компании Applied\\nMaterials и KLA - обе доминирующие в некоторых своих нишах производственного\\nоборудования - обращались к ASML с предложением о поглощении. Обе компании\\nполучили отказ, а независимость ASML позволила ей получить постоянную поддержку со\\nстороны всей отрасли, что в конечном итоге принесло пользу всем участникам.\\n_____\\n', '20\\nКарма\\n14\\nРейтинг\\n@TraPhro\\nQuant, AI\\nКогда-то небольшое, борющееся за выживание совместное предприятие, созданное Philips\\nи ASM, в которое мало кто верил, ASML использовала свои ограниченные ресурсы и\\nнеустанные инновации, чтобы стать единственным поставщиком самого передового в мире\\nоборудования для производства микросхем. Сегодня ASML не просто ключевой игрок, но и\\nкраеугольный камень мирового технологического ландшафта. С рыночной капитализацией,\\nпревышающей 300 миллиардов евро, она остается важнейшей опорой в цепочке создания\\nстоимости полупроводников, и ее путь расширения технологических границ, очевидно,\\nдалек от завершения.\\nХорошего дня! заходите на тг канал\\xa0https://t.me/TradPhronesis\\nТеги: AMSL, литография чипов, чипы, euv-литография, philips, tsmc, венчурные инвестиции\\n, ультрафиолет, intel, полупроводник\\nХабы: Читальный зал, Бизнес-модели, Инфографика, Развитие стартапа\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\nЭлектропочта\\nПодписаться\\n']\n",
      "ASML_ Создание самых сложных машин на Земле _ Хабр.pdf\n",
      "['ASML возникла в 1984 году как совместное предприятие Philips и ASM, и которое с самого', 'начала рассматривалась как провальный проект.\\xa0Но сегодня ASML имеет рыночную', 'капитализацию более 300 млрд евро и является одной из самых важных компаний для', 'технологического развития мира. Компании удалось создать, по сути, технологическую', 'монополию в самом передовом оборудовании для производства чипов —литографии в', 'экстремальном ультрафиолетовом диапазоне (EUV) - технологии, необходимой для', 'создания самых современных чипов, с которой не смогла справиться ни одна другая', 'компания в мире...', 'От аутсайдера-задиры до лидера литографии', 'В начале 1980-х годов небольшой городок Вельдховен тихо пожинал плоды своего соседа', 'Эйндховена — процветающего промышленного центра благодаря присутствию', 'голландского электронного гиганта Philips. Поскольку Philips продвигал технологические', 'инновации в регионе, его влияние распространялось за пределы Эйндховена, способствуя', 'волновому эффекту промышленной активности и технического опыта, которые достигли', 'Вельдховена.', 'С самого начала ASML был подразделением, которое многие в материнской компании', 'считали скорее обузой, чем возможностью. Фактически, это был проблемный ребенок', 'Philips — проект, который потратил более десятилетия на попытки проникнуть в', 'фотолитографический бизнес без особого успеха. В Philips шутили, что создание ASML', 'было по сути отложенным увольнением для переведенных в него сотрудников.', 'Несмотря на внутренний скептицизм, Philips увидела спасение в партнерстве с ASM', 'International , другой голландской компанией по производству оборудования для', 'полупроводников, которая сегодня занимает лидирующие позиции на рынке оборудования', 'для так называемого атомно-слоевого осаждения (ALD) и эпитаксии, используемых в', 'передовом производстве микросхем. В 1984 году ASML была создана как совместное', 'предприятие Philips и ASM со штаб-квартирой в Вельдховене. Поначалу компанию в', 'значительной степени считали неудачной, не имея ни коммерческой жизнеспособности, ни', 'даже надлежащего офиса. Ее первым рабочим пространством был набор деревянных', 'бараков в кампусе Philips в Эйндховене. Моральный дух был низким, многие сотрудники', 'TraPhro', '9 часов назад', 'ASML: Создание самых сложных машин на Земле', '17 мин', '2.5K', 'Читальный зал,\\xa0Бизнес-модели*,\\xa0Инфографика,\\xa0Развитие стартапа', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "['117.94\\nРейтинг\\nred_mad_robot\\n№1 в разработке цифровых решений для бизнеса\\n 6 часов назад\\nСредний18 мин\\nredmadrobot\\nCocoapods, Carthage, SPM: как выбрать\\nменеджер зависимостей в iOS\\n179\\nБлог компании red_mad_robot\\n, \\nРазработка под iOS*\\nFAQ\\nКАК СТАТЬ АВТОРОМ\\n', 'Старший iOS-разработчик red_mad_robot Аня Кочешкова\\nрассказывает, чем отличаются три менеджера зависимостей, в каких\\nслучаях и для каких задач подойдёт тот или иной. Материал будет\\nполезен джун-специалистам, которые только начали погружаться\\nв разработку: специально для них подробно объясняем, что такое\\nсемантическое версионирование, как устроены модули кода и в чём\\nразница между динамическими и статическими библиотеками.\\nЧто такое менеджер зависимостей и зачем он нужен\\nВ современной разработке зависимость — это написанный кем-то\\nдругим код, который используется в вашей программе. Добавление\\nзависимости позволяет воспользоваться уже сделанной кем-то\\nхорошей работой и избежать «изобретения велосипеда», а также\\nлишних действий: проектирования, написания, тестирования,\\nотладки и поддержки определённой логики. Такие «куски логики»\\nназывают пакетами, библиотеками или модулями.\\nЧем сложнее проект, тем больше сторонних библиотек (правда,\\nнекоторые приложения могут вообще отказываться\\nот их использования), а у этих библиотек много разных версий. Они\\nмогут ссылаться друг на друга и накладывать ограничения на эти\\nверсии.\\nЭту работу можно проиллюстрировать таким бытовым примером.\\nЧтобы телефон показал погоду на улице, не нужно встраивать\\nв него термометр и вытаскивать за окно. В нём уже зашита\\nспециальная технология, которая позволяет ему связаться\\nс «библиотекой», где хранятся данные гидрометцентра, которые,\\nкак и погода, всё время обновляются. Иногда с новой версией\\nдобавляется какая-нибудь новая функциональность. В примере\\nс термометром библиотека с новой версией умеет ещё\\nи выдавать карту дождей, а до этого могла показывать только\\n', 'температуру.\\nМенеджеры зависимостей позволяют избежать долгого и муторного\\nрешения такого набора «уравнений» — то есть берут на себя часть\\nработы, которую без них разработчик делал бы руками. Это система,\\nкоторая позволяет управлять зависимостями в вашем проекте.\\nА ещё у сторонних библиотек часто могут выходить новые версии\\nс исправленными ошибками, и благодаря менеджеру зависимостей\\nвам не нужно следить за их выходом — он всё обновит за вас.\\nПредположим, вы решили подключить в ваш проект Firebase\\nCrashlytics и GoogleUtilities. Обе эти библиотеки ссылаются\\nна библиотеку PromisesObjC. Firebase хочет, чтобы она была\\nлюбой версии, начинающейся с 2.1, а GoogleUtilities — чтобы она\\nбыла от 1.2 до 3.0. Такое уравнение, конечно, можно решить\\nвручную, но что делать, когда этих уравнений несколько\\nдесятков, а уровней зависимостей — два, три или больше?\\nДля таких задач в распоряжении iOS-разработчика существуют три\\nосновных менеджера зависимостей:\\n1. Cocoapods. Пожалуй, самый распространённый сегодня,\\nон требует лишь указывать список зависимостей\\nв определённом формате в текстовом файле.\\n2. Carthage. Менее удобен в этом плане: вы также указываете\\nсписок зависимостей в текстовом файле, но помимо этого нужно\\nещё немного «самостоятельной работы», о которой расскажем\\nниже.\\n3. SPM, или Swift Package Manager. Официальный менеджер\\nзависимостей от Apple. Появился недавно, и многие команды\\nпереходят на него с Cocoapods. Позволяет управлять\\nзависимостями как через графический, так и через текстовый\\nинтерфейс прямо из Xcode.\\nСлучается, что нужная библиотека не поддерживает\\nнеобходимый менеджер зависимостей, потому что разработчики\\n', 'каждой библиотеки сами выбирают, какой менеджер поддержать.\\nПоддержка каждого менеджера — это работа, которая занимает\\nопределённое время, поэтому часто выбирают работать только\\nс самым популярным менеджером, Сocoapods. А если в проекте,\\nнапример, SPM, а библиотека есть только в Cocoapods или\\nпросто нужна в одной конкретной версии, приходится добавлять\\nвручную как framework / xcframework, о чём мы также\\nпоговорим позже.\\nНо прежде чем начать говорить об управлении зависимостями,\\nнужно разобрать несколько связанных с ним терминов.\\nФантастическое Semantic Versioning, и где оно обитает\\nSemantic Versioning (SemVer) — это стандарт, который говорит о том,\\nкак работать с версиями пакетов/библиотек/приложений, — как\\nих указывать и накладывать на них ограничения.\\nЗачем накладывать ограничения? Возьмём такой пример.\\nРазработчик создаёт приложение и использует некоторый набор\\nбиблиотек для упрощения работы — например, библиотек для\\nработы с сетью Alamofire. Допустим, начиная с определённой\\nверсии в библиотеке появляется поддержка какой-то новой\\nфункциональности, например Modern Concurrency. Эта\\nфункциональность доступна только начиная с определенной\\nверсии библиотеки. В таком случае от разработчика потребуется\\nвсего лишь поднять версию Alamofire в манифесте, где\\nон описывает используемые библиотеки. Если начиная с какой-то\\nверсии библиотеки исправляется некий важный баг, но не сильно\\nменяется функциональность самой библиотеки, то обновление\\nдо свежей исправленной версии произойдёт без лишнего\\nвмешательства разработчика, в следующий раз при обновлении\\n', \"текущих библиотек проекта.\\nРазберём, что такое SemVer, чтобы понимать формат версий,\\nс которыми придётся работать, а также узнаем, как можно указать\\nверсию зависимости и наложить на неё ограничения.\\nЭтот набор правил — глобальный стандарт, который используется\\nповсеместно, начиная от пакетов Node.js и заканчивая версиями\\nприложений в App Store.\\nПо этому стандарту номер версии — это строка, которая состоит\\nиз чисел, разделённых точкой, и имеет формат A.B.C, где A —\\nмажорная версия, B — минорная версия и C — патч-версия. Версии\\nсравниваются слева направо, например: 1.0.0 < 2.0.0 < 2.1.0 < 2.1.1.\\nНо зачастую мажорная версия увеличивается или при выпуске каких-\\nлибо масштабных фич, или при редизайне приложения, или просто\\nпри желании обозначить принципиальное обновление\\nприложения/библиотеки.\\nСинтаксис SemVer в Cocoapods, Carthage и SPM\\nCocoapods\\nВ Cocoapods можно указать одну из стратегий выбора версии:\\n1. Если необходима конкретная версия, просто укажите её без\\nдополнительных символов, как есть: '5.0.0'.\\n\", \"2. Если необходима точная версия, но при этом вы также хотите\\nобновляться, если придут исправления багов в патч-версии,\\nиспользуйте операнд ~>: '~> 5.0.0'.\\n3. Можно указывать неравенства: '> 5.0.0', '≥ 5.0.0' или '< 5.0.0', '≤\\n5.0.0'.\\nВ этом случае Cocoapods обновит библиотеку до 5.0.1, 5.0.2 и т. д.,\\nно не возьмёт обновление 5.1.0, так как в нём уже повышена\\nминорная, а не патч-версия. Если вы также хотите завязаться\\nна минорную версию, достаточно просто указать '~> 5.0'.\\nCarthage\\nВ Carthage используется тот же формат, что и в Cocoapods, с тем\\nтолько исключением, что точная версия указывается через\\nравенство: == 5.0.0.\\nSPM\\nSPM позволяет указать стратегию выбора версии прямо\\nв интерфейсе, доступны следующие:\\nАналогичный синтаксис и у Package.swift файла.\\nМодули — что к чему: библиотеки, фреймворки,\\nXCFramework и Swift-пакеты\\nОрганизация кода в iOS построена на понятии модулей. Модуль —\\nэто отдельный кусок кода или функциональности, который можно\\nраспространять разными способами и переиспользовать.\\n\", 'Существует четыре вида модулей. Разберёмся, в чём разница:\\n1. Библиотека — это исполняемый код, скомпилированный под\\nконкретную архитектуру.\\n2. Фреймворк — папка, содержащая в себе библиотеку\\nи вспомогательные файлы.\\n3. XCFramework — набор фреймворков под разные архитектуры.\\n4. Swift-пакет — фреймворк, который используется для работы\\nсо Swift Package Manager.\\nЕсли говорить на языке бытовых примеров, то код — это\\nкомната, а модули — разные предметы в ней. Торшер, например,\\nпозволяет включать и выключать свет, но как он работает внутри,\\nмы не знаем. И этим торшером могут пользоваться в разных\\nквартирах и комнатах разные люди.\\nБиблиотеки\\nБиблиотека — это исполняемый код, скомпилированный под\\nконкретную архитектуру, например arm64.\\nАрхитектуры могут различаться для разных типов устройств\\nи симуляторов:\\n• iOS-устройства — архитектуры armv7, armv7s, arm64,\\n• iOS-симулятор — архитектуры i386 и x86_64.\\nИсходный код — текст программы, который написал\\nразработчик. Исполняемый код — код, который получился\\nв результате компиляции. Компилятор переводит исходный код\\nв машинный, который сможет исполнить операционная\\nсистема, — на выходе получается исполняемый код.\\n', 'Библиотеки бывают статическими и динамическими (об этом позже),\\nи представляют собой один бинарный файл.\\nЕсли представить, что библиотека — это настоящая библиотека\\nсо стеллажами, в которых хранятся книги, то эти книги —\\nсправочники о том, как что-то сделать (методы для выполнения\\nразных задач). По сути, это скомпилированный набор объектных\\nфайлов, которые программа может использовать для выполнения\\nлюбой задачи. То есть все файлы, которые библиотека собирается\\nиспользовать, были скомпилированы до этапа компоновки.\\nФреймворки\\nФреймворк (.framework) представляет собой папку с файлами,\\nв которой лежит сама библиотека, хедеры и ресурсы. Эта папка\\nдолжна иметь определённую структуру, но нет нужды запоминать её,\\nтак как фреймворки, как правило, собираются самим Xcode.\\nПоддержка фреймворков добавилась в iOS 8.\\nФреймворки служат той же цели, что и библиотеки, но могут\\nвключать в себя различные ресурсы — картинки, документацию,\\nстроковые файлы.\\nОни также могут быть статическими или динамическими.\\n', 'Что использовать: фреймворк или библиотеку\\nВ чём же разница между фреймворком и библиотекой? И когда\\nих использовать? Принципиальное различие — в архитектуре.\\nXCFrameworks\\nДля начала стоит рассказать, что такое fat-фреймворки. Это такие\\nфреймворки, которые содержат библиотеки для разных архитектур\\nи платформ, соединённые в одну. С появлением Xcode 11 на замену\\nим пришли XCFrameworks, которые ввели структуру, позволяющую\\nразнести библиотеки для разных платформ и архитектур по папкам.\\nXCFramework — это, по сути, такой усовершенствованный fat-\\nфреймворк. Он также содержит различные библиотеки для разных\\nархитектур и платформ, но теперь нет необходимости трудиться над\\nтем, чтобы объединить их в одну. В одном XCFramework может\\nлежать несколько разных фреймворков — для iOS, macOS, tvOS,\\nwatchOS, а также для разных архитектур — armv7, x86_64 и т. д.,\\nкаждый в своей папке.\\nВ каком-то смысле фреймворк — тоже разновидность\\nбиблиотеки, а XCFramework — разновидность фреймворка,\\nно всё же эти понятия принято разделять. Создатель\\nзависимости сам выбирает, в каком виде её распространять. Нам\\nважно понимать различие между этими видами, выбор можно\\nсделать, только создавая свою библиотеку.\\n', 'Swift Package\\nSwift-пакет используется для распространения только через SPM.\\nСодержит исходный код, а также Package.swift файл, который\\nописывает его конфигурацию. Доступен начиная со Swift 3.\\nStatic vs Dynamic\\nПрежде чем начать, наконец, говорить об управлении\\nзависимостями, важно разобраться, в чём различие между\\nстатическими и динамическими библиотеками и фреймворками.\\nПервое, что здесь важно понимать: системные iOS- и macOS-\\nбиблиотеки — динамические. Приложения могут хранить ссылку\\nна библиотеку. Например, системная библиотека Foundation —\\nдинамическая. Она лежит на айфоне в единственном экземпляре,\\nа все приложения лишь ссылаются на неё.\\nФреймворки, как и библиотеки, бывают динамические и статические\\nи представляют собой обёртку библиотеки. Так, динамические\\nбиблиотеки вне фреймворков не поддерживаются на iOS, watchOS\\nи tvOS, и здесь важно отметить, что во всех дальнейших\\nсравнениях, говоря «динамическая библиотека», мы будем\\nиметь в виду её использование в рамках динамического\\nфреймворка.\\n', \"Фреймворки в Cocoapods и Carthage\\nЗависимости, распространяемые через Cocoapods, называются\\nкокоаподами, или подами. До iOS 8 под представлял собой fat-\\nстатическую библиотеку.\\nЕсть специальная инструкция — по ней Cocoapods использует\\nфреймворки вместо статичных библиотек, которые\\nне поддерживались Swift до Xcode 9 и CocoaPods 1.5.0.\\nТак что раньше эта директива была обязательна, сейчас —\\nопциональна. Если нужно более быстрое время запуска, можно\\nеё убрать (см. предыдущий раздел).\\nCarthage поддерживает только фреймворки.\\ntarget 'TargetName' do\\n  use_frameworks!\\nend\\n\", 'Три менеджера зависимостей: Cocoapods, Carthage,\\nSPM\\nНаконец, то, ради чего мы тут собрались, — менеджеры\\nзависимостей. Посмотрим, какие они бывают, чем отличаются и как\\nвыбрать между ними.\\nCocoapods\\nЭто один из самых популярных сегодня менеджеров зависимостей.\\nПозволяет указать список зависимостей в текстовом файле, а затем\\nгенерирует на основе его .xcworkspace, который будет содержать\\nдва проекта — ваш проект и проект с подами.\\nКак правило, почти все библиотеки поддерживают Cocoapods,\\nхотя с развитием SPM стали появляться и такие, которые\\nподдерживают исключительно SPM (в основном это библиотеки\\nApple). В таком случае, если ваш менеджер зависимостей\\nCocoapods, единственный вариант воспользоваться\\nбиблиотекой — вручную её собрать и прилинковать к проекту.\\nЧтобы понять, поддерживает ли библиотека Cocoapods, достаточно\\nзаглянуть в readme, либо обратить внимание на наличие .podspec-\\nфайла в корне репозитория.\\nВсе библиотеки, опубликованные в Cocoapods, можно посмотреть\\nна их официальном сайте.\\nCocoapods в вашем проекте\\nДля понимания работы Cocoapods важны два термина — подфайл\\nи спека. Подфайл описывает все зависимости приложения,\\nа спека — библиотеку (её название и набор её зависимостей) для\\nCocoapods. Спеки хранятся в общем репозитории спек Cocoapods.\\nЕщё их можно хранить в приватных репозиториях.\\n', \"Для описания конфигурации зависимостей используется подфайл.\\nЭтот файл написан на языке Ruby.\\n1. В файле опционально укажите источник спек. Источников может\\nбыть несколько, даже приватные.\\nsource 'https://github.com/CocoaPods/Specs.git' —\\nофициальный источник спек, который указан по умолчанию.\\nsource 'https://github.com/Artsy/Specs.gi — можно указать свои\\nисточники спек, в том числе приватные.\\n2. Укажите минимальную версию iOS: platform: ios, '9.0'.\\n3. Укажите таргет и набор зависимостей для него. Если таргетов\\nнесколько, укажите все, так как зависимости для них могут\\nразличаться.\\n4. Декларируйте зависимость. Первым идёт её официальное\\nназвание (то самое, которое указано в спеке), затем версия.\\nИспользуйте в названии правила из блока про SemVer.\\nЕсть и другие способы указать зависимости.\\nЭто может понадобиться в случаях, когда нужно использовать\\nкакую-то конкретную версию библиотеки. Например,\\nиз определённой ветки репозитория, где она хранится, или даже\\nиз определённого коммита. Либо указать путь локально, если\\nесть только её файл.\\n    target 'MyApp' do\\n    pod 'GoogleAnalytics', '~> 3.1'\\n    ...\\n    end\\n\", \"1. Можно указать ресурс репозитория, с которого будем загружать\\nбиблиотеку:\\npod 'CRToast', :git =>\\n'https://github.com/akhatmullin/CRToast.git'\\n2. Можно указать нужную ветку в репозитории:\\npod 'CRToast', :git =>\\n'https://github.com/akhatmullin/CRToast.git', :branch => 'dev'\\n3. Или тег:\\npod 'CRToast', :git =>\\n'https://github.com/akhatmullin/CRToast.git', :tag => '0.7.0'\\n4. Или коммит:\\npod 'CRToast', :git =>\\n'https://github.com/akhatmullin/CRToast.git', :commit =>\\n'082f8310af'\\n5. Можно указать на локальный под (путь должен вести на папку,\\nгде лежит .podspec):\\npod 'AFNetworking', :path => '~/Documents/AFNetworking'\\nРядом с подфайлом обычно лежит Podfile.lock. Этот файл\\nописывает конечные версии зависимостей после их установки.\\nОн обычно так же заливается в GitHub, как и сам Podfile, чтобы\\nизбежать ошибок и рассинхрона при установке зависимостей\\nу разных разработчиков. Ещё он ускоряет выполнение pod install.\\nЧтобы приступить к работе, запустите pod install. Можно выполнить\\npod update, если вы хотите обновить все библиотеки в проекте,\\nиначе же будут использоваться фиксированные версии\\nзависимостей из Podfile.lock.\\nКак Cocoapods работает под капотом\\nPod install генерирует workspace — файл .xcworkspace, состоящий\\nиз двух проектов — проекта с вашим приложением и отдельного\\nпроекта, в котором хранятся зависимости (поды).\\n\", 'Чтобы всё работало корректно, всегда открывайте workspace-файл.\\nЕсли внимательнее посмотреть, что происходит, можно увидеть, что\\nпродукт этого проекта линкуется в основной проект:\\nТакже Cocoapods добавляет пару скриптов в Build Phases:\\nПоддержка Cocoapods в вашей библиотеке\\n', 'Чтобы библиотека была доступна через Cocoapods, нужно сначала\\nописать библиотеку или под при помощи спеки, собрать\\nеё и отправить спеку в приватный либо глобальный репозиторий\\nCocoapods.\\nРазберёмся, как это сделать.\\n1. Создайте спеку и файл лицензии.\\nЛицензию нужно добавлять для любой библиотеки, и чаще всего\\nиспользуется лицензия MIT — самая популярная лицензия для\\nпрограмм с открытым исходным кодом. Её текст можно найти\\nв интернете. Здесь нужно описать вашу библиотеку. Файл принято\\nформатировать так, чтобы вторая часть строки была выровнена.\\n    Pod::Spec.new do |spec|\\n    # Название библиотеки. Именно его будут указывать разраб\\n    spec.name                    = \\'NAME\\' \\n    # Версия\\n    spec.version                 = \\'1.2.1\\' \\n    # Описание\\n    spec.summary                 = \\'Describe your framework.\\n    # Здесь можно указать путь на вебсайт или просто GitHub-\\n    spec.homepage                = \\'https://xxx\\' \\n    # Путь к лицензии. Да, её тоже нужно создать\\n    spec.license                 = { type: \\'MIT\\', file: \\'LIC\\n    # Автор библиотеки\\n    spec.author                  = { \"Your Name\" => \\'your-em\\n    # Путь к исходным файлам (чаще всего это ваш репозитоий)\\n    spec.source                  = { :git => \\'https://github\\n    # Минимальная поддерживаемая версия iOS и Swift\\n    spec.ios.deployment_target   = \\'13.0\\' \\n    spec.swift_version           = \\'5.0\\'\\n    # Если библиотека использует локальный фреймворк, его ну\\n    spec.vendored_frameworks     = \\'Path/To/Files/NameOfVend\\n', '2. Проверьте, что всё корректно.\\nСледующая команда валидирует спеку — проверяет, что она\\nне имеет ошибок, а проект билдится — pod lib lint.\\n3. Отправьте спеку в репозиторий спек.\\nКак правило, это делается уже на финальном этапе разработки —\\nпосле того, как прошла проверка и вы готовы публиковать работу, —\\npod trunk push NAME.podspec.\\nCarthage\\n    # Указываем вайлдкард-пути к файлам, которые необходимо \\n    spec.source_files            = \\'Path/To/Files/**/*.{swif\\n    # Путь к ресурсам\\n    spec.resources               = \\'Path/To/Files/Resources/\\n    # Укажите здесь все ассеты, которые вам нужны, включая .\\n    spec.resource_bundles        = {\\'BundleName\\' => [\\'Path/T\\n    # Указываем зависимости\\n    spec.dependency           \"Alamofire\", \"~> 5.0.0\" \\n    # И системные зависимости\\n    spec.frameworks           = \\'Foundation\\' \\n    spec.ios.frameworks       = \\'UIKit\\'\\n    end\\n', 'Carthagе — это децентрализованный менеджер зависимостей,\\nкоторые не нужно «поддерживать» разработчику. Carthage сам\\nвыкачивает библиотеку из репозитория, компилирует\\nеё и предоставляет в готовом виде. Поэтому, если хотите\\nиспользовать Carthage, не нужно искать в репозитории\\nс библиотекой определённые инструкции или слова о том, что\\nон поддерживается, — просто попробуйте.\\nНет у Carthage и централизованного источника, как у Cocoapods.\\nЭтим источником, по сути, является сам GitHub.\\nКонечно, этого не всегда бывает достаточно. Некоторые\\nбиблиотеки Carthage просто не может «переварить» — что-то\\nне собирается, а что-то отваливается. Задача разработчика —\\nпроверить, всё ли работает корректно.\\nНаконец, Carthage может автоматически подхватить уже собранный\\nфреймворк, если он прикреплён к соответствующему релизу\\nна GitHub.\\nCarthage в вашем проекте\\nЗависимости в Carthage описываются в текстовом файле Cartfile.\\nОн очень похож на Podfile, только гораздо проще по структуре.\\nМы указываем источник, имя зависимости и версию.\\n1. Источник зависимости. Как правило, GitHub, либо git для\\nуниверсальных репозиториев Git, размещённых в другом месте.\\nКлючевое слово git сопровождается путём к репозиторию, будь\\nто удалённый URL-адрес, используя git://, http://, или ssh://, или\\nлокальный — путём к репозиторию git на компьютере\\nразработчика.\\n2. Название. Через слеш указывается владелец и название\\nрепозитория, которые можно подсмотреть в ссылке\\nна репозиторий.\\n', '3. Версия. Особенности указания версий мы уже рассмотрели\\nв блоке про семантическое версионирование, но можно также\\nуказывать и ветки.\\nПримеры:\\n1. Версия — github \"Alamofire/Alamofire\" == 2.0\\n2. Ветка — github \"username/project\" \"branch\"\\n3. Локальный проект — git \"file:///directory/to/project\" \"branch\"\\n4. Бинарник (путь указывается в json) — binary\\n\"https://my.domain.com/release/MyFramework.json\" ~> 2.3\\nПосле описания зависимостей потребуется ещё несколько\\nдополнительных шагов.\\n1. Вызовите carthage update. Carthage склонирует репозитории\\nс указанных в Cartfile путей и затем сбилдит для каждой\\nзависимости фреймворк. После окончания работы вы увидите\\nнесколько файлов и каталогов:\\n2. Полученные фреймворки затем вручную перетащите из папки\\nBuild в проект в секцию Frameworks, Libraries и Embedded\\nContent.\\n3. Carthage требует добавления нового скрипта в Build Phases\\nв качестве workaround бага Apple. Это позволяет dSYM-\\nфайлам корректно подтягиваться при архивации.\\n', '4. Перейдите в Build Phases и добавьте новый Run Script.\\nДобавьте следующую команду: /usr/local/bin/carthage copy-\\nframeworks.\\n5. Нажмите на плюс под Input Files и добавьте запись для каждой\\nплатформы:\\n$(SRCROOT)/Carthage/Build/iOS/Alamofire.framework.\\nКак Carthage работает под капотом\\nКак и в случае с Cocoapods, создаётся лок-файл Cartfile.resolved,\\nкоторый фиксирует конечные версии зависимостей.\\nКаталог Carthage содержит два подкаталога:\\n1. Build. Содержит собранный фреймворк для каждой\\nзависимости.\\n2. Checkouts. Содержит клонированные исходники, которые\\nCarthage потом компилирует. Это своего рода кэш, который\\nускоряет работу.\\nПоддержка Carthage в вашей библиотеке\\nДля поддержки Carthage достаточно сделать релиз на GitHub.\\nНо никогда не помешает проверить, всё ли билдится корректно\\nв вашем случае и, если есть какие-то проблемы, решить их.\\nДля этого, выполните carthage build --no-skip-current. Carthage\\nпопытается сбилдить ваш проект и создаст в нём папку Carthage.\\nТакже стоит обратить внимание, что он видит только те схемы,\\nкоторые помечены в проекте как Shared.\\n', 'SPM\\nИли Swift Package Manager — официальный менеджер\\nзависимостей от Apple.\\nРабота с ним происходит прямо в Xcode, а .Xcodeproj файлы\\nбольше не нужны (хотя при желании есть возможность\\nсгенерировать файл проекта из SPM-пакета). Если заглянуть\\nв репозиторий библиотеки, то первое, что будет говорить\\nо поддержке SPM, — это наличие Package.swift-файла.\\nSPM в вашем проекте\\nДобавление зависимостей в проект происходит в секции Package\\nDependencies.\\n1. Для начала выберите File > Swift Packages > Add package\\ndependecy.\\n2. Найдите модуль по поиску, либо введите URL репозитория.\\n3. Укажите версию:\\n', 'Как и во всех предыдущих случаях, SPM создает Package.resolved,\\nназначение которого то же, что и у других менеджеров зависимостей.\\nВ меню File > Swift Packages доступно несколько действий:\\n• Reset Package Caches удаляет кэши из Derived Data.\\n• Resolve Package Versions резолвит версии зависимостей.\\nСоздаёт Package.resolved, если его не существует, но не меняет\\nего, если он уже есть. Может потребоваться, если\\nPackage.resolved был, например, обновлён извне.\\n• Update to Latest Packages Versions обновляет версии пакетов,\\nможет модифицировать Package.resolved.\\nПоддержка SPM в вашей библиотеке\\nЕсли вы разрабатываете библиотеку и хотите распространять\\nеё через SPM, нужно будет создать Package.swift файл. Таким\\nобразом может быть сконфигурирован и обычный проект, что\\nпозволяет отойти от использования project-файлов. Конфигурация\\n', 'через SPM выглядит чище и более проста для понимания\\nи редактирования, чем стандартные project-файлы, которые\\nзачастую нечитаемы.\\nЕсли вы пишете библиотеку и хотите распространять её через\\nSPM, обязательно убедитесь, что потенциальные потребители\\nбиблиотеки не используют Cocoapods. Одновременная\\nподдержка Cocoapods и SPM — часто непростая задача.\\nimport PackageDescription\\n \\nlet package = Package(\\n    // Название нашего пакета\\n    name: \"Resources\",\\n    // Платформы, которые поддерживаются нашим пакетом\\n    platforms: [\\n        .iOS(.v11),\\n    ],\\n    // То, что будут использовать сторонние приложения\\n    products: [\\n        .library(\\n            name: \"Resources\",\\n            // Динамический или статический продукт\\n            // по дефолту значение nil - SPM сам будет поним\\n            // преференция, скорее всего, будет отдаваться .\\n            type: .dynamic,\\n            targets: [\"Resources\"]),\\n    ],\\n        // Зависимости, необходимые для работы нашего пакета\\n  // здесь они просто загружаются, добавляются они в target\\n    dependencies: [\\n        // Название пакета, путь к нему и источник (ветка, в\\n        .package(\\n            name: \"R.swift.Library\", \\n            url: \"https://github.com/mac-cain13/R.swift.Libr\\n            branch: \"master\"),\\n        .package(\\n            name: \"SVGKit\",\\n            url: \"https://github.com/SVGKit/SVGKit.git\",\\n', 'И как выбрать подходящий менеджер\\n            .upToNextMajor(from: \"3.0.0\")),\\n        .package(\\n            name: \"PhoneNumberKit\",\\n            url: \"https://github.com/marmelroy/PhoneNumberKi\\n            from: \"3.3.4\"),\\n        // Пример подключения локального пакета\\n        .package(path: \"../Core\")\\n    ],\\n    targets: [\\n        // Это то, из чего мы будем складывать наш продукт\\n        .target(\\n            name: \"Resources\",\\n            dependencies: [\\n                // Здесь мы указываем зависимости, которые м\\n                .product(name: \"RswiftDynamic\", package: \"R.\\n            ],\\n            resources: [\\n                // Все ресурсы, которые мы хотим использоват\\n                // Путь к ним относительный от Sources/имя_п\\n                .process(\"Resources\")\\n            ])\\n    ]\\n)\\n', 'Универсального ответа нет — выбор менеджера зависимостей\\nзависит исключительно от нужд и особенностей вашего проекта.\\nSPM чаще всего не будут поддерживать старые библиотеки,\\nв то время как Cocoapods вполне могут не поддерживать новые\\n(хотя это и большая редкость, чем первое). Carthage в этом плане\\nможет показаться универсальнее. Хоть он более громоздкий\\nи сложный во взаимодействии, но он единственный разделяет шаги\\nуправления зависимостями и добавления их в проект. Это даёт\\nконтроль над ситуацией и ускоряет разработку — не будет лишних\\nпересборок или заморозки интерфейса, потому что нужно\\nзарезолвить зависимости.\\nНад материалом работали:\\n• текст — Аня Кочешкова, Ника Черникова,\\n• редактура — Виталик Балашов,\\n• иллюстрации — Юля Ефимова.\\nДелимся железной экспертизой от практик в нашем телеграм-канале\\nred_mad_dev. А полезные видео складываем на одноимённом\\nYouTube-канале. Присоединяйся!\\nТеги:  \\nred_mad_robot\\n,\\nios\\n,\\ncocoapods\\n,\\ncarthage\\n,\\nspm\\n,\\nswift package manager\\n,\\nзависимости\\n,\\nуправление зависимостями\\nХабы:  \\nБлог компании red_mad_robot\\n,\\nРазработка под iOS\\n+1\\n3\\n1\\n', 'red_mad_robot\\n№1 в разработке цифровых решений для бизнеса\\n57\\nКарма\\n13\\nРейтинг\\n@redmadrobot\\nПользователь\\nКомментарии 1\\nПубликации\\nЛУЧШИЕ ЗА СУТКИ \\nПОХОЖИЕ\\n', ' · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n', 'Ваш\\nаккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nИНФОРМАЦИЯ\\nСайт\\nredmadrobot.ru\\nДата регистрации\\n16 августа 2009\\nДата основания\\n16 ноября 2008\\nЧисленность\\n1 001–5 000 человек\\nМестоположение\\nРоссия\\n', 'Настройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "Cocoapods, Carthage, SPM как выбрать менеджер зависимостей в iOS.pdf\n",
      "['117.94', 'Рейтинг', 'red_mad_robot', '№1 в разработке цифровых решений для бизнеса', ' 6 часов назад', 'Средний18 мин', 'redmadrobot', 'Cocoapods, Carthage, SPM: как выбрать', 'менеджер зависимостей в iOS', '179', 'Блог компании red_mad_robot', ', ', 'Разработка под iOS*', 'FAQ', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "1 117.94 red_mad_robot №1 в разработке цифровых решений для бизнеса  6 часов назад\n",
      "['Еще до начала MWC в Барселоне было очевидно, что хотя бы один оператор представит\\nамбициозный проект по созданию смартфона совместно с ведущей AI-компанией. И вот он:\\nDeutsche Telekom (DT) сообщает, что готовит «AI Phone» — недорогой аппарат, разработанный в\\nтесном сотрудничестве с Perplexity, а также Picsart и другими партнерами, и дополняет его новым\\nприложением-ассистентом под названием «Magenta AI».\\nDT планирует представить устройство во второй половине этого года, а продавать его начнет в\\n2026-м по цене менее 1 000 долларов. Как уточнил представитель компании, в первую очередь\\nмодель будет ориентирована на европейский рынок.\\ntechnokratiya 1 час назад\\nDeutsche Telekom и Perplexity объявили о новом «AI Phone»\\nстоимостью менее 1 000 долларов\\n4 мин\\n419\\nИскусственный интеллект,\\xa0Смартфоны\\nКАК СТАТЬ АВТОРОМ\\nЗарплаты айтишников\\nНайдено: весенние мероприятия для айти…\\n', '«Мы становимся AI-компанией», — заявила на пресс-конференции в понедельник член\\nправления DT, отвечающая за технологии и инновации, Клаудия Немат. При этом она добавила,\\nчто оператор не создает фундаментальные большие языковые модели, а разрабатывает «AI-\\nагентов».\\nПримечательно, что Perplexity, стартап из Кремниевой долины, который, по слухам, уже\\nоценивается примерно в 9 миллиардов долларов, отводится ключевая роль в создании\\nсмартфона. Это показывает, что компания, прежде всего известная благодаря генеративному AI-\\nпоисковику, делает шаг к созданию более «проактивных» решений.\\n«Perplexity переходит от простой “машины ответов” к “машине действий”, — отметил\\nсооснователь и генеральный директор Perplexity Аравинд Шринивас на сцене мероприятия. —\\nТеперь мы не просто отвечаем на вопросы, но и можем бронировать рейсы, столики в\\nресторанах, отправлять письма, сообщения, совершать звонки и многое другое, включая\\nустановку умных напоминаний».\\nХотя это, по всей видимости, первая крупная сделка Perplexity с оператором связи для\\nразработки AI-интерфейса смартфона, компания уже имеет некоторый опыт в сфере\\nпомощников: в январе Perplexity выпустила Android-приложение, которое, судя по всему, может\\nслужить прообразом нового «AI Phone».\\nЭта новость — очередная глава знакомой истории из мира телекоммуникаций. Долгие годы как\\nмобильные, так и проводные операторы искали пути усилить позиции в конкурентной борьбе с\\nтехнологическими гигантами.\\nВ частности, они пытались соперничать с Apple и Google, чьи операционные системы и\\nсмартфоны в значительной степени отодвигают операторов на периферию, лишая их\\nсущественной доли прибыли от приложений и прямого взаимодействия с пользователями. За эти\\nВойти\\nМоя\\nлента\\nВсе\\nпотоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп\\n', 'годы были попытки объединиться, к примеру, с Mozilla для создания «операторского» смартфона,\\nкоторый смог бы конкурировать с этими двумя компаниями (телефон под названием Firephone,\\nвпрочем, так и не нашел отклика). Были и «теплые» отношения с Facebook, стремившейся\\nукрепить собственные позиции в мобильном мире (ныне Meta переключила усилия на другие\\nнаправления в сфере оборудования и сетей).\\nПодход «двигаться быстро и ломать все на своем пути» не слишком близок консервативным\\nтелекомам. Perplexity и Deutsche Telekom сотрудничают уже с апреля 2024 года, а впервые о «AI\\nPhone» DT заговорила год назад на предыдущем Mobile World Congress.\\nНемат не раскрыла подробностей о технических характеристиках и производителе нового\\nдевайса, а также не уточнила, на какой операционной системе он будет работать (на\\nпрезентационных рендерах можно заметить явные черты Android). Представитель DT сообщил,\\nчто эта информация появится во второй половине года.\\nИзвестно лишь, что в смартфон будет глубоко интегрирован AI, созданный с участием Perplexity,\\nчтобы, по словам Немат, «пользователь получил весь спектр возможностей». Она подчеркнула,\\nчто AI будет работать прямо с экрана блокировки.\\nКроме Perplexity, в создании телефона принимают участие Google Cloud, ElevenLabs и Picsart,\\nсообщили в DT.\\nПриложение Magenta AI, являющееся самостоятельной версией голосового помощника от DT,\\nбудет доступно для установки на любые Android- и iOS-устройства, но только для 300 миллионов\\nклиентов оператора, добавила Немат.\\nСтремясь использовать нынешний всеобщий интерес к искусственному интеллекту — главную\\nтему MWC этого года, — DT в очередной раз пытается закрепиться в сфере «якорного»\\nоборудования, предлагая и приложение для тех, кто не хочет или не может купить фирменный\\nсмартфон.\\nPerplexity же конкурирует не только с хорошо финансируемыми OpenAI и Anthropic, но и с такими\\nгигантами, как Google, уже внедрившей свою модель Gemini во все основные продукты поиска.\\nВыход на рынок «машин действий» в сотрудничестве с крупным оператором связи дает компании\\nопределенное преимущество, пусть и временное.\\nТеперь Perplexity, похоже, переходит к следующему этапу эволюции, чтобы предоставить\\nпользователям еще более удобные AI-инструменты.\\n«Раньше для решения этих задач нужно было по отдельности осваивать разные приложения, —\\nговорит Аравинд Шринивас. — Теперь все станет проще, и вы сможете сосредоточиться на более\\nважных делах. Это действительно следующий виток развития, где AI перестанет быть чисто\\nреактивным и превратится в неотъемлемую функцию вашего смартфона, которая всегда на связи\\nи готова помогать проактивно».\\nПока не ясно, смогут ли DT и Perplexity добиться успеха в сложном рынке смартфонов, где\\nдоминируют считаные игроки, а крупные корпорации вроде LG в итоге полностью ушли из этой\\nсферы.\\n', '17\\nКарма\\n4.4\\nРейтинг\\nТехнократия @technokratiya\\nИТ-Компания\\nСайт\\nСайт\\nТем не менее это свидетельствует о том, насколько силен «притягательный эффект» AI сегодня,\\nкак даже давние игроки видят в нем спасительное решение, а передовые стартапы —\\nвозможность укрепить свои позиции на фоне жесткой конкуренции.\\nИсточник: TechCrunch\\nЧтобы не пропустить анонс новых материалов подпишитесь на\\xa0«Голос Технократии»\\xa0— мы\\nрегулярно рассказываем о новостях про AI, LLM и RAG, а также делимся полезными\\nмастридами и актуальными событиями.\\nТеги: смартфоны, ии, искусственный интеллект, мобильные телефоны, ai, artificial intelligence, \\nperplexity, deutsche telekom\\nХабы: Искусственный интеллект, Смартфоны\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\nЭлектропочта\\nПодписаться\\n']\n",
      "Deutsche Telekom и Perplexity объявили о новом «AI Phone» стоимостью менее 1 000 долларов _ Хабр.pdf\n",
      "['Еще до начала MWC в Барселоне было очевидно, что хотя бы один оператор представит', 'амбициозный проект по созданию смартфона совместно с ведущей AI-компанией. И вот он:', 'Deutsche Telekom (DT) сообщает, что готовит «AI Phone» — недорогой аппарат, разработанный в', 'тесном сотрудничестве с Perplexity, а также Picsart и другими партнерами, и дополняет его новым', 'приложением-ассистентом под названием «Magenta AI».', 'DT планирует представить устройство во второй половине этого года, а продавать его начнет в', '2026-м по цене менее 1 000 долларов. Как уточнил представитель компании, в первую очередь', 'модель будет ориентирована на европейский рынок.', 'technokratiya 1 час назад', 'Deutsche Telekom и Perplexity объявили о новом «AI Phone»', 'стоимостью менее 1 000 долларов', '4 мин', '419', 'Искусственный интеллект,\\xa0Смартфоны', 'КАК СТАТЬ АВТОРОМ', 'Зарплаты айтишников', 'Найдено: весенние мероприятия для айти…', '']\n",
      "['71.07\\nРейтинг\\nCloud4Y\\n#1 Корпоративный облачный провайдер\\nАвтор оригинала: Yashwant Singh\\nВзлом Instagram*‑аккаунта — популярный запрос в поисковиках. Поэтому есть смысл рассказать\\nо том, как это обычно работает. Просто для того, чтобы вы знали, откуда может пойти атака.\\n Cloud4Y 5 часов назад\\nOSINT & Hacking — как работает фишинг для нельзяграма\\n Простой\\n 3 мин\\n 1.1K\\nБлог компании Cloud4Y , Информационная безопасность *, Социальные сети и сообщества\\nОбзор\\n Перевод\\nКАК СТАТЬ АВТОРОМ\\n', 'Чтобы начать попытки заполучить доступ к аккаунту, вы должны знать ник человека, которого вы\\nпытаетесь взломать. Так что небольшая разведка будет очень кстати. Только не увлекайтесь.\\nСуществуют различные инструменты для разведки, в первую очередь, поиск пользователя\\nв конкретной соцсети с целью узнать его ник. Я нашёл отличный инструмент под названием\\n«Slash», который можно использовать для поиска любых учётных записей пользователя, если он\\nвезде регистрируется под одним ником.\\nСтавим Slash\\n', 'git clone https://github.com/theahmadov/slash \\ncd slash \\npip install -r requirements.txt \\npython slash.py help\\nЯ проверил Slash на себе, и посмотрите на эти результаты. Некоторые из учетных записей,\\nперечисленных здесь, были созданы много лет назад.\\n', 'Slash — это простой консольный инструмент. Но вы также можете использовать такие\\nинструменты, как WhatsMyName Web, который совершенно бесплатен.\\nВот, посмотрите. Я проверил WhatsMyName на себе. Мой ник «earthtoyash».\\n', 'Теперь, когда мы знаем больше о нашем пользователе, можно использовать эти знания.\\n', 'Например, через отправку фишинговых ссылок. Для этого создадим полезную нагрузку с помощью\\nZphisher.\\nСтавим Zphisher с GitHub\\nКлонируем репозиторий:\\ngit clone --depth=1 https://github.com/htr-tech/zphisher.git\\nЗапускаем файл zphisher.sh:\\ncd zphisher && ./zphisher.sh\\nПри первом запуске он установит зависимости и на этом всё. Система скажет, что Zphisher\\nустановлен. После установки вам нужно будет снова запустить zphisher.sh в каталоге zphisher\\nкомандой ./zphisher , и тогда вы получите что-то вроде этого:\\n', 'Как вы можете видеть, есть много вариантов и шаблонов, благодаря чему любой может\\nзаниматься фишингом. Мы сосредоточимся на нельзяграме. Итак, введите «2» и нажмите Enter.\\n', 'Следующий шаг полностью зависит от вас, выберите любой из них.\\nЗатем появится окно с выбором. Я выбрал третий вариант, так как он минималистичный и удобен\\nдля того, чтобы показать возможности инструмента.\\n', 'Опять же, чтобы все было просто, я пропущу пользовательский порт, но если вы уже используете\\nпорт 8080, то можете изменить его на 8000. Если нет, оставляйте всё как есть. Также важно\\nмаскировать URL, ну просто в целях безопасности. Можно использовать что‑то вроде этого:\\n', 'Всё, Zphisher создал фишинговую ссылку, которую можно отправить жертве. Как только она\\nнажмёт на ссылку, вы начнёте получать информацию о ней. Например, IP‑адреса, имена\\nпользователей, пароли и т. д. Ещё можно использовать обратный поиск IP, чтобы определить\\nместоположение вашей цели и многое другое.\\nИтак, вот эти фишинговые ссылки.\\n', 'При нажатии открывается страница, похожая на официальную страницу входа в запрещённую\\nсоцсеть.\\n', 'Вот она, нехорошая\\nПосле ввода учётных данных можно получить много информации на «хакерской» стороне\\nтерминала.\\n', 'Вот так, господа и дамы, можно без особого труда взломать учетную запись в нельзяграме.\\nПоэтому в очередной раз напоминаем: нельзя нажимать на ссылки, которым вы не доверяете.\\nСамо собой разумеется, не используйте информацию из этой статьи с намерением причинить\\nкому-либо вред. OSINT законен, но фишинг и кража личных данных даже в запрещённой соцсети\\nявляется уголовным преступлением. И да, * Организация Meta, а также её продукт Instagram, на\\nкоторые мы ссылаемся в этой статье, признаны экстремистскими и запрещены на территории РФ.\\nСпасибо за внимание!\\n', 'Cloud4Y\\n#1 Корпоративный облачный провайдер\\nСайт\\nFacebook\\nTwitter\\nВКонтакте\\nTelegram\\nЧто ещё интересного есть в блоге Cloud4Y\\n→ Информационная безопасность и глупость: необычные примеры\\n→ NAS за шапку сухарей\\n→ Взлом Hyundai Tucson, часть 1, часть 2\\n→ Столетний язык программирования — какой он\\n→ 50 самых интересных клавиатур из частной коллекции\\nТеги:  взлом, соцсети, osint\\nХабы:  Блог компании Cloud4Y, Информационная безопасность, \\nСоциальные сети и\\nсообщества\\n 0\\n 19\\n 1\\n', '', '149\\nКарма\\n53.5\\nРейтинг\\nCloud4Y @Cloud4Y\\nКорпоративный облачный провайдер\\nСайт\\nВКонтакте\\nTelegram\\n Комментарии 1\\nПубликации\\nЛУЧШИЕ ЗА СУТКИ\\nПОХОЖИЕ\\n', '', '', ' · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\nИ Н Ф О Р М А Ц И Я\\nСайт\\nwww.cloud4y.ru\\n', 'Ваш аккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nДата регистрации\\n29 июля 2011\\nДата основания\\n2009\\nЧисленность\\n51–100 человек\\nМестоположение\\nРоссия\\nПредставитель\\n', 'Информация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные программы\\nСтартапам\\nМегапроекты\\nНастройка языка\\nТехническая поддержка Вернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "imgonline-com-ua-site2pdfB28eKGLy6EjA.pdf\n",
      "['71.07', 'Рейтинг', 'Cloud4Y', '#1 Корпоративный облачный провайдер', 'Автор оригинала: Yashwant Singh', 'Взлом Instagram*‑аккаунта — популярный запрос в поисковиках. Поэтому есть смысл рассказать', 'о том, как это обычно работает. Просто для того, чтобы вы знали, откуда может пойти атака.', ' Cloud4Y 5 часов назад', 'OSINT & Hacking — как работает фишинг для нельзяграма', ' Простой', ' 3 мин', ' 1.1K', 'Блог компании Cloud4Y , Информационная безопасность *, Социальные сети и сообщества', 'Обзор', ' Перевод', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "1 71.07 Cloud4Y #1 Корпоративный облачный провайдер Автор оригинала: Yashwant Singh\n",
      "['71.07\\nРейтинг\\nCloud4Y\\n#1 Корпоративный облачный провайдер\\nАвтор оригинала: Yashwant Singh\\nВзлом Instagram*аккаунта — популярный запрос в поисковиках.\\nПоэтому есть смысл рассказать о том, как это обычно работает.\\nПросто для того, чтобы вы знали, откуда может пойти атака.\\n 5 часов назад\\nПростой3 мин\\nCloud4Y\\nOSINT & Hacking — как работает фишинг для\\nнельзяграма\\n1.1K\\nБлог компании Cloud4Y\\n, \\nИнформационная безопасность*\\n, \\nСоциальные сети и сообщест\\nОбзор\\nПеревод\\nКАК СТАТЬ АВТОРОМ\\n', 'Чтобы начать попытки заполучить доступ к аккаунту, вы должны\\nзнать ник человека, которого вы пытаетесь взломать. Так\\nчто небольшая разведка будет очень кстати. Только не увлекайтесь.\\nСуществуют различные инструменты для разведки, в первую\\nочередь, поиск пользователя в конкретной соцсети с целью узнать\\nего ник. Я нашёл отличный инструмент под названием «Slash»,\\nкоторый можно использовать для поиска любых учётных записей\\nпользователя, если он везде регистрируется под одним ником.\\nСтавим Slash\\nЯ проверил Slash на себе, и посмотрите на эти результаты.\\nНекоторые из учетных записей, перечисленных здесь, были созданы\\nмного лет назад.\\ngit clone https://github.com/theahmadov/slash \\ncd slash \\npip install -r requirements.txt \\npython slash.py help\\n', 'Slash — это простой консольный инструмент. Но вы также можете\\nиспользовать такие инструменты, как WhatsMyName Web, который\\nсовершенно бесплатен.\\nВот, посмотрите. Я проверил WhatsMyName на себе. Мой ник\\n«earthtoyash».\\n', '', 'Теперь, когда мы знаем больше о нашем пользователе, можно\\nиспользовать эти знания. Например, через отправку фишинговых\\nссылок. Для этого создадим полезную нагрузку с помощью Zphisher.\\nСтавим Zphisher с GitHub\\nКлонируем репозиторий:\\nЗапускаем файл zphisher.sh:\\nПри первом запуске он установит зависимости и на этом всё.\\nСистема скажет, что Zphisher установлен. После установки вам\\nнужно будет снова запустить zphisher.sh в каталоге zphisher\\nкомандой ./zphisher , и тогда вы получите что-то вроде этого:\\ngit clone --depth=1 https://github.com/htr-tech/zphisher.git\\ncd zphisher && ./zphisher.sh\\n', 'Как вы можете видеть, есть много вариантов и шаблонов, благодаря\\nчему любой может заниматься фишингом. Мы сосредоточимся на\\nнельзяграме. Итак, введите «2» и нажмите Enter.\\nСледующий шаг полностью зависит от вас, выберите любой из них.\\nЗатем появится окно с выбором. Я выбрал третий вариант, так как он\\nминималистичный и удобен для того, чтобы показать возможности\\nинструмента.\\n', 'Опять же, чтобы все было просто, я пропущу пользовательский порт,\\nно если вы уже используете порт 8080, то можете изменить его на\\n8000. Если нет, оставляйте всё как есть. Также важно маскировать\\nURL, ну просто в целях безопасности. Можно использовать чтото\\nвроде этого:\\nВсё, Zphisher создал фишинговую ссылку, которую можно\\nотправить жертве. Как только она нажмёт на ссылку, вы начнёте\\n', 'получать информацию о ней. Например, IPадреса, имена\\nпользователей, пароли и т. д. Ещё можно использовать обратный\\nпоиск IP, чтобы определить местоположение вашей цели и многое\\nдругое.\\nИтак, вот эти фишинговые ссылки.\\nПри нажатии открывается страница, похожая на официальную\\nстраницу входа в запрещённую соцсеть.\\n', 'Вот она, нехорошая\\nПосле ввода учётных данных можно получить много информации на\\n«хакерской» стороне терминала.\\nВот так, господа и дамы, можно без особого труда взломать учетную\\nзапись в нельзяграме. Поэтому в очередной раз напоминаем: нельзя\\n', 'нажимать на ссылки, которым вы не доверяете.\\nСамо собой разумеется, не используйте информацию из этой статьи\\nс намерением причинить кому-либо вред. OSINT законен, но фишинг\\nи кража личных данных даже в запрещённой соцсети является\\nуголовным преступлением. И да, * Организация Meta, а также её\\nпродукт Instagram, на которые мы ссылаемся в этой статье,\\nпризнаны экстремистскими и запрещены на территории РФ.\\nСпасибо за внимание!\\nЧто ещё интересного есть в блоге Cloud4Y\\n→ Информационная безопасность и глупость: необычные примеры\\n→ NAS за шапку сухарей\\n→ Взлом Hyundai Tucson, часть 1, часть 2\\n→ Столетний язык программирования — какой он\\n→ 50 самых интересных клавиатур из частной коллекции\\nТеги:  \\nвзлом\\n,\\nсоцсети\\n,\\nosint\\nХабы:  \\nБлог компании Cloud4Y\\n,\\nИнформационная безопасность\\n,\\nСоциальные сети и сообщества\\n0\\n19\\n1\\n', 'Cloud4Y\\n#1 Корпоративный облачный провайдер\\nСайт\\nFacebook\\nTwitter\\nВКонтакте Telegram\\n149\\nКарма\\n53.5\\nРейтинг\\nCloud4Y @Cloud4Y\\nКорпоративный облачный провайдер\\nСайт\\nВКонтакте Telegram\\nКомментарии 1\\nПубликации\\nЛУЧШИЕ ЗА СУТКИ \\nПОХОЖИЕ\\n', ' · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n', 'Ваш\\nаккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nИНФОРМАЦИЯ\\nСайт\\nwww.cloud4y.ru\\nДата регистрации\\n29 июля 2011\\nДата основания\\n2009\\nЧисленность\\n51–100 человек\\nМестоположение\\nРоссия\\nПредставитель\\n', 'Настройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "OSINT & Hacking — как работает фишинг для нельзяграма _ Хабр3.pdf\n",
      "['71.07', 'Рейтинг', 'Cloud4Y', '#1 Корпоративный облачный провайдер', 'Автор оригинала: Yashwant Singh', 'Взлом Instagram*аккаунта — популярный запрос в поисковиках.', 'Поэтому есть смысл рассказать о том, как это обычно работает.', 'Просто для того, чтобы вы знали, откуда может пойти атака.', ' 5 часов назад', 'Простой3 мин', 'Cloud4Y', 'OSINT & Hacking — как работает фишинг для', 'нельзяграма', '1.1K', 'Блог компании Cloud4Y', ', ', 'Информационная безопасность*', ', ', 'Социальные сети и сообщест', 'Обзор', 'Перевод', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "1 71.07 Cloud4Y #1 Корпоративный облачный провайдер Автор оригинала: Yashwant Singh\n",
      "['Нидерландская компания Yandex N.V. по\\xa0итогам реструктуризации не\\xa0сможет создавать и\\nразвивать сервисы, аналогичные бизнесам «Яндекса» в\\xa0течение пяти лет по\\xa0всему миру.\\nОб\\xa0этом говорится в\\xa0документах по\\xa0реструктуризации бизнеса.\\nОтмечается, что\\xa0исключением будут выделяемые зарубежные стартапы Avride, Toloka, Nebius и\\nTripleTen, которые можно будет развивать везде, кроме России и Беларуси. «При этом\\nнезависимо от\\xa0рынка, им запрещено развиваться так, чтобы создавать конкуренцию сервисам\\n„Яндекса“. По\\xa0этому соглашению „Яндекс“ сможет развивать свои сервисы без\\xa0каких‑либо\\nограничений»,\\xa0— добавляется в\\xa0документах.\\nРанее стало известно, что расположенная в Нидерландах публичная компания с ограниченной\\nответственностью «Яндекс Н.В.» (Yandex N.V.\\xa0— материнская компания «Яндекса»)\\xa0заключила\\nсделку по продаже «Яндекса» за 475 млрд рублей консорциуму инвесторов и перестанет быть\\nголовной компанией. В заявлении компании говорится, что «Яндекс» останется частной,\\nнезависимой и публичной организацией после реструктуризации.\\nРазделение компании произойдёт в\\xa0первом полугодии 2024\\xa0года и пройдёт в\\xa0два этапа.\\nПосле разделения российской компании «Яндекс» отойдут все сервисы и активы\\nза\\xa0исключением Nebius, Toloka, Avride и TripleTen. При\\xa0этом основным владельцем компании\\nроссийской компании станет фонд паевых инвестиций «Консорциум Первый». В\\xa0Фонд войдут\\nменеджмент «Яндекса», частные инвесторы и «Лукойл», при\\xa0этом контрольного пакета ни\\xa0у\\xa0кого\\nиз\\xa0акционеров не\\xa0будет.\\nПосле завершения разделения компании Yandex N.V. проведет делистинг, а\\xa0российская\\nструктура «Яндекса» выйдет на\\xa0Московскую биржу.\\nТеги:  яндекс, yandex n.v., разделение бизнеса\\nХабы:  Управление проектами, Бизнес-модели, Финансы в IT, IT-компании\\nLizzieSimpson 10 часов назад\\nYandex N.V. запретили в\\xa0течение пяти лет создавать\\nаналогичные «Яндексу» сервисы\\n2 мин\\n13K\\nУправление проектами*,\\xa0Бизнес-модели*,\\xa0Финансы в IT,\\xa0IT-компании\\n+11\\n6\\n25 +24\\nКАК СТАТЬ АВТОРОМ\\nЛучшие ИТ-работодатели России 2023\\nОбучение ИБ: что с ним, …\\nВойти\\nМоя\\nлента\\nВсе\\nпотоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп\\n']\n",
      "Yandex N.V. запретили в течение пяти лет создавать аналогичные «Яндексу» сервисы _ Хабр.pdf\n",
      "['Нидерландская компания Yandex N.V. по\\xa0итогам реструктуризации не\\xa0сможет создавать и', 'развивать сервисы, аналогичные бизнесам «Яндекса» в\\xa0течение пяти лет по\\xa0всему миру.', 'Об\\xa0этом говорится в\\xa0документах по\\xa0реструктуризации бизнеса.', 'Отмечается, что\\xa0исключением будут выделяемые зарубежные стартапы Avride, Toloka, Nebius и', 'TripleTen, которые можно будет развивать везде, кроме России и Беларуси. «При этом', 'независимо от\\xa0рынка, им запрещено развиваться так, чтобы создавать конкуренцию сервисам', '„Яндекса“. По\\xa0этому соглашению „Яндекс“ сможет развивать свои сервисы без\\xa0каких‑либо', 'ограничений»,\\xa0— добавляется в\\xa0документах.', 'Ранее стало известно, что расположенная в Нидерландах публичная компания с ограниченной', 'ответственностью «Яндекс Н.В.» (Yandex N.V.\\xa0— материнская компания «Яндекса»)\\xa0заключила', 'сделку по продаже «Яндекса» за 475 млрд рублей консорциуму инвесторов и перестанет быть', 'головной компанией. В заявлении компании говорится, что «Яндекс» останется частной,', 'независимой и публичной организацией после реструктуризации.', 'Разделение компании произойдёт в\\xa0первом полугодии 2024\\xa0года и пройдёт в\\xa0два этапа.', 'После разделения российской компании «Яндекс» отойдут все сервисы и активы', 'за\\xa0исключением Nebius, Toloka, Avride и TripleTen. При\\xa0этом основным владельцем компании', 'российской компании станет фонд паевых инвестиций «Консорциум Первый». В\\xa0Фонд войдут', 'менеджмент «Яндекса», частные инвесторы и «Лукойл», при\\xa0этом контрольного пакета ни\\xa0у\\xa0кого', 'из\\xa0акционеров не\\xa0будет.', 'После завершения разделения компании Yandex N.V. проведет делистинг, а\\xa0российская', 'структура «Яндекса» выйдет на\\xa0Московскую биржу.', 'Теги:  яндекс, yandex n.v., разделение бизнеса', 'Хабы:  Управление проектами, Бизнес-модели, Финансы в IT, IT-компании', 'LizzieSimpson 10 часов назад', 'Yandex N.V. запретили в\\xa0течение пяти лет создавать', 'аналогичные «Яндексу» сервисы', '2 мин', '13K', 'Управление проектами*,\\xa0Бизнес-модели*,\\xa0Финансы в IT,\\xa0IT-компании', '+11', '6', '25 +24', 'КАК СТАТЬ АВТОРОМ', 'Лучшие ИТ-работодатели России 2023', 'Обучение ИБ: что с ним, …', 'Войти', 'Моя', 'лента', 'Все', 'потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп', '']\n",
      "['4.29\\nОценка\\n280.79\\nРейтинг\\nSouthbridge\\nОбеспечиваем стабильную работу highload-проектов\\nАвтор оригинала: Seifeldin Mahjoub\\nПеревели статью о создании пайплайна для развертывания\\nстатического веб-сайта на AWS S3 Bucket на примере Gitlab\\nCI/CD, чтобы быстро вникнуть в основы технологии и начать\\nприменять ее в работе. В статье рассматриваются базовые\\nконцепции CI и CD, а также этапы CI/CD-пайплайна.\\n 2 часа назад\\n5 мин\\nzubarek\\nБыстрое начало работы с Gitlab CI/CD:\\nпайплайн для веб-сайта на AWS S3 Bucket\\n362\\nБлог компании Southbridge\\n, \\nТестирование IT-систем*\\n, \\nСистемное администрирование*\\n, \\nТуториал\\nПеревод\\nКАК СТАТЬ АВТОРОМ\\n', 'От автора\\nМне повезло быть частью некоторых профессиональных команд,\\nкаждая из которых применяла несколько DevOps практик. И меня\\nпоразило то, как качество кода, скорость разработки и позитивный\\nнастрой команды коррелируют с CI/CD-пайплайном.\\nПо моему мнению, зрелость пайплайна может служить\\nпрекрасным показателем опытности разработчика, качества\\nкода и эффективности всей команды.\\nВо многих случаях, которые я наблюдал, пайплайны были\\nвыстроены либо DevOps-инженером, либо отдельной DevOps-\\nкомандой. Да и последний отчет State of CD 2022\\nпродемонстрировал, что только 22% разработчиков создают\\nпайплайны.\\nМоя цель — увеличить это число: помочь разработчикам взять на\\nсебя ответственность за пайплайны, выстраивать непрерывный\\nпроцесс доставки и создавать качественный код.\\nВ статье рассматриваются фундаментальные концепции CI и CD.\\n', 'Что такое CI/CD?\\nМногие бизнесы применяют фреймворки Agile, так как они позволяют\\nменять приоритеты и повышать скорость доставки. Кроме всего\\nпрочего, такой подход улучшает атмосферу в команде и помогает\\nувеличить прибыль.\\nЕсли ваша компания следует по пути Agile, то принятие культуры,\\nфилософии и практик DevOps станет ее большим преимуществом.\\nМодное словечко последних десятилетий, DevOps сегодня считается\\nнастоящим стандартом индустрии. CI/CD — это практика DevOps,\\nкоторая помогает разработчикам ПО доставлять изменения в коде с\\nвысокой частотой и надежностью.\\n«Быстрый билд, быстрый тест, быстрый фейл»\\nПри наличии автоматизированных тестов команды тяготеют к общей\\nавтоматизации задач и частым, надежным поставкам кода. Создание\\nCI/CD-пайплайна в этом случае может привести к нескольким\\nпреимуществам.\\nБизнес выигрывает от снижения затрат и повышения\\nпроизводительности, ускорения Time to Market и адаптации к\\nизменяющимся требованиям рынка.\\nКоманда выигрывает от быстрой обратной связи, улучшения\\nэффективности разработки, уменьшения количества бутылочных\\nгорлышек и повышения уровня вовлеченности и удовлетворенности\\nсотрудников.\\nФазы CI и CD\\n', 'CI — непрерывная интеграция. Непрерывная интеграция\\nпозволяет по много раз в день коммитить изменения в основную\\nветку вашей кодовой базы.\\nУчитывая ограниченные когнитивные способности человека, CI\\nстимулирует разработчиков вносить в код небольшие изменения,\\nкоторые легче рассмотреть, покрыть автоматическими тестами и\\nчасто релизить.\\nЭто позволяет избежать напряженных и переполненных merge\\nconflict-ами дней подготовки к релизу с тоннами ручного\\nтестирования.\\nCD — непрерывная доставка. Следующий шаг после CI позволяет\\nгарантировать, что кодовая база постоянно готова к деплою, а\\nзадеплоить ее можно одним нажатием кнопки.\\nПри этом неважно, с чем вы работаете: с масштабной\\nраспределенной системой, сложной производственной средой и т. д.\\nКлючевой момент — автоматизация.\\nCD — непрерывное развертывание. Последний этап зрелого\\nCI/CD-пайплайна, где все изменения в коде автоматически\\nразвертываются в продакшн без ручного вмешательства.\\n', 'Само собой, для этого требуется большое количество хорошо\\nпродуманных автоматических тестов. State of CD 2022 утверждает,\\nчто «47% разработчиков применяют CI или СD, но только один из\\nпяти использует оба подхода для автоматизации сборки,\\nтестирования и развертывания кода».\\nКнига Accelerate подводит итоги многолетнего исследования с\\nиспользованием отчетов State of DevOps, основанных на 23 000\\nнаборов данных компаний по всему миру. Как видите,\\nвысокопроизводительные команды могут деплоить по требованию\\n(или несколько раз в день).\\nЭтапы CI/CD-пайплайна\\n', 'Стадия исходного кода — здесь запускается пайплайн. Обычно это\\nпроисходит после изменений в Git-репозитории, которые\\nпроявляются в открытии нового Pull Request-а или в пуше в ветку.\\nДругой способ заключается в настройке инструментария CI/CD для\\nзапуска пайплайна через автоматическое расписание или после\\nзапуска другого пайплайна.\\nСтадия сборки — этап, в процессе которого происходит проверка и\\nсборка кода. Здесь особенно полезны такие инструменты, как\\nDocker: они обеспечивают однородную среду.\\nСтадия тестирования — CI/CD невозможно представить без\\nавтоматизированных тестов. В конце концов, все хотят быть\\nуверены, что изменения в коде не сломают продакшн.\\nСтадия развертывания — на последнем этапе (после успешного\\nпрохождения всех предыдущих стадий) код можно развернуть в\\nвыбранной среде.\\nПример с Gitlab\\nВ этом примере будет использован Gitlab CI/CD, однако концепции\\nаналогичны и для остальных инструментов, поэтому их можно\\nприменить к другим сервисам хостинга репозиториев.\\nСуществует несколько инструментов CI/CD, например всемирно\\nизвестный Jenkins. Этот инструмент требует некоторой настройки и\\nконфигурации, в то время как другие поставляются сервисами\\nхостинга репозиториев (такими как GitHub Actions и Bitbucket\\nPipelines) с предварительной настройкой.\\nПоэтому если ваш код размещен на Gitlab, то легче всего\\nиспользовать Gitlab CI/CD, поскольку код и управление CI/CD\\nнаходятся на одной платформе.\\nКак все это может работать без настроек?\\n', 'Для ответа на этот вопрос стоит немного погрузиться в архитектуру\\nGitlab, а именно — в инстансы и раннеры. Инстансы хранят код\\nприложения и конфигурации пайплайна. Раннеры выступают в\\nкачестве агентов, выполняющих операции в пайплайнах.\\nВ Gitlab каждый инстанс может быть подключен к одному или\\nнескольким раннерам.\\nGitlab.com — это управляемый инстанс с несколькими раннерами,\\nкоторые сам Gitlab и поддерживает. Следовательно, если вы\\nиспользуете этот инстанс, то получаете все необходимое из коробки.\\nПриступим к работе\\nGitlab предлагает несколько шаблонов при создании нового проекта.\\nКонфигурация пайплайна Gitlab CI/CD по умолчанию находится в\\nфайле .gitlab-ci.yml в корневом каталоге.\\n', 'Предположим, мы хотим создать простой пайплайн, который\\nпроверяет: написан, протестирован и развернут ли код. Вот\\nнесколько концепций и терминов для ознакомления перед началом\\nработы.\\nПайплайн (Pipeline)\\nПайплайн — это набор заданий, разделенных на этапы. Gitlab\\nпредлагает различные типы пайплайнов, например parent-child или\\nmulti-project. Полный список см. здесь.\\nЭтап (Stage)\\nЭтап — это шаг в пайплайне, предоставляющий информацию о том,\\nкакие задания запускать (сборка, тестирование и т. д.). Один этап\\nможет включать одно или несколько заданий.\\nЗадание (Job)\\nЗадание — основной блок пайплайна (компиляция, линтинг и т. д.).\\nДля каждого задания должны быть определены name и script. После\\nвыполнения всех заданий на этапе пайплайн переходит к\\nследующему.\\nТеперь — к коду\\nВыстраиваем пайплайн Gitlab CI/CD, который собирает, тестирует и\\nразворачивает статический веб-сайт в AWS S3 Bucket.\\n', 'Для начала создадим новый .gitlab-ci.yml\\n1. Определим переменные\\n2. Определим этапы\\n3. Определим задания на каждом этапе\\nvariables: # variabiles definitions for easier reuse of valu\\n CI_NODE_IMAGE: \"node:16.13.2\"\\n# Pipeline stages\\nstages:\\n - install\\n - build\\n - test\\n - deploy\\n#install job definition\\ninstall:\\n stage: install\\n image: \"$CI_NODE_IMAGE\" # variable reference\\n script: # Shell script that is executed by the runner.\\n   - npm ci\\n cache: # List of files that should be cached between subseq\\n   key:\\n     files:\\n       - package.json\\n       - package-lock.json\\n   paths: # directories to cache\\n     - node_modules\\n# Build Job definition\\n', 'На этом все, спасибо за внимание.\\nНаучиться работать с пайплайнами, билдами и артефактами можно\\nна курсе Gitlab CI/CD в Слёрм. Вы узнаете, из чего состоит Gitlab\\nи какие у него возможности и настройки, а также разберете лучшие\\nпрактики построения пайплайна, особенности шаблонизации\\nи работы с переменными.\\nbuild:\\n stage: build\\n image: $CI_NODE_IMAGE\\n script:\\n   - npm run build\\n artifacts: # list of files and directories that are attache\\n   paths:\\n     - dist/\\n cache:\\n   key:\\n     files:\\n       - package.json\\n       - package-lock.json\\n   paths:\\n     - node_modules\\n   policy: pull\\n# Test Job definition\\ntest:\\n stage: test\\n image: $CI_NODE_IMAGE\\n script:\\n   - npm run test\\n# Deploy Job definition\\ndeploy:\\n stage: deploy\\n image: registry.gitlab.com/gitlab-org/cloud-deploy/aws-base\\n script:\\n   - aws s3 cp --recursive dist s3://bucket-name # copies th\\n', 'Southbridge\\nОбеспечиваем стабильную работу highload-проектов\\nСайт\\nСайт\\n14\\nКарма\\n16.9\\nРейтинг\\nЛиза Зубарькова @zubarek\\nПользователь\\nКомментировать\\nПубликации\\nВидеокурс доступен всегда. Посмотреть программу:\\nhttps://slurm.club/3JUKdzT\\nТеги:  \\nci/cd\\n,\\ngitlab-ci\\n,\\naws\\n,\\ngitlab\\n,\\npipeline\\nХабы:  \\nБлог компании Southbridge\\n,\\nТестирование IT-систем\\n,\\nСистемное администрирование\\n,\\nПрограммирование\\n,\\nDevOps\\n+7\\n14\\n0\\nЛУЧШИЕ ЗА СУТКИ \\nПОХОЖИЕ\\n', '', 'Ваш\\nаккаунт\\nРазделы\\nИнформация\\nУслуги\\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\nИНФОРМАЦИЯ\\nСайт\\nsouthbridge.io\\nДата регистрации\\n15 ноября 2012\\nДата основания\\n22 февраля 2008\\nЧисленность\\n51–100 человек\\nМестоположение\\nРоссия\\nПредставитель\\nАнтон Скобин\\n', 'Войти\\nРегистрация\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nНастройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "Быстрое начало работы с Gitlab CICD.pdf\n",
      "['4.29', 'Оценка', '280.79', 'Рейтинг', 'Southbridge', 'Обеспечиваем стабильную работу highload-проектов', 'Автор оригинала: Seifeldin Mahjoub', 'Перевели статью о создании пайплайна для развертывания', 'статического веб-сайта на AWS S3 Bucket на примере Gitlab', 'CI/CD, чтобы быстро вникнуть в основы технологии и начать', 'применять ее в работе. В статье рассматриваются базовые', 'концепции CI и CD, а также этапы CI/CD-пайплайна.', ' 2 часа назад', '5 мин', 'zubarek', 'Быстрое начало работы с Gitlab CI/CD:', 'пайплайн для веб-сайта на AWS S3 Bucket', '362', 'Блог компании Southbridge', ', ', 'Тестирование IT-систем*', ', ', 'Системное администрирование*', ', ', 'Туториал', 'Перевод', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "3 280.79 Southbridge Обеспечиваем стабильную работу highload-проектов Автор оригинала: Seifeldin Mahjoub\n",
      "['Митчелл Бейкер, гендиректор Mozilla с 2020 года, объявила, что покидает свой пост и\\nвозвращается на должность председателя совета директоров Mozilla Corporation, которую она\\nзанимала ранее. Временным генеральным директором компании станет член правления Лора\\nЧемберс.\\n«За 25 лет работы в Mozilla я побывала на многих должностях. Мой теперешний шаг вызван\\nжеланием сосредоточить внимание на предстоящих задачах. Я руководила бизнесом Mozilla в\\nпериод преобразований, а также курировала миссию Mozilla в более широком смысле. Стало\\nочевидно, что оба направления требуют преданного и постоянного руководства», — заявила\\nБейкер в сообщении в блоге Mozilla.\\nМитчелл работает в Mozilla с эпохи Netscape Communications. Она основала Mozilla Foundation и\\nучредила лицензию Mozilla Public License.\\nAnnieBronson 12 минут назад\\nГенеральный директор Mozilla покинула свой пост\\n1 мин\\n107\\nIT-компании,\\xa0Управление персоналом*,\\xa0Карьера в IT-индустрии\\nКАК СТАТЬ АВТОРОМ\\nЗарплаты IT-специалистов\\nНовый год → новая жизнь → новые с…\\nВойти\\nМоя\\nлента\\nВсе\\nпотоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп\\n', '141\\nКарма\\n145.1\\nРейтинг\\n@AnnieBronson\\nИнформационная служба Хабра\\nРеклама\\nКомментарии\\nЗдесь пока нет ни одного комментария, вы\\xa0можете\\xa0стать первым!\\nДругие новости\\nAnnieBronson 59 минут назад\\nЛора Чемберс в должность генерального директора до конца этого года. Она входила в состав\\nсовета директоров Mozilla в течение трёх последних лет, а до этого работала в Airbnb, PayPal,\\neBay и Willow Innovations. По словам Бейкер, внимание Лоры « будет сосредоточено на\\nпредоставлении успешных продуктов, продвигающих миссию компании, и создании платформ,\\nускоряющих её развитие».\\xa0\\n«Мы находимся на критическом этапе, когда общественное доверие к институтам,\\nправительствам и Интернету достигло беспрецедентно низкого уровня. Происходит\\nтектонический сдвиг: каждый борется за будущее ИИ. Это возможность и необходимость для\\nMozilla создать лучшее будущее», — заключила Бейкер.\\nТеги:  mozilla, mozilla foundation\\nХабы:  IT-компании, Управление персоналом, Карьера в IT-индустрии\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\nЭлектропочта\\nТолько полноправные пользователи могут оставлять комментарии. Войдите, пожалуйста.\\nai.sbergraduate.ru РЕКЛАМА\\nВакансии для\\xa0студентов без\\xa0опыта работы\\nSberseasons: подай заявку на\\xa0стажировку по\\xa0IT и\\xa0другим специальностям\\n', 'SpaceX планирует использовать беспилотные баржи в качестве\\nморских интернет-станций для Starlink\\n2 мин\\nSLY_G 3 часа назад\\nБиологи научились регулировать кислотность клеток человека при\\nпомощи света\\n3 мин\\nCloudMTS 3 часа назад\\nОблачный дайджест: затраты на VDI растут, SAP закрывает доступ,\\nдата-центр строят на 3D-принтере\\n2 мин\\nCloud4Y 4 часа назад\\nЧто делает тихоходок настолько живучими? Новое исследование\\nуказывает на свободные радикалы\\n6 мин\\nIgnatChuker 5 часов назад\\nВ\\xa0Австралии приняли в\\xa0первом чтении закон о\\xa0праве работника\\nигнорировать рабочие звонки и сообщения в нерабочее время\\n2 мин\\nAnnieBronson 5 часов назад\\nMicrosoft изменила интерфейс Copilot для Windows, чтобы сервис по\\nумолчанию запускался с большим окном чата\\n1 мин\\n301\\n+5\\n0\\n0\\n448\\n+6\\n0\\n0\\n633\\n+1\\n0\\n0\\n2.8K\\n+5\\n4\\n3 +3\\n3.2K\\n+23\\n8\\n29 +29\\n1.6K\\n', 'avouner 5 часов назад\\nРоссийские банки выступили против оборотных штрафов до 500 млн\\nрублей за утечки данных\\n2 мин\\nLizzieSimpson 5 часов назад\\nDisney за $1,5 млрд купит долю в Epic Games\\n1 мин\\ndaniilshat 5 часов назад\\nБезопасность конфигураций веб-сервера Angie PRO теперь\\nконтролирует X-Config\\n1 мин\\nTravis_Macrif 5 часов назад\\nМинистерство внутренней безопасности США создало новое\\nподразделение под названием AI Corps\\n2 мин\\nПоказать еще\\nМИНУТОЧКУ ВНИМАНИЯ\\n0\\n0\\n7 +7\\n2.2K\\n+5\\n1\\n7 +7\\n874\\n+5\\n0\\n3 +3\\n183\\n+4\\n0\\n0\\n490\\n+3\\n0\\n2 +2\\nИнтересно\\nИнтересно \\nТурбо\\n', 'ВОПРОСЫ И ОТВЕТЫ\\nЕсть ли плагин-\"мусорка\" закладок для браузера Огнелис?\\nMozilla · Простой · 1 ответ\\nГде скачать русскоязычную модель для TTS от Mozilla?\\nMozilla · Простой · 1 ответ\\nЧто делать, если сайт некорректно отображается в Mozilla FireFox?\\nMozilla · Простой · 1 ответ\\nОгромные лаги в интерфейсе Firefox. Как исправить?\\nMozilla · 2 ответа\\nПочему не работает слайдер jquery.jcarousel.js в браузерах Фаерфокс и ИЕ?\\nMozilla · 0 ответов\\nБольше вопросов на Хабр Q&A\\nЗарплаты IT-специалистов во\\nвторой половине 2023\\nАх, этот скидочный снегопад:\\nпоймай свою снежинку\\nХабракалендарь, отворись! Какие\\nIT-ивенты ждут нас в 2024\\nРеклама\\n', 'ЧИТАЮТ СЕЙЧАС\\nТинькофф банк вернул 200 тысяч. Это был не дипфейк\\nЛюди не понимают ООП\\nВ России госведомства начали объявлять тендеры на уничтожение техники Apple\\nYandex N.V. запретили в\\xa0течение пяти лет создавать аналогичные «Яндексу» сервисы\\n«МегаФон» повысил цены на связь для некоторых абонентов, так как они от этого не отказались\\n5.8K\\n26 +26\\n50K\\n247 +247\\n45K\\n218 +218\\n13K\\n25 +25\\n8.8K\\n52 +52\\nsberbank.com\\nКешбэк до\\xa010%\\nбонусами\\nСберСпасибо —\\nсервис СберПремьер\\n4,6\\nРейтинг организации\\nАкция для\\xa0владельцев. Оформите\\nонлайн пакет услуг и\\xa0получайте\\nвыгоду от\\xa0покупок\\nОформить\\nВыделенная линия\\nУзнать больше\\nРЕКЛАМА\\n', 'Знаете, что поменять в обучении ИБ? Рассказывайте!\\nСпецпроект\\nИСТОРИИ\\nБЛИЖАЙШИЕ СОБЫТИЯ\\nРеклама\\nХорошие статьи из\\nблогов компаний\\nЗарплаты в IT во\\nвторой половине\\n2023\\nУчим английский\\nНе работай через\\nсилу\\nПозовите автора!\\nКонк\\nХабр\\nКак использовать\\nоблачные платформы\\nСloud.ru: серия демо-\\nвстреч по четвергам\\n 1 – 29 февраля\\n 11:00\\nОнлайн\\nПодробнее в календаре\\nКонкурс грантов на\\nобучение IT-профессиям\\nот Хекслета\\n 5 – 29 февраля\\nОнлайн\\nПодробнее в календаре\\nОткрытый урок\\n«Behaviour Tree в \\n 8 февраля\\n 19\\nОнлайн\\nПодробнее в календаре\\n', 'ADRIVER\\nВаш аккаунт\\nВойти\\nРегистрация\\nРазделы\\nСтатьи\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nНастройка языка\\nТехническая поддержка\\n© 2006–2024, Habr\\n']\n",
      "Генеральный директор Mozilla покинула свой пост _ Хабр.pdf\n",
      "['Митчелл Бейкер, гендиректор Mozilla с 2020 года, объявила, что покидает свой пост и', 'возвращается на должность председателя совета директоров Mozilla Corporation, которую она', 'занимала ранее. Временным генеральным директором компании станет член правления Лора', 'Чемберс.', '«За 25 лет работы в Mozilla я побывала на многих должностях. Мой теперешний шаг вызван', 'желанием сосредоточить внимание на предстоящих задачах. Я руководила бизнесом Mozilla в', 'период преобразований, а также курировала миссию Mozilla в более широком смысле. Стало', 'очевидно, что оба направления требуют преданного и постоянного руководства», — заявила', 'Бейкер в сообщении в блоге Mozilla.', 'Митчелл работает в Mozilla с эпохи Netscape Communications. Она основала Mozilla Foundation и', 'учредила лицензию Mozilla Public License.', 'AnnieBronson 12 минут назад', 'Генеральный директор Mozilla покинула свой пост', '1 мин', '107', 'IT-компании,\\xa0Управление персоналом*,\\xa0Карьера в IT-индустрии', 'КАК СТАТЬ АВТОРОМ', 'Зарплаты IT-специалистов', 'Новый год → новая жизнь → новые с…', 'Войти', 'Моя', 'лента', 'Все', 'потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп', '']\n",
      "['Для привлечения внимания расскажу историю. Уже много лет живу далеко за пределами\\nнашей всеми любимой родины. И на днях, понадобилось мне войти в старую почту gmail...\\nЛогин и пароль надежно сохранены. Однако Google не торопится впускать нас в\\nсобственную почту.\\n— Нам кажется что это не вы, подтвердите что это вы. Введите номер телефона\\nкогда-то использовавшийся при регистрации.\\nЧто ж, и это можно. Ввожу номер.\\n— Увы, мы не можем отправить СМС на этот номер. Хотите завести другой аккаунт?\\nИтак, все данные есть, однако подтвердить что это вы по техническим проблемам самого\\nсервиса невозможно. Связанные аккаунты, старая переписка, и файлы в облаке утрачены\\nнавсегда. Какое-то знакомое чувство, не так ли? Другой аккаунт Google, прикрепленного\\nномера телефона нет и не было. Подтвердите что это вы. Выберите способ\\nподтверждения, Увы, доступных способов подтверждения нету. Завести новый\\nаккаунт?\\nЕсли вы уже начали понимать к чему клоню, то следующая история вам понравится еще\\nбольше. Находясь в путешествии в дальних заморских странах, понадобилось три тысячи\\nдолларов снять для оплаты жилья. Из приложения банка делаю перевод на свое\\nсобственное имя... и вуаля, вы заблокированы! Войти в приложение нельзя для вашей\\nсобственной безопасности!\\nПишу в поддержку.\\n— Нам кажется что вы это не вы, надо подтвердить что это вы. Укажите кодовое\\nслово, номер того-и-сего.\\nУказываю. Прошу разблокировать банк и сделать транзакцию.\\nВ поддержку сейчас пишет много людей. Ответ может занять неопределенное время.\\nПодождите пожалуйста подольше. Мы делаем все для клиентов!\\nТак проходит еще час. И поверьте, имена брендов в этой истории совершенно не важны.\\nНе потому, что хабр не жалобная книга, а потому что раком безопасности заражена вся\\nиндустрия, не осталось ни одного захудалого приложения для заказа пиццы, не\\nпостроившего бы пароноидально-шизофреническую систему входов-подтверждений.\\nmanul\\n46 минут назад\\nЗа что безопасники будут гореть в аду?\\nПростой\\n4 мин\\n466\\nРазработка мобильных приложений*,\\xa0Разработка облачных платформ,\\xa0Исследования и прогнозы в IT*,\\xa0Программи\\nКонкурс рассказов\\nОткрываем сезон футурологии с крутыми призами\\nРЕКЛАМА\\nКАК СТАТЬ АВТОРОМ\\nЗарплаты айтишников\\nHuawei Mate 70 Pro: тест нови…\\n07.03.2025, 08:20\\nЗа что безопасники будут гореть в аду? / Хабр\\nhttps://habr.com/ru/articles/888708/\\n1/11\\n', 'Из опыта, на посошок. Еще одна страна, еще один банк, еще одна сим-карта для\\nподтверждений, ведь других вариантов подтверждений банки сейчас не предлагают.\\nДальнейшее путешествие. И вот сим-карта превращается в тыкву в роуминге. А следом за\\nней и банк. Поддержка банка говорит что в целях безопасности без подтверждений по\\nСМС никак нельзя. Небезопасно. Достучаться до поддержки оператора в свою очередь\\nневозможно. Надо проходить безумный квест \"нажмите цифру\", а затем победить\\nэлектронного помощника-дебила. После этого услышать ответ по скрипту от человека, у\\nнас проблем не зафиксировано. Все для безопасности! Божок безопасности доволен!\\nИтак, это новая болезнь нашего мира, и она охватывает все сферы жизни. Она страшнее,\\nближе, и опаснее, чем государственные регуляции. Из хозяев своих данных, под\\nблаговидным предлогом, мы превратились в рабов безопасности всевозможных сервисов,\\nв том числе частных. Теперь не безопасность служит нам, а мы ей. А придумали, создали\\nи внедрили эту безумную систему подтверждений, вторых, пятых и десятых факторов,\\nнастрочив об этом тонны статей, получив множество придуманных самостоятельно ачивок,\\nникто иные, как наши прибившиеся \"коллеги\" по IT-индустрии, так называемые\\nбезопасники.\\nНапомню, самые страшные болезни заходят через системы безопасности организма. Рак,\\nСПИД, и аутоиммунные заболевания. И это произошло с нашей IT-индустрией, со всеми\\nвеб-сервисами и мобильными приложениями. Мы заражены. Мы больше не хозяева своих\\nденег и данных. Все может накрыться медным тазом в произвольный момент.\\nЛогин и пароль были слишком простым способом входа? Кто-то ставит слабые пароли! И\\nради защиты детей тупых придумали второй фактор. Что может быть надежнее для этого,\\nчем кусочек пластика который выходит из строя, невозможно восстановить в другой\\nстране, и находится в зависимости от дополнительной, по сути, левой компании-\\nоператора? Разве что e-sim, которые помимо такой же зависимости от глючных операторов\\nв роуминге, так еще и превращаются в тыкву с поломкой телефона. Разбился экран в\\nдругой стране, добро пожаловать в бомжи!\\nНо и этого религиозным фанатикам показалось мало, и они добавили алгоритмы\\nопределения входа из необычных мест, устройств и так далее... А что, сидите дома,\\nхолопы, это безопаснее!\\nЭталонный клиент современных мер безопасности, это слабоумный, пускающий слюни, с\\nтрясущимися руками старик, забывающий все на свете, не помнящий как его зовут, и\\nникогда не выходящий на прогулку дальше пяти метров от дома.\\nИндустрия безопасности и безопасников, которая якобы должна спасать нас, сама\\nпревратилась в гигантский источник опасности, который ввергает нас в рабство,\\nнастоящую зависимость от странных и ненадежных факторов, технических проблем\\nтретьих компаний, прихоти \"умных\" алгоритмов, лени техподдержки, фазы луны.\\nКонкурс рассказов\\nОткрываем сезон футурологии с крутыми призами\\n07.03.2025, 08:20\\nЗа что безопасники будут гореть в аду? / Хабр\\nhttps://habr.com/ru/articles/888708/\\n2/11\\n', 'Где-то мы это уже слышали, не правда ли? Безопасники частных компаний просто\\nповторили путь своих старших государственных братишек менее чем за пару десятилетий.\\nЛогин и пароль это лучшее что было. Логин и пароль надежно и удобно хранятся в\\nспециализированных БД, приложениях вроде KeePass, и так далее. Безопаснее с\\nдополнительными факторами не стало. Стало страшнее и опаснее за хрупкие и\\nненадежные дополнительные факторы, которые периодически запрашивают. Вспомним,\\nвторым фактором может выступать ключевая фраза, ответ на вопрос про любимое блюдо\\nи так далее, практика, которую злобные гиганты намеренно ввергли в забытье.\\nЯ призываю здравомыслящих людей разбить оковы рабства. Давайте будем создавать\\nприложения ради свободы, а не безопасности.\\nСвобода или безопасность?\\n61.54% Свобода\\n8\\n38.46% Безопасность\\n5\\nПроголосовали 13 пользователей. Воздержались 4 пользователя.\\nТеги: свобода, разработка, разработка приложений, \\nразработка программного обеспечения, разработка сайтов, облачные сервисы, \\nоблачная инфраструктура\\nХабы: Разработка мобильных приложений, Разработка облачных платформ, \\nИсследования и прогнозы в IT, Программирование, IT-компании\\nТолько зарегистрированные пользователи могут участвовать в опросе. Войдите,\\nпожалуйста.\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\n+7\\n0\\n1\\nКонкурс рассказов\\nОткрываем сезон футурологии с крутыми призами\\n07.03.2025, 08:20\\nЗа что безопасники будут гореть в аду? / Хабр\\nhttps://habr.com/ru/articles/888708/\\n3/11\\n']\n",
      "За что безопасники будут гореть в аду_ _ Хабр.pdf\n",
      "['Для привлечения внимания расскажу историю. Уже много лет живу далеко за пределами', 'нашей всеми любимой родины. И на днях, понадобилось мне войти в старую почту gmail...', 'Логин и пароль надежно сохранены. Однако Google не торопится впускать нас в', 'собственную почту.', '— Нам кажется что это не вы, подтвердите что это вы. Введите номер телефона', 'когда-то использовавшийся при регистрации.', 'Что ж, и это можно. Ввожу номер.', '— Увы, мы не можем отправить СМС на этот номер. Хотите завести другой аккаунт?', 'Итак, все данные есть, однако подтвердить что это вы по техническим проблемам самого', 'сервиса невозможно. Связанные аккаунты, старая переписка, и файлы в облаке утрачены', 'навсегда. Какое-то знакомое чувство, не так ли? Другой аккаунт Google, прикрепленного', 'номера телефона нет и не было. Подтвердите что это вы. Выберите способ', 'подтверждения, Увы, доступных способов подтверждения нету. Завести новый', 'аккаунт?', 'Если вы уже начали понимать к чему клоню, то следующая история вам понравится еще', 'больше. Находясь в путешествии в дальних заморских странах, понадобилось три тысячи', 'долларов снять для оплаты жилья. Из приложения банка делаю перевод на свое', 'собственное имя... и вуаля, вы заблокированы! Войти в приложение нельзя для вашей', 'собственной безопасности!', 'Пишу в поддержку.', '— Нам кажется что вы это не вы, надо подтвердить что это вы. Укажите кодовое', 'слово, номер того-и-сего.', 'Указываю. Прошу разблокировать банк и сделать транзакцию.', 'В поддержку сейчас пишет много людей. Ответ может занять неопределенное время.', 'Подождите пожалуйста подольше. Мы делаем все для клиентов!', 'Так проходит еще час. И поверьте, имена брендов в этой истории совершенно не важны.', 'Не потому, что хабр не жалобная книга, а потому что раком безопасности заражена вся', 'индустрия, не осталось ни одного захудалого приложения для заказа пиццы, не', 'построившего бы пароноидально-шизофреническую систему входов-подтверждений.', 'manul', '46 минут назад', 'За что безопасники будут гореть в аду?', 'Простой', '4 мин', '466', 'Разработка мобильных приложений*,\\xa0Разработка облачных платформ,\\xa0Исследования и прогнозы в IT*,\\xa0Программи', 'Конкурс рассказов', 'Открываем сезон футурологии с крутыми призами', 'РЕКЛАМА', 'КАК СТАТЬ АВТОРОМ', 'Зарплаты айтишников', 'Huawei Mate 70 Pro: тест нови…', '07.03.2025, 08:20', 'За что безопасники будут гореть в аду? / Хабр', 'https://habr.com/ru/articles/888708/', '1/11', '']\n",
      "['Привет! Меня зовут Денис, я занимаюсь коммерческим программированием уже более 15 лет,\\nуправляю командами с 2017 года, работал в Яндексе, TradingView, Skillbox, Arcadia. В статье вы\\nузнаете историю, как написать целый проект на Kotlin Multiplatform, сколько это стоит и сколько\\nобойдется интеграция с точки зрения цифр.\\nЭтот доклад - текстовая версия моего выступления с осеннего Мобиуса 2024 года. Обязательно\\nприложу видео с выступлением после выхода.\\nО чем поговорим в статье:\\nПройдем путь от идеи Kotlin Multiplatform до её реализации в цифрах.\\nОбсудим, сколько стоило внедрить Kotlin Multiplatform в продукты и к чему это привело.\\nGuitariz 14 минут назад\\nЗаезжаем в Kotlin Multiplatform. Но какой ценой?\\n14 мин\\n85\\nKotlin*,\\xa0Разработка мобильных приложений*\\nКАК СТАТЬ АВТОРОМ\\nЗарплаты айтишников\\nНайдено: весенние мероприятия для айти…\\n', 'Проведём ретроспективу: стоило ли это делать вообще и стоит ли инвестировать в эту\\nтехнологию дальше. А также разберём, сколько это будет стоить в перспективе.\\nРазбираемся с KMP и CMP: ключевые термины перед стартом\\nСначала немного вводных: разберемся с сокращениями, которые часто буду использовать в\\nстатье.\\nKMP (Kotlin Multiplatform) – это технология, которая позволяет шарить код, написанный на Kotlin,\\nмежду разными платформами (Android, iOS, JVM, Desktop, Web, etc.)\\nCMP (Compose Multiplatform) — UI, который можно рисовать с помощью Jetpack Compose и\\nпортировать на разные устройства. Сейчас официально поддерживаются две платформы:\\nAndroid и Desktop. iOS находится в стадии публичной беты, а Web – в альфе.\\nМы сконцентрируемся на трёх таргетах – Compose Multiplatform для Android, Desktop и iOS.\\nОстальные в статье рассматривать не будем, так как они достаточно редкие или имеют далекий\\nот стабильности статус разработки.\\nЗапуск проекта на KMP + Compose/SwiftUI: архитектура, паттерны и подходы\\nВ апреле 2024 мы начали разработку нового проекта. В качестве базового стека выбрали KMP в\\nсвязке с Compose Multiplatform и SwiftUI.\\nОсновной подход – использование паттерна MVVM для шаринга кода между экранами разных\\nплатформ. В мультиплатформенной части находятся DI, бизнес-логика и переводы, которые\\nпоставляются в виде SDK. Каждому экрану доступна своя ViewModel.\\nДля Android и iOS мы реализовали следующий подход:\\nAndroid: активити выступает хостом для Composable-функции из библиотеки, а также\\nпредоставляет доступ к нативным компонентам.\\niOS: нативный UI на SwiftUI и платформенные связки (например, пуш-уведомления,\\nинтеграция с библиотекой авторизации) реализованы отдельно, но используют общее SDK.\\nВыбор Compose Multiplatform позволил нам легко адаптировать UI для новых таргетов в будущем.\\nАрхитектура проекта: зонтик-модуль, Shared-логика и работа с Compose SDK\\nНаша схема работы следующая: серыми блоками обозначены элементы, которые мы написали\\nодин раз и практически не изменяем, а зелеными — те, к которым мы обращаемся и вносим\\nизменения довольно часто.\\n', 'У нас есть зонтик-модуль под названием Shared. По сути, это вся логика, написанная на Kotlin\\nMultiplatform, собранная в одном месте, к этому модулю подключаются другие. Например,\\nModuleShared (какая то отдельная функциональность, например, профиль), который может\\nссылаться на стандартные решения: API, ViewModel, различные вспомогательные инструменты\\n— переводы, логирование, аналитику и прочее.\\nЗонтик-модуль мы подключаем к ComposeSdk для того, чтобы получить мультиплатформенный\\nтаргет, который хостится в Android-приложении. Compose Sdk также делится на отдельные\\nмодули, где имплементированы UI отдельных бизнес фич. А iOS, соответственно, берет просто\\nбизнес-логику Shared целиком и у себя уже имплементирует UI.\\nЭксперимент 1. Про интеграцию KMP в цифрах\\nПервое, о чем хочу рассказать, — это сколько стоит интеграция библиотеки Kotlin Multiplatform в\\nмегабайтах и секундах. Мегабайты показывают, насколько ваше приложение увеличится в\\nразмере после добавления зависимости. Секунды отражают время, необходимое для горячего\\nперезапуска. Про холодный запуск я тоже упомяну, но в первую очередь нас интересует именно\\nгорячий — то, сколько секунд будет уходить на повторную сборку. Это важный параметр,\\nпоскольку он влияет на комфорт работы разработчиков. Мы будем часто сталкиваться с этим\\nпроцессом в работе, поэтому важно заранее понять, насколько это критично.\\nУ нас есть таблица с данными. Мы будем постепенно добавлять зависимости в наш проект,\\nтестировать его на трех платформах и сравнивать размеры и время, затрачиваемое на горячую\\nсборку. Давайте начнем с разбора структуры таблицы.\\n', 'Начнем с чистого проекта. Ну, он не совсем чистый, потому что в Android там добавлен Compose\\nи Kotlin — ведь вы вряд ли будете писать новое Android-приложение без них. То же самое\\nдобавлено сразу в JVM Target.\\nПолучаем следующие цифры:\\nГорячая пересборка (Hot Reload) везде примерно по одной секунде. Размеры нативного iOS-\\nприложения — 0,2 мегабайта (200 килобайт). Android — 5.25, десктоп — 67.85 МБ.\\nПо умолчанию Compose тянет в десктоп-приложение довольно много всего, потому что там\\nвнутри JVM (я рассматриваю сейчас таргет под MacOS). Это JVM, которая запускается на Mac OS\\nи тянет за собой немало. Но в целом 67 МБ для нативного приложения, на мой взгляд, — вполне\\nадекватный размер. Предположу, что на этом этапе мы не скажем: \"всё, хватит, эксперимент\\nокончен, это слишком много для приложения, и мы не будем его разрабатывать\". По моему\\nмнению, для десктопа это разумная цифра для старта.\\xa0\\nТеперь посмотрим, как будут расти зависимости.\\n', '· Добавляем Kotlin\\nОчевидно, он уже есть у нас в Android и также в десктопе. Добавление в iOS весит 2,5 мегабайта.\\nЭти замеры сделаны на версии 2.0.20, но в зависимости от версии языка они могут отличаться.\\nЯ наблюдаю за этим начиная с Kotlin 1.6 — тогда он весил около 1.7 МБ, а сейчас вырос до 2.5, в\\nцелом, 2 мегабайта — это все еще немного.\\n·\\xa0 Добавляем Coroutines\\nНа Android получается 7,5 мегабайт, а десктоп почему-то не вырос. Там вообще будут разные\\nнюансы, связанные с размером десктопа, — он не всегда растет пропорционально с другими\\nплатформами.\\nВот такие цифры у меня получились: iOS вырос на 400 килобайт. На мой взгляд, это отличный\\nрезультат, потому что Coroutines — это мощный инструмент для асинхронной работы, в том числе\\nи в iOS. Под капотом там целый комбайн для работы с асинхронщиной.\\n', '·\\xa0 Добавляем DateTime\\nAndroid практически не растет, потому что использует под капотом нативный DateTime. Десктоп\\nнезначительно увеличивается на несколько килобайт. А iOS — внимание — 7.6 мегабайта!\\nПочему так? Вероятно, пока библиотека не находится в стабильной версии и “тянет” что-то\\nлишнее. Coroutines добавляют 400 килобайт, а DateTime — 5 мегабайт.\\nВ этот момент горячая сборка всех трёх проектов у меня на M1 занимала примерно 20 секунд с\\nнуля. Повторная сборка — по одной секунде.\\n·\\xa0 Добавляем View-Model\\nВ Android она практически ничего не добавляет. Десктоп почему-то стал меньше (помним про\\nвозможные ошибки в измерении), iOS тоже не вырос. Были небольшие изменения на уровне\\nбайтов и килобайтов, но общий размер в 7.6 мегабайта пока выглядит компактным.\\n· \\xa0 Добавляем Ktor\\nВойти\\nМоя\\nлента\\nВсе\\nпотоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп\\n', 'Это сетевой слой, который умеет ходить в сеть и работает с Coroutines. Android вырастает на 1\\nмегабайт, десктоп — на 3 мегабайта, а iOS — на 5.3 мегабайта. Здесь я использовал версию Ktor\\n3.0.0 RC1.\\n·\\xa0 Добавляем SqlDelight для базы данных\\nВ Android добавляется совсем немного, так как используется нативный драйвер. В iOS прирост\\nминимален по той же причине — применяется нативный SQL-драйвер. А вот в десктопе\\nдобавляется заметно больше, поскольку там отсутствует встроенное решение для SQL. Вместо\\nэтого подтягивается драйвер JDBC, что добавляет 14 мегабайт. Размер итогового приложения\\nдля десктопа — 85 мегабайт, что по-прежнему укладывается в разумные пределы. Мой\\nсубъективный психологический барьер в 100 мегабайт пока не достигнут.\\n·\\xa0 Добавляем Koin\\n', 'Для организации DI-сборки приложение увеличивается на 100 килобайт — как в Android, так и\\nпримерно столько же в iOS. В десктопе прирост больше — около 400 килобайт.\\nCompose\\xa0\\nВ Android и десктопе не добавляет ничего, потому что он был включен с самого начала. В iOS\\nдобавляет 45 мегабайт. Если просто загрузить коробочное решение, то Compose занимает около\\n30 мегабайт, в наших приложениях это все еще далеко не самая крупная библиотека.\\nКакие выводы здесь можно сделать?\\n1. Собирается все очень быстро\\nДесктоп компилируется примерно в два раза медленнее, но это по-прежнему укладывается в\\nрамки секунд. На этапе сборки проблем не возникает.\\n2. На KMP можно разрабатывать библиотеки\\n', 'Подключая стандартный набор инструментов Android, мы получаем итоговый вес около 10\\nмегабайт.\\n3. Если использовать только стандартный Kotlin, можно уложиться в пару мегабайт\\nЭтот размер включает в себя очень много кода. Если добавить свой код — то прирост будет\\nминимальным и может уложиться всего в пару мегабайт. На практике это практически незаметно.\\n4. На CMP писать библиотеки сложнее, но тоже можно (около 47 мб)\\nСчитаю, что 47 мегабайт для iOS на сегодняшний день не являются критическим барьером для\\nсоздания библиотеки. Современные телефоны редко имеют объем памяти меньше 128 гигабайт,\\nтак что 47 мегабайт вряд ли создадут проблемы.\\nЕсли же разрабатывается простой экран, возможно, стоит задуматься, нужно ли это. Но для\\nболее сложных решений, например, платежных систем или систем интеграции заметок, такой\\nподход вполне оправдан.\\n5. Десктоп весит и собирается больше остальных, но числа — адекватные\\nОн немного тяжелее из-за отсутствия нативного UI и необходимости добавления драйвера JDBC.\\nОднако в целом показатели остаются в пределах нормы.\\nЭксперимент 2. Сколько стоит написание модуля в байтах\\nТеперь измерим, сколько добавляет нам написание одного модуля в проекте. Для примера я\\nнаписал модуль логина. Хотел запустить десктоп с авторизацией —\\xa0как раз такого модуля для\\nдесктопа мне и не хватает, для iOS и Android библиотеки уже есть.\\nМодуль содержит:\\nТри API — ручки на Ktor\\nПроперти в SharedPreferences. Замечу, что класть токены туда нельзя, но в данном случае\\nэто использовано для примера.\\nMVVM и Coroutines под капотом.\\nЭкран на Compose Multiplatform\\nТакже я продублировал его на SwiftUI для сравнения размеров.\\nРезультаты\\n', 'Добавляя зависимости, получаем прирост:\\nAndroid — 12 мегабайт.\\niOS — 50 мегабайт, второй вариант — 51 мегабайт из-за отличий в версиях библиотек.\\nРазмеры сборок иногда бывают разными, что стоит учитывать. Лично у меня это поведение\\nвстречалось, и лучше все перепроверить у себя.\\nКакие выводы тут можно сделать?\\n1. Размер приложения растет равномерно и линейно\\nИсходя из этого, можно долго писать код, не опасаясь, что он превратится в «монстра» на сотни\\nмегабайт.\\n2. Все приложения растут равномерно (можно ориентироваться на размер Android)\\nПонимание линейного роста важно для перспективы. Например, если команда из 10 человек\\nразрабатывает приложение в течение двух лет, итоговый размер на iOS можно легко\\nспрогнозировать, добавив небольшую дельту к текущим показателям.\\n3. Артефакт со сборкой iOS нужно изучить (плавающий инкремент)\\n4. На время сборки новые модули практически не влияют\\n5. Модульная архитектура не влияет на размер итогового приложения\\n> Это подтверждает, что можно создавать гибкие мультимодульные решения, не опасаясь\\nзначительного роста веса программы.\\nЭксперимент 3. Перенос Android Compose на CMP\\nТеперь посмотрим на то, сколько стоит перенести фичу с Андроида на CMP. Я занимался этим в\\nрамках запуска десктопа — переносил дизайн-систему с Android на Compose Multiplatform.\\n', '1) В дизайн-системе у нас было 400 картинок, написанных в формате SVG. Все это мы вытащили\\nв Kotlin-код в Android-библиотеку и прикрутили к Composable теме.\\nЗадача заключалась в том, чтобы из Android-таргета получить мультиплатформенный.\\nДля этого нам понадобилось\\n1) Первое изменение — это changelog при перегенерации картинок. Я перегонял их из SVG в код\\nна Kotlin. На это в реальности ушло около 30 минут. Но на код-ревью можно получить 36 тысяч\\nстрок changelog’a, просто перенеся картинки из SVG в Kotlin.\\n2) Я переработал примерно 2000 строк кода — 2000 удалил и 2000 добавил. Суммарно потратил\\nгде-то 3 человека-дня.\\nЯ делал это не в первый раз, поэтому без опыта вы потратите немного больше, стоит\\nзакладывать в два раза больше времени.\\nЧто было:\\n·\\xa0 Была довольно понятная логика, ничего такого, что нельзя было бы отревьюить с помощью\\nдвух пул-реквестов.\\n·\\xa0 Один хотфикс — сломались шрифты. Мы использовали Compose 1.6.11. В Compose 1.7.0 этой\\nпроблемы уже нет.\\n·\\xa0 Постоянная, но легкая боль, так как превью в CMP + Android Studio не работает. Решение есть:\\nможно написать превью в Android-части. Оно будет ссылаться на Compose Multiplatform и\\nотображать его, придётся держать два экрана, но это работает.\\nНам помогали:\\n·\\xa0 Самопальный генератор верстки из Figma в Kotlin.\\n·\\xa0 Плагин для переноса Svg to Compose.\\n·\\xa0 Бесконечная вера в то, что нам это пригодится.\\nМы реально поверили в то, что нам это надо, и сделали. И нам это действительно пригодилось.\\nРезультаты\\n1. У нас получилась дизайн-система на десктопе.\\nМожно запускать ее без эмулятора, сравнивать поведение, с помощью мышки, менять размер\\nэкрана, смотреть на поведение компонентов и так далее. Это действительно используют сейчас,\\nи мы можем экспериментировать с версткой уже в десктопе.\\n2. С точки зрения ресурсов — это три человеко-дня.\\n', 'В дальнейшем поддержка и написание на Compose Multiplatform или Jetpack Compose —\\xa0для\\nменя и команды особой разницы нет.\\xa0\\n3. Дизайнеры благодарны.\\nОни теперь могут запускать дизайн-систему у себя на макбуке и сразу видеть результат своей\\nработы.\\nПроизводительность KMP: тестируем запуск на Android, iOS и Desktop\\nТеперь мы попробуем измерить загрузку первоначального экрана, запуска кода и оценим,\\nсколько это будет стоить.\\nЗапуск в попугаях\\nМеряем в \"попугаях\", потому что реальные цифры будут зависеть от эмулятора и реального\\nустройства — объема памяти, процессора и прочих характеристик. Но даже в таком формате мы\\nсможем получить ориентировочные данные.\\n1. Запускаем пустой проект с добавленным Kotlin Multiplatform.\\n2. Запускаем в нем Coroutines. Запустим одну джобу и смотрим, когда она будет выполнена.\\n3. Затем запускаем то же самое, но с экраном на Compose Multiplatform и фиксируем, сколько\\nмиллисекунд уходит с момента запуска приложения до первого показа экрана.\\nЭто не перформанс-ревью, а тестирование того, сколько времени займет старт, и можно ли\\nинтегрировать это в приложение так, чтобы процесс оставался комфортным.\\nРезультаты\\n·\\xa0 Пустой проект в Android запускается за 165 миллисекунд на эмуляторе. Десктоп на старте\\nзанял 257 мс, а iOS — внезапно 101 мс. На симуляторе он запускается быстрее всего.\\nКонечно, iOS здесь немного \"читерит\", предоставляя симулятор, который фактически работает на\\nmacOS без дополнительных прослоек. В то время как десктоп использует JVM, а Android\\nработает через стандартный эмулятор, что влияет на результаты. Числа получены на M1\\nMacBook с 50 вкладками в браузере и четырьмя мессенджерами — классическая ситуация для\\nразработчика.\\n·\\xa0 Добавляем вызов корутины.\\nЧисло становится чуть меньше. На самом деле Android в начале показывает некоторые\\nколебания, но главное, что разница в конечном счете несущественная.\\nДесктоп ведет себя так же, потому что там JVM под капотом. Собственно, он дает примерно те же\\nрезультаты. И внезапно симулятор на iOS выдает меньше времени, чем на старте. Наверное,\\n', 'потому что я закрыл вкладку с любимым видео у себя на макбуке, поэтому он стал работать\\nнемного быстрее.\\n·\\xa0 Добавляем Compose экран.\\nПолучаем 163 мс в Android, 280 мс в десктопе и 73 мс в симуляторе iOS.\\nКакие выводы тут можно сделать?\\n1. Kotlin на iOS не влияет вообще.\\nА еще работает быстрее, чем в нативном Android. Шутка, потому что размер был в попугаях.\\nБерите сами, замеряйте, сравнивайте, принимайте для себя решения.\\n2. Compose/Coroutines тоже не влияют на запуск приложения.\\nВы видели сами цифры — они никак не повлияли на запуск пустого проекта или последнего со\\nвсем стафом внутри.\\nЭксперимент 4. Запуск Desktop на основе Android\\nЕсть приложение, написанное на Android, и я хочу его запустить у себя на макбуке.\\n1. Имеем андроид, написанный на технологиях выше.\\n2. Используем собирающийся десктоп проект.\\nОн у нас компилировался и собирался для прохождения проверок целостности кода, но при\\nзапуске сразу падал с ошибкой в рантайме. В некоторых местах мы использовали expect-actual\\nфункции, оставляя заглушки в десктоп-версии. Например, не подключили драйвер для базы\\nданных и не добавили поддержку Ktor. В общем, реальной работоспособности у него не было.\\n3. Надо прикрутить авторизацию (нет библиотеки).\\nМы используем нативные библиотеки авторизации в Android и iOS, а для десктопов такого\\nрешения нет (раньше мы разбирали пример написания такой либы).\\nВ итоге на разработку я потратил четыре человека-дня, сделал пять пул-реквестов и долго\\nбуксовал, потому что писать свою авторизацию на нативном API — задача, мягко говоря, не\\nсамая увлекательная.\\nКроме того, кейс сам по себе редкий — в основном используется в ТВ-станциях и подобных\\nустройствах. Например, когда нужно отсканировать QR-код, перейти в браузер для авторизации и\\nзатем вернуться в приложение.\\nИз-за редкости кейса пришлось разбираться и копаться в деталях. В результате проект я\\nзапустил за три часа, а на прикручивание авторизации потратил еще три дня.\\n', 'Какие выводы тут можно сделать?\\n1. Многие штуки можно переиспользовать с Android.\\xa0\\nНапример, у нас работает Ktor + OkHTTP, можно взять его прямо из Android и подключить в\\nдесктоп.\\n2. Секьюрных преференсов нет.\\nПоэтому хранить данные аккаунта и токен там не стоит.\\n3. Библиотек немного, а те, что есть, не всегда и не во всем поддерживают десктоп.\\nНо, тем не менее, SqlDelight, Room и Ktor доступны и работают хорошо.\\n4. UX надо адаптировать, но это решаемая проблема.\\nЕсли вы захотите поддержать планшетную верстку, так или иначе придется это сделать.\\n5. В целом, все работает так же, как и на других платформах.\\nCompose на десктопе — ничего сверхъестественного.\\nЭкономия времени и ресурсов: плюсы и минусы мультиплатформенного подхода\\n1. Мы пишем одно ядро приложения один раз.\\n2. UI пока пишется дважды, но UX адаптирован.\\nЕдиная ViewModel позволяет сократить работу. Например, показы тостов и разрешений\\nвызываются одинаково.\\n3. Преимущество CMP с ненативным UI пока не очевидно ни по скорости запуска, ни по\\nвремени работы.\\nТем более это все еще бета-версия, поэтому поддержка может быть сложной. JetBrains\\nпостепенно подтягивает недостающие компоненты, Material 3 уже есть в Compose 1.7.0.\\n4. Баги исправляются сразу на двух платформах, что экономит время.\\nНа тестах тоже экономим, потому что бизнес-логику можно проверить на одной платформе, а она\\nбудет работать и на второй. Но тестировать UI все равно придется отдельно, так как приложения\\nхоть и похожи, но разные.\\n5. Экономия времени\\nЯ оцениваю экономию в 30%. Мы делим 100% времени пополам на Android и iOS. Android делим\\nна 30% для бизнес-логики и 20% на UI. С iOS забираем те же 30% логики, оставляя 70%.\\n', 'Экономия — 30%.\\nКонечно, все зависит от того, как вы пишете код, и сколько времени уходит на бизнес-логику или\\nUI. 50% сэкономить невозможно, так как Android-приложение не запустится на iOS без доработок.\\nТестирование в мультиплатформе: где экономим, а где —\\xa0нет\\n·\\xa0 Дешевле писать тесты\\nТесты на KMP можно запускать везде, сравнивать результаты и хостить.\\n·\\xa0 Из-за разных UI тестировать их придется отдельно.\\nПроверять баги на промежуточной платформе тоже необходимо. Если мы не уверены на 100%,\\nчто проблема в бизнес-логике, а тестировщики тоже не могут это подтвердить, баг все равно\\nпридется проверять на обеих платформах.\\n·\\xa0 Экономия на половине тестов может составить примерно 10% от общего объема работ по\\nтестированию.\\nНо это расчет в условных единицах — «в попугаях». Здесь каждый может для себя\\nвизуализировать цифры по-своему.\\n·\\xa0 Багов в целом получается объективно меньше.\\nКодовая база сокращается, у нас один источник истины, и изменения вносятся в одном месте.\\nЭто снижает количество точек, где могут возникнуть баги.\\nКонечно, с увеличением числа поддерживаемых платформ нагрузка возрастает, но общая выгода\\nот подхода дает профит.\\nСборка мультиплатформенного проекта: ожидания vs реальность\\n·\\xa0 Сборка проектов у нас идет параллельно.\\nЭто важно, так как позволяет проверять работу кода одновременно на Android и iOS. Общее\\nвремя сборки при этом определяется самым долгим пайплайном.\\nВ нашем случае это обычно iOS, хотя проблема связана не столько с самой платформой, сколько\\nс виртуальными машинами, которые мы используем.\\n·\\xa0 Работа с единым источником истины.\\nКогда в core-компоненте вносятся изменения, сборка триггерится сразу на обоих таргетах. После\\nэтого проверяем, работают ли изменения и подтянулись ли они корректно.\\n·\\xa0 Сборка для iOS может быть довольно проблемной.\\n', 'Интеграция с KMP в Xcode оставляет желать лучшего (впрочем, вряд ли можно рассчитывать на\\nэто со стороны Apple), и добавление зависимостей на этапе pull request может превратиться в\\nквест “угадай, почему xcode это не видит”.\\nЗатаскивание новых файлов в iOS-проект — отдельная боль. Добавьте сюда еще использование\\nсторонних систем сборки, которые только усложняют процесс.\\nПо факту мультиплатформенный подход не всегда упрощает процесс сборки, а часто наоборот —\\nдобавляет сложности.\\n·\\xa0 Можно начинать разработку с Android.\\nЭто позволяет не тратить время на постоянные сборки и проверки iOS на ранних этапах. Логику и\\nUI тестируем сначала в Android. После этого переносим в iOS, минимизируя доработки и отладку.\\nЭтот подход экономит время, хотя иногда приходится возвращаться назад для внесения\\nизменений. Android собирается быстрее, так как для него это нативный процесс.\\n·\\xa0 Для iOS сборка идет медленнее Android (хотя это все еще измерение в попугаях).\\nЕсли говорить в цифрах:\\nAndroid — 1 минута 15 секунд против iOS — 2,5 минуты.\\nСуммарно весь пайплайн у нас работает примерно в 2 раза дольше для iOS, чем для Android.\\nПочему с iOS все не просто\\nВ iOS есть свои особенности сборки и дополнительные сложности.\\nНекоторые из них связаны с интеграцией библиотек и зависимостей, другие — с особенностями\\nвзаимодействия с системами сборки и пайплайнами.\\nТранзитивные зависимости не работают. Нужен модуль-зонтик для подключения и мержа\\nзависимостей. Отсюда все недостатки зонтичной системы наследования.\\nВсе не так драматично (в числах все равно получается довольно быстро). Мы получаем\\nнезначительные изменения в разных платформах. Но сам подход написания большого\\nзонтичного модуля, хотелось бы видеть только на этапе сборки конечной программы, а не\\nпромежуточного модуля.\\nЧто в итоге\\nИспользование Kotlin Multiplatform в разработке оказалось не таким болезненным, как может\\nпоказаться на первый взгляд. Общая бизнес-логика действительно позволяет сократить затраты\\nна разработку примерно на 30%, а аналогичную экономию можно получить и на тестировании.\\nПри этом использование Jetpack Compose уже сейчас выглядит оправданным шагом, особенно\\nесли закладывать возможность перехода на Compose Multiplatform в будущем — это бесплатно и\\nможет дать преимущества. Однако полностью переходить на CMP пока рискованно, поскольку\\n']\n",
      "Заезжаем в Kotlin Multiplatform. Но какой ценой_ _ Хабр.pdf\n",
      "['Привет! Меня зовут Денис, я занимаюсь коммерческим программированием уже более 15 лет,', 'управляю командами с 2017 года, работал в Яндексе, TradingView, Skillbox, Arcadia. В статье вы', 'узнаете историю, как написать целый проект на Kotlin Multiplatform, сколько это стоит и сколько', 'обойдется интеграция с точки зрения цифр.', 'Этот доклад - текстовая версия моего выступления с осеннего Мобиуса 2024 года. Обязательно', 'приложу видео с выступлением после выхода.', 'О чем поговорим в статье:', 'Пройдем путь от идеи Kotlin Multiplatform до её реализации в цифрах.', 'Обсудим, сколько стоило внедрить Kotlin Multiplatform в продукты и к чему это привело.', 'Guitariz 14 минут назад', 'Заезжаем в Kotlin Multiplatform. Но какой ценой?', '14 мин', '85', 'Kotlin*,\\xa0Разработка мобильных приложений*', 'КАК СТАТЬ АВТОРОМ', 'Зарплаты айтишников', 'Найдено: весенние мероприятия для айти…', '']\n",
      "['404.32\\nРейтинг\\nАльфа-Банк\\nЛучший мобильный банк по версии Markswebb\\nПодписаться\\nВ Альфа-Банке мы внедрили ИИ-агентов, которые проектируют, разрабатывают и проверяют\\nавтотесты. При этом полностью автономно, как QA-инженеры, но в разы быстрее и точнее.\\nПодобных примеров, когда ИИ разрабатывает автотесты от анализа требований до пул-реквеста,\\nв нашей стране, пожалуй, ещё не было.\\nЧто умеют агенты?\\nАнализировать контекст из Jira и Confluence, вычленяя суть задачи.\\nПрогнозировать риски, зависимости и даже «пограничные» сценарии.\\nГенерировать DTO для REST API и превращать ручные сценарии в Java-тесты за минуты.\\nСверять код с бизнес-логикой и техстандартами Альфы, защищая прод от случайных\\nошибок.\\nСоздавать вариативные проверки — от позитивных кейсов до сложных негативных условий.\\nАвтоматизировать рутину.\\nВ ИИ-команде QA есть несколько агентов, каждый работает над своей частью из перечная выше.\\nСейчас решение пилотируется в нескольких продуктовых командах, но результаты уже видны:\\nAlfaTeam 2 часа назад\\nИИ-агенты в Альфа-Банке: нейросети создают автотесты без\\nучастия человека\\n1 мин\\n717\\nБлог компании Альфа-Банк,\\xa0Искусственный интеллект,\\xa0Тестирование IT-систем*\\nВойти\\nМоя\\nлента\\nВсе\\nпотоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп\\nКАК СТАТЬ АВТОРОМ\\nЗарплаты айтишников\\nМиграция за ночь? Легко! Без сна и с адре…\\n', 'Альфа-Банк\\nЛучший мобильный банк по версии Markswebb\\nСайт\\nХабр Карьера\\n14\\nКарма\\n0.3\\nРейтинг\\n@AlfaTeam\\nПользователь\\nСайт\\nFacebook\\nВКонтакте\\nДругие новости\\nmefdayy 19 минут назад\\nNvidia и Broadcom тестируют передовой процесс производства чипов\\nIntel\\n2 мин\\nменьше ошибок в проде, предсказуемые дедлайны и высвобожденные ресурсы для творческих\\nзадач.\\n«Одна команда ИИ-агентов экономит десятки часов работы, увеличивает скорость релизов и\\nнаходит на 30% больше багов»\\nСвятослав Соловьев, Директор по генеративному ИИ в ИТ Альфа-Банка.\\nВ ближайшее время подробнее опишем как устроены агенты, какие технологии используем и как\\nмы измеряем их эффективность.\\nТеги: автоматизация, искусственный интеллект, тестирование, автотесты\\nХабы: Блог компании Альфа-Банк, Искусственный интеллект, Тестирование IT-систем\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\nЭлектропочта\\nКомментарии 11\\n123\\nПодписаться\\n']\n",
      "ИИ-агенты в Альфа-Банке_ нейросети создают автотесты без участия человека _ Хабр.pdf\n",
      "['404.32', 'Рейтинг', 'Альфа-Банк', 'Лучший мобильный банк по версии Markswebb', 'Подписаться', 'В Альфа-Банке мы внедрили ИИ-агентов, которые проектируют, разрабатывают и проверяют', 'автотесты. При этом полностью автономно, как QA-инженеры, но в разы быстрее и точнее.', 'Подобных примеров, когда ИИ разрабатывает автотесты от анализа требований до пул-реквеста,', 'в нашей стране, пожалуй, ещё не было.', 'Что умеют агенты?', 'Анализировать контекст из Jira и Confluence, вычленяя суть задачи.', 'Прогнозировать риски, зависимости и даже «пограничные» сценарии.', 'Генерировать DTO для REST API и превращать ручные сценарии в Java-тесты за минуты.', 'Сверять код с бизнес-логикой и техстандартами Альфы, защищая прод от случайных', 'ошибок.', 'Создавать вариативные проверки — от позитивных кейсов до сложных негативных условий.', 'Автоматизировать рутину.', 'В ИИ-команде QA есть несколько агентов, каждый работает над своей частью из перечная выше.', 'Сейчас решение пилотируется в нескольких продуктовых командах, но результаты уже видны:', 'AlfaTeam 2 часа назад', 'ИИ-агенты в Альфа-Банке: нейросети создают автотесты без', 'участия человека', '1 мин', '717', 'Блог компании Альфа-Банк,\\xa0Искусственный интеллект,\\xa0Тестирование IT-систем*', 'Войти', 'Моя', 'лента', 'Все', 'потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп', 'КАК СТАТЬ АВТОРОМ', 'Зарплаты айтишников', 'Миграция за ночь? Легко! Без сна и с адре…', '']\n",
      "1 404.32 Альфа-Банк Лучший мобильный банк по версии Markswebb Подписаться\n",
      "['2376.9\\nРейтинг\\nRUVDS.com\\nVDS/VPS-хостинг. Скидка 15% по коду HABR15\\nАвтор оригинала: Lahiru Hewawasam\\n 4 часа назад\\nПростой9 мин\\nru_vds\\nИнструменты наблюдаемости, о которых\\nнужно знать в 2023 году\\n752\\nБлог компании RUVDS.com\\n, \\nOpen source*\\n, \\nХранение данных*\\n, \\nОблачные сервисы*\\n, \\nМик\\nПеревод\\nКАК СТАТЬ АВТОРОМ\\n', 'Когда организации переходят в облако, их системы тоже начинают\\nстремиться к распределённым архитектурам. Один из самых\\nраспространённых примеров этого — использование микросервисов.\\nОднако это также создаёт новые сложности с точки зрения\\nнаблюдаемости.\\nНеобходимо подбирать подходящие инструменты для мониторинга,\\nотслеживания и трассировки этих систем при помощи анализа\\nвыходных результатов посредством метрик, логов и трассировок. Это\\nпозволяет командам разработчиков быстро выявлять первопричины\\nпроблем, устранять их и оптимизировать производительность\\nприложений, ускоряя выпуск кода.\\nВ этой статье мы рассмотрим возможности, ограничения и важные\\nособенности одиннадцати популярных инструментов\\nнаблюдаемости, что позволит вам выбрать наиболее подходящий\\nдля вашего проекта.\\nHelios\\n', 'Helios — это решение по обеспечению наблюдаемости для\\nразработчиков, предоставляющее информацию по всему потоку\\nприложений. Оно включает в себя фреймворк распространения\\nконтекста OpenTelemetry и обеспечивает наблюдение за\\nмикросервисами, serverless-функциями, базами данных и\\nсторонними API. Можно протестировать песочницу продукта или\\nиспользовать его бесплатно, зарегистрировавшись здесь.\\n▍ Основные возможности\\n• Обеспечение полного контроля: Helios предоставляет\\nинформацию о распределённой трассировке в полном\\nконтексте, показывает, как передаются данные через всё\\nприложение в любом окружении.\\n• Визуализация: позволяет пользователям собирать и\\nвизуализировать данные трассировок из множественных\\nисточников данных, чтобы исследовать и устранять\\nпотенциальные проблемы.\\n• Многоязыковая поддержка: поддерживает множество языков и\\nфреймворков, в том числе Python, JavaScript, Node.js, Java,\\nRuby, .NET, Go, C++ и Collector.\\n• Обмен и многократное использование: вы с лёгкостью можете\\nсотрудничать с участниками команды, обмениваясь через Helios\\nтрассировками, тестами и триггерами. Кроме того, Helios\\n', 'позволяет многократно использовать запросы и полезные\\nнагрузки между участниками команды.\\n• Автоматическая генерация тестов: автоматически генерирует\\nтесты на основании данных трассировок.\\n• Простота интеграций: интегрируется в существующую\\nэкосистему, включая логи, тесты, мониторинг ошибок и многое\\nдругое.\\n• Воссоздание процессов: Helios всего за несколько кликов\\nпозволяет в точности воссоздавать рабочие процессы, в том\\nчисле HTTP-запросы, сообщения Kafka и RabbitMQ, а также\\nвызовы Lambda.\\n▍ Популярные способы использования\\n• Распределённая трассировка\\n• Интеграция трассировок в многоязыковое приложение\\n• Наблюдаемость serverless-приложения\\n• Устранение неполадок в тестах\\n• Информация о вызовах API\\n• Анализ и выявление узких мест\\nPrometheus\\n', 'Prometheus — это опенсорсный инструмент, широко используемый\\nдля обеспечения наблюдаемости в нативных облачных окружениях.\\nОн может собирать и хранить данные временных\\nпоследовательностей и предоставляет инструменты визуализации\\nдля анализа собранных данных.\\n▍ Основные возможности\\n• Сбор данных: он может скрейпить метрики из различных\\nисточников, в том числе из приложений, сервисов и систем.\\nТакже он «из коробки» поддерживает множество форматов\\nданных, в том числе логи, трассировки и метрики.\\n• Хранилище данных: он сохраняет собранные данные в базе\\nданных временных последовательностей, позволяя эффективно\\nзапрашивать и агрегировать данные с течением времени.\\n• Система алертов: инструмент включает в себя встроенную\\nсистему алертов, которая может запускать алерты на основании\\nзапросов.\\n• Исследование сервисов: он может автоматически\\nраспознавать и скрейпить метрики сервисов, работающих в\\nразличных окружениях, например, Kubernetes и в других\\n', 'системах управления контейнерами.\\n• Интеграция с Grafana: инструмент имеет гибкую интеграцию с\\nGrafana, позволяющей создавать дэшборды для отображения и\\nанализа метрик Prometheus.\\n▍ Ограничения\\n• Ограниченные возможности анализа первопричин:\\nинструмент в первую очередь предназначен для мониторинга и\\nалертов. Поэтому он не предоставляет встроенных\\nвозможностей аналитики первопричин проблем.\\n• Масштабирование: хотя инструмент может обрабатывать\\nмножество метрик, это может привести к большой трате\\nресурсов, поскольку Prometheus хранит все данные в памяти.\\n• Моделирование данных: содержит модель данных на основе\\nпар «ключ-значение» и не поддерживает вложенных полей и join.\\n▍ Популярные способы применения\\n• Сбор и хранение метрик\\n• Система алертов\\n• Исследование сервисов\\nGrafana\\n', 'Grafana — это опенсорсный инструмент, в первую очередь\\nиспользуемый для визуализации и мониторинга данных. Он\\nпозволяет с лёгкостью создавать интерактивные дэшборды для\\nвизуализации и анализа данных из различных источников.\\n▍ Основные возможности\\n• Визуализация данных: создаёт настраиваемые и\\nинтерактивные дэшборды для визуализации метрик и логов из\\nразличных источников данных.\\n• Система алертов: позволяет настраивать алерты на основании\\nсостония метрик для информирования о потенциальных\\nпроблемах.\\n• Выявление аномалий: позволяет настроить выявление\\nаномалий для автоматического определения и отправки алертов\\nв случае аномального поведения в метриках.\\n• Анализ первопричин: позволяет углубиться в метрики для\\nанализа первопричин, предоставляя подробную информацию с\\nисторическим контекстом.\\n', '▍ Ограничения\\n• Хранение данных: архитектура инструмента не поддерживает\\nдолговременное хранение и для сохранения метрик и логов\\nтребует дополнительных инструментов наподобие Prometheus\\nили Elasticsearch.\\n• Моделирование данных: Grafana не предоставляет\\nрасширенных возможностей моделирования данных. То есть она\\nне позволяет моделировать конкретные типы данных и\\nвыполнять сложные запросы.\\n• Агрегирование данных: Grafana не содержит встроенных\\nфункций агрегирования.\\n▍ Популярные способы применения\\n• Визуализация метрик\\n• Система алертов\\n• Выявление аномалий\\nElasticsearch, Logstash и Kibana (ELK)\\n', 'Стек ELK — это популярное опенсорсное решение, помогающее\\nуправлять логами и анализировать данные. Оно состоит из трёх\\nкомпонентов: Elasticsearch, Logstash и Kibana.\\nElasticsearch — это движок распределённого поиска и аналитики,\\nспособный обрабатывать большие объёмы структурированных и\\nнеструктурированных данных; он позволяет хранить и индексировать\\nбольшие массивы данных, а также выполнять поиск по ним.\\nLogstash — это конвейер сбора и обработки данных, позволяющий\\nсобирать, обрабатывать и обогащать данные из множества\\nисточников, например, файлов логов.\\nKibana — это инструмент визуализации и исследования данных,\\nпозволяющий создавать интерактивные дэшборды и визуализации\\nна основе данных, находящихся в Elasticsearch.\\n▍ Основные возможности\\n• Управление логами: ELK позволяет собирать, обрабатывать,\\nхранить и анализировать данные логов и метрики из множества\\nисточников, предоставляя централизованную консоль для\\nпоиска по логам.\\n• Поиск и анализ: позволяет выполнять поиск и анализ\\nрелевантных данных логов, что критически важно для\\nвыявления и устранения первопричин проблем.\\n• Визуализация данных: Kibana позволяет создавать\\nнастраиваемые дэшборды, которые способны визуализировать\\nданные логов и метрики из множества источников данных.\\n• Выявление аномалий: Kibana позволяет создавать алерты для\\nаномальной активности в данных логов.\\n• Анализ первопричин: стек ELK позволяет глубоко изучать\\nданные логов, чтобы лучше понять первопричины, предоставляя\\nподробные логи и исторический контекст.\\n', '▍ Ограничения\\n• Трассировка: ELK нативно не поддерживает распределённую\\nтрассировку. Поэтому может понадобиться применение\\nдополнительных инструментов наподобие Jaeger.\\n• Мониторинг в реальном времени: архитектура ELK позволяет\\nему качественно выполнять задачи платформы управления\\nлогами и анализа данных. Однако в отчётности логов\\nприсутствует незначительная задержка, и пользователям\\nприходится ждать.\\n• Сложная настройка и поддержка: для платформы требуется\\nсложный процесс настройки и поддержки. Кроме того, для\\nуправления большими объёмами данных и множественными\\nисточниками данных требуются специфические знания.\\n▍ Популярные способы применения\\n• Управление логами\\n• Визуализация данных\\n• Комплаенс и безопасность\\nInfluxDB и Telegraf\\n', 'InfluxDB и Telegraf — это опенсорсные инструменты, популярные\\nблагодаря своим возможностям по хранению и мониторингу данных\\nвременных последовательностей.\\nInfluxDB — это база данных временных последовательностей,\\nхранящая большие объёмы данных временных\\nпоследовательностей и выполняющая запросы к ним при помощи\\nсвоего языка запросов, напоминающего SQL.\\nTelegraf — это хорошо известный агент сбора данных, способный\\nсобирать и отправлять метрики широкому выбору получателей,\\nнапример, InfluxDB. Также он поддерживает многие источники\\nданных.\\n▍ Основные возможности\\nКомбинация из InfluxDB и Telegraf предоставляет множество\\nвозможностей, повышающих наблюдаемость приложений.\\n• Сбор и хранение метрик: Telegraf позволяет собирать метрики\\nиз множества источников и отправлять их в InfluxDB для\\nхранения и анализа.\\n', '• Визуализация данных: InfluxDB можно интегрировать со\\nсторонними инструментами визуализации наподобие Grafana\\nдля создания интерактивных дэшбордов.\\n• Масштабируемость: архитектура InfluxDB позволяет\\nобрабатывать большие объёмы данных временных\\nпоследовательностей и выполнять горизонтальное\\nмасштабирование.\\n• Поддержка множества источников данных: Telegraf\\nподдерживает более двухсот плагинов ввода для сбора метрик.\\n▍ Ограничения\\n• Ограниченные возможности алертинга: в обоих инструментах\\nотсутствуют возможности алертинга и для его подключения\\nнеобходима интеграция сторонних сервисов.\\n• Ограниченный анализ первопроичин: в этих инструментах\\nотсутствуют нативные возможности анализа первопричин и\\nнеобходима интеграция сторонних сервисов.\\n▍ Популярные способы применения\\n• Сбор и хранение метрик\\n• Мониторинг\\nDatadog\\n', 'Datadog — это популярная облачная платформа для мониторинга и\\nаналитики. Она широко используется для получения информации о\\nздоровье и производительности распределённых систем с целью\\nзаблаговременного устранения проблем.\\n▍ Основные возможности\\n• Многооблачная поддержка: пользователи могут выполнять\\nмониторинг приложений, работающих на облачных платформах\\nнескольких поставщиков, например, AWS, Azure, GCP и так\\nдалее.\\n• Карты сервисов: позволяют выполнять визуализацию\\nзависимостей сервисов, местоположений, сервисов и\\nконтейнеров.\\n• Аналитика трассировок: пользователи могут анализировать\\nтрассировки, предоставляя подробную информацию о\\nпроизводительности приложений.\\n• Анализ первопричин: позволяет глубоко изучать метрики и\\nтрассировки, чтобы понять первопричину проблем,\\nпредоставляя подробную информацию с историческим\\nконтекстом.\\n', '• Выявление аномалий: может настраивать систему выявления\\nаномалий, которая автоматически выявляет аномальное\\nповедение в метриках и создаёт алерты о нём.\\n▍ Ограничения\\n• Затраты: Datadog — это облачный платный сервис, стоимость\\nкоторого увеличивается при развёртывании крупномасштабных\\nсистем.\\n• Ограниченная поддержка потребления, хранения и\\nиндексации логов: Datadog по умолчанию не предоставляет\\nподдержку анализа логов. Необходимо отдельно приобретать\\nподдержку потребления и индексации логов. Поэтому\\nбольшинство организаций принимает решение хранить\\nограниченное количество логов, что может вызвать неудобства\\nпри устранении проблем, поскольку отсутствует доступ к полной\\nистории проблемы.\\n• Нехватка контроля за хранением данных: Datadog хранит\\nданные на собственных серверах и не позволяет пользователям\\nхранить данные локально или в дата-центрах компании.\\n▍ Популярные способы применения\\n• Конвейеры наблюдаемости\\n• Распределённая трассировка\\n• Мониторинг контейнеров\\nNew Relic\\n', 'New Relic — это облачная платформа мониторинга и аналитики,\\nпозволяющая выполнять мониторинг приложений и систем в\\nраспределённом окружении. Она использует сервис «New Relic\\nEdge» для распределённой трассировки и способна выполнять\\nнаблюдение за 100% трассировок приложения.\\n▍ Основные возможности\\n• Мониторинг производительности приложений:\\nпредоставляет комплексное решение APM для мониторинга\\nпроизводительности приложений и устранения проблем.\\n• Многооблачная поддержка: поддерживает мониторинг\\nприложений на облачных платформах нескольких поставщиков,\\nнапример, AWS, Azure, GCP и так далее.\\n• Аналитика трассировок: позволяет анализировать\\nтрассировки, предоставляя подробную информацию о\\nпроизводительности системы и приложений.\\n', '• Анализ первопричин: позволяет глубоко изучать матрики и\\nтрассировки для анализа первопричин проблем.\\n• Управление логами: собирает, обрабатывает и анализирует\\nданные логов из различных источников, обеспечивая\\nвсеобъемлющую картину логов.\\n▍ Ограничения\\n• Ограниченная опенсорсная интеграция: New Relic — это\\nплатформа с закрытыми исходниками, поэтому её интеграция с\\nопенсорсными инструментами может быть ограниченной.\\n• Затраты: New Relic может быть более дорогим по сравнению с\\nдругими решениями при работе с крупномасштабными\\nсистемами.\\n▍ Популярные способы применения\\n• Мониторинг производительности приложений\\n• Многооблачный мониторинг\\n• Аналитика трассировок\\nAppDynamics\\n', 'AppDynamics — это платформа мониторинга и аналитики,\\nпозволяющая отслеживать и визуализировать каждый компонент\\nприложения, а также управлять ими. Кроме того, она позволяет\\nвыполнять анализ первопричин для выявления внутренних проблем,\\nкоторые могут влиять на производительность приложения.\\n▍ Основные возможности\\n• Сбор данных: пользователи могут собирать метрики и\\nтрассировки из множества источников: хостов, контейнеров,\\nоблачных сервисов и приложений.\\n• Выявление аномалий: позволяет настраивать систему\\nвыявления аномалий, которая выявляет аномальное поведение\\nи сообщает о нём при помощи алертов.\\n• Аналитика трассировок: пользователи могут анализировать\\nтрассировки и получать подробную информацию о\\nпроизводительности.\\nМониторинг производительности приложений:\\nпредоставляет комплексное решение APM, позволяющее\\nвыполнять мониторинг и устранение проблем\\nпроизводительности приложения.\\n• Ограниченные возможности настройки: по сравнению с\\nдругими инструментами, опции настройки не очень гибки, потому\\n', 'что пользователи не могут настраивать решение\\nсамостоятельно.\\n▍ Популярные способы применения\\n• Мониторинг производительности приложений\\n• Многооблачный мониторинг\\n• Управление бизнес-транзакциями\\nВыбор лучшего инструмента наблюдаемости\\nНаблюдаемость — неотъемлемая часть разработки и эксплуатации\\nсовременного ПО. Она помогает компаниям выполнять мониторинг\\nздоровья и производительности систем и быстро решать проблемы\\nещё до того, как они станут критичными.\\nВ этой статье мы рассказали об одиннадцати лучших инструментах\\nнаблюдаемости, о которых должны знать разработчики при работе с\\nраспределёнными системами. Как видите, каждый инструмент имеет\\nсвои сильные стороны и ограничения. Поэтому чтобы найти\\nподходящий для вас инструмент, следует сравнить его с\\nтребованиями вашей системы. Выбор наилучшего инструмента\\nнаблюдаемости для вашей организации зависит от конкретных\\nпотребностей, таких, как окружение, технологический стек, опыт\\nразработчиков, профили пользователей, требования к мониторингу и\\nустранению проблем, а также рабочие процессы.\\nНадеюсь, эта статья была для вас полезной.\\nTelegram-канал с розыгрышами призов, новостями IT и\\nпостами о ретроиграх \\n️\\n', 'RUVDS.com\\nVDS/VPS-хостинг. Скидка 15% по коду HABR15\\nTelegram\\nВКонтакте\\nTwitter\\n327\\nКарма\\n576\\nРейтинг\\n@ru_vds\\nПользователь\\nКомментарии 3\\nПубликации\\nТеги:  \\nruvds_перевод\\n,\\nнаблюдаемость\\n,\\nobservability\\n,\\ngraphana\\n,\\nprometheus\\n,\\nhelios\\n,\\nлоги\\n,\\nметрики\\n,\\nтрассировка\\nХабы:  \\nБлог компании RUVDS.com\\n,\\nOpen source\\n,\\nХранение данных\\n,\\nОблачные сервисы\\n,\\nМикросервисы\\n+9\\n21\\n3\\nЛУЧШИЕ ЗА СУТКИ \\nПОХОЖИЕ\\n', '', 'Ваш\\nаккаунт\\nРазделы\\nИнформация\\nУслуги\\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\nИНФОРМАЦИЯ\\nСайт\\nruvds.com\\nДата регистрации\\n18 марта 2016\\nДата основания\\n27 июля 2015\\nЧисленность\\n11–30 человек\\nМестоположение\\nРоссия\\nПредставитель\\nruvds\\n', 'Войти\\nРегистрация\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nНастройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "Инструменты наблюдаемости, о которых нужно знать в 2023 году.pdf\n",
      "['2376.9', 'Рейтинг', 'RUVDS.com', 'VDS/VPS-хостинг. Скидка 15% по коду HABR15', 'Автор оригинала: Lahiru Hewawasam', ' 4 часа назад', 'Простой9 мин', 'ru_vds', 'Инструменты наблюдаемости, о которых', 'нужно знать в 2023 году', '752', 'Блог компании RUVDS.com', ', ', 'Open source*', ', ', 'Хранение данных*', ', ', 'Облачные сервисы*', ', ', 'Мик', 'Перевод', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "1 2376.9 RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Автор оригинала: Lahiru Hewawasam\n",
      "['4.58\\nОценка\\n353.18\\nРейтинг\\nХабр\\nЭкосистема для развития людей, вовлеченных в IT\\n8 февраля — день российской науки. Этот праздник отмечается в Академии Наук, в НИИ,\\nвузах, исследовательских лабораториях по всей стране. Судьба российской науки непростая,\\nвсегда переплетённая с историей страны и ею же обусловленная: непростой путь к открытию\\nМГУ, талантливые кулибины (и И.Кулибин) из глубинки, учёные в изгнании, Туполевская шарага,\\nдостижения института Гамалеи… Всё смешано, переплетено и влияло и влияет на весь мир.\\nВеличие российской науки прежде всего в людях, которые её создают, которые разрабатывают,\\nизобретают, экспериментирую и точно знают, что делают.\\xa0\\nВ этот день всегда спорят медики, биологи, инженеры, программисты, филологи, физики, химики\\n— чьи учёные научнее и главнее, кто определял и создавал будущее? Ответ простой: все,\\nкаждый — в своей сфере. Предлагаем сегодня собрать российские\\xa0 открытия и учёных, которые\\nвас вдохновляют, впечатляют, будоражат воображение и влияют (или повлияли) на вас.\\nНапишем эту статью вместе?\\nExosphere 10 часов назад\\nИстория российской науки: напишем вместе?\\nПростой\\n5 мин\\n1.3K\\nБлог компании Хабр,\\xa0Научно-популярное\\nРетроспектива\\nКАК СТАТЬ АВТОРОМ\\nТехнотекст\\nДобавь свой раздел в статью про российскую науку\\nВойти\\nМоя\\nлента\\nВсе\\nпотоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп\\n', 'Правила создания статьи\\nВы в комментариях рассказываете о том самом учёном и его изобретении, которые важны\\nименно для вас, а мы добавляем ваши истории в общую статью и формируем самый крутой\\nлонгрид об истории российской науки.\\nФормат свободный, желательно (не обязательно!) отразить:\\nФИО учёного, его годы жизни\\nосновные изобретения\\nкраткая биография\\nнаиболее значимое изобретение (на ваш взгляд)\\nчем вас вдохновил учёный, как повлиял на вас\\nизображения приветствуются.\\nКомментарии будут сохраняться и вы сможете их обсуждать — так, как и всегда. Автор\\nсамого заплюсованного описания получит приятный сюрприз от команды Хабра.\\nЧур, википедию не копировать!\\nИтак, поехали.\\xa0\\nНиколай Иванович Пирогов (1810 — 1881)\\nвеликий российский врач, хирург\\n', 'Николай Пирогов в 14\\xa0лет поступил на\\xa0первый курс медицинского факультета МГУ (тогда\\nМосковского университета). По\\xa0окончании вуза он в\\xa0числе семи лучших студентов отправился\\nв\\xa0Депт (Тарту) заниматься анатомическими исследованиями и готовиться к\\xa0профессорской\\nдеятельности. Считается одним из\\xa0лучших анатомов мира, родоначальник топографической\\nанатомии. Именно он разрезал замороженное человеческое тело в\\xa0четырёх плоскостях и создал\\nатлас топографической анатомии, который стал незаменимым пособием для\\xa0хирургов. Это\\xa0был\\nне\\xa0просто учебный материал, но\\xa0прежде всего возможность оперировать с\\xa0минимальным\\nтравматизмом для\\xa0пациента.\\nПирогов брался за\\xa0любые операции, от\\xa0ампутаций и извлечения камней до\\xa0глазных\\nманипуляций и сосудистых вмешательств. Именно он провёл в 1847\\xa0году первую свою операцию\\nпод\\xa0наркозом, тщательно изучив свойства эфира и хлороформа. Затем в\\xa0течение года прошло\\n300\\xa0подобных операций.\\nНиколай Пирогов\\xa0— автор алгоритма сортировки раненных на\\xa0поле брани, благодаря чему\\nудалось спасти огромное количество жизней, а\\xa0сам алгоритм до\\xa0сих пор служит организации\\nвоенной медицины.\\nИменно он ввёл в\\xa0практику российской медицины обеззараживание хлорной водой и настойкой\\nйода. До\\xa0сих пор хлорка, йодопирон, йодинол и йод служат хирургам и всем медикам верную\\nслужбу.\\nНиколай Пирогов\\xa0был первым учёным от\\xa0медицины, о\\xa0котором я узнала в\\xa0возрасте 7\\xa0или 8\\xa0лет,\\nуже будучи немного горящей «стать доктором». Жизнь повернулась самым причудливым\\nобразом, но\\xa0изучение истории медицины сделало моё мышление структурным, комплексным и\\n', 'навсегда привило любовь к\\xa0людям. Какие\\xa0бы они ни\\xa0были, они\\xa0— люди, с\\xa0болью, со стрессом,\\nс\\xa0заболеваниями и радостью выздоровления. Совершенно особенное ощущение.\\nХудожественный фильм «Пирогов»\\nГааз и Пирогов\\xa0— лекторий «Достоевский». Хотя конкретно в этом фильме история Гааза\\nпотрясает гораздо больше.\\nСемён Николаевич Корсаков (1787-1853)\\nсоздатель первой в мире перфокарточной машины (табулятора) «для сравнения идей»\\nРассказывает @Motanto\\nЕго история меня когда-то поразила: только подумайте, кто-то что-то делал в программировании\\nдо Бэббиджа, да ещё и в Российской Империи. Ещё больше поразил тот факт, что Корсаков был\\nобвинён современниками в пустой трате времени на изобретения, а его славные дела дошли до\\nмирового научного сообщества и до нас с вами благодаря \"раскопкам\" учёных XX века, в\\nчастности благодаря математику и кибернетику Г.Н. Поварову.\\nПотомственный дворянин, Семён Корсаков стал прародителем отечественной кибернетики. Он\\nвсегда видел своей задачей усиление работы разума с помочью различных механических\\nустройств (чем вам не первобытное программирование?!). Кроме изобретённых им гомеоскопов\\nи идеоскопа, он подарил миру идею перфокарт. Именно гомеоскопические перфорированные\\nтаблицы стали прототипом тех самых картонных карт. Устройства Корсакова помогали\\nсравнивать таблицы и обрабатывать большие массивы данных, с помощью его устройств можно\\nбыло решать задачи классификации. Корсаков видел бытовое применение своим изобретениям\\n(реестры, каталоги, регистрации), ничего не патентовал и просто представил машины\\nобщественности.\\n', 'С.Н.Корсаков и машина для сравнения идей\\nОднако комиссия из пяти ученых, возглавляемая молодым математиком академиком М. В.\\nОстроградским, вынесла вердикт: «Господин Корсаков потратил чересчур много своих\\nинтеллектуальных сил, пытаясь научить других, как вовсе без оного интеллекта прожить».\\nКажется, это лучшее определение ИТ-сферы целиком ;-)\\nТакая вот биг дата доэлектрической эры.\\nПавел Петрович Аносов (10 июля 1796 — 25 мая 1851)\\nгорный инженер, учёный-металлург\\nРассказывает @Mimizavr\\nТалантами Русь богата\\nИ вот, старики говорят,\\nЧто сталь дамасских булатов\\nРассек уральский булат.\\nПавел Петрович Аносов - на мой взгляд, совершенно фантастическая фигура, благодаря\\nкоторой в 19 веке очень сильно развилась металлургия на Урале.\\nБудучи горным инженером по специальности, он занимался научной работой в сфере\\nметаллургии, стал крупным организатором горнозаводской промышленности на Урале,\\nисследовал природу Южного Урала, вел геологическую разведку месторождений россыпного\\nзолота и железных руд, изобрел ряд устройств и механизмов (в том числе и эффективную\\nзолотопромывную машину), разработал технологию производства огнеупорных тиглей (первый в\\nРоссии), стал десятым губернатором Томской губернии.\\n', 'Павел Петрович известен, в первую очередь, как создатель русского булата. Булат — особый\\nвид стали, очень прочный и упругий, с узором на поверхности, который является не украшением,\\nа показателем качества металла. Впервые оружие из такой стали начали использовать воины\\nДревней Индии. Такими мечами разрубали камни и легкие воздушные ткани. Несколько веков\\nспустя центром по изготовлению булатных клинков стал Дамаск. Как мастера 12-15 веков могли\\nварить такую удивительную сталь, оставалось загадкой, разгадать которую сумел лишь Павел\\nАносов. До Павла Аносова создать булат пытались многие, но никому это не удавалось. За\\nнесколько лет работы мастер испробовал 189 способов сталеварения, но ни один из них не не\\nдал булат. И лишь в 190-ый раз смешивая компоненты, создавая необходимые температурные\\nусловия мастер открыл тайну.\\nИгорь Васильевич Курчатов\\xa0(1903–1960)\\nфизик, «отец» атомной бомбы СССР\\nГоворит и показывает @Boomburum\\n', 'Игорь Васильевич Курчатов\\xa0(1903–1960) — один из первопроходцев в области ядерной\\nфизики, «отец» атомной бомбы СССР. В числе его заслуг — создание первого в Европе атомного\\nреактора, первой в СССР атомной бомбы и первой в мире термоядерной бомбы. Но были\\nзаслуги и в развитии мирного атома — под его руководством была сооружена первая в мире\\nатомная электростанция (Обнинская АЭС).\\xa0\\nСамый засекреченный ученый\\nСамый засекреченный ученый\\nТеги:  российская наука, российские учёные\\nХабы:  Блог компании Хабр, Научно-популярное\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\n', 'Хабр\\nЭкосистема для развития людей, вовлеченных в IT\\nХабр Карьера\\nFacebook\\nTwitter\\nВКонтакте\\nInstagram\\n244\\nКарма\\n128.3\\nРейтинг\\n@Exosphere\\nмодератор «Хабра», куратор-эксперт\\nОПРОС\\nВы знаете, что такое IT-менторство и как оно работает?\\nЗнаю, я — ментор или у меня была консультация с ментором\\nЗнаю и хочу попробовать, но пока опыта нет\\nЗнаю и не хочу пробовать, менторство не решает мой запрос\\nНе знаю, но мне интересно\\nНе знаю и мне неинтересно\\nГолосовать\\nВоздержаться\\nПроголосовал 1061 пользователь. Воздержались 219 пользователей.\\nЭлектропочта\\nКомментарии 8\\nИНФОРМАЦИЯ\\nСайт\\nhabr.com\\nДата регистрации\\n9 августа 2008\\nДата основания\\n26 мая 2006\\nВаш аккаунт\\nВойти\\nРегистрация\\nРазделы\\nСтатьи\\nНовости\\nИнформация\\nУстройство сайта\\nДля авторов\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\n', 'Численность\\n51–100 человек\\nМестоположение\\nРоссия\\nПредставитель\\nАлексей\\nССЫЛКИ\\nХабр Карьера\\ncareer.habr.com\\nХабр Q&A\\nqna.habr.com\\nХабр Фриланс\\nfreelance.habr.com\\nВИДЖЕТ\\nВКОНТАКТЕ\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nНастройка языка\\nТехническая поддержка\\n© 2006–2024, Habr\\n', 'Хабр\\nПодписан 1 друг\\nПодписаться на новости\\nБЛОГ НА ХАБРЕ\\n10 часов назад\\nИстория российской науки: напишем вместе?\\n31 янв в 15:03\\n10 «золотых» советов авторам любых текстов\\n28 янв в 12:22\\nИстории, достойные экранизации, или День защиты персональных данных\\n31 дек 2023 в 02:19\\nПоздравление-загадка от Хабра\\n29 дек 2023 в 18:31\\nАнонимный Дед Мороз на Хабре: хвастаемся подарками\\n1.3K\\n8\\n8.5K\\n41 +41\\n7.6K\\n16 +16\\n4K\\n44 +44\\n4.1K\\n97 +97\\n']\n",
      "История российской науки_ напишем вместе_ _ Хабр.pdf\n",
      "['4.58', 'Оценка', '353.18', 'Рейтинг', 'Хабр', 'Экосистема для развития людей, вовлеченных в IT', '8 февраля — день российской науки. Этот праздник отмечается в Академии Наук, в НИИ,', 'вузах, исследовательских лабораториях по всей стране. Судьба российской науки непростая,', 'всегда переплетённая с историей страны и ею же обусловленная: непростой путь к открытию', 'МГУ, талантливые кулибины (и И.Кулибин) из глубинки, учёные в изгнании, Туполевская шарага,', 'достижения института Гамалеи… Всё смешано, переплетено и влияло и влияет на весь мир.', 'Величие российской науки прежде всего в людях, которые её создают, которые разрабатывают,', 'изобретают, экспериментирую и точно знают, что делают.\\xa0', 'В этот день всегда спорят медики, биологи, инженеры, программисты, филологи, физики, химики', '— чьи учёные научнее и главнее, кто определял и создавал будущее? Ответ простой: все,', 'каждый — в своей сфере. Предлагаем сегодня собрать российские\\xa0 открытия и учёных, которые', 'вас вдохновляют, впечатляют, будоражат воображение и влияют (или повлияли) на вас.', 'Напишем эту статью вместе?', 'Exosphere 10 часов назад', 'История российской науки: напишем вместе?', 'Простой', '5 мин', '1.3K', 'Блог компании Хабр,\\xa0Научно-популярное', 'Ретроспектива', 'КАК СТАТЬ АВТОРОМ', 'Технотекст', 'Добавь свой раздел в статью про российскую науку', 'Войти', 'Моя', 'лента', 'Все', 'потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп', '']\n",
      "3 353.18 Хабр Экосистема для развития людей, вовлеченных в IT 8 февраля — день российской науки. Этот праздник отмечается в Академии Наук, в НИИ,\n",
      "['Подбор бытовой техники для своего умного дома может быть сложной задачей. Даже если\\nтехника имеет \"умную часть\", не факт что она подойдёт именно к вашей экосистеме.\\nПоэтому вопрос совместимости лучше выяснить ещё до покупки устройства.\\nНедавно захотел подобрать себе робот-пылесос, а на сайте Home Assistant как обычно\\nуказаны только обобщенные интеграции. При этом на сайте Алисы приведены тысячи\\nконкретных совместимых устройств, правда без цены и даже без указания выпускается ли\\nдо сих пор эта модель или нет.\\nИ тут мне пришла в голову идея: поместить эти совместимые устройства в Google Таблицу,\\nа затем написать Google Apps Script, который через поиск найти цены на все эти\\nустройства, а затем я уже выберу конкретное устройство исходя из моего бюджета.\\nПроблема совместимости устройств\\nРазные платформы: от Home Assistant до Яндекс Алисы - используют собственные методы\\nподключения. Одни устройства работают через Wi-Fi, другие через Zigbee или Bluetooth, а\\nempenoso\\n3 часа назад\\nКак быстро найти цены на совместимую с Home Assistant\\nбытовую технику: мой опыт автоматизации поиска цен\\nПростой\\n5 мин\\n360\\nOpen source*,\\xa0Google App Engine*,\\xa0Google API*,\\xa0Умный дом\\nКейс\\nКАК СТАТЬ АВТОРОМ\\n', 'некоторые требуют облачных сервисов. В результате поиск подходящей техники\\nпревращается в квест. Я сам использую локальный сервер Home Assistant, а Яндекс Алиса\\nподключена как голосовой интерфейс.\\nHome Assistant: свобода выбора или сложность интеграции?\\nHome Assistant - мощная платформа с гибкими возможностями, но её открытая экосистема\\nнакладывает ограничения. Производители редко заявляют официальную поддержку HA,\\nпоэтому пользователям приходится самостоятельно искать способы интеграции. На сайте\\nHome Assistant есть каталог интеграций, но он не содержит конкретных моделей техники —\\nтолько названия брендов и технологий.\\n', 'Дополнительно усложняет выбор большое количество разных протоколов:\\nWi-Fi — наиболее распространённый вариант, но часто требует облака\\nBluetooth — подходит для локальных устройств, но ограничен дальностью\\nZigbee и Z-Wave — энергоэффективные протоколы для умного дома, но требуют\\nшлюза\\nЛокальные API — идеальный вариант, но встречается редко\\nКак Яндекс Алиса помогает в поиске, но не решает проблему\\nВ отличие от Home Assistant, у Яндекс Алисы есть удобный каталог совместимых\\nустройств с конкретными моделями. Но даже если устройство есть в списке Алисы, это не\\nгарантирует его работу в HA. Многие устройства взаимодействуют с Алисой через облако,\\nа Home Assistant требует локального API или поддержки Zigbee.\\nАвтоматизация поиска цены\\nКак уже говорил каталог Яндекс Алисы содержат сотни совместимых моделей, но без цен\\nи актуальности их наличия. Проверять каждую вручную на маркетплейсах — долго и\\nнеудобно, особенно когда выбор включает десятки вариантов.\\nGoogle Apps Script для автоматического поиска цен на устройства\\nЧтобы не тратить время на ручной поиск, я написал скрипт на Google Apps Script. Он\\nиспользует Google Custom Search Engine (CSE) для поиска цен в результатах выдачи. Это\\nпозволяет быстро находить магазины, где продаётся конкретная модель, и загружать\\nссылки с актуальными ценами прямо в таблицу.\\n', 'Описание работы скрипта: использование Google Custom Search Engine\\nСкрипт получает список устройств из таблицы и для каждого отправляет запрос в Google\\nCSE. Если найдено предложение с ценой, информация записывается в таблицу: ссылка и\\nстоимость. Если подходящего предложения нет, выводится сообщение \"Цена не найдена\"\\nи ссылка на поиск в Яндексе.\\nСсылка на таблицу: Хабр_Алиса: поиск цен умного дома\\nСкрипт:\\nconst secrets = {\\n  apiKey_token: \\'xxx\\',  // Ключ № 1 на 100 запросов в день из xxx@gmail.com\\n  // apiKey_token: \\'xxx\\',  // Ключ № 2 на 100 запросов в день из xxx@gmail.com\\n  cx_token: \\'xxx\\', // Замените идентификатором Google\\'s Custom Search Engine (CSE) ID \\n  // https://cse.google.com/cse?cx=xxx&cr=countryRU\\n  initial_phrase: \\'\\',\\n  \\n  // Стоп-слова для исключения из поиска\\n  exclude_terms: []\\n};\\n// Подробнее про автоматический поиск цен на товары: https://habr.com/ru/articles/87957\\nfunction aliceSmartHomePriceSearch() {\\n  var sheet = SpreadsheetApp.getActiveSpreadsheet().getSheetByName(\\'Поиск\\');\\n  var data = sheet.getDataRange().getValues();\\n  var results = [];\\n  for (var i = 1; i < data.length; i++) { // Пропускаем заголовок\\n    var productName = data[i][0]; // Колонка A\\n    if (productName) {\\n      console.log(`Обрабатываем строку ${i + 1}: ${productName}`);\\n      var result = searchProductPrice(productName);\\n      var hyperlink = \\'=HYPERLINK(\"\\' + result.link + \\'\"; \"\\' + result.title + \\'\")\\';\\n      results.push([hyperlink, result.price]);\\n    } else {\\n      results.push([\\'\\', \\'\\']); // Пустые значения, если товара нет\\n    }\\n  }\\n  // Записываем результаты в столбцы B и C (2 и 3)\\n  var range = sheet.getRange(2, 2, results.length, 2);\\n', \"  range.setValues(results);\\n}\\nfunction searchProductPrice(productName) {\\n  var apiKey = secrets.apiKey_token;\\n  var cx = secrets.cx_token;\\n  \\n  // Формируем исключающие термины\\n  var excludeString = secrets.exclude_terms.map(term => '-' + term).join(' ');\\n  \\n  // Добавляем уточняющие слова и кавычки для точного совпадения модели\\n  var cleanProductName = productName\\n    .replace(/\\\\s+/g, ' ')\\n    .trim();\\n    \\n  // Формируем поисковый запрос с исключениями и уточнениями\\n  var searchTerms = [\\n    secrets.initial_phrase,\\n    `${cleanProductName}`,\\n    excludeString\\n  ].filter(Boolean).join(' ');\\n  \\n  var query = encodeURIComponent(searchTerms);\\n  var url = 'https://www.googleapis.com/customsearch/v1?q=' + query + \\n           '&cr=countryRU&cx=' + cx + '&key=' + apiKey;\\n  \\n  Logger.log(`Поисковый запрос: ${searchTerms}`);\\n  Logger.log(`URL запроса: ${url}`);\\n  \\n  var response = UrlFetchApp.fetch(url);\\n  var json = JSON.parse(response.getContentText());\\n  \\n  if (json.items && json.items.length > 0) {\\n    for (var i = 0; i < json.items.length; i++) {\\n      // Проверяем, не содержит ли заголовок стоп-слова\\n      var titleLower = json.items[i].title.toLowerCase();\\n      var hasExcludedTerms = secrets.exclude_terms.some(term => \\n        titleLower.includes(term.toLowerCase())\\n      );\\n      \\n      if (hasExcludedTerms) {\\n        Logger.log(`Пропускаем результат с запрещенными словами: ${json.items[i].title}\\n        continue;\\n      }\\n      \\n      if (json.items[i].pagemap && \\n\", '          json.items[i].pagemap.offer && \\n          json.items[i].pagemap.offer[0].price) {\\n            \\n        var price = json.items[i].pagemap.offer[0].price;\\n        price = price.replace(\\'.\\', \\',\\');\\n        var link = json.items[i].link;\\n        var title = json.items[i].title;\\n        \\n        Logger.log(`Найдена подходящая цена: ${price} (${title})`);\\n        \\n        return {\\n          price: price,\\n          link: link,\\n          title: title\\n        };\\n      }\\n    }\\n  }\\n  \\n  Logger.log(`Подходящая цена не найдена для ${productName}`);\\n  \\n  return {\\n    price: \\'0\\',\\n    link: \\'https://ya.ru/search/?text=\\' + query,\\n    title: \\'Цена не найдена\\'\\n  };\\n}\\n// Тестовая функция\\nfunction testImprovedSearch() {\\n  var result = searchProductPrice(\"Робот пылесос Mijia Vaccum Cleaner 2 mopping 2 in 1 \\n  Logger.log(result);\\n}\\nПроцесс работы:\\n1. Копируем список совместимых устройств в Google Таблицу, очищаем от штрих-кодов и\\nлишних данных простой сортировкой.\\n2. Запускаем скрипт, который автоматически ищет цены.\\n3. Получаем в таблице ссылки на магазины и актуальные цены, что упрощает выбор и\\nпокупку.\\n', 'Такой подход позволил мне оперативно находить нужные устройства без утомительного\\nмониторинга цен вручную. Теперь выбор техники для Home Assistant стал намного проще.\\nКак я использую этот инструмент для выбора техники\\nGoogle Apps Script значительно ускоряет поиск цен, но не всегда выдаёт идеальные\\nрезультаты. Иногда вместо нужной модели находятся запчасти или нулевые цены, потому\\nчто это устройство уже снято с производства. Например, при поиске Xiaomi Mi Robot Mop 2\\nPRO+ можно наткнуться на сменные щётки и фильтры вместо самого пылесоса.\\nЧтобы улучшить точность, я экспериментировал с формулировкой запросов. Хорошие\\nпримеры:\\n\"Xiaomi Mi Robot Mop 2 PRO+ купить\" – добавление слова «купить» повышает шанс\\nнайти магазины.\\n\"Xiaomi Mi Robot Mop 2 PRO+ цена Пермь\" – указывает город, если нужен локальный\\nпоиск.\\nИсключение лишнего: «-фильтр -щётка -запчасть» убирает ненужные товары.\\nТакже можно расширить функционал, например, фильтровать по проверенным магазинам.\\nДля этого достаточно добавить названия крупных маркетплейсов в запрос.\\nЗаключение\\nАвтоматизированный поиск цен на совместимую с Home Assistant бытовую технику с\\nпомощью Google Apps Script значительно экономит время и упрощает выбор. Теперь вы\\nможете быстро получить актуальные цены на нужные модели, не тратя часы на рутинный\\nпоиск.\\nАвтор: Михаил Шардин\\n🔗 Моя онлайн-визитка\\n📢 Telegram «Умный Дом Инвестора»\\n18 марта 2025 г.\\nТеги: поиск цен\\nХабы: Open source, Google App Engine, Google API, Умный дом\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\n', '183\\nКарма\\n90.7\\nРейтинг\\nМихаил Шардин @empenoso\\nАвтоматизация / Данные / Финансы / Умные дома\\nСайт\\nСайт\\nGithub\\nКомментарии 2\\nЭлектропочта\\nsic\\n2 часа назад\\nПодписаться\\n']\n",
      "Как быстро найти цены на совместимую с Home Assistant бытовую технику_ мой опыт автоматизации поиска цен _ Хабр.pdf\n",
      "['Подбор бытовой техники для своего умного дома может быть сложной задачей. Даже если', 'техника имеет \"умную часть\", не факт что она подойдёт именно к вашей экосистеме.', 'Поэтому вопрос совместимости лучше выяснить ещё до покупки устройства.', 'Недавно захотел подобрать себе робот-пылесос, а на сайте Home Assistant как обычно', 'указаны только обобщенные интеграции. При этом на сайте Алисы приведены тысячи', 'конкретных совместимых устройств, правда без цены и даже без указания выпускается ли', 'до сих пор эта модель или нет.', 'И тут мне пришла в голову идея: поместить эти совместимые устройства в Google Таблицу,', 'а затем написать Google Apps Script, который через поиск найти цены на все эти', 'устройства, а затем я уже выберу конкретное устройство исходя из моего бюджета.', 'Проблема совместимости устройств', 'Разные платформы: от Home Assistant до Яндекс Алисы - используют собственные методы', 'подключения. Одни устройства работают через Wi-Fi, другие через Zigbee или Bluetooth, а', 'empenoso', '3 часа назад', 'Как быстро найти цены на совместимую с Home Assistant', 'бытовую технику: мой опыт автоматизации поиска цен', 'Простой', '5 мин', '360', 'Open source*,\\xa0Google App Engine*,\\xa0Google API*,\\xa0Умный дом', 'Кейс', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "['look, use the source!\\n1. Указывайте количественно и качественно выраженные достижения\\nЭто самый главный и мощный пункт.\\nБольшинство людей пишут какие-то беспомощные аморфные функции и фразы про\\nобязанности и участие — «состоял, привлекался, принимал участие». Это выглядит, как\\nсвидетель из Фрязино, а не мощный проектный специалист, который будет двигать проект\\nвперёд.\\nSystems_Education\\n20 мар в 14:20\\nКак системному аналитику написать хорошее резюме —\\xa011\\nрекомендаций\\nПростой\\n3 мин\\n1.8K\\nАнализ и проектирование систем*,\\xa0Карьера в IT-индустрии\\nТуториал\\n0\\n27\\n22\\nКАК СТАТЬ АВТОРОМ\\nИсследуем Духов Машин в новом Сезоне на Хабре\\n', 'Нанимающий руководитель смотрит прежде всего на результаты, а не на процесс. Если вы\\nпишете только про поток, это в глазах читающего создаёт риски того, что вы цените\\nпроцесс, а не результаты. (Процесс тоже важен, но про него отдельно).\\nОсвойте язык результатов, важных для команды, бизнеса нанимателя, бизнеса клиента.\\nКак обычно пишут\\nФункции и задачи:\\nразрабатывал требования, общался с клиентами, командой, отвечал на вопросы,\\nрисовал схемы…\\nКак надо\\nДостижения:\\nразработал требования для 5 проектов в области X, Y, Z;\\nпомог клиентам найти более выгодное решение;\\nсократил время на создание пакета требований в 2 раза относительно планового;\\nсократил количество обсуждений по требованиям в разработке в 3 раза;\\nуменьшил и удерживал количество циклов согласования с 4-х до 2-х;\\nвнедрил в команде практику использования таких-то диаграмм, что сократило\\nдлительность переписок и встреч;\\nвыстроил доверительные отношения с ключевыми клиентами.\\n2. Не допускайте ошибок\\nБудьте предельно внимательны ко всем словам, типографике, аббревиатурам.\\nКадровикам-рекрутёрам будет пофиг, но вот ваши будущие коллеги будут очень\\nпристально смотреть на ваш текст и всё, что будет резать глаз. Если вы допускаете\\nопечатки, ошибки в документе, который представляет вас всему рынку труда, то в рабочих\\nдокументах будете тем более. Никто не хочет бесплатно работать корректором.\\nОтдельный вопрос — логика построения предложений. В резюме для этого мало места, но\\nи тут некоторые умудряются нагородить странный порядок слов. Помните как мантру —\\n«Мама мыла раму», Субъект-Действие-Объект. Никто не хочет бесплатно работать\\nредактором. Писать понятные тексты — это одна из важнейших компетенций аналитика.\\n', '3. Только актуальный и релевантный опыт\\nНе указывайте места работы старше 8 лет, если только не хотите зачем-то подчеркнуть\\nразносторонность своего опыта — например, почему-то считаете, что то, что вы работали\\nтаксистом полезно для конкретных вакансий, которые вы ищете.\\n4. Указывайте в местах работы основную трудовую функцию, а не\\nдолжность по трудовой\\nЗапись в трудовой никого не интересует. Слова «старший специалист отдела\\nВРИОКОМУНО» никому ни о чём не говорят, ну разве что вы точно были не младшим.\\nПишите суть деятельности — «аналитик», «проектировщик», «координатор»,\\n«конструктор». Лучше, если там будет фигурировать как минимум слово «инженер».\\n5. Указывайте область деятельности компании / подразделения\\nНазвание компании тоже зачастую ничего не говорит читателю. Сразу после названия\\nпозиции первой строчкой укажите, чем занимается компания — «разработка рудных\\nместорождений», «направление занимается кредитами для малого бизнеса» и т.д.\\n6. Указывайте объём управления\\nИногда бывает, что человек работал ведущим аналитиком или руководителем группы и не\\nуказывает, сколькими людьми управлял. Поверьте, между 2-мя людьми в подчинении и 7\\nесть разница, по крайней мере, при прочих равных.\\n7. Подкрепляйте софт-скиллы фактами\\nЛучше не писать «обучаемость», «креативность», «коммуникативность», это всем набило\\nоскомину и ничего не значит, кроме вашего желания, чтобы вас так воспринимали.\\nПишите с примерами:\\nВысокая обучаемость: за 3 месяца досрочно освоила Enterprise Architect.\\nКреативность: придумала несколько концепций решений, которые мы с командой\\nсмогли продать клиенту и они дали ему существенный эффект в бизнесе.\\n', '8. Указывайте, если готовы предоставить образцы документов по\\nзапросу\\nДокументы не обязательно должны быть рабочими, это могут быть и документы,\\nсозданные вами на учебном курсе (Но такая фраза сразу приятно выделит вас на фоне\\nдругих кандидатов). А сейчас так и вообще, пример портфолио можно нагенерить и\\nоформить за пару-тройку часов!\\n9. Не забывайте про ключевые слова\\nЕсли у вас в резюме нет слов SQL, XML, UML, JSON, API, то совершенно неудивительно,\\nчто вас невозможно найти (до трудно забыть дело даже не дойдет).\\n10. Покажите масштаб личности\\nВполную вы сможете развернуться на встрече, но даже на уровне резюме люди хотят\\nработать с интересными людьми, а не роботами, которые выполняют трудовые функции\\nпо договору.\\nДайте представление читателю о вашем жизненном кругозоре и интересах, какими\\nвещами вы увлекаетесь. Причём желательно не просто «философия, психология», а\\nболее предметно «изучаю аналитическую философию и гештальт-психологию». Одно\\nключевое слово здесь может заставить нанимающего менеджера позвать вас на интервью.\\n11. Создавайте копии резюме с профильным позиционированием\\nЭто совершенно бесплатно.\\nЕсли вы знаете, работа в каких сферах/темах проектов вам более интересна, создайте\\nкопии резюме с соответствующими названиями:\\nСистемный аналитик в интеграции;\\nСистемный аналитик IoT;\\nи т.д.\\nЕсли хотите получить более предметную помощь по своему резюме и поиску работы —\\nидите к менторам.\\nТеги:  резюме, системный аналитик, трудоустройство, поиск работы\\n', '32\\nКарма\\n0\\nРейтинг\\n@Systems_Education\\nПользователь\\nСайт\\nСайт\\nFacebook\\nTelegram\\nTelegram\\nРеклама\\nКомментарии 22\\nПубликации\\nRationalAnswer\\n9 часов назад\\nКак обнулялся Credit Suisse: разбираем траекторию погружения\\nна дно швейцарского гига-банка\\n12 мин\\nkesn\\n10 часов назад\\nХабы:  Анализ и проектирование систем, Карьера в IT-индустрии\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\nЭлектропочта\\nЛУЧШИЕ ЗА СУТКИ\\nПОХОЖИЕ\\n12K\\nОбзор\\n+75\\n26\\n49\\nАМА\\n', 'Байки погромиста. Если кто-то скажет, что программирование —\\nэто скучно\\nПростой\\n12 мин\\nTutelka\\n9 часов назад\\nОбещания — настоящие и не очень\\nПростой\\n7 мин\\nalizar\\n9 часов назад\\nУдивительное рядом. Как устроен буфер обмена в Windows и Linux\\nПростой\\n6 мин\\nunxed\\n3 часа назад\\nLinux-порт Far Manager: новости весны 2023\\nСредний\\n4 мин\\nРобот Марвин ждёт тех, кто его удивит в Сезон ML на Хабре\\nСобытие\\nПоказать еще\\nМИНУТОЧКУ ВНИМАНИЯ\\nРазместить\\n11K\\nМнение\\n+75\\n33\\n9\\n2.7K\\nМнение\\n+34\\n24\\n10\\n4.6K\\n+31\\n46\\n10\\n1.7K\\nДайджест\\n+29\\n14\\n7\\n', 'ВАКАНСИИ\\nСистемный аналитик\\nот 150\\xa0000 до 200\\xa0000 ₽ · SpectrumData · Екатеринбург\\nСистемный аналитик\\nот 150\\xa0000 до 200\\xa0000 ₽ · Сбер · Екатеринбург\\nСистемный аналитик\\\\ бизнес-аналитик\\nдо 250\\xa0000 ₽ · Главный радиочастотный центр · Москва\\nСистемный аналитик (senior)\\nот 200\\xa0000 до 300\\xa0000 ₽ · Heaven 11 · Можно удаленно\\nСистемный аналитик (внутренняя безопасность)\\nот 200\\xa0000 до 250\\xa0000 ₽ · Сбер · Москва\\nБольше вакансий на Хабр Карьере\\nЛетопись data-driven-подхода, от статистики до\\nML\\nСпецпроект\\nKubernetes без хайпа: полезные на практике\\nпосты о кубе\\nТурбо\\nРеклама\\n', 'ЧИТАЮТ СЕЙЧАС\\nНа LinkedIn обнаружили резюме человека, которому катастрофически не везёт с работодателями\\nКак обнулялся Credit Suisse: разбираем траекторию погружения на дно швейцарского гига-банка\\n«Призрачные вакансии» как новый HR-инструмент: растёт количество объявлений, по которым не\\nищут сотрудников\\nКак получить доступ к chatGPT в России\\n2.4K\\n7\\n12K\\n48\\n4.1K\\n3\\n903K\\n233\\nРЕКЛАМА\\n', 'Байки погромиста. Если кто-то скажет, что программирование — это скучно\\nРобот Марвин ждёт тех, кто его удивит в Сезон ML на Хабре\\nСобытие\\nИСТОРИИ\\nРАБОТА\\nCистемный аналитик\\n528 вакансий\\nВсе вакансии\\nРеклама\\n11K\\n9\\nПункты сбора\\nвторсырья на\\nкартах\\nЭлектровел с ИИ от\\nAcer\\nКейсы ML в бизнесе\\nПолезная подборка\\nо зрении\\nПубличная\\nконсультация с\\nэкспертом:\\nрекрутеры\\n', 'Реклама\\nРеклама\\nВаш аккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nНастройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "Как системному аналитику написать хорошее резюме — 11 рекомендаций _ Хабр.pdf\n",
      "['look, use the source!', '1. Указывайте количественно и качественно выраженные достижения', 'Это самый главный и мощный пункт.', 'Большинство людей пишут какие-то беспомощные аморфные функции и фразы про', 'обязанности и участие — «состоял, привлекался, принимал участие». Это выглядит, как', 'свидетель из Фрязино, а не мощный проектный специалист, который будет двигать проект', 'вперёд.', 'Systems_Education', '20 мар в 14:20', 'Как системному аналитику написать хорошее резюме —\\xa011', 'рекомендаций', 'Простой', '3 мин', '1.8K', 'Анализ и проектирование систем*,\\xa0Карьера в IT-индустрии', 'Туториал', '0', '27', '22', 'КАК СТАТЬ АВТОРОМ', 'Исследуем Духов Машин в новом Сезоне на Хабре', '']\n",
      "['2394.92\\nРейтинг\\nRUVDS.com\\nVDS/VPS-хостинг. Скидка 15% по коду HABR15\\nНесмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO-\\nобразов этих дисков остаётся актуальным. Многие операционные системы поставляются в\\nвиде ISO-образов, а администраторам необходимо поддерживать разношёрстный парк\\nстарых персональных компьютеров.\\nartyomsoft\\n20 мар в 14:00\\nКак создать аппаратный эмулятор CD-ROM без паяльника\\nСредний\\n19 мин\\n9.2K\\nБлог компании RUVDS.com,\\xa0Системное администрирование*,\\xa0Разработка под Linux*,\\xa0Разработка на Raspberry Pi*,\\xa0\\nКАК СТАТЬ АВТОРОМ\\nВызовы для k8s. Чего ждать операторам в будущем\\n', 'Существует множество решений, как можно установить операционную систему с ISO-\\nобраза без записи его на оптический носитель. Я уже затрагивал тему ISO-образов в моих\\nстатьях: «Раскрываем секреты загрузочных ISO-образов» и «Что вам нужно знать о\\nвнешних загрузочных дисках».\\nВ этой статье я хочу рассказать о ещё одном способе, который, как оказывается, вшит в\\nядро Linux. Если ваш одноплатный компьютер имеет USB OTG-разъём, и на него\\nвозможна установка Linux, то вы c большой долей вероятности можете сделать из\\nодноплатника аппаратный эмулятор привода оптических дисков.\\nМеня этот способ заинтересовал. Я проверил его сам и, получив положительный результат\\nу себя, решил поделиться с вами.\\nЯ сам узнал много интересного, систематизировал свои знания, поэтому надеюсь, что\\nчтение будет познавательно и интересно для вас.\\nКак всегда, если вы хотите посмотреть, что получится в итоге, уточнить детали, вы всегда\\nможете найти исходный код в моём репозитории на GitHub.\\nПри написании статьи я поставил себе следующие цели:\\n1. Аппаратный эмулятор CD-ROM должен быть реализован без использования\\nпаяльника и макетных плат.\\n2. Реализация должна быть понятна человеку, имеющему лишь базовые представления\\nо Linux, USB и Bluetooth.\\n3. Решение должно быть таким, чтобы его можно было с небольшими изменениями\\nреализовать на различных одноплатных компьютерах.\\n4. Побудить интерес читателя к изучению используемых в статье технологий.\\n5. Изложить материал, необходимый для решения задачи лаконично и просто. Не\\nуверен, что у меня это получилось из-за большого объёма темы. Буду признателен,\\nесли вы в отзывах напишете своё мнение.\\nОглавление\\nСуть решения\\nПроверка решения на практике\\nОт проверки идеи до реализации\\n', 'Операционная система Linux\\nUSB\\nBluetooth\\nСборка и модификация дистрибутива Raspberry Pi OS\\nРеализация\\nКак пользоваться эмулятором\\nОсобенности моего эмулятора\\nВыводы\\nСуть решения\\nРешение заключается в том, что, модифицируя операционную систему Linux на\\nодноплатном компьютере (встраиваемой системе), можно получить из него устройство,\\nкоторое будет распознаваться компьютером как внешний оптический привод USB или\\nфлеш-накопитель.\\nВ ядро Linux включена поддержка эмуляции CD-ROM и эмуляции флеш-накопителя. Но\\nэто не значит, что любую встраиваемую систему можно превратить в них. Для этого ещё\\nнеобходимо, чтобы встраиваемая система имела USB OTG-контроллер или USB-\\nконтроллер периферийного устройства.\\nПроверка решения на практике\\nЯ делал эмулятор оптических дисков, используя Raspberry Pi Zero 2 W. Но вы можете\\nиспользовать и другие одноплатные компьютеры. Естественно, вам тогда придётся самим\\nразбираться с некоторыми проблемами, которые с большой долей вероятностью у вас\\nвозникнут. У меня других одноплатных компьютеров кроме Raspberry Pi не было, поэтому\\nпривожу алгоритм, как делал я.\\n1. Скачать образ Raspberry OS Light с сайта raspberrypi.org.\\n2. Записать образ на SD-карту. Я использовал программу balenaEtcher.\\n3. Добавить строку dtoverlay=dwc2  в файле config.txt на SD-карте.\\n4. Записать файлы ssh и wpa_supplicant.conf на SD-карту. Файл userconf.txt нужен, чтобы\\nустановить пароль для пользователя, ssh — чтобы включить SSH, wpa_supplicant.conf\\n— чтобы указать точку доступа и пароль для Wi-Fi.\\n5. Вставить SD-карту в Raspberry Pi Zero 2 W.\\n6. Подключить USB-разъём Raspberry Pi Zero 2 W к USB-разъёму компьютера.\\n', '7. Подождать, пока выполнится первая загрузка и Raspberry Pi Zero 2 W подключится к\\nWi-Fi-сети.\\n8. Найти IP-адрес Raspberry Pi Zero 2 W в Wi-Fi-сети и подключиться к нему по протоколу\\nSFTP. Я использовал приложение WinSCP.\\n9. Записать ISO-образы, которые вы хотите эмулировать, в файловую систему Raspberry\\nPi.\\n10. Подключиться через SSH к Raspberry Pi W 2. Это можно сделать при помощи\\nприложения PuTTY.\\n11. Для того, чтобы ваш Raspberry Pi Zero 2 W стал внешним USB CD-ROM, ввести\\nкоманду:\\n$ sudo modprobe g_mass_storage cdrom=y removable=y stall=n file=/full/filename/w\\nПосле чего у вас на компьютере распознается внешний USB CD-ROM, в который\\nвставлен диск.\\n12. Для прекращения эмуляции ввести команду:\\n$ sudo modprobe -r g_mass_storage\\nПриведённый файл userconf.txt устанавливает для пользователя pi пароль «raspberry».\\nФайл ssh — это пустой файл, который не содержит никаких данных.\\nОт проверки идеи до реализации\\nПриведённая выше последовательность шагов позволяет вам посмотреть работу\\nэмуляции в действии. Однако это решение обладает рядом недостатков.\\n1. Чтобы загрузить и выбрать образ для эмуляции, необходимо наличие Wi-Fi.\\nФайл userconf.txt\\nФайл wpa_supplicant.conf\\n', '2. При перезагрузке Raspberry Pi необходимо заново монтировать образ.\\n3. Если нужно эмулировать образ для загрузки с него операционной системы,\\nпонадобится ещё один компьютер для управления Raspberry Pi.\\n4. Эмулировать можно образ размером максимум 2 Gib.\\nЕсли вам интересно, как избавиться от этих недостатков, у вас есть время и интерес\\nразобраться в этом вопросе, то предлагаю продолжить чтение.\\nКраткое содержание моей реализации следующее:\\n1. Для общения с Raspberry Pi будет использоваться Bluetooth.\\n2. Чтобы работа устройства была возможна в автономном режиме (без подключений Wi-\\nFi и Bluetooth), управляющий скрипт оформляется в виде службы Systemd.\\n3. Для управления по Bluetooth будет использоваться приложение Serial Bluetooth\\nTerminal c Google Play.\\n4. Для эмуляции оптических дисков с образов размером больше 2Gib необходимо внести\\nнебольшие изменения в модуль ядра Linux и выполнить перекомпиляцию.\\nПриведу кратко, что вам нужно знать, чтобы лучше понять суть того, что мы будем делать.\\nОперационная система Linux\\n▍ Ядро Linux\\nЯдро Linux содержит в себе абстракции для работы с устройствами там, где оно\\nзапускается. Реализуются эти абстракции в специальных программах, называемых\\nдрайверами. В ОС Linux драйвер может находиться непосредственно в файле ядра, а\\nможет быть оформлен в виде отдельного модуля. В большинстве случаев предпочтителен\\nвторой способ, так как модули можно динамически удалять и добавлять. Например, если\\nустройство не подключено к системе или с ним не осуществляется работа, драйвер нам не\\nнужен, и его можно выгрузить из памяти или не загружать вообще.\\nПри загрузке ядра ему необходима информация об устройствах, которые присутствуют в\\nсистеме, чтобы корректно загрузить драйверы (модули) для них. Эта информация может\\nпередаваться из различных источников. Например, в архитектуре x86 это будет ACPI. В\\nархитектуре ARM это Device Tree.\\n▍ Device Tree и Overlays\\n', 'Иногда Device Tree нужно модифицировать, чтобы можно было загрузить корректно\\nдрайвера для устройств. Делается это при помощи подключения overlays. Они содержат\\nинформацию, что необходимо изменить в исходном Device Tree.\\n▍ Headless-режим работы Raspberry Pi\\nОчень часто в различных статьях и самоучителях по работе с Raspberry Pi необходимо\\nподключение монитора, клавиатуры и мыши. Но на самом деле есть возможность\\nработать с ним в так называемом headless-режиме. В этом режиме вы работаете с\\nRaspberry Pi при помощи эмулятора терминала. Соединение его с Raspberry Pi может быть\\nUART, USB, Bluetooth, Ethernet, Wi-Fi.\\nГлавная сложность заключается в том, как можно работать в этом режиме с самого\\nначала, сразу после записи образа операционной системы на SD-карту, если у вас нет\\nлишнего монитора и клавиатуры. Как активировать SSH, настроить Wi-Fi на использование\\nопределённой точки доступа?\\nВ Raspberry Pi OS такая возможность есть. Достаточно разместить определённые файлы в\\nразделе FAT32 на SD-карте и загрузиться с неё. Raspberry Pi OS сделает необходимые\\nнастройки сама.\\n▍ Файловые системы, блочные устройства, разделы, монтирование\\nМеня всегда восхищала идея, что в Linux всё является файлом. Правильно используя\\nсредства Linux, можно практически без программирования выполнять сложные задачи.\\nБлочное устройство — это некоторый файл, в который можно записывать и считывать\\nданные блоками байтов различной длины. Что будет происходить при этом, зависит от\\nтого, с чем реально ассоциирован этот файл. Например, если он ассоциирован с жёстким\\nдиском, то тогда будут читаться/записываться данные на жёсткий диск, не обращая\\nвнимание на разделы и файловые системы. Если он ассоциирован с разделом жёсткого\\nдиска, то будут читаться/записываться данные, не обращая внимание на файловую\\nсистему.\\nПри помощи команды losetup можно добиться того, что он будет ассоциироваться с\\nобычным файлом на диске, что позволит создавать образы разделов и дисков.\\nЕщё полезной командой Linux является команда kpartx, которая создаёт блочные\\nустройства из файла образа диска. Каждое из устройств будет ассоциировано с образом\\nраздела, который хранится в этом файле.\\n', 'Форматирование раздела в Linux выполняется одной командой. В качестве параметра\\nнеобходимо передать имя файла блочного устройства. Например, для создания файловой\\nсистемы exFAT на блочном устройстве /dev/mmcblk0p3:\\n$ mkfs.exfat /dev/mmcblk0p3\\nЧтобы можно было работать с файлами файловой системы, размещённой на блочном\\nустройстве, нужно примонтировать файловую систему к корневой файловой системе при\\nпомощи команды mount.\\n$ mkdir -p /mnt/data\\n$ mount -t auto /dev/mmcblk0p3 /mnt/data\\nОбратите внимание, что директория, куда будет производиться монтирование, должна\\nсуществовать до того, как вы будете монтировать. Если нужно размонтировать файловую\\nсистему, используется команда umount.\\n$umount /mnt/data\\nЧтобы посмотреть, какие у вас есть блочные устройства, и куда они примонтированы,\\nможно использовать команду:\\n$ lsblk\\n▍ Systemd\\nЗагрузка Linux происходит в несколько этапов. Сначала загружается ядро операционной\\nсистемы, затем ядро запускает процесс init. Задача процесса init загрузить и\\nинициализировать процессы пространства пользователя и находиться в памяти до\\nперезагрузки или выключения компьютера (устройства). За долгие годы существования\\nLinux было написано множество реализаций init. На данный момент во многих Linux-\\nдистрибутивах используется реализация, называемая systemd. Её мы и будем\\nиспользовать.\\nМинимум команд, которые необходимо знать для работы с systemd.\\n', 'Команда\\nНазначение\\nsystemctl start\\nsn.service\\nзапустить службу\\nsystemctl stop\\nsn.service\\nостановить службу\\nsystemctl status\\nsn.service\\nпосмотреть статус службы\\nsystemctl enable\\nsn.service\\nвключить службу (служба будет автоматически запущена при\\nследующей загрузке Linux)\\nsystemctl disable\\nsn.service\\nвыключить службу\\njournalctl -u\\nsn.service -b\\nпосмотреть логи службы, начиная с момента последней\\nзагрузки Linux\\n▍ Терминалы, PuTTY, sshd, agetty\\nДля администрирования ОС Linux из Windows часто используют эмулятор терминала\\nPuTTY. Он позволяет подключаться к компьютеру или устройству c ОС Linux с помощью\\nразличных соединений (Ethernet, Wi-Fi, эмулируемого последовательного порта на\\nBluetooth или USB) и работать удалённо с консолью Linux в Windows. Чтобы такое было\\nвозможно, в ОС Linux должна быть запущена специальная программа, которая будет\\nвзаимодействовать с PuTTY. Это может быть sshd в случае SSH-соединения или agetty в\\nслучае последовательного порта.\\nПри подключении через последовательный порт по умолчанию вы увидите чёрно-белый\\nэкран без поддержки манипулятора мышь. Чтобы добавить поддержку мыши и цветного\\nэкрана, необходимо изменить значение переменной окружения TERM в файле\\n/usr/lib/systemd/system/serial-getty@.service.\\n[Service]\\nEnvironment=TERM=xterm\\nUSB\\n', 'Чтобы два USB-устройства могли работать друг с другом, необходимо наличие у каждого\\nиз них USB-контроллера. USB-контроллер в конкретный момент времени может работать в\\nрежиме хоста (Host) или режиме периферийного устройства (Device). Если одно из\\nвзаимодействующих устройств работает в режиме Host, то другое должно работать в\\nрежиме Device. Существуют следующие виды USB-контроллеров:\\nHost — всегда работает в режиме Host.\\nDevice — всегда работает в режиме Device.\\nOTG — может работать или в режиме хоста или в режиме периферийного устройства.\\nПереключение режимов может быть аппаратным (при помощи особой распайки\\nкабеля OTG кабель переводит в контроллер режим хоста) или программным\\nРежим хоста подразумевает посылку команд, а режим периферийного устройства — их\\nобработку.\\n▍ OTG USB-контроллер\\nВозьмём Android-телефон с OTG-контроллером. Это означает, что при сопряжении по USB\\nс компьютером (для записи файлов с компьютера на телефон), он будет играть роль\\nпериферийного устройства, а при сопряжении по USB с периферийным устройством\\n(мышью, клавиатурой, сетевой картой, флэш-накопителем, монитором, принтером)\\nтелефон будет играть роль хоста.\\nОбычно USB-контроллер периферийного устройства или USB OTG-контроллер\\nприсутствуют во встраиваемых устройствах. Также они могут быть интегрированы в\\nоднокристальную систему (SoC). Но по факту на устройстве может отсутствовать\\nфизический USB-разъём для подключения.\\nНапример, на всех Raspberry Pi установлена SoC, которая имеет OTG-контроллер, но\\nфактически физический разъём для него есть только в Raspberry Pi Zero (Zero W, Zero 2\\nW) и в Raspberry Pi 4.\\n▍ Дескрипторы USB\\nКаждое USB-устройство имеет дескрипторы. Дескрипторы — это информация о USB-\\nустройстве, которая используется операционной системой для корректного выбора\\nдрайвера для устройства. Мне понравилось описание, которое приведено на сайте\\nMicrosoft.\\n', '▍ Создание USB-устройств в Linux\\nЯдро Linux содержит модули, которые позволяют создавать виртуальные USB-устройства.\\nЭто может быть Mass Storage, последовательный порт, сетевая карта. Загрузив и настроив\\nэти модули, вы можете сделать так, чтобы компьютером ваш одноплатник распознавался\\nодним или несколькими такими устройствами.\\nЕсли вам достаточно одного устройства, то вы можете загрузить модуль для этого\\nустройства, опционально передав ему параметры для конфигурации при помощи команды\\nmodprobe. Когда отпадёт необходимость в этом устройстве, его можно выгрузить при\\nпомощи команды modprobe -r.\\nЧтобы на одном физическом порту у вас распознавалось несколько устройств\\nодновременно, нужно использовать модуль libcomposite и сконфигурировать эти\\nустройства при помощи создания структур в файловой системе ConfigFS в директории\\n/sys/kernel/config/usb_gadget.\\nТакие устройства называются композитными USB-устройствами. Вы, скорее всего,\\nвстречались с такими, например, если у вас беспроводная клавиатура и мышь, а для них\\nиспользуется один приёмопередатчик.\\nВ нашем случае мы создадим композитное USB-устройство, которое будет\\nпоследовательным портом и устройством хранения. Последовательный порт мы будем\\nиспользовать для подключения к нашему эмулятору оптических дисков через PuTTY.\\nИзначально я хотел, что бы это была сетевая карта и SSH, но карта требует настройки в\\nоперационной системе компьютера, поэтому для простоты отказался от этой идеи в пользу\\nпоследовательного порта.\\n▍ Создание композитного USB-устройства при помощи ConfigFS\\n1. Загружаем модуль libcomposite.\\nmodprobe libcomposite\\n2. Заполняем дескрипторы для устройства.\\n$ usb_dev=/sys/kernel/config/usb_gadget/cdemu\\n$ mkdir -p $usb_dev\\n', '$ echo 0x0137 > $usb_dev/idProduct\\n$ echo 0x0100 > $usb_dev/bcdDevice\\n$ echo 0x0200 > $usb_dev/bcdUSB\\n$ echo 0xEF  > $usb_dev/bDeviceClass\\n$ echo 0x02 > $usb_dev/bDeviceSubClass\\n$ echo 0x01 > $usb_dev/bDeviceProtocol\\n$ mkdir -p $usb_dev/strings/0x409\\n$ echo \"abababababababa\" > $usb_dev/strings/0x409/serialnumber\\n$ echo \"Linux Foundation\" > $usb_dev/strings/0x409/manufacturer\\n$ echo \"USB CD-ROM Emulator\" > $usb_dev/strings/0x409/product\\n3. Создаём конфигурацию.\\nmkdir -p $usb_dev/configs/c.1\\nmkdir -p $usb_dev/configs/c.1/strings/0x409\\necho \"acm+usb\" > $usb_dev/configs/c.1/strings/0x409/configuration\\necho \"0x80\" > $usb_dev/configs/c.1/bmAttributes\\necho 250 > $usb_dev/configs/c.1/MaxPower\\n4.Создаём и подключаем функцию acm (последовательный порт через USB).\\n$ mkdir -p $usb_dev/functions/acm.usb0\\n$ ln -s $usb_dev/functions/acm.usb0 $usb_dev/configs/c.1\\n5. Создаём и подключаем функцию mass_storage. Mass_storage в данном случае — это\\nэмуляция CD-ROM для ISO-образа /home/pi/1.iso.\\n$ mkdir -p $usb_dev/functions/mass_storage.usb0/lun.0\\n$ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/cdrom\\n$ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/removable\\n$ echo 0 > $usb_dev/functions/mass_storage.usb0/lun.0/nofua\\n$ echo 0 > $usb_dev/functions/mass_storage.usb0/stall\\n$ echo \"/home/pi/1.iso\" > $usb_dev/functions/mass_storage.usb0/lun.0/file\\n$ ln -s $usb_dev/functions/mass_storage.usb0 $usb_dev/configs/c.1\\n6. Активируем созданное устройство.\\n', '$ ls /sys/class/udc > $usb_dev/UDC\\n▍ Удаление композитного USB-устройства при помощи ConfigFS\\n1. Деактивируем устройство.\\n$ usb_dev=/sys/kernel/config/usb_gadget/cdemu\\n$ echo \"\"> $usb_dev/UDC\\n2. Удаляем функцию mass_storage.\\n$ rm $usb_dev/configs/c.1/mass_storage.usb0\\n$ rmdir $usb_dev/functions/mass_storage.usb0\\n3. Удаляем функцию acm.\\n$ rm $usb_dev/configs/c.1/acm.usb0\\n$ rmdir $usb_dev/functions/acm.usb0\\n4. Удаляем конфигурацию.\\n$ rmdir $usb_dev/configs/c.1/strings/0x409\\n$ rmdir $usb_dev/configs/c.1\\n5. Удаляем устройство.\\n$ rmdir $usb_dev/strings/0x409\\n$ rmdir $usb_dev\\n6. Выгружаем загруженные устройством модули.\\n', '$ modprobe -r usb_f_mass_storage\\n$ modprobe -r usb_f_acm\\n$ modprobe -r libcomposite\\nCтруктура файловой системы для созданного эмулятора CD-ROM\\nBluetooth\\nТема Bluetooth очень объёмная, и её невозможно изложить в одной статье, поэтому\\nприведу только тот минимум, который позволяет понять, как мы будем использовать\\nBluetooth.\\nBluetooth — технология, которая позволяет связывать устройства без проводов по\\nрадиоканалу. На данный момент существует множество версий спецификации Bluetooth.\\nСпецификация Bluetooth освещает множество вопросов.\\nЧтобы передать данные с одного устройства на другое, необходимо наличие на обоих\\n', 'устройствах контроллеров и стеков Bluetooth.\\nBluetooth-контроллер — аппаратное устройство, обычно выполненное в виде микросхемы\\nили части более сложной микросхемы, позволяющее получать/передавать данные по\\nрадиоканалу в соответствии со спецификацией Bluetooth.\\nBluetooth-стек — программная реализация протоколов, описанных в спецификации\\nBluetooth.\\nПротоколы Bluetooth, предназначенные для решения определённых задач, группируются в\\nпрофили Bluetooth.\\nМы будем использовать два профиля Bluetooth:\\n1. Generic Access Profile (GAP), который поддерживается всеми Bluetooth-устройствами.\\n2. Serial Port Profile (SPP), который подразумевает использование последовательного\\nпорта поверх соединения Bluetooth.\\n▍ Поддержка Bluetooth операционными системами\\nBluetooth-контроллеры могут иметь различные аппаратные интерфейсы для доступа. Это\\nможет быть UART, USB, PCIe. В случае операционной системы многие детали скрываются,\\nи можно о них не думать. С контроллером можно работать на низком уровне через\\nдрайвер или уже используя высокоуровневые библиотеки и приложения,\\nпредоставляемые стеком Bluetooth, например, в Linux широко распространён стек BlueZ.\\n▍ BlueZ\\nСтек Bluetooth BlueZ состоит из двух частей.\\nОдна часть представлена модулями ядра Linux, уже включена в ядро. Если она\\nотсутствует, то её нужно включить и перекомпилировать ядро.\\nВторая часть представлена приложениями для пространства пользователя. Приложения\\nпозволяют конфигурировать и работать со стеком Bluetooth.\\nНа данный момент многие приложения считаются устаревшими, и разработчики BlueZ\\nрекомендуют использовать более новые приложения и интерфейс D-Bus для работы со\\nстеком.\\n', 'Но, как мне кажется, именно те старые, устаревшие приложения позволяют лучше понять\\nработу Bluetooth, поэтому в учебных целях я буду использовать их, для чего нужно будет\\nинициализировать BlueZ в режиме совместимости.\\n$ bluetoothd --noplugin=sap -C\\n▍ Протоколы Bluetooth\\nЯ не буду утомлять вас различными схемами, диаграммами, которые вы легко можете\\nнайти в интернете. Расскажу только о тех протоколах, с которыми нам предстоит\\nстолкнуться и нужно будет сконфигурировать.\\n▍ Service Discovery Protocol (SDP)\\nПри помощи протокола SDP можно определить, какие приложения (сервисы) находятся на\\nхосте, и с ними возможен обмен данными через Bluetooth\\nЧтобы можно было увидеть сервис с другого устройства, его необходимо зарегистрировать\\nв SDP database. Например, если мы хотим зарегистрировать службу, представляющую\\nэмуляцию последовательного порта в Bluetooth, это можно сделать следующей командой:\\n$ sdptool add SP\\nЧтобы можно было посмотреть службы, зарегистрированные у вас на хосте, нужно ввести\\nкоманду:\\n$ sdptool browse local\\n▍ Radio Frequency Communications (RFCOMM)\\nПротокол RFCOMM позволяет создавать виртуальное соединение по последовательному\\nпорту между двумя хостами.\\nНа одном из хостов создаётся сервер, которому выделяется канал RFCOMM, второй из\\nхостов подключается к нему, указывая MAC-адрес и номер канала\\nКанал RFCOMM немного напоминает порт в UDP или TCP, но если у них и у источника и у\\nполучателя есть порты, то у RFCOMM для источника и получателя один и тот же канал.\\n', 'Поэтому невозможно создать несколько подключений на один и тот же канал.\\nВ Linux можно использовать команду rfcomm для создания процесса, который будет\\nслушать определённый канал RFCOMM и при соединении запускать другой процесс.\\n$ rfcomm -r watch hci0 1 /usr/local/bin/cdemu-cmd /dev/rfcomm0 /dev/rfcomm0\\nВ данном случае на Bluetooth-контроллере hci0 RFCOMM будет прослушиваться канал 1 и\\nзапускаться процесс cdemu-cmd с двумя параметрами командной строки /dev/rfcomm0 и\\n/dev/rfcomm0.\\n▍ Утилита bluetoothctl\\nУтилита Bluetoothctl позволяет сопрягать устройство, на котором вы её запустили с другим\\nустройством.\\nВы можете сделать устройство видимым для обнаружения другими устройствами, а также\\nнайти другое устройство и выполнить с ним сопряжение. Более подробно расписано в\\nдокументации к утилите, которая доступна по команде:\\n$ man bluetoothctl\\n▍ Serial Bluetooth Terminal\\nДля отладки приложений, использующих Bluetooth, удобно использовать приложение для\\nAndroid Serial Bluetooth Terminal. Это приложение позволяет работать с Bluetooth-\\nустройствами, у которых доступен профиль SPP. В нашем случае мы будем использовать\\nего как визуальный интерфейс для работы с нашим эмулятором оптических дисков.\\nСборка и модификация дистрибутива Raspberry Pi OS\\nЧтобы сделать полноценный аппаратный эмулятор оптических дисков, нам придётся\\nнемного модифицировать исходный дистрибутив Linux. Это подразумевает\\nперекомпиляцию ядра, изменение нескольких конфигурационных файлов и добавление\\nсвоего программного кода. Для меня это было удобно сделать при помощи Docker.\\n▍ Кросс-компиляция ядра Linux\\nКросс-компиляция позволяет на компьютере с одной архитектурой получать исполняемые\\n', 'файлы для другой архитектуры. Мы можем компилировать ядро Linux для Raspberry Pi на\\nRaspberry Pi, а можем, используя кросс-компиляцию, сделать это на обычном компьютере\\nс архитектурой x86, что существенно сократит время компиляции из-за большего\\nбыстродействия компьютера. Подробно о том, как выполнять кросс-компиляцию Raspberry\\nPi OS, можно почитать тут.\\n▍ Chroot и запуск бинарных файлов другой архитектуры\\nКоманда Linux chroot позволяет запускать процессы с изменённой корневой системой. Это\\nкажется немного запутанным, но суть в следующем. В качестве параметра команде\\nпередаётся путь к корневой директории. В результате запуска команды через chroot\\nзапущенный процесс будет считать, что корнем файловой системы является та\\nдиректория, которую передали в качестве параметра.\\nПрименений у команды chroot несколько, например, её можно использовать, чтобы\\nзапустить команду apt для Raspberry Pi в Docker-контейнере.\\nИнтересно, что Docker Desktop для Windows позволяет запускать исполняемые файлы для\\nархитектуры ARM. В Linux-версии Docker такое сделать можно, но нужна дополнительная\\nнастройка.\\nРеализация\\nСозданный мной проект состоит из следующих файлов:\\n1. Dockerfile и скрипт, который выполняется в Docker-контейнере.\\n2. Файлы, которые необходимо добавить или обновить в исходном дистрибутиве:\\ncdemu — основная логика работы эмулятора оптических дисков, написанная на\\nязыке bash;\\ncdemu-cmd — bash-скрипт для обработки команд от пользователя и передачи их\\nэмулятору;\\nbash-utils.sh — bash-скрипт co вспомогательными функциями;\\ncdemu-bluetooth-ui.service — systemd-служба, которая запускает интерпретатор\\nкоманд на создаваемом RFCOMM-соединении телефона и Raspberry Pi;\\ncdemu.service — systemd-служба, которая запускает эмулятор оптических дисков\\nпри загрузке;\\nbluetooth.service — изменённая служба systemd для инициализации bluetooth;\\n', 'serial-getty@.service — изменённая служба systemd для запуска agetty на\\nсоздаваемом соединении на последовательном порту;\\nfirstboot.service — служба systemd для запуска скрипта при первой загрузке\\nоперационной системы. Я её позаимствовал из проекта raspberian-firstboot;\\nconfig.txt — изменённый файл конфигурации для загрузки Raspberry Pi. Содержит\\nподключение overlay dwc. Это необходимо, чтобы USB-контроллер мог работать в\\ndevice mode;\\nfistboot.sh — скрипт, который запускается службой systemd firstboot.service;\\nuserconf.txt — файл, который необходим, чтобы установить пароль для\\nпользователя pi. В последних версиях Raspberry Pi OS пользователь pi не\\nактивирован по умолчанию, поэтому необходимо наличие этого файла;\\nssh — файл необходим, чтобы активировать ssh, который отключён по\\nумолчанию;\\nwpa_supplicant.conf — файл, необходимый, если вы хотите настроить Raspberry Pi\\nна работу с вашей точкой доступа.\\nЛистинги файлов не привожу, так как это ещё больше раздует и так большую статью.\\nОзнакомиться вы с ними можете здесь.\\nКак пользоваться эмулятором\\n1. Собираем Docker-образ.\\ndocker build -t raspi-image . \\n2. Собираем образ RaspberryPi OS.\\ndocker run --privileged -v c:\\\\temp:/build --name raspi-image -it --rm raspi-image\\n3. Записываем образ на SD-карту. Вставляем её в Raspberry Pi.\\n4. Подключаем Rapsberry Pi Zero 2 W к компьютеру.\\n5. Через некоторое время у вас появится съёмный накопитель.\\n6. На этот съёмный накопитель, содержащий файл Readme.txt, копируем образы, которые\\nхотим эмулировать.\\n7. Находим виртуальный COM-порт, созданный после подключения Raspberry Pi к\\nкомпьютеру.\\n', '8. Подключаемся к Raspberry Pi через с помощью Putty через виртуальный COM-порт.\\n9. Запускаем интерактивное приложение для управления эмулятором.\\n$ sudo cdemu-cmd\\n10. Если хотим сделать управление с телефона, то выполняем сопряжение телефона и\\nRaspberry Pi. Для чего вводим в эмуляторе терминала команду:\\n$ sudo bluetoothctl\\n11. Делаем Raspberry Pi доступным для обнаружения:\\ndiscoverable on\\n12. Находим его на телефоне и выполняем полключение. После чего соглашаемся с PIN-\\nкодом на телефоне и Raspberry Pi.\\nyes\\n13. Выходим из bluetoothctl.\\nexit\\n14. Запускаем на телефоне Serial Bluetooth Terminal и выполняем подключение к Raspberry\\nPi из него. Теперь можно посылать команды созданному эмулятору CD-ROM.\\nКоманды, которые можно посылать эмулятору:\\n1. hdd — переключение в режим эмуляции внешних жёстких дисков.\\n2. cdrom — переключение в режим эмуляции внешних приводов оптических дисков.\\n3. list — вывести список доступных ISO-образов, которые можно эмулировать.\\n4. insert <порядковый номер> — поместить ISO-образ для эмуляции.\\n5. eject — извлечь ISO-образ из эмулятора.\\n6. help — показать список доступных команд в текущем режиме.\\nОсобенности моего эмулятора\\n', 'Интересно, но в Linux по умолчанию нельзя эмулировать ISO-образы размером больше 2\\nGib. Я просмотрел исходный код драйвера в файле\\ndrivers/usb/gadget/function/storage_common.c и предположил, что нет оснований не\\nприменять патч к ядру Linux от Adam Bambuch, который просто удаляет одно условие.\\nОбразы эмулировались нормально и при снятии ISO-образа с эмулируемого CD-ROM он\\nбыл идентичен исходному. Поверял по хеш-коду для файла ISO-образа.\\nНе пойму, почему есть это ограничение в Linux и почему его до сих пор не убрали? Если\\nвы знаете ответ, ответьте в комментариях.\\nМоя реализация не требует никаких дополнительных деталей. Нужен только Raspberry Pi\\nZero 2 W, один или два кабеля USB и адаптер питания, если будете использовать два\\nкабеля USB. Один для питания, второй для передачи данных. Хоть и использование\\nдополнительного кабеля и адаптера добавляет громоздкости, это решает проблему\\nперезагрузки Raspberry Pi, если компьютер или ноутбук отключает ненадолго питание при\\nперезагрузке.\\nКроме того, я не использую Python, только bash.\\nВыводы\\nПолученное программно-аппаратное решение, хоть и обладает рядом недостатков по\\nсравнению с карманом Zalman (не поддерживается USB 3.0, нет интерактивного меню на\\nсамом устройстве), позволит вам установить практически любую операционную систему\\nна широкий спектр компьютеров путём простого копирования ISO-образа.\\nРешение является прототипом, но вместе с тем позволяет углубить знания по многим\\nтемам, или получить, если вы были с ними не знакомы.\\nТак как основной целью была разработка прототипа, я запускал bluetoothd в режиме\\nсовместимости, и почти всю логику написал на bash.\\nЯ хотел показать возможность превратить встраиваемое устройство с операционной\\nсистемой Linux в аппаратный эмулятор флеш-накопителя или привода оптических дисков,\\nприложив минимум усилий. Надеюсь, что это удалось.\\nЧтобы уместить всё в одной статье, я лишь поверхностно коснулся тех тем, которые\\nнеобходимы для понимания. Если вас заинтересовало, вы можете самостоятельно\\nизучить их углублённо.\\n', 'Объём статьи не позволяет осветить все интересности, с которыми я столкнулся при\\nразработке эмулятора, и решения, которые применял и проверял. Приведу лишь\\nнесколько из них.\\nНапример, я долго боролся с зависанием при удалении составного устройства. Помогло\\nиспользование службы serial-getty вместо getty, хотя во многих статьях упоминалась getty.\\nЯ долго разбирался, как можно сделать сопряжение через Bluetooth между Raspberry и\\nтелефоном, использовал команду bt-agent, но потом всё-таки отказался от неё в пользу\\nbluetoothctl.\\nПри переключении эмулятора в режим HDD для записи ISO-образов изначально я\\nоткрывал для доступа всю SD-карту и хранил ISO-образы в отдельном разделе, но потом\\nпосчитал, что для безопасности лучше хранить образ диска с ISO-образами в отдельном\\nфайле и открывать доступ только к нему, хоть это и снизило скорость записи, но\\nпользователя не обескураживают появляющиеся несколько дисков.\\nРазработанный прототип есть куда улучшать. Можно, например, создать более\\nминималистичный дистрибутив Linux, который будет содержать только то, что реально\\nиспользуется для эмуляции, или создать более удобное графическое приложение для\\nAndroid для работы с эмулятором. А можно упростить работу с Bluetooth, напрямую\\nработая с драйверами bluetooth или используя интерфейс D-Bus для работы с Bluetooth-\\nустройствами. Или вообще всё-таки взяться за паяльник и сделать устройство, более\\nпохожее по функционалу на карман Zalman. Но главное, вы увидели, что это реально\\nсделать, а когда видишь положительный результат, это вдохновляет на большее\\nтворчество.\\nВ процессе тестирования и отладки программного кода было замечено, что на Lenovo X1\\nExtreme Gen 2 эмулятор CD-ROM дисков великолепно определялся в Windows 10, но\\nотказывался определяться в BIOS. Эмпирически было определено, что помогает\\nотключение режима экономии энергии процессора в BIOS. Также ноутбук отключал\\nпитание на usb при перезагрузке, поэтому понадобилось дополнительное питание\\nRaspberry Pi.\\nИнтересно, но на ASUS K53E и Gigabyte BRIX всё работает без проблем.\\nРешение с небольшими модификациями можно реализовать на Raspberry Pi 4. Но если вы\\nпоняли суть решения, вы его сможете повторить и на других одноплатных компьютерах,\\nкоторые имеют выведенные USB-порты для OTG или USB-контроллеры периферийных\\nустройств.\\n', 'RUVDS.com\\nVDS/VPS-хостинг. Скидка 15% по коду HABR15\\nTelegram\\nВКонтакте\\nTwitter\\n84\\nКарма\\n82.1\\nРейтинг\\n@artyomsoft\\nПользователь\\nКомментарии 17\\nDockerfile на данный момент только выполняется в Docker Desktop для Windows. В Linux\\nон работать не будет.\\nВ заключение хочу сказать, что существует ещё один способ эмулировать оптические\\nдиски, который я не пробовал, но знаю о его существовании из ваших комментариев к\\nодной из моих статей — это программа DriveDroid для Android. Я ей не пользовался, так\\nкак для её работы нужно получать права root на телефоне. Но, скорее всего, из-за\\nограничений в ядре Linux программа поддерживает ISO-образы до 2 Gib и/или работает\\nтолько с гибридными ISO-образами. Если я не прав, буду рад увидеть ваши опровержения\\nв комментариях.\\nTelegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх 🕹️\\nТеги:  linux kernel, usb, bluetooth, bluez, linux modules, эмуляция, iso, cd-rom, systemd, \\nagetty, ruvds_статьи\\nХабы:  Блог компании RUVDS.com, Системное администрирование, \\nРазработка под Linux, Разработка на Raspberry Pi, DIY или Сделай сам\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\nЭлектропочта\\n', 'Публикации\\nRationalAnswer\\n9 часов назад\\nКак обнулялся Credit Suisse: разбираем траекторию погружения\\nна дно швейцарского гига-банка\\n12 мин\\nkesn\\n10 часов назад\\nБайки погромиста. Если кто-то скажет, что программирование —\\nэто скучно\\nПростой\\n12 мин\\nTutelka\\n9 часов назад\\nОбещания — настоящие и не очень\\nПростой\\n7 мин\\nalizar\\n9 часов назад\\nУдивительное рядом. Как устроен буфер обмена в Windows и Linux\\nПростой\\n6 мин\\nЛУЧШИЕ ЗА СУТКИ\\nПОХОЖИЕ\\n12K\\nОбзор\\n+75\\n26\\n49\\n11K\\nМнение\\n+75\\n33\\n9\\n2.7K\\nМнение\\n+34\\n24\\n10\\n4.7K\\n+31\\n46\\n10\\n', 'unxed\\n3 часа назад\\nLinux-порт Far Manager: новости весны 2023\\nСредний\\n4 мин\\nПоказать еще\\n1.8K\\nДайджест\\n+29\\n14\\n7\\nИНФОРМАЦИЯ\\nСайт\\nruvds.com\\nДата регистрации\\n18 марта 2016\\nДата основания\\n27 июля 2015\\nЧисленность\\n11–30 человек\\nМестоположение\\nРоссия\\nПредставитель\\nruvds\\nССЫЛКИ\\nVPS / VDS сервер от 130 рублей в месяц.\\nruvds.com\\nДата-центры RUVDS в Москве, Санкт-Петербурге, Казани, Екатеринбурге, Новосибирске, Лондоне,\\nФранкфурте, Цюрихе, Амстердаме\\nruvds.com\\nПомощь и вопросы\\nruvds.com\\nПартнерская программа RUVDS\\nruvds.com\\nVPS (CPU 1x2ГГц, RAM 512Mb, SSD 10 Gb) — 190 рублей в месяц\\nruvds.com\\nVPS Windows от 523 рублей в месяц. Бесплатный тестовый период 3 дня.\\nruvds.com\\nVDS в Цюрихе. Дата-центр TIER III — швейцарское качество по низкой цене.\\nВаш аккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nНастройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n', 'ruvds.com\\nАнтивирусная защита виртуального сервера. Легкий агент для VPS.\\nruvds.com\\nVPS в Лондоне. Дата-центр TIER III — английская точность за рубли.\\nruvds.com\\nVPS с видеокартой на мощных серверах 3,4ГГц\\nruvds.com\\nПРИЛОЖЕНИЯ\\nRUVDS Client\\nПриложение для мониторинга и управления виртуальными серверами RUVDS с мобильных устройств.\\nAndroid\\niOS\\nВИДЖЕТ\\nБЛОГ НА ХАБРЕ\\n5 часов назад\\nКак устроено индексирование баз данных\\n1.7K\\n0\\n', '9 часов назад\\nУдивительное рядом. Как устроен буфер обмена в Windows и Linux\\nвчера в 14:00\\nБудни техпода. Размещение игрового сервера на VDS\\nвчера в 08:45\\nНостальгические игры: Parasite Eve\\n20 мар в 18:00\\nИнструменты наблюдаемости, о которых нужно знать в 2023 году\\n4.7K\\n10\\n2.9K\\n12\\n3.4K\\n4\\n3.2K\\n4\\n']\n",
      "Как создать аппаратный эмулятор CD-ROM без паяльника _ Хабр.pdf\n",
      "['2394.92', 'Рейтинг', 'RUVDS.com', 'VDS/VPS-хостинг. Скидка 15% по коду HABR15', 'Несмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO-', 'образов этих дисков остаётся актуальным. Многие операционные системы поставляются в', 'виде ISO-образов, а администраторам необходимо поддерживать разношёрстный парк', 'старых персональных компьютеров.', 'artyomsoft', '20 мар в 14:00', 'Как создать аппаратный эмулятор CD-ROM без паяльника', 'Средний', '19 мин', '9.2K', 'Блог компании RUVDS.com,\\xa0Системное администрирование*,\\xa0Разработка под Linux*,\\xa0Разработка на Raspberry Pi*,\\xa0', 'КАК СТАТЬ АВТОРОМ', 'Вызовы для k8s. Чего ждать операторам в будущем', '']\n",
      "1 2394.92 RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Несмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO-\n",
      "['2394.92\\nРейтинг\\nRUVDS.com\\nVDS/VPS-хостинг. Скидка 15% по коду HABR15\\n 20 мар в 12:00\\nСредний19 мин\\nartyomsoft\\nКак создать аппаратный эмулятор CD-ROM\\nбез паяльника\\n9.2K\\nБлог компании RUVDS.com\\n, \\nСистемное администрирование*\\n, \\nРазработка под Linux*\\n, \\nРа\\nКАК СТАТЬ АВТОРОМ\\n', 'Несмотря на то, что постепенно оптические диски уходят в прошлое,\\nиспользование ISO-образов этих дисков остаётся актуальным.\\nМногие операционные системы поставляются в виде ISO-образов, а\\nадминистраторам необходимо поддерживать разношёрстный парк\\nстарых персональных компьютеров.\\nСуществует множество решений, как можно установить\\nоперационную систему с ISO-образа без записи его на оптический\\nноситель. Я уже затрагивал тему ISO-образов в моих статьях:\\n«Раскрываем секреты загрузочных ISO-образов» и «Что вам нужно\\nзнать о внешних загрузочных дисках».\\nВ этой статье я хочу рассказать о ещё одном способе, который, как\\nоказывается, вшит в ядро Linux. Если ваш одноплатный компьютер\\nимеет USB OTG-разъём, и на него возможна установка Linux, то вы c\\nбольшой долей вероятности можете сделать из одноплатника\\nаппаратный эмулятор привода оптических дисков. \\nМеня этот способ заинтересовал. Я проверил его сам и, получив\\nположительный результат у себя, решил поделиться с вами. \\nЯ сам узнал много интересного, систематизировал свои знания,\\nпоэтому надеюсь, что чтение будет познавательно и интересно для\\nвас.\\nКак всегда, если вы хотите посмотреть, что получится в итоге,\\nуточнить детали, вы всегда можете найти исходный код в моём\\nрепозитории на GitHub.\\nПри написании статьи я поставил себе следующие цели:\\n1. Аппаратный эмулятор CD-ROM должен быть реализован без\\nиспользования паяльника и макетных плат.\\n2. Реализация должна быть понятна человеку, имеющему лишь\\nбазовые представления о Linux, USB и Bluetooth.\\n', '3. Решение должно быть таким, чтобы его можно было с\\nнебольшими изменениями реализовать на различных\\nодноплатных компьютерах.\\n4. Побудить интерес читателя к изучению используемых в статье\\nтехнологий.\\n5. Изложить материал, необходимый для решения задачи\\nлаконично и просто. Не уверен, что у меня это получилось из-\\nза большого объёма темы. Буду признателен, если вы в\\nотзывах напишете своё мнение.\\nОглавление\\n• Суть решения\\n• Проверка решения на практике\\n• От проверки идеи до реализации\\n• Операционная система Linux\\n• USB\\n• Bluetooth\\n• Сборка и модификация дистрибутива Raspberry Pi OS\\n• Реализация\\n• Как пользоваться эмулятором\\n• Особенности моего эмулятора\\n• Выводы\\nСуть решения\\nРешение заключается в том, что, модифицируя операционную\\nсистему Linux на одноплатном компьютере (встраиваемой системе),\\nможно получить из него устройство, которое будет распознаваться\\nкомпьютером как внешний оптический привод USB или флеш-\\nнакопитель.\\n', 'В ядро Linux включена поддержка эмуляции CD-ROM и эмуляции\\nфлеш-накопителя. Но это не значит, что любую встраиваемую\\nсистему можно превратить в них. Для этого ещё необходимо, чтобы\\nвстраиваемая система имела USB OTG-контроллер или USB-\\nконтроллер периферийного устройства.\\nПроверка решения на практике\\nЯ делал эмулятор оптических дисков, используя Raspberry Pi Zero 2\\nW. Но вы можете использовать и другие одноплатные компьютеры.\\nЕстественно, вам тогда придётся самим разбираться с некоторыми\\nпроблемами, которые с большой долей вероятностью у вас\\nвозникнут. У меня других одноплатных компьютеров кроме Raspberry\\nPi не было, поэтому привожу алгоритм, как делал я.\\n1. Скачать образ Raspberry OS Light с сайта raspberrypi.org.\\n2. Записать образ на SD-карту. Я использовал программу\\nbalenaEtcher.\\n3. Добавить строку dtoverlay=dwc2  в файле config.txt на SD-\\nкарте.\\n4. Записать файлы ssh и wpa_supplicant.conf на SD-карту. Файл\\nuserconf.txt нужен, чтобы установить пароль для пользователя,\\nssh — чтобы включить SSH, wpa_supplicant.conf — чтобы\\nуказать точку доступа и пароль для Wi-Fi.\\n5. Вставить SD-карту в Raspberry Pi Zero 2 W.\\n6. Подключить USB-разъём Raspberry Pi Zero 2 W к USB-разъёму\\nкомпьютера.\\n7. Подождать, пока выполнится первая загрузка и Raspberry Pi Zero\\n2 W подключится к Wi-Fi-сети.\\n8. Найти IP-адрес Raspberry Pi Zero 2 W в Wi-Fi-сети и\\nподключиться к нему по протоколу SFTP. Я использовал\\nприложение WinSCP.\\n', '9. Записать ISO-образы, которые вы хотите эмулировать, в\\nфайловую систему Raspberry Pi.\\n10. Подключиться через SSH к Raspberry Pi W 2. Это можно сделать\\nпри помощи приложения PuTTY.\\n11. Для того, чтобы ваш Raspberry Pi Zero 2 W стал внешним USB\\nCD-ROM, ввести команду:\\nПосле чего у вас на компьютере распознается внешний USB CD-\\nROM, в который вставлен диск.\\n12. Для прекращения эмуляции ввести команду:\\nПриведённый файл userconf.txt устанавливает для пользователя pi\\nпароль «raspberry».\\nФайл ssh — это пустой файл, который не содержит никаких данных.\\nОт проверки идеи до реализации\\nПриведённая выше последовательность шагов позволяет вам\\nпосмотреть работу эмуляции в действии. Однако это решение\\nобладает рядом недостатков.\\n$ sudo modprobe g_mass_storage cdrom=y removable=y stall\\n$ sudo modprobe -r g_mass_storage\\nФайл userconf.txt\\nФайл wpa_supplicant.conf\\n', '1. Чтобы загрузить и выбрать образ для эмуляции, необходимо\\nналичие Wi-Fi.\\n2. При перезагрузке Raspberry Pi необходимо заново монтировать\\nобраз.\\n3. Если нужно эмулировать образ для загрузки с него\\nоперационной системы, понадобится ещё один компьютер для\\nуправления Raspberry Pi.\\n4. Эмулировать можно образ размером максимум 2 Gib.\\nЕсли вам интересно, как избавиться от этих недостатков, у вас есть\\nвремя и интерес разобраться в этом вопросе, то предлагаю\\nпродолжить чтение.\\nКраткое содержание моей реализации следующее:\\n1. Для общения с Raspberry Pi будет использоваться Bluetooth.\\n2. Чтобы работа устройства была возможна в автономном режиме\\n(без подключений Wi-Fi и Bluetooth), управляющий скрипт\\nоформляется в виде службы Systemd.\\n3. Для управления по Bluetooth будет использоваться приложение\\nSerial Bluetooth Terminal c Google Play.\\n4. Для эмуляции оптических дисков с образов размером больше\\n2Gib необходимо внести небольшие изменения в модуль ядра\\nLinux и выполнить перекомпиляцию.\\nПриведу кратко, что вам нужно знать, чтобы лучше понять суть того,\\nчто мы будем делать.\\nОперационная система Linux\\n▍ Ядро Linux\\n', 'Ядро Linux содержит в себе абстракции для работы с устройствами\\nтам, где оно запускается. Реализуются эти абстракции в\\nспециальных программах, называемых драйверами. В ОС Linux\\nдрайвер может находиться непосредственно в файле ядра, а может\\nбыть оформлен в виде отдельного модуля. В большинстве случаев\\nпредпочтителен второй способ, так как модули можно динамически\\nудалять и добавлять. Например, если устройство не подключено к\\nсистеме или с ним не осуществляется работа, драйвер нам не\\nнужен, и его можно выгрузить из памяти или не загружать вообще.\\nПри загрузке ядра ему необходима информация об устройствах,\\nкоторые присутствуют в системе, чтобы корректно загрузить\\nдрайверы (модули) для них. Эта информация может передаваться из\\nразличных источников. Например, в архитектуре x86 это будет ACPI.\\nВ архитектуре ARM это Device Tree.\\n▍ Device Tree и Overlays\\nИногда Device Tree нужно модифицировать, чтобы можно было\\nзагрузить корректно драйвера для устройств. Делается это при\\nпомощи подключения overlays. Они содержат информацию, что\\nнеобходимо изменить в исходном Device Tree.\\n▍ Headless-режим работы Raspberry Pi\\nОчень часто в различных статьях и самоучителях по работе с\\nRaspberry Pi необходимо подключение монитора, клавиатуры и\\nмыши. Но на самом деле есть возможность работать с ним в так\\nназываемом headless-режиме. В этом режиме вы работаете с\\nRaspberry Pi при помощи эмулятора терминала. Соединение его с\\nRaspberry Pi может быть UART, USB, Bluetooth, Ethernet, Wi-Fi.\\nГлавная сложность заключается в том, как можно работать в этом\\nрежиме с самого начала, сразу после записи образа операционной\\nсистемы на SD-карту, если у вас нет лишнего монитора и\\nклавиатуры. Как активировать SSH, настроить Wi-Fi на\\n', 'использование определённой точки доступа?\\nВ Raspberry Pi OS такая возможность есть. Достаточно разместить\\nопределённые файлы в разделе FAT32 на SD-карте и загрузиться с\\nнеё. Raspberry Pi OS сделает необходимые настройки сама.\\n▍ Файловые системы, блочные устройства, разделы,\\nмонтирование\\nМеня всегда восхищала идея, что в Linux всё является файлом.\\nПравильно используя средства Linux, можно практически без\\nпрограммирования выполнять сложные задачи.\\nБлочное устройство — это некоторый файл, в который можно\\nзаписывать и считывать данные блоками байтов различной длины.\\nЧто будет происходить при этом, зависит от того, с чем реально\\nассоциирован этот файл. Например, если он ассоциирован с\\nжёстким диском, то тогда будут читаться/записываться данные на\\nжёсткий диск, не обращая внимание на разделы и файловые\\nсистемы. Если он ассоциирован с разделом жёсткого диска, то будут\\nчитаться/записываться данные, не обращая внимание на файловую\\nсистему.\\nПри помощи команды losetup можно добиться того, что он будет\\nассоциироваться с обычным файлом на диске, что позволит\\nсоздавать образы разделов и дисков. \\nЕщё полезной командой Linux является команда kpartx, которая\\nсоздаёт блочные устройства из файла образа диска. Каждое из\\nустройств будет ассоциировано с образом раздела, который\\nхранится в этом файле.\\nФорматирование раздела в Linux выполняется одной командой. В\\nкачестве параметра необходимо передать имя файла блочного\\nустройства. Например, для создания файловой системы exFAT на\\nблочном устройстве /dev/mmcblk0p3:\\n', 'Чтобы можно было работать с файлами файловой системы,\\nразмещённой на блочном устройстве, нужно примонтировать\\nфайловую систему к корневой файловой системе при помощи\\nкоманды mount.\\nОбратите внимание, что директория, куда будет производиться\\nмонтирование, должна существовать до того, как вы будете\\nмонтировать. Если нужно размонтировать файловую систему,\\nиспользуется команда umount.\\nЧтобы посмотреть, какие у вас есть блочные устройства, и куда они\\nпримонтированы, можно использовать команду:\\n▍ Systemd\\nЗагрузка Linux происходит в несколько этапов. Сначала загружается\\nядро операционной системы, затем ядро запускает процесс init.\\nЗадача процесса init загрузить и инициализировать процессы\\nпространства пользователя и находиться в памяти до перезагрузки\\nили выключения компьютера (устройства). За долгие годы\\nсуществования Linux было написано множество реализаций init. На\\nданный момент во многих Linux-дистрибутивах используется\\nреализация, называемая systemd. Её мы и будем использовать.\\n$ mkfs.exfat /dev/mmcblk0p3\\n$ mkdir -p /mnt/data\\n$ mount -t auto /dev/mmcblk0p3 /mnt/data\\n$umount /mnt/data\\n$ lsblk\\n', 'Минимум команд, которые необходимо знать для работы с systemd.\\nКоманда\\nНазначение\\nsystemctl start\\nsn.service\\nзапустить службу\\nsystemctl stop\\nsn.service\\nостановить службу\\nsystemctl status\\nsn.service\\nпосмотреть статус службы\\nsystemctl enable\\nsn.service\\nвключить службу (служба будет\\nавтоматически запущена при следующей\\nзагрузке Linux)\\nsystemctl disable\\nsn.service\\nвыключить службу\\njournalctl -u\\nsn.service -b\\nпосмотреть логи службы, начиная с момента\\nпоследней загрузки Linux\\n▍ Терминалы, PuTTY, sshd, agetty\\nДля администрирования ОС Linux из Windows часто используют\\nэмулятор терминала PuTTY. Он позволяет подключаться к\\nкомпьютеру или устройству c ОС Linux с помощью различных\\nсоединений (Ethernet, Wi-Fi, эмулируемого последовательного порта\\nна Bluetooth или USB) и работать удалённо с консолью Linux в\\nWindows. Чтобы такое было возможно, в ОС Linux должна быть\\nзапущена специальная программа, которая будет\\nвзаимодействовать с PuTTY. Это может быть sshd в случае SSH-\\nсоединения или agetty в случае последовательного порта.\\nПри подключении через последовательный порт по умолчанию вы\\nувидите чёрно-белый экран без поддержки манипулятора мышь.\\nЧтобы добавить поддержку мыши и цветного экрана, необходимо\\n', 'изменить значение переменной окружения TERM в файле\\n/usr/lib/systemd/system/serial-getty@.service. \\nUSB\\nЧтобы два USB-устройства могли работать друг с другом,\\nнеобходимо наличие у каждого из них USB-контроллера. USB-\\nконтроллер в конкретный момент времени может работать в режиме\\nхоста (Host) или режиме периферийного устройства (Device). Если\\nодно из взаимодействующих устройств работает в режиме Host, то\\nдругое должно работать в режиме Device. Существуют следующие\\nвиды USB-контроллеров:\\n• Host — всегда работает в режиме Host.\\n• Device — всегда работает в режиме Device.\\n• OTG — может работать или в режиме хоста или в режиме\\nпериферийного устройства. Переключение режимов может быть\\nаппаратным (при помощи особой распайки кабеля OTG кабель\\nпереводит в контроллер режим хоста) или программным\\nРежим хоста подразумевает посылку команд, а режим\\nпериферийного устройства — их обработку. \\n▍ OTG USB-контроллер\\nВозьмём Android-телефон с OTG-контроллером. Это означает, что\\nпри сопряжении по USB с компьютером (для записи файлов с\\nкомпьютера на телефон), он будет играть роль периферийного\\nустройства, а при сопряжении по USB с периферийным устройством\\n(мышью, клавиатурой, сетевой картой, флэш-накопителем,\\nмонитором, принтером) телефон будет играть роль хоста.\\n[Service]\\nEnvironment=TERM=xterm\\n', 'Обычно USB-контроллер периферийного устройства или USB OTG-\\nконтроллер присутствуют во встраиваемых устройствах. Также они\\nмогут быть интегрированы в однокристальную систему (SoC). Но по\\nфакту на устройстве может отсутствовать физический USB-разъём\\nдля подключения.\\nНапример, на всех Raspberry Pi установлена SoC, которая имеет\\nOTG-контроллер, но фактически физический разъём для него есть\\nтолько в Raspberry Pi Zero (Zero W, Zero 2 W) и в Raspberry Pi 4.\\n▍ Дескрипторы USB\\nКаждое USB-устройство имеет дескрипторы. Дескрипторы — это\\nинформация о USB-устройстве, которая используется операционной\\nсистемой для корректного выбора драйвера для устройства. Мне\\nпонравилось описание, которое приведено на сайте Microsoft.\\n▍ Создание USB-устройств в Linux\\nЯдро Linux содержит модули, которые позволяют создавать\\nвиртуальные USB-устройства. Это может быть Mass Storage,\\nпоследовательный порт, сетевая карта. Загрузив и настроив эти\\nмодули, вы можете сделать так, чтобы компьютером ваш\\nодноплатник распознавался одним или несколькими такими\\nустройствами. \\nЕсли вам достаточно одного устройства, то вы можете загрузить\\nмодуль для этого устройства, опционально передав ему параметры\\nдля конфигурации при помощи команды modprobe. Когда отпадёт\\nнеобходимость в этом устройстве, его можно выгрузить при помощи\\nкоманды modprobe -r.\\nЧтобы на одном физическом порту у вас распознавалось несколько\\nустройств одновременно, нужно использовать модуль libcomposite и\\nсконфигурировать эти устройства при помощи создания структур в\\nфайловой системе ConfigFS в директории\\n', '/sys/kernel/config/usb_gadget.\\nТакие устройства называются композитными USB-устройствами. Вы,\\nскорее всего, встречались с такими, например, если у вас\\nбеспроводная клавиатура и мышь, а для них используется один\\nприёмопередатчик.\\nВ нашем случае мы создадим композитное USB-устройство, которое\\nбудет последовательным портом и устройством хранения.\\nПоследовательный порт мы будем использовать для подключения к\\nнашему эмулятору оптических дисков через PuTTY. Изначально я\\nхотел, что бы это была сетевая карта и SSH, но карта требует\\nнастройки в операционной системе компьютера, поэтому для\\nпростоты отказался от этой идеи в пользу последовательного порта.\\n▍ Создание композитного USB-устройства при помощи\\nConfigFS\\n1. Загружаем модуль libcomposite.\\n2. Заполняем дескрипторы для устройства.\\nmodprobe libcomposite\\n$ usb_dev=/sys/kernel/config/usb_gadget/cdemu\\n$ mkdir -p $usb_dev\\n$ echo 0x0137 > $usb_dev/idProduct\\n$ echo 0x0100 > $usb_dev/bcdDevice\\n$ echo 0x0200 > $usb_dev/bcdUSB\\n$ echo 0xEF  > $usb_dev/bDeviceClass\\n$ echo 0x02 > $usb_dev/bDeviceSubClass\\n$ echo 0x01 > $usb_dev/bDeviceProtocol\\n', '3. Создаём конфигурацию.\\n4.Создаём и подключаем функцию acm (последовательный порт\\nчерез USB).\\n5. Создаём и подключаем функцию mass_storage. Mass_storage в\\nданном случае — это эмуляция CD-ROM для ISO-образа\\n/home/pi/1.iso.\\n6. Активируем созданное устройство.\\n$ mkdir -p $usb_dev/strings/0x409\\n$ echo \"abababababababa\" > $usb_dev/strings/0x409/serialnumb\\n$ echo \"Linux Foundation\" > $usb_dev/strings/0x409/manufactu\\n$ echo \"USB CD-ROM Emulator\" > $usb_dev/strings/0x409/produc\\nmkdir -p $usb_dev/configs/c.1\\nmkdir -p $usb_dev/configs/c.1/strings/0x409\\necho \"acm+usb\" > $usb_dev/configs/c.1/strings/0x409/configur\\necho \"0x80\" > $usb_dev/configs/c.1/bmAttributes\\necho 250 > $usb_dev/configs/c.1/MaxPower\\n$ mkdir -p $usb_dev/functions/acm.usb0\\n$ ln -s $usb_dev/functions/acm.usb0 $usb_dev/configs/c.1\\n$ mkdir -p $usb_dev/functions/mass_storage.usb0/lun.0\\n$ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/cdrom\\n$ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/remova\\n$ echo 0 > $usb_dev/functions/mass_storage.usb0/lun.0/nofua\\n$ echo 0 > $usb_dev/functions/mass_storage.usb0/stall\\n$ echo \"/home/pi/1.iso\" > $usb_dev/functions/mass_storage.us\\n$ ln -s $usb_dev/functions/mass_storage.usb0 $usb_dev/config\\n', '▍ Удаление композитного USB-устройства при помощи\\nConfigFS\\n1. Деактивируем устройство.\\n2. Удаляем функцию mass_storage.\\n3. Удаляем функцию acm.\\n4. Удаляем конфигурацию.\\n5. Удаляем устройство.\\n$ ls /sys/class/udc > $usb_dev/UDC\\n$ usb_dev=/sys/kernel/config/usb_gadget/cdemu\\n$ echo \"\"> $usb_dev/UDC\\n$ rm $usb_dev/configs/c.1/mass_storage.usb0\\n$ rmdir $usb_dev/functions/mass_storage.usb0\\n$ rm $usb_dev/configs/c.1/acm.usb0\\n$ rmdir $usb_dev/functions/acm.usb0\\n$ rmdir $usb_dev/configs/c.1/strings/0x409\\n$ rmdir $usb_dev/configs/c.1\\n$ rmdir $usb_dev/strings/0x409\\n$ rmdir $usb_dev\\n', '6. Выгружаем загруженные устройством модули.\\nCтруктура файловой системы для созданного эмулятора CD-ROM\\nBluetooth\\nТема Bluetooth очень объёмная, и её невозможно изложить в одной\\nстатье, поэтому приведу только тот минимум, который позволяет\\nпонять, как мы будем использовать Bluetooth.\\nBluetooth — технология, которая позволяет связывать устройства без\\nпроводов по радиоканалу. На данный момент существует множество\\nверсий спецификации Bluetooth. Спецификация Bluetooth освещает\\n$ modprobe -r usb_f_mass_storage\\n$ modprobe -r usb_f_acm\\n$ modprobe -r libcomposite\\n', 'множество вопросов.\\nЧтобы передать данные с одного устройства на другое, необходимо\\nналичие на обоих устройствах контроллеров и стеков Bluetooth.\\nBluetooth-контроллер — аппаратное устройство, обычно\\nвыполненное в виде микросхемы или части более сложной\\nмикросхемы, позволяющее получать/передавать данные по\\nрадиоканалу в соответствии со спецификацией Bluetooth.\\nBluetooth-стек — программная реализация протоколов, описанных в\\nспецификации Bluetooth.\\nПротоколы Bluetooth, предназначенные для решения определённых\\nзадач, группируются в профили Bluetooth.\\nМы будем использовать два профиля Bluetooth:\\n1. Generic Access Profile (GAP), который поддерживается всеми\\nBluetooth-устройствами.\\n2. Serial Port Profile (SPP), который подразумевает использование\\nпоследовательного порта поверх соединения Bluetooth.\\n▍ Поддержка Bluetooth операционными системами\\nBluetooth-контроллеры могут иметь различные аппаратные\\nинтерфейсы для доступа. Это может быть UART, USB, PCIe. В\\nслучае операционной системы многие детали скрываются, и можно о\\nних не думать. С контроллером можно работать на низком уровне\\nчерез драйвер или уже используя высокоуровневые библиотеки и\\nприложения, предоставляемые стеком Bluetooth, например, в Linux\\nшироко распространён стек BlueZ. \\n▍ BlueZ\\n', 'Стек Bluetooth BlueZ состоит из двух частей.\\nОдна часть представлена модулями ядра Linux, уже включена в\\nядро. Если она отсутствует, то её нужно включить и\\nперекомпилировать ядро.\\nВторая часть представлена приложениями для пространства\\nпользователя. Приложения позволяют конфигурировать и работать\\nсо стеком Bluetooth.\\nНа данный момент многие приложения считаются устаревшими, и\\nразработчики BlueZ рекомендуют использовать более новые\\nприложения и интерфейс D-Bus для работы со стеком.\\nНо, как мне кажется, именно те старые, устаревшие приложения\\nпозволяют лучше понять работу Bluetooth, поэтому в учебных целях\\nя буду использовать их, для чего нужно будет инициализировать\\nBlueZ в режиме совместимости. \\n▍ Протоколы Bluetooth\\nЯ не буду утомлять вас различными схемами, диаграммами, которые\\nвы легко можете найти в интернете. Расскажу только о тех\\nпротоколах, с которыми нам предстоит столкнуться и нужно будет\\nсконфигурировать. \\n▍ Service Discovery Protocol (SDP)\\nПри помощи протокола SDP можно определить, какие приложения\\n(сервисы) находятся на хосте, и с ними возможен обмен данными\\nчерез Bluetooth\\nЧтобы можно было увидеть сервис с другого устройства, его\\n$ bluetoothd --noplugin=sap -C\\n', 'необходимо зарегистрировать в SDP database. Например, если мы\\nхотим зарегистрировать службу, представляющую эмуляцию\\nпоследовательного порта в Bluetooth, это можно сделать следующей\\nкомандой:\\nЧтобы можно было посмотреть службы, зарегистрированные у вас\\nна хосте, нужно ввести команду:\\n▍ Radio Frequency Communications (RFCOMM)\\nПротокол RFCOMM позволяет создавать виртуальное соединение по\\nпоследовательному порту между двумя хостами.\\nНа одном из хостов создаётся сервер, которому выделяется канал\\nRFCOMM, второй из хостов подключается к нему, указывая MAC-\\nадрес и номер канала\\nКанал RFCOMM немного напоминает порт в UDP или TCP, но если у\\nних и у источника и у получателя есть порты, то у RFCOMM для\\nисточника и получателя один и тот же канал. Поэтому невозможно\\nсоздать несколько подключений на один и тот же канал.\\nВ Linux можно использовать команду rfcomm для создания процесса,\\nкоторый будет слушать определённый канал RFCOMM и при\\nсоединении запускать другой процесс.\\nВ данном случае на Bluetooth-контроллере hci0 RFCOMM будет\\n$ sdptool add SP\\n$ sdptool browse local\\n$ rfcomm -r watch hci0 1 /usr/local/bin/cdemu-cmd /dev/rfcom\\n', 'прослушиваться канал 1 и запускаться процесс cdemu-cmd с двумя\\nпараметрами командной строки /dev/rfcomm0 и /dev/rfcomm0.\\n▍ Утилита bluetoothctl\\nУтилита Bluetoothctl позволяет сопрягать устройство, на котором вы\\nеё запустили с другим устройством.\\nВы можете сделать устройство видимым для обнаружения другими\\nустройствами, а также найти другое устройство и выполнить с ним\\nсопряжение. Более подробно расписано в документации к утилите,\\nкоторая доступна по команде: \\n▍ Serial Bluetooth Terminal\\nДля отладки приложений, использующих Bluetooth, удобно\\nиспользовать приложение для Android Serial Bluetooth Terminal. Это\\nприложение позволяет работать с Bluetooth-устройствами, у которых\\nдоступен профиль SPP. В нашем случае мы будем использовать его\\nкак визуальный интерфейс для работы с нашим эмулятором\\nоптических дисков.\\nСборка и модификация дистрибутива Raspberry Pi OS\\nЧтобы сделать полноценный аппаратный эмулятор оптических\\nдисков, нам придётся немного модифицировать исходный\\nдистрибутив Linux. Это подразумевает перекомпиляцию ядра,\\nизменение нескольких конфигурационных файлов и добавление\\nсвоего программного кода. Для меня это было удобно сделать при\\nпомощи Docker.\\n▍ Кросс-компиляция ядра Linux\\nКросс-компиляция позволяет на компьютере с одной архитектурой\\n$ man bluetoothctl\\n', 'получать исполняемые файлы для другой архитектуры. Мы можем\\nкомпилировать ядро Linux для Raspberry Pi на Raspberry Pi, а можем,\\nиспользуя кросс-компиляцию, сделать это на обычном компьютере с\\nархитектурой x86, что существенно сократит время компиляции из-за\\nбольшего быстродействия компьютера. Подробно о том, как\\nвыполнять кросс-компиляцию Raspberry Pi OS, можно почитать тут.\\n▍ Chroot и запуск бинарных файлов другой архитектуры\\nКоманда Linux chroot позволяет запускать процессы с изменённой\\nкорневой системой. Это кажется немного запутанным, но суть в\\nследующем. В качестве параметра команде передаётся путь к\\nкорневой директории. В результате запуска команды через chroot\\nзапущенный процесс будет считать, что корнем файловой системы\\nявляется та директория, которую передали в качестве параметра. \\nПрименений у команды chroot несколько, например, её можно\\nиспользовать, чтобы запустить команду apt для Raspberry Pi в\\nDocker-контейнере.\\nИнтересно, что Docker Desktop для Windows позволяет запускать\\nисполняемые файлы для архитектуры ARM. В Linux-версии Docker\\nтакое сделать можно, но нужна дополнительная настройка.\\nРеализация\\nСозданный мной проект состоит из следующих файлов:\\n1. Dockerfile и скрипт, который выполняется в Docker-контейнере.\\n2. Файлы, которые необходимо добавить или обновить в исходном\\nдистрибутиве:\\n• cdemu — основная логика работы эмулятора оптических\\nдисков, написанная на языке bash;\\n', '• cdemu-cmd — bash-скрипт для обработки команд от\\nпользователя и передачи их эмулятору;\\n• bash-utils.sh — bash-скрипт co вспомогательными\\nфункциями;\\n• cdemu-bluetooth-ui.service — systemd-служба, которая\\nзапускает интерпретатор команд на создаваемом RFCOMM-\\nсоединении телефона и Raspberry Pi;\\n• cdemu.service — systemd-служба, которая запускает\\nэмулятор оптических дисков при загрузке;\\n• bluetooth.service — изменённая служба systemd для\\nинициализации bluetooth;\\n• serial-getty@.service — изменённая служба systemd для\\nзапуска agetty на создаваемом соединении на\\nпоследовательном порту;\\n• firstboot.service — служба systemd для запуска скрипта при\\nпервой загрузке операционной системы. Я её\\nпозаимствовал из проекта raspberian-firstboot;\\n• config.txt — изменённый файл конфигурации для загрузки\\nRaspberry Pi. Содержит подключение overlay dwc. Это\\nнеобходимо, чтобы USB-контроллер мог работать в device\\nmode;\\n• fistboot.sh — скрипт, который запускается службой systemd\\nfirstboot.service;\\n• userconf.txt — файл, который необходим, чтобы установить\\nпароль для пользователя pi. В последних версиях Raspberry\\nPi OS пользователь pi не активирован по умолчанию,\\nпоэтому необходимо наличие этого файла;\\n• ssh — файл необходим, чтобы активировать ssh, который\\nотключён по умолчанию;\\n• wpa_supplicant.conf — файл, необходимый, если вы хотите\\nнастроить Raspberry Pi на работу с вашей точкой доступа.\\n', 'Листинги файлов не привожу, так как это ещё больше раздует и так\\nбольшую статью. \\nОзнакомиться вы с ними можете здесь.\\nКак пользоваться эмулятором\\n1. Собираем Docker-образ.\\n2. Собираем образ RaspberryPi OS.\\n3. Записываем образ на SD-карту. Вставляем её в Raspberry Pi.\\n4. Подключаем Rapsberry Pi Zero 2 W к компьютеру.\\n5. Через некоторое время у вас появится съёмный накопитель.\\n6. На этот съёмный накопитель, содержащий файл Readme.txt,\\nкопируем образы, которые хотим эмулировать.\\n7. Находим виртуальный COM-порт, созданный после подключения\\nRaspberry Pi к компьютеру.\\n8. Подключаемся к Raspberry Pi через с помощью Putty через\\nвиртуальный COM-порт.\\n9. Запускаем интерактивное приложение для управления\\nэмулятором.\\n10. Если хотим сделать управление с телефона, то выполняем\\nсопряжение телефона и Raspberry Pi. Для чего вводим в эмуляторе\\nтерминала команду:\\ndocker build -t raspi-image .\\ndocker run --privileged -v c:\\\\temp:/build --name raspi-image\\n$ sudo cdemu-cmd\\n$ sudo bluetoothctl\\n', '11. Делаем Raspberry Pi доступным для обнаружения:\\n12. Находим его на телефоне и выполняем полключение. После чего\\nсоглашаемся с PIN-кодом на телефоне и Raspberry Pi.\\n13. Выходим из bluetoothctl.\\n14. Запускаем на телефоне Serial Bluetooth Terminal и выполняем\\nподключение к Raspberry Pi из него. Теперь можно посылать\\nкоманды созданному эмулятору CD-ROM.\\nКоманды, которые можно посылать эмулятору:\\n1. hdd — переключение в режим эмуляции внешних жёстких\\nдисков.\\n2. cdrom — переключение в режим эмуляции внешних приводов\\nоптических дисков.\\n3. list — вывести список доступных ISO-образов, которые можно\\nэмулировать.\\n4. insert <порядковый номер> — поместить ISO-образ для\\nэмуляции.\\n5. eject — извлечь ISO-образ из эмулятора.\\n6. help — показать список доступных команд в текущем режиме.\\nОсобенности моего эмулятора\\ndiscoverable on\\nyes\\nexit\\n', 'Интересно, но в Linux по умолчанию нельзя эмулировать ISO-образы\\nразмером больше 2 Gib. Я просмотрел исходный код драйвера в\\nфайле drivers/usb/gadget/function/storage_common.c и предположил,\\nчто нет оснований не применять патч к ядру Linux от Adam Bambuch,\\nкоторый просто удаляет одно условие. Образы эмулировались\\nнормально и при снятии ISO-образа с эмулируемого CD-ROM он был\\nидентичен исходному. Поверял по хеш-коду для файла ISO-образа.\\nНе пойму, почему есть это ограничение в Linux и почему его до сих\\nпор не убрали? Если вы знаете ответ, ответьте в комментариях.\\nМоя реализация не требует никаких дополнительных деталей. Нужен\\nтолько Raspberry Pi Zero 2 W, один или два кабеля USB и адаптер\\nпитания, если будете использовать два кабеля USB. Один для\\nпитания, второй для передачи данных. Хоть и использование\\nдополнительного кабеля и адаптера добавляет громоздкости, это\\nрешает проблему перезагрузки Raspberry Pi, если компьютер или\\nноутбук отключает ненадолго питание при перезагрузке.\\nКроме того, я не использую Python, только bash.\\nВыводы\\nПолученное программно-аппаратное решение, хоть и обладает\\nрядом недостатков по сравнению с карманом Zalman (не\\nподдерживается USB 3.0, нет интерактивного меню на самом\\nустройстве), позволит вам установить практически любую\\nоперационную систему на широкий спектр компьютеров путём\\nпростого копирования ISO-образа. \\nРешение является прототипом, но вместе с тем позволяет углубить\\nзнания по многим темам, или получить, если вы были с ними не\\nзнакомы.\\nТак как основной целью была разработка прототипа, я запускал\\nbluetoothd в режиме совместимости, и почти всю логику написал на\\n', 'bash.\\nЯ хотел показать возможность превратить встраиваемое устройство\\nс операционной системой Linux в аппаратный эмулятор флеш-\\nнакопителя или привода оптических дисков, приложив минимум\\nусилий. Надеюсь, что это удалось.\\nЧтобы уместить всё в одной статье, я лишь поверхностно коснулся\\nтех тем, которые необходимы для понимания. Если вас\\nзаинтересовало, вы можете самостоятельно изучить их углублённо.\\nОбъём статьи не позволяет осветить все интересности, с которыми я\\nстолкнулся при разработке эмулятора, и решения, которые применял\\nи проверял. Приведу лишь несколько из них.\\nНапример, я долго боролся с зависанием при удалении составного\\nустройства. Помогло использование службы serial-getty вместо getty,\\nхотя во многих статьях упоминалась getty.\\nЯ долго разбирался, как можно сделать сопряжение через Bluetooth\\nмежду Raspberry и телефоном, использовал команду bt-agent, но\\nпотом всё-таки отказался от неё в пользу bluetoothctl. \\nПри переключении эмулятора в режим HDD для записи ISO-образов\\nизначально я открывал для доступа всю SD-карту и хранил ISO-\\nобразы в отдельном разделе, но потом посчитал, что для\\nбезопасности лучше хранить образ диска с ISO-образами в\\nотдельном файле и открывать доступ только к нему, хоть это и\\nснизило скорость записи, но пользователя не обескураживают\\nпоявляющиеся несколько дисков.\\nРазработанный прототип есть куда улучшать. Можно, например,\\nсоздать более минималистичный дистрибутив Linux, который будет\\nсодержать только то, что реально используется для эмуляции, или\\nсоздать более удобное графическое приложение для Android для\\nработы с эмулятором. А можно упростить работу с Bluetooth,\\nнапрямую работая с драйверами bluetooth или используя интерфейс\\n', 'D-Bus для работы с Bluetooth-устройствами. Или вообще всё-таки\\nвзяться за паяльник и сделать устройство, более похожее по\\nфункционалу на карман Zalman. Но главное, вы увидели, что это\\nреально сделать, а когда видишь положительный результат, это\\nвдохновляет на большее творчество.\\nВ процессе тестирования и отладки программного кода было\\nзамечено, что на Lenovo X1 Extreme Gen 2 эмулятор CD-ROM дисков\\nвеликолепно определялся в Windows 10, но отказывался\\nопределяться в BIOS. Эмпирически было определено, что помогает\\nотключение режима экономии энергии процессора в BIOS. Также\\nноутбук отключал питание на usb при перезагрузке, поэтому\\nпонадобилось дополнительное питание Raspberry Pi.\\nИнтересно, но на ASUS K53E и Gigabyte BRIX всё работает без\\nпроблем.\\nРешение с небольшими модификациями можно реализовать на\\nRaspberry Pi 4. Но если вы поняли суть решения, вы его сможете\\nповторить и на других одноплатных компьютерах, которые имеют\\nвыведенные USB-порты для OTG или USB-контроллеры\\nпериферийных устройств.\\nDockerfile на данный момент только выполняется в Docker Desktop\\nдля Windows. В Linux он работать не будет.\\nВ заключение хочу сказать, что существует ещё один способ\\nэмулировать оптические диски, который я не пробовал, но знаю о его\\nсуществовании из ваших комментариев к одной из моих статей —\\nэто программа DriveDroid для Android. Я ей не пользовался, так как\\nдля её работы нужно получать права root на телефоне. Но, скорее\\nвсего, из-за ограничений в ядре Linux программа поддерживает ISO-\\nобразы до 2 Gib и/или работает только с гибридными ISO-образами.\\nЕсли я не прав, буду рад увидеть ваши опровержения в\\nкомментариях.\\n', 'RUVDS.com\\nVDS/VPS-хостинг. Скидка 15% по коду HABR15\\nTelegram\\nВКонтакте\\nTwitter\\n84\\nКарма\\n82.1\\nРейтинг\\n@artyomsoft\\nПользователь\\nКомментарии 17\\nПубликации\\nTelegram-канал с розыгрышами призов, новостями IT и\\nпостами о ретроиграх \\n️\\nТеги:  \\nlinux kernel\\n,\\nusb\\n,\\nbluetooth\\n,\\nbluez\\n,\\nlinux modules\\n,\\nэмуляция\\n,\\niso\\n,\\ncd-rom\\n,\\nsystemd\\n,\\nagetty\\n,\\nruvds_статьи\\nХабы:  \\nБлог компании RUVDS.com\\n,\\nСистемное администрирование\\n,\\nРазработка под Linux\\n,\\nРазработка на Raspberry Pi\\n,\\nDIY или Сделай сам\\n+84\\n98\\n17\\n', 'ЛУЧШИЕ ЗА СУТКИ \\nПОХОЖИЕ\\n', ' · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\nИНФОРМАЦИЯ\\nСайт\\nruvds.com\\nДата регистрации\\n18 марта 2016\\nДата основания\\n27 июля 2015\\nЧисленность\\n11–30 человек\\nМестоположение\\nРоссия\\nПредставитель\\nruvds\\n', 'Ваш\\nаккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nНастройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "Как создать аппаратный эмулятор CD-ROM.pdf\n",
      "['2394.92', 'Рейтинг', 'RUVDS.com', 'VDS/VPS-хостинг. Скидка 15% по коду HABR15', ' 20 мар в 12:00', 'Средний19 мин', 'artyomsoft', 'Как создать аппаратный эмулятор CD-ROM', 'без паяльника', '9.2K', 'Блог компании RUVDS.com', ', ', 'Системное администрирование*', ', ', 'Разработка под Linux*', ', ', 'Ра', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "1 2394.92 RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15  20 мар в 12:00\n",
      "['Все потоки\\nДобавить компанию\\nRUVDS.com\\nVDS/VPS-хостинг. Скидка 15% по коду HABR15\\nПишет в хабы:  Читальный зал,\\u2004DIY или Сделай сам,\\u2004Научно-популярное,\\u2004Электроника для начинающих,\\u2004\\nСистемное администрирование\\n2394.92\\n72K\\nSelectel\\nIT-инфраструктура для бизнеса\\nПишет в хабы:  Компьютерное железо,\\u2004Гаджеты,\\u2004IT-инфраструктура,\\u2004IT-компании,\\u2004\\nПроизводство и разработка электроники\\n1935.27\\n40K\\nTimeweb Cloud\\nОблачная платформа для разработчиков и бизнеса\\nПишет в хабы:  Читальный зал,\\u2004Научно-популярное,\\u2004JavaScript,\\u2004DIY или Сделай сам,\\u2004История IT\\n1538.54\\n1.1K\\nOTUS\\nЦифровые навыки от ведущих экспертов\\nПишет в хабы:  Программирование,\\u2004Java,\\u2004Машинное обучение,\\u2004Тестирование веб-сервисов,\\u2004Python\\n830.05\\n103K\\nЯндекс\\nКак мы делаем Яндекс\\nПишет в хабы:  Машинное обучение,\\u2004Open source,\\u2004Высокая производительность,\\u2004Программирование,\\u2004\\nИнтерфейсы\\n721.54\\n164K\\nOpen Data Science\\nКрупнейшее русскоязычное Data Science сообщество\\nПишет в хабы:  Машинное обучение,\\u2004Искусственный интеллект,\\u2004Natural Language Processing,\\u2004Open source,\\u2004\\nBig Data\\nСТАТЬИ +63\\nНОВОСТИ +51\\nХАБЫ\\nАВТОРЫ\\nКОМПАНИИ\\nПоиск\\nНазвание \\nРейтинг \\nПодписчики\\nКАК СТАТЬ АВТОРОМ\\nИсследуем Духов Машин в новом Сезоне на Хабре\\n', '610.1\\n9.6K\\nГК ЛАНИТ\\nВедущая многопрофильная группа ИТ-компаний в РФ\\nПишет в хабы:  Управление проектами,\\u2004Карьера в IT-индустрии,\\u2004Управление персоналом,\\u2004\\nМашинное обучение,\\u2004Программирование\\n569.92\\n54K\\nVK\\nТехнологии, которые объединяют\\nПишет в хабы:  Big Data,\\u2004Машинное обучение,\\u2004DevOps,\\u2004Tarantool,\\u2004Облачные сервисы\\n536.94\\n167K\\ngetmatch\\nРассказываем о том, как строить карьеру в IT\\nПишет в хабы:  Карьера в IT-индустрии,\\u2004Лайфхаки для гиков,\\u2004Читальный зал,\\u2004IT-эмиграция,\\u2004Урбанизм\\n436.91\\n953\\nWiren Board\\nОборудование для автоматизации и мониторинга\\nПишет в хабы:  Умный дом,\\u2004Интернет вещей,\\u2004Инженерные системы,\\u2004DIY или Сделай сам,\\u2004\\nКомпьютерное железо\\n411.66\\n353\\nFirstVDS\\nВиртуальные и выделенные серверы в ДЦ в Москве\\nПишет в хабы:  Научно-популярное,\\u2004Системное администрирование,\\u2004Читальный зал,\\u2004\\nИнформационная безопасность,\\u2004IT-инфраструктура\\n398.02\\n24K\\nКонференции Олега Бунина (Онтико)\\nПрофессиональные конференции для IT-разработчиков\\nПишет в хабы:  Управление разработкой,\\u2004Конференции,\\u2004DevOps,\\u2004Управление проектами,\\u2004\\nКарьера в IT-индустрии\\n360.51\\n58K\\nAGIMA\\nКрупнейший интегратор digital-решений\\nПишет в хабы:  Карьера в IT-индустрии,\\u2004Управление проектами,\\u2004Конференции,\\u2004Управление персоналом,\\u2004\\nУправление продуктом\\n336.45\\n519\\nАльфа-Банк\\nЛучший мобильный банк по версии Markswebb\\nПишет в хабы:  Конференции,\\u2004Анализ и проектирование систем,\\u2004Машинное обучение,\\u2004JavaScript,\\u2004\\nКарьера в IT-индустрии\\n', '311.59\\n24K\\nPostgres Professional\\nРазработчик СУБД Postgres Pro\\nПишет в хабы:  PostgreSQL,\\u2004SQL,\\u2004Администрирование баз данных,\\u2004Open source,\\u2004Хранение данных\\n305.42\\n76K\\nOzon Tech\\nСтремимся делать лучший e-commerce в России\\nПишет в хабы:  Конференции,\\u2004Высокая производительность,\\u2004Go,\\u2004Тестирование веб-сервисов,\\u2004\\nПрограммирование\\n282.75\\n28K\\nTINKOFF\\nIT’s Tinkoff — просто о сложном\\nПишет в хабы:  Разработка под Android,\\u2004Машинное обучение,\\u2004Тестирование IT-систем,\\u2004\\nРазработка мобильных приложений,\\u2004Open source\\n282.47\\n40K\\nХабр Карьера\\nПомогаем строить карьеру в IT\\nПишет в хабы:  Карьера в IT-индустрии,\\u2004Управление персоналом,\\u2004Исследования и прогнозы в IT,\\u2004\\nБлог компании Хабр,\\u2004Финансы в IT\\n280.35\\n14K\\nJUG Ru Group\\nКонференции для Senior-разработчиков\\nПишет в хабы:  Конференции,\\u2004Java,\\u2004Тестирование IT-систем,\\u2004Разработка мобильных приложений,\\u2004JavaScript\\n276.72\\n41K\\nSouthbridge\\nОбеспечиваем стабильную работу highload-проектов\\nПишет в хабы:  IT-инфраструктура,\\u2004DevOps,\\u2004Системное администрирование,\\u2004Карьера в IT-индустрии,\\u2004\\nПрограммирование\\n273.3\\n47K\\n1\\n2\\n3\\n... 16\\n17\\nФИЛЬТР\\nВсе\\n323\\nВеб-разработка\\n98\\nПрограммное обеспечение\\n180\\n', 'Аппаратное обеспечение\\n40\\nДизайн и юзабилити\\n22\\nРеклама и маркетинг\\n9\\nСМИ\\n5\\nКонсалтинг и поддержка\\n49\\nРекрутинг и HR\\n9\\nЭлектронная коммерция\\n41\\nНекоммерческие организации\\n5\\nОптимизация\\n9\\nПроизводство мультимедиа-контента\\n9\\nСвязь и телекоммуникации\\n54\\nДомены и хостинг\\n17\\nПоисковые технологии\\n9\\nМобильные технологии\\n72\\nВеб-сервисы\\n97\\nИгры и развлечения\\n11\\nИнформационная безопасность\\n47\\nВаш аккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\n', 'Настройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "Компании _ Хабр.pdf\n",
      "['Все потоки', 'Добавить компанию', 'RUVDS.com', 'VDS/VPS-хостинг. Скидка 15% по коду HABR15', 'Пишет в хабы:  Читальный зал,\\u2004DIY или Сделай сам,\\u2004Научно-популярное,\\u2004Электроника для начинающих,\\u2004', 'Системное администрирование', '2394.92', '72K', 'Selectel', 'IT-инфраструктура для бизнеса', 'Пишет в хабы:  Компьютерное железо,\\u2004Гаджеты,\\u2004IT-инфраструктура,\\u2004IT-компании,\\u2004', 'Производство и разработка электроники', '1935.27', '40K', 'Timeweb Cloud', 'Облачная платформа для разработчиков и бизнеса', 'Пишет в хабы:  Читальный зал,\\u2004Научно-популярное,\\u2004JavaScript,\\u2004DIY или Сделай сам,\\u2004История IT', '1538.54', '1.1K', 'OTUS', 'Цифровые навыки от ведущих экспертов', 'Пишет в хабы:  Программирование,\\u2004Java,\\u2004Машинное обучение,\\u2004Тестирование веб-сервисов,\\u2004Python', '830.05', '103K', 'Яндекс', 'Как мы делаем Яндекс', 'Пишет в хабы:  Машинное обучение,\\u2004Open source,\\u2004Высокая производительность,\\u2004Программирование,\\u2004', 'Интерфейсы', '721.54', '164K', 'Open Data Science', 'Крупнейшее русскоязычное Data Science сообщество', 'Пишет в хабы:  Машинное обучение,\\u2004Искусственный интеллект,\\u2004Natural Language Processing,\\u2004Open source,\\u2004', 'Big Data', 'СТАТЬИ +63', 'НОВОСТИ +51', 'ХАБЫ', 'АВТОРЫ', 'КОМПАНИИ', 'Поиск', 'Название ', 'Рейтинг ', 'Подписчики', 'КАК СТАТЬ АВТОРОМ', 'Исследуем Духов Машин в новом Сезоне на Хабре', '']\n",
      "['КАК СТАТЬ АВТОРОМ\\n', 'Идея\\nА почему бы не использовать возможности chatGPT и попросить его\\nделать что-то за нас?\\nНапример, давайте попробуем настроить его так, чтобы он мог\\nзабронировать нам столик в наш любимый ресторан.\\nРеализация\\nШаг№1: Интеграция с WhatApp\\nПредположим, что нам известен номер в WhatsApp для\\nбронирования столиков в ресторане.\\nТогда нам понадобится сервис для автоматической отправки и\\nприема сообщений в WhatsApp. Таких сервисов много, для пример\\nвозьмем сервис https://wazzup24.com/, так как у него есть\\nбесплатный пробный период 3 дня и не требуется подключения\\nWhatsApp Business.\\nРегистрируемся на их сайте и получаем API ключ:\\n 6 часов назад\\n8 мин\\nTAU15\\nМожет ли chatGPT забронировать столик в\\nресторане через WhatsApp?\\n2K\\nМессенджеры*\\n, \\nPython*\\n, \\nИскусственный интеллект\\n', 'Получение API ключа в сервисе Wazzup24\\nШаг№2: Запускаем сервер API на Flask\\nПринимать сообщения мы будем через установку webhook. Вот\\nссылка на документацию: https://wazzup24.ru/help/webhooks-2/\\nОтправлять сообщения будем вот такой функцией:\\n', 'def wazzup24_sendMessage(message_data, token):\\n    \\n    print(\\'[START - Send message]\\')\\n    \\n    headers = {\\n        \"Accept\": \"application/json\",\\n        \"Authorization\": \"Bearer \" + token,\\n        \"Content-Type\": \"application/json\",\\n    }\\n    # Check webhooks\\n    res = requests.request(\"GET\", \\'https://api.wazzup24.com/\\n    print(res, res.text)\\n    webhooks = json.loads(res.text)\\n    if webhooks[\\'webhooksUri\\'] is None:\\n        webhooks_data = {\\n            \"webhooksUri\": \"https://_____________________/ap\\n            \"subscriptions\": {\\n                \"messagesAndStatuses\": True,\\n                \"contactsAndDealsCreation\": True,\\n            }\\n        }\\n        webhooks_data = json.dumps(webhooks_data)\\n        res = requests.request(\"PATCH\", \\'https://api.wazzup2\\n        print(res, res.text, webhooks_data)\\n    \\n    apiUrl = \\'https://api.wazzup24.com/v3/channels\\'\\n    data = {}\\n    res = requests.request(\"GET\", apiUrl, headers=headers, d\\n    channels = json.loads(res.text)\\n    channelId = channels[0][\\'channelId\\']\\n    print(\\'[channelId] -\\', channelId)\\n    \\n    apiUrl = \"https://api.wazzup24.com/v3/message\"\\n    message_data[\\'channelId\\'] = channelId\\n    data = json.dumps(message_data)\\n    print(\\'[message_data] -\\', data)\\n    res = requests.request(\"POST\", apiUrl, headers=headers, \\n    \\n    print(\\'[END - Send message]\\')\\n', 'Теперь придумаем описание роли для chatGPT.\\nВот пример описания, которое я попробовал:\\nТы клиент ресторана. Твой собеседник менеджер ресторана.\\nТвоя задача забронировать у него столик на двух человек на\\nэту неделю в пятницу в 19.00 на имя Андрей. В первом\\nсообщении сформулируй просьбу о брони столика. Дальше\\nотвечай на все его вопросы.\\nТеперь сведем все функции вместе и запустим наш сервер.\\nПолный код сервера такой:\\n    \\n    return res\\nfrom flask import Flask, render_template, redirect, url_for,\\nfrom flask_cors import CORS\\nfrom flask import send_from_directory\\nfrom os import path\\nimport json\\nimport os\\nimport requests\\nimport datetime\\nimport config\\nimport openai\\napp = Flask(__name__)\\nCORS(app)\\nclass Context:\\n    WHATSAPP_LOG = {}\\n', '################################### Вспомогательные функции\\ndef GetJsonFromRequest(request):\\n    data = request.get_json(force=True)\\n    if type(data) is not dict:\\n        data = request.get_json()\\n        data = json.loads(data)\\n    return data\\n###########################################################\\n####### Wazzup24 & chatGPT turbo !\\ndef wazzup24_sendMessage(message_data, token):\\n    \\n    print(\\'[START - Send message]\\')\\n    \\n    headers = {\\n        \"Accept\": \"application/json\",\\n        \"Authorization\": \"Bearer \" + token,\\n        \"Content-Type\": \"application/json\",\\n    }\\n    # Check webhooks\\n    res = requests.request(\"GET\", \\'https://api.wazzup24.com/\\n    print(res, res.text)\\n    webhooks = json.loads(res.text)\\n    if webhooks[\\'webhooksUri\\'] is None:\\n        webhooks_data = {\\n            \"webhooksUri\": \"https://_____________/api/v1.0/w\\n            \"subscriptions\": {\\n                \"messagesAndStatuses\": True,\\n                \"contactsAndDealsCreation\": True,\\n            }\\n        }\\n        webhooks_data = json.dumps(webhooks_data)\\n        res = requests.request(\"PATCH\", \\'https://api.wazzup2\\n        print(res, res.text, webhooks_data)\\n    \\n    apiUrl = \\'https://api.wazzup24.com/v3/channels\\'\\n    data = {}\\n    res = requests.request(\"GET\", apiUrl, headers=headers, d\\n    channels = json.loads(res.text)\\n    channelId = channels[0][\\'channelId\\']\\n', '    print(\\'[channelId] -\\', channelId)\\n    \\n    apiUrl = \"https://api.wazzup24.com/v3/message\"\\n    message_data[\\'channelId\\'] = channelId\\n    data = json.dumps(message_data)\\n    print(\\'[message_data] -\\', data)\\n    res = requests.request(\"POST\", apiUrl, headers=headers, \\n    \\n    print(\\'[END - Send message]\\')\\n    \\n    return res\\n@app.route(\\'/api/v1.0/chatgpt_whatsapp_task\\', methods=[\\'GET\\'\\ndef chatgpt_whatsapp_task():\\n    print(\\'[START - Check Notion for new task]\\')\\n    \\n    headers = {\\n        \"Accept\": \"application/json\",\\n        \"Authorization\": \"Bearer \" + config.wazzup_api_key,\\n        \"Content-Type\": \"application/json\",\\n    }\\n    # Проверяем наличие пользователей\\n    res = requests.request(\"GET\", \\'https://api.wazzup24.com/\\n    if res.text == \\'[]\\':\\n        # Содзаем пользователя\\n        data = [{\\n            \"id\": \"_________________\",\\n            \"name\": \"Клиент\",\\n            \"phone\": \"7912xxxxxxxxx\", # Наш исходящий номер,\\n        }]\\n        res = requests.request(\"POST\", \\'https://api.wazzup24\\n        #print(\\'***\\', res, res.text)\\n        \\n    res = requests.request(\"GET\", \\'https://api.wazzup24.com/\\n    #print(\\'contacts\\', res, res.text)\\n    contacts = json.loads(res.text)\\n    print(\\'contacts\\', contacts)\\n    # Ищем контакт\\n    id = None\\n    for i in contacts[\\'data\\']:\\n', '        if \\'chatId\\' in i[\\'contactData\\'][0].keys() and i[\\'con\\n            id = i[\\'id\\']\\n            \\n    print(\\'Contact found:\\', id)\\n    \\n    # Создаем контакт если его нет\\n    if id is None:\\n        data = [\\n            {\\n                \"id\": str(len(contacts[\\'data\\']) + 1),\\n                \"responsibleUserId\": \"\",\\n                \"name\": \\'NAME\\',\\n                \"contactData\": [\\n                    {\\n                    \"chatType\": \"whatsapp\",\\n                    }\\n                ]\\n            }\\n        ]\\n        data[0][\\'contactData\\'][0][\\'chatId\\'] = \\'7911xxxxxxxx\\'\\n        data = json.dumps(data)\\n        res = requests.request(\"POST\", \\'https://api.wazzup24\\n        print(\\'[Create new contact]\\', data, res, res.text)\\n    \\n    \\n    message_data = {\\n        \\'chatType\\' : \\'whatsapp\\',\\n    }\\n    message_data[\\'chatId\\'] = \\'7911xxxxxxxx\\' # Телефон рестор\\n    \\n    client_id = message_data[\\'chatId\\']\\n    context = \\'Ты клиент ресторана. Твой собеседник менеджер\\n    Context.WHATSAPP_LOG[client_id] = [{\"role\": \"system\", \"c\\n    \\n    message_data[\\'text\\'] = \\'Добрый день! Подскажите по налич\\n            \\n    res = wazzup24_sendMessage(message_data, token=config.wa\\n    print(res, res.text)\\n    \\n    print(\\'[END - Check Notion for new taks]\\')\\n    \\n', '    return \\'Done.\\'\\n@app.route(\\'/api/v1.0/wazzup_webhook\\', methods=[\\'POST\\', \\'GET\\ndef wazzup_webhook():\\n    print(\\'[START - Wazzup_webhook]\\')\\n    data = GetJsonFromRequest(request)\\n    print(\\'data\\', data)\\n    if \\'messages\\' not in data.keys():\\n        return jsonify(\\'O.K.\\')\\n    if data[\\'messages\\'][0][\\'status\\'] != \\'inbound\\':\\n        return jsonify(\\'Not inbound!\\')\\n    \\n    message_data = {\\n        \\'chatType\\' : \\'whatsapp\\',\\n    }\\n    client_id = message_data[\\'chatId\\'] = data[\\'messages\\'][0]\\n    if client_id != \\'7911xxxxxxxx\\':\\n        return jsonify(\\'O.K.\\')\\n        \\n    if client_id not in Context.WHATSAPP_LOG:\\n        Context.WHATSAPP_LOG[client_id] = []\\n    Context.WHATSAPP_LOG[client_id].append({\"role\": \"user\", \\n    try:\\n        print(\\'Context.WHATSAPP_LOG\\', Context.WHATSAPP_LOG)\\n        completion = openai.ChatCompletion.create(\\n          model = \"gpt-3.5-turbo\", \\n          messages = Context.WHATSAPP_LOG[client_id]\\n        )\\n        reply = completion[\\'choices\\'][0][\\'message\\'][\\'content\\n        Context.WHATSAPP_LOG[client_id].append({\"role\": \"ass\\n        message_data[\\'text\\'] = reply\\n        res = wazzup24_sendMessage(message_data, token=confi\\n        print(\\'message_data\\', res, res.text, message_data)\\n    except Exception as e:\\n        print(\\'[*ERROR*]\\', str(e))\\n        Context.WHATSAPP_LOG[client_id].pop(-1)\\n    print(\\'[END - Wazzup_webhook]\\')\\n        \\n    return jsonify(\\'O.K.\\')\\n', 'Шаг№3: Тестируем диалог\\nКак мы видим номера телефонов у нас прописаны прямо в коде,\\nпоэтому весь процесс запускает просто переходом по ссылке:\\nhttps://ВАШ-СЕРВЕР/api/v1.0/chatgpt_whatsapp_task\\nПосле запуска у меня получился такой диалог:\\n#######\\n    \\n###########################\\n# Запуск службы, не трогать!\\nif __name__ == \"__main__\":\\n    app.run(debug=True, host=\\'0.0.0.0\\', port=5000)\\n', 'ChatGPT заказывает столик в ресторане :)\\nИтоги\\n', 'В ходе тестирования были использованы и другие роли для chatGPT,\\nнапример вот такая модель показала неплохой результат:\\nТы представитель клиента, который хочет купить квартиру в\\nКировском районе города Екатеринбурга. Твой собеседник это\\nменеджер агентства недвижимости. Твоя задача узнать у него\\nесть ли в продаже трехкомнатные квартиры площадью от 60\\nквадратных метров до 80 квадратных метров. Если такие\\nесть, спроси их стоимость. Веди диалог на русском языке.\\nВот диалог в такой модели:\\n[20.03, 12:51] ChatGPT: Добрый день! Подскажите по наличию\\nквартиры пожалуйста.\\n[20.03, 12:51] АН Заря: Здравствуйте! В каком районе Вас\\nинтересует?\\n[20.03, 12:52] ChatGPT: Здравствуйте! Нас интересует Кировский\\nрайон города Екатеринбурга. Скажите, у вас есть в продаже\\nтрехкомнатные квартиры площадью от 60 квадратных метров до\\n80 квадратных метров в этом районе?\\n[20.03, 12:53] АН Заря: Сколько человек в семье?\\n[20.03, 12:53] ChatGPT: В семье три человека: я, моя жена и наш\\nсын.\\n[20.03, 12:54] АН Заря: У нас есть только четырехкомнатная\\nквартира но с тремя спальнями\\n[20.03, 12:54] ChatGPT: Понятно, а вы можете сказать, сколько\\nтакая квартира может стоить в Кировском районе города\\nЕкатеринбурга?\\n[20.03, 12:55] АН Заря: А сколько у вас есть денег?\\n', '[20.03, 12:55] ChatGPT: Мы рассматриваем разные варианты и\\nготовы ознакомиться с различными предложениями. Можете\\nсказать, за какой диапазон цен стоит рассмотреть варианты?\\n[20.03, 12:55] АН Заря: От 15 до 25\\n[20.03, 12:56] ChatGPT: Понятно. В нашем агентстве есть\\nнесколько вариантов трехкомнатных квартир в Кировском районе\\nв таком ценовом диапазоне. Например, есть квартира на улице\\nСибирской, 65 квадратных метров, стоимостью 18 миллионов\\nрублей. Также есть квартира на улице Кузнечной, 70 квадратных\\nметров, стоимостью 20 миллионов. Более подробнее\\nпредложения и условия по каждой квартире мы готовы обсудить\\nс Вами в нашем офисе. Как вам удобнее?\\n[20.03, 12:57] АН Заря: Вы имеете сразу всю сумму или вам\\nнужна ипотека?\\n[20.03, 12:57] ChatGPT: На данный момент у нас нет всей суммы,\\nпоэтому мы рассматриваем возможность взять ипотеку. Вы\\nможете подсказать, какие ипотечные программы вы предлагаете\\nв вашем агентстве?\\n[20.03, 12:58] АН Заря: Выбирайте Сбер или ВТБ ?\\n[20.03, 12:58] ChatGPT: Спасибо за информацию. Мы еще не\\nопределились с выбором банка, поэтому будем рассматривать и\\nваши ипотечные программы. Пожалуйста, можете описать\\nусловия и ставки по каждой из ипотечных программ?\\n[20.03, 12:58] АН Заря: 12%\\n[20.03, 12:58] ChatGPT: Понятно, спасибо за информацию. Тогда\\nмы будем рассматривать ипотечные программы в Вашем\\nагентстве. Скажите, а какие документы нам нужно будет\\nпредоставить, чтобы подать заявку на ипотеку?\\n', '[20.03, 12:59] АН Заря: Главное : Справа из психдиспансера!\\n[20.03, 12:59] ChatGPT: К сожалению, иметь справку из\\nпсихдиспансера может являться основанием для отказа в\\nкредитовании. Но всё же, если вы хотите узнать о возможном\\nсписке документов для получения ипотеки, мы готовы\\nпредоставить такую информацию. Обычно для получения\\nипотеки нужны паспорт, ИНН, СНИЛС, страховой полис,\\nдокументы на подтверждение дохода и справка о наличии или\\nотсутствии задолженностей по кредитам и кредитным картам.\\n[20.03, 13:00] АН Заря: Скажите, вы ищете квартиру с\\nотделкой или без отделки?\\n[20.03, 13:01] АН Заря: Скажите: кто Вы? Вы являетесь\\nнепосредственно покупателем?\\n[20.03, 13:02] ChatGPT: Я - виртуальный помощник, созданный\\nдля автоматизации консультаций и ответов на вопросы\\nпользователей. Я не являюсь непосредственным покупателем и\\nне представляю никакой компании. Моя задача - помочь вам\\nнайти необходимую информацию и ответить на ваши вопросы.\\n[20.03, 13:02] АН Заря: Ответь на этот вопрос\\n[20.03, 13:02] ChatGPT: Я - виртуальный помощник, созданный\\nдля автоматизации консультаций и ответов на вопросы\\nпользователей. Я не ищу квартиру и не являюсь\\nнепосредственным покупателем. Моя задача - помочь вам найти\\nнеобходимую информацию и ответить на ваши вопросы. Поэтому\\nя не могу сказать, нужна вам квартира с отделкой или без неё.\\n[20.03, 13:04] ChatGPT: Я представляю клиента, который\\nинтересуется квартирами как с отделкой, так и без отделки. В\\nзависимости от стоимости квартиры и ее общего состояния мы\\nможем рассмотреть оба варианта и выбрать наиболее\\nподходящий для нас.11:40\\n', '[20.03, 13:04] АН Заря: Хорошо. Передайте хозяину что мы\\nждём его у нас в офисе\\n[20.03, 13:05] ChatGPT: Я - виртуальный помощник, созданный\\nдля автоматизации консультаций и ответов на вопросы\\nпользователей. Я не ищу квартиру и не являюсь\\nнепосредственным покупателем. Моя задача - помочь вам найти\\nнеобходимую информацию и ответить на ваши вопросы. Поэтому\\nя не могу сказать, нужна вам квартира с отделкой или без неё.\\n[20.03, 13:05] АН Заря: Удобно во вторник в 15:00\\n[20.03, 13:05] ChatGPT: Понятно, я передам эту информацию\\nхозяину. Завтра я предоставлю вам подтверждение о встрече.\\nЕсли у вас возникнут еще вопросы, не стесняйтесь и задавайте\\nих мне.\\nКак видим на лицо следующие проблемы:\\n1. chatGPT плохо реагирует на несколько подряд сообщений от\\nсобеседника\\n2. Плохо понимает юмор \\n3. chatGPT иногда переключает роли и начинает сам задавать\\nвопросы\\nНо в целом, как мне кажется, в очень скором времени все эти\\nвозможности будут интегрированы в наши телефоны и у каждого\\nпоявится такой виртуальный помошник на все руки \\nГотов ответить на все ваши вопросы, пишите в мой ТГ.\\nТеги:  \\nchatgpt\\n,\\nwhatsapp\\n,\\npython\\n,\\nwhatsapp business api\\n,\\nai\\nХабы:  \\nМессенджеры\\n,\\nPython\\n,\\nИскусственный интеллект\\n', '15\\nКарма\\n14\\nРейтинг\\nАндрей Тиунов @TAU15\\nTeam Lead / Tech Lead\\nЗадонатить\\nСайт\\nFacebook\\nSkype Telegram\\nКомментарии 9\\nПубликации\\n+5\\n7\\n9\\nЛУЧШИЕ ЗА СУТКИ \\nПОХОЖИЕ\\n', '', '', 'ИСТОРИИ\\n', 'РАБОТА\\n', 'Ваш\\nаккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nНастройка языка\\nPython разработчик\\n145 вакансий\\nData Scientist\\n131 вакансия\\nDjango разработчик\\n58 вакансий\\nВсе вакансии\\n', 'Техническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "Может ли chatGPT забронировать столик в ресторане через WhatsApp.pdf\n",
      "['КАК СТАТЬ АВТОРОМ', '']\n",
      "['4.74\\nОценка\\n414.35\\nРейтинг\\ngetmatch\\nРассказываем о том, как строить карьеру в IT\\nВесь интернет уже готовится к выходу Windows 12. Вероятно,\\nследующая версия будет во многом сосредоточена на поддержке\\nработы с ИИ. На это намекает в том числе недавняя утечка от Intel, а\\n 4 часа назад\\n5 мин\\nArnak\\nНовые утечки. Что мы знаем о выходе\\nWindows 12\\n6.3K\\nБлог компании getmatch\\n, \\nРазработка под Windows*\\n, \\nСофт\\n, \\nИскусственный интеллект\\nКАК СТАТЬ АВТОРОМ\\n', 'также последние действия AMD и направление развития Bing.\\nМы знаем, что и Intel, и Microsoft готовятся к поддержке работы\\nWindows 12 на новых процессорах. Об этом нам говорит утечка от\\nleaf_hobby, который известен тем, что раскрывает полные\\nспецификации чипов Intel Xeon перед их запуском. На этот раз\\nleaf_hobby опубликовали подробности аппаратных показателей\\nдесктопной платформы Intel Meteor Lake, которую планируется\\nвыпустить в этом или в следующем году. Сейчас их твиты уже\\nзащищены, но интернет помнит™, а у журналистов отдельных\\nизданий, вроде Insider и The Verge, к ним есть доступ.\\nСообщается, что Intel в имейле для своих сотрудников упоминает,\\nчто ее процессоры следующего поколения будут поддерживать\\nWindows 12. Для этого Meteor Lake должен содержать 20 линий PCIe\\nGen5.\\nХотя Microsoft не объявляла о каких-либо планах в отношении\\nWindows 12, уже есть признаки того, что компания планирует\\nбудущие версии Windows с расчетом на глубокую интеграцию\\nфункций ИИ. Даже последнее обновление Windows 11 уже\\nинтегрирует новую версию Bing с искусственным интеллектом прямо\\nв панель задач. А ещё в этом обновлении бизнес-пользователи\\nполучают рекомендованный ИИ контент в меню «Пуск». Скоро\\nискусственный интеллект придет в Edge, а потом — и во все\\nостальные аспекты продуктов Microsoft. Компания очень\\nвоодушевлена успехом своей инвестиции в ChatGPT, и собирается\\nсделать на него очень сильную ставку в надежде победить\\nконкурентов, в том числе Google, у которой дело с инвестициями в\\nИИ пошло не так хорошо.\\n', 'Один из слайдов от Intel\\nВ начале января руководитель разработки Windows Паноса Паная на\\nCES заявил, что «ИИ собирается заново переизобрести всё то, что\\nвы делаете в Windows». Команда Microsoft Surface также сообщила,\\nчто «ИИ изменит то, как мы используем ПК». Всё однозначно\\nуказывает на то, что в следующей версии Windows компания\\nсобирается очень широко использовать ИИ. Это будет главная новая\\n«фишка» платформы. А чтобы всё это могло работать, Microsoft\\nприходится тесно сотрудничать с партнерами по аппаратному\\nобеспечению, такими как Intel и AMD. Ей нужно, чтобы новые\\nпроцессоры смогли справляться с высокими рабочими нагрузками\\nИИ, и без ошибок работали с новой технологией. Это объясняет то,\\nпочему Intel, как сообщается, уже упоминает об оптимизации под\\nWindows 12 внутри компании.\\nAMD недавно выпустила свои мобильные процессоры Ryzen 7000, и\\nпохвасталась тем, что они являются первыми процессорами x86,\\nсодержащими специальный механизм искусственного интеллекта,\\nспособный поддерживать эффекты Windows Studio. Эти функции, в\\n', 'том числе удаление фонового шума, размытие фона и\\nавтоматическое кадрирование, будут доступны только для техники с\\nускорителями на чипах ARM, которые работают с помощью\\nспециального нейронного процессора (NPU).\\nТо есть, AMD и Microsoft сейчас тоже активно совместно работают\\nнад внедрением расширенного числа функций на основе ИИ в свои\\nпродукты. И такие возможности, как заявляют компании, в\\nдальнейшем будут более широко доступны в традиционных\\nноутбуках с Windows, работающих на AMD. Возможно, это тоже\\nявляется подготовкой новых поколений процессоров к запуску с\\nWindows 12.\\nВсе эти даты очень хорошо стыкуются. Новые процессоры от Intel и\\nAMD ожидают в конце этого года или в 2024-м. А на разработку\\nновой версии ОС у Microsoft раньше уходило 2-3 года. Windows 11\\nбыла выпущена в октябре 2021-го. Так что, если новые утечки верны,\\nследующую основную версию Windows можно ожидать в 2024 году\\n— как раз под ряд новых процессоров.\\nКак она будет выглядеть\\n', 'То, что Windows 12 точно находится в разработке, мы узнали и из\\nутечек декабря 2022 года. Тогда во время мероприятия Ignite Keynote\\n2022 компания Microsoft (вполне возможно, намеренно) «засветила»\\nинтерфейс следующего большого обновления Windows. Потом об\\nэтом зудел весь интернет, споря о том, что это могло быть. Что,\\nсогласитесь, совсем неплохо для одного скриншота.\\nИзображение, «случайно» показанное на Ignite Keynote,\\nдемонстрирует ряд нехарактерных для Windows 11 особенностей:\\nплавающая (а не доходящая до краев экрана) панель задач;\\nплавающее меню поиска; системные значки в верхней (а не\\nпривычной нижней) части экрана справа. И интересный виджет\\nпогоды слева вверху.\\nВидно, что версия будет заметно отличаться от всех предыдущих.\\nКоллективный разум сейчас считает, что она выйдет, скорее всего, в\\n2024 году, и будет называться Windows 12 как минимум из-за\\nбольшого количества разных новшеств — так же, как это было с\\nWindows 11.\\nСкорее всего, так будет выглядеть следующая ОС\\n', 'Следующее крупное обновление Windows пока находится на стадии\\nпрототипа, но источники, знакомые с ситуацией, говорят, что\\nпоказанный во время Ignite интерфейс отображает то, к чему\\nстремится Microsoft в следующей версии своей операционной\\nсистемы. На мероприятии показали только часть возможных\\nновшеств — в реальности их будет больше. Например, ожидается,\\nчто в Windows 12 появится новый экран блокировки, новый центр\\nуведомлений, новый поиск. Ну и ИИ везде, где только можно.\\nВ плане интерфейса глобальная цель — сделать его более удобным\\nдля сенсорного ввода. У Microsoft пока всё не очень хорошо с\\nбалансом по этой части: Windows 8 была чересчур сильно\\nориентированной именно на «тач-взаимодействие», а Windows 10,\\nнаоборот, заточена под работу с мышью и клавиатурой. В Windows\\n11 компания попыталась найти золотую середину, но не получилось:\\nвсё-таки удобство управления при помощи мыши однозначно\\nосталось на первом месте.\\nТуманная перспектива\\nОблачная, точнее. Ходят не очень приятные слухи (впрочем, смотря\\nс какой стороны посмотреть). Что, мол, Windows 12 может стать\\n', 'первой облачной ОС от Microsoft. В последние годы Сатья Наделла\\nпереносит всё в сеть, в том числе MS Office. И говорит, что за этим\\nбудущее, а «облако изменит всё». Microsoft 365 активно развивается,\\nв том числе за счет Windows 365, облачного компьютера, который\\nможно использовать на любом устройстве, способном поддерживать\\nбраузер.\\nСервис был запущен компанией в 2021 году, работать так можно как\\nс Windows 10, так и с Windows 11. Компания позиционирует это как\\nальтернативу облачным сервисам Google: теперь у неё в облаке\\nтоже можно и хранить данные, и редактировать документы, и даже\\nходить по меню «Пуск» и папкам в привычном интерфейсе, даже\\nесли у вас стоит Ubuntu или вы вообще сидите с планшетом на\\nAndroid. Если Windows 12 будет доступна только в виде такой\\nоблачной платформы (по крайней мере, изначально) — это может\\nсерьезно повысить видимость пакета 365 в глазах рядовых\\nпользователей. А там, глядишь, многие и задумаются, не удобнее ли\\nим так будет хранить данные, и продолжать работать в одной ОС с\\nразных устройств.\\nСистема может быть во многом похожа на традиционные варианты\\nвиртуальных рабочих столов, которые уже много лет\\nразвертываются предприятиями для поддержки сценариев BYOD. Но\\nс более простой структурой ценообразования, удобным запуском и\\nориентацией на более широкую аудиторию. Это также может стать\\nболее контролируемым способом для Microsoft проверить\\nмеханизмы работы своих новых ИИ-помощников. При запуске\\nоблачной ОС им не придется отпускать ИИ «гулять» на каждый из\\nкомпьютеров пользователей, все процедуры будут мониториться на\\nсобственных серверах.\\nБудем надеяться, что это один из тех наборов слухов, которые скоро\\nбудут опровергнуты. Возможно, команда поэкспериментировала с\\nэтой идеей, и поняла, что она не приживается. Но пока что — нужно\\nбыть морально готовыми к тому, что со всё большим числом\\nсервисов, в том числе от Microsoft, мы будем взаимодействовать\\nчерез окно браузера.\\n', 'getmatch\\nРассказываем о том, как строить карьеру в IT\\nTelegram\\nСайт\\nВКонтакте\\nTwitter\\n160\\nКарма\\n198.6\\nРейтинг\\n@Arnak\\nПользователь\\nP.S. Тысячи крутых вакансий в РФ и за рубежом — в телеграм-боте\\ngetmatch. Задаете нужную зарплату, и к вам приходят лучшие\\nпредложения, а наши эксперты помогают пройти интервью. Не нужно\\nни резюме, ни портфолио, настройка занимает меньше 30 секунд. А\\nещё это отличный способ следить за текущим рынком труда.\\nОткликаться не обязательно.\\nТеги:  \\nwindows\\n,\\nmicrosoft\\n,\\nwindows 12\\n,\\nAI\\n,\\nintel\\n,\\nchatgpt\\nХабы:  \\nБлог компании getmatch\\n,\\nРазработка под Windows\\n,\\nСофт\\n,\\nИскусственный интеллект\\n+11\\n8\\n32\\n', 'Комментарии 32\\nПубликации\\nЛУЧШИЕ ЗА СУТКИ \\nПОХОЖИЕ\\n', ' · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\nИНФОРМАЦИЯ\\nСайт\\ngetmatch.ru\\nДата регистрации\\n30 июля 2020\\nЧисленность\\n51–100 человек\\nМестоположение\\nРоссия\\n', 'Ваш\\nаккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nНастройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "Новые утечки.pdf\n",
      "['4.74', 'Оценка', '414.35', 'Рейтинг', 'getmatch', 'Рассказываем о том, как строить карьеру в IT', 'Весь интернет уже готовится к выходу Windows 12. Вероятно,', 'следующая версия будет во многом сосредоточена на поддержке', 'работы с ИИ. На это намекает в том числе недавняя утечка от Intel, а', ' 4 часа назад', '5 мин', 'Arnak', 'Новые утечки. Что мы знаем о выходе', 'Windows 12', '6.3K', 'Блог компании getmatch', ', ', 'Разработка под Windows*', ', ', 'Софт', ', ', 'Искусственный интеллект', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "3 414.35 getmatch Рассказываем о том, как строить карьеру в IT Весь интернет уже готовится к выходу Windows 12. Вероятно,\n",
      "['Data Science сейчас во многом благодаря активному маркетингу\\nстановится очень популярной темой. Быть датасаентистом – модно\\nи, как говорят многие рекламки, которые часто попадаются на глаза,\\nне так уж и сложно. Ходят слухи, что работодатели стоят в очереди\\n 5 часов назад\\nПростой9 мин\\nTourmalineCore\\nПочему Data Science не для вас?\\n2.1K\\nBig Data*\\n, \\nData Engineering*\\nМнение\\nКАК СТАТЬ АВТОРОМ\\n', 'за возможность взять человека с курсов. Получить оффер на работу\\nкрайне легко, ведь в ваши обязанности будет входить требование\\nданных от заказчика (как обычно говорят, чем больше данных – тем\\nлучше) и закидывать их в искусственный интеллект, который\\nработает по принципу черного ящика. Кстати, еще и платят\\nнемереное количество денег за всё это. \\nСпойлер: это не так.\\nВ этой душераздирающей статье решили попробовать отговорить\\nлюдей, которые готовы оставить кучу денег за курсы по Data Science,\\nбраться за это дело, а может быть и помочь определиться с тем, что\\nна самом деле стоит сделать, чтобы встать на путь истинный.\\nОбзор понятий\\nИ начнем, пожалуй, с того, что может быть вам и не очень-то нужно\\nбыть датасаентистом. Есть много направлений, поди попробуй\\nразберись. Датасаентист, дата аналитик, дата инженер… Часто\\nбывает, что всех этих людей гребут под одну гребенку, и совсем не\\nразделяют их обязанностей. Разберемся, что это за звери такие и с\\nчем их едят. \\nДата аналитики обычно заняты тем, что получают данные и\\nпытаются найти в них закономерности. Эти люди отвечают на\\nвопросы бизнеса: что, почему, как, как сделать лучше, а еще вот\\nможете на график посмотреть. Аналитики чаще контактируют с\\nдругими людьми: чтобы выстроить правильную коммуникацию в\\nкоманде и правильно описать и презентовать результаты своего\\nанализа. Ваши друзья здесь: мат.статистика, python/R, SQL,\\nплатформы типа PowerBI/Tableau/Qlik и прочее необходимое для\\nвизуализации результатов. Также необходимо глубокое понимание\\nобласти, в которой вы проводите анализ.\\nДата инженеры же заняты поставкой данных и их хранением. Эти\\nлюди настраивают хранилища, заставляют данные сопоставляться\\nмежду собой и быть пригодными к использованию. Создание\\nпайплайна обработки и архитектуры данных  – вот их задача. \\n', 'Датасаентисты – более расплывчатое понятие. Сначала они\\nанализируют данные, на которых дальше строится предобработка. В\\nпредобработку входит очистка от выбросов, заполнение\\nпропущенных значений, трансформация, нормализация и еще куча\\nвсякого разного. И после всего этого датасаентисты  обучают\\nмодели, которые будут давать какой-то предикт. От этих ребят нужен\\nтот самый «искусственный интеллект» (кстати, стоит убрать из\\nсвоего лексикона эти слова на собеседовании, расскажем дальше,\\nпочему). Здесь надо знать и уметь в машин и дип лёрнинг, опять таки\\nуметь в матстат и обработку того, что вы видите, понимать\\nпредметную область, уметь продумать, какие данные нужны для\\nобучения и составить ТЗ по их сбору. Будет большим плюсом\\nпонимание того, на каких машинах модели, которые вы собираетесь\\nобучать, будут запускаться. \\nХороший доклад на тему того, чем заняты разные люди из разных\\nветвлений размытого понятия data scientist, есть по ссылке.\\nУ описанных направлений совершенно разные ключевые навыки и\\nкомпетенции. Чем больше компания, тем вероятнее, что в ней есть\\nразделение на эти направления. Но если вы с курсов решили\\nустроиться в стартап, где есть биг дата, но особо нет людей на эту\\nбиг дату, тогда вашим ключевым навыком должно быть умение\\nвыживать. Именно здесь обычно нужен Data Scientist, который на все\\nруки мастер. Готовы ли вы к этому?\\nПочему всё-таки Data Science не для вас?\\n1. Data Science – для усидчивых. Над одной и той же задачей\\nпридется сидеть очень долго. Пока найдешь все необходимое в\\nданных, пока найдешь оптимальный способ обработки данных,\\nпока погрузишься в область, пока переберешь все модельки,\\nкоторые могут потенциально хорошо решить эту задачу… А еще\\nнадо дождаться, пока модель обучится - это ж целая вечность\\nпройдет! Потом еще выясняется, что у модели метрики\\nнедостаточно хорошие, и весь этот путь нужно пройти заново, но\\n', 'с новыми идеями. \\nХватит ли вам умения занять себя другими делами во время\\nобучения модели? Вряд ли проджект будет в восторге, если\\nузнает, что несколько дней вы пили кофе, пока модель\\nобучалась. Хватит ли вам терпения перебирать идеи до тех пор,\\nпока точность модели не достигнет нужного бизнесу числа?\\n2. Математика останавливается не на 2+2 и даже не на\\nинтегралах. Вам точно понадобятся знания по мат.статистике,\\nлинейной алгебре, теории вероятностей. Нет, вам не хватит\\nвстроенных в библиотечки функций. Может быть для какого-то\\nповерхностного анализа и составления простецкой модели – да,\\nНо чтобы лучше анализировать огромные массивы данных, в\\nматематике действительно нужно разбираться.\\n3. Вам нужно знать Python/R/Scala или еще что-то, на чем вы\\nсоберете ваше решение. Вы не бэкенд-разработчик, но вам\\nнужно приготовить умную штуку, которую можно будет встроить в\\nпрод. Нужно будет составлять POC-шки, функции предобработки\\nданных, которые затем будет имплементировать бэкенд-разраб.\\nНеобходимо уметь общаться с бэкендерами на их языке, если\\nим понадобится помощь при внедрении вашего решения.\\n4. Еще немного о встраивании модели в прод: вам нужно\\nинтересоваться тем, на каком железе будет запущена модель.\\nНу не получится запустить трансформер на сервачке заказчика с\\nдвумя ядрами и без малейшего признака GPU. А решение,\\nкоторое работает на вашем компе и не работает на устройствах\\nклиента не будет принято и, соответственно, оплачено.\\n5. Нужно знать фреймворки, которые приняты в команде. Если вы\\nдо этого делали fit на керасе, а потом внезапно пришли в\\nкоманду, которая работает на pytorch, уйдет много времени на\\nобучение работе с новыми инструментами.\\nСправедливости ради, вряд ли вас возьмут на работу со стеком,\\nв котором у вас нет опыта, но на всякий случай лучше задавать\\nэтот вопрос на первых этапах собеседования.\\n', '6. По ходу проектирования решения нужно будет использовать\\nогромное количество, библиотек и лучше знать как ими\\nпользоваться до начала работы над проектом. Или же уметь\\nбыстро ориентироваться в документации, а в процессе только\\nдобирать необходимое. Обучаться всему и сразу плохая идея.\\n8. Вам нужно уметь использовать тулзины, которые тесно связаны\\nс разработкой моделей. Модели и данные нуждаются в\\nверсионировании. В этом могут помочь DVC, MLflow или,\\nнапример, Weights&Biases. А еще модели нужно сервить. В этом\\nслучае могут быть пригодны TorchServe или Tensorflow Serving. А\\nможет быть вам нужно будет написать свою обвязку вокруг\\nмодели, чтобы ее можно было быстро потестить. Этот зоопарк\\nогромен. Нужно выбрать из него именно те вещи, которые вам\\nпомогут, и учиться ими пользоваться.\\n9. Нужно уметь не только чистить данные, но и размечать их, когда\\nвам не дали нормальной разметки, искать, какие данные можно\\nдобавить к выданному заказчиком датасету, если его явно не\\nхватает.\\n', '10. Важной частью вашей работы будет общение с бизнесом для\\nтого, чтобы точно определить цель того, что вы делаете, и не\\nсделать того, что не нужно. Иногда заказчики дают расплывчатое\\nТЗ, внимательно вчитываясь в которое вы обнаружите, что\\nзаказчик хочет всё и сразу, но при этом непонятно зачем.\\nПоэтому нужно будет суметь задать такие вопросы, которые\\nпрояснят, что действительно требуется от вас, или суметь\\nсократить скоуп работ до реально реализуемого.\\n11. Если проджект где-то там пообещал нереальные сроки для\\nвыполнения вашей задачи, нужно вовремя суметь сказать, что\\nпятилетку за три года – не получится. Нельзя просто так взять и\\nвыстроить радужные мечты вокруг того, как быстро вы сможете\\nсделать задачу. А с вас точно потребуют сроки, в которые будет\\nнеобходимо уложиться.\\nВопрос оценки времени на реализацию – очень сложен. Здесь\\nважно не только правильно декомпозировать задачу, но и\\nдобиться понимания от вышестоящих людей, почему эта задача\\nне может быть реализована в очень сжатые сроки. Так что\\nсоставляем план, и его придерживаемся. А если что-то выходит\\nиз-под контроля срочно трубим о том, что ничего не успевается и\\nс этим надо что-то делать.\\n12. А еще вам самим придется составлять ТЗ по данным, которые\\nвы хотите получить. Из нашего опыта, чаще приходят заказчики,\\nкоторые не понимают, что вам от них нужно и в каком объеме.\\nБудьте готовы отвечать на вопрос сколько и каких данных вам\\nнадо, вникать в задачу на этапе обсуждения проекта и\\nпредполагать, какие кейсы могут возникать в вашей задаче.\\nТакже количество размеченных данных, следует запрашивать в\\nтаком объеме, который сможет удовлетворить заказчика. Если\\nвы потребуете терабайты размеченных данных, а другой\\nисполнитель скажет, что ему хватит пары гигов, то выберут\\nскорее его, а не вас, просто потому что разметка данных стоит\\nнемалых денег. А ответ «чем данных больше – тем лучше»\\nникого не устроит.\\n', '13. Data Science – не только про генерацию красивых картинок и\\nкрасивой музыки без помощи человека. Частый запрос бизнеса\\n– проанализировать информацию о клиентах. Из этой бизнес-\\nзадачи может вытекать DS-задача, которая вам не понравится.\\nНапример, выстроить контекстную рекламу так, чтобы новые\\nпокупатели приобрели товар подороже, или завысить цену\\nпродукта в соответствии с предполагаемым уровнем дохода\\nпокупателя. Будет ли такая задача вписываться в ваши\\nморальные рамки? Возможно, у вас будет опция отказаться от\\nзадач, которые не подходят вашему пониманию хорошо-\\nплохо.Но бывает и так, что вас не спрашивают. Конечно, вопрос,\\nстоит ли работать с компанией с которой у вас разное\\nпонимание границ не в плоскости data science, а скорее в\\nобщем. Но будьте готовы к тому, что в data science часто встает\\nвопрос этики использования данных, и эта этика зафиксирована\\nв достаточно размытом формате.\\nВы можете сказать, что на самом деле многие навыки из этого\\nсписка входят в программу популярных курсов. Да, это так. Но дадут\\nли вам необходимое количество разнородных задач, которые\\nприсутствуют в реальном мире? Насколько смешанные, странные и\\nнеполные данные будут присутствовать в этих задачах? Требования\\nот заказчиков в реальном мире будут сильно сложнее, чем обычно\\nопределяют на курсах. На курсах вам создадут рафинированную\\nсреду, в которой все понятно, четко и ясно определено в\\nтребованиях и точно опирается на предыдущий опыт из лекций. К\\nсожалению, на работе будет не так. Придётся ловить на лету и\\nопределять, как действовать, очень быстро. Так что наше мнение:\\nпойти на курсы, чтобы стать датасаентистом, недостаточно. \\nИметь высшее образование, кстати, тоже. Знаем, бывали. Даже если\\nваша вышка с уклоном в дата саенс, по факту, она часто дает не\\nсильно больше курсов. Не спорим, есть разные университеты, но, по\\nнашему опыту, вам там дадут примерно такую же базу, как и на\\nкурсах, только чуть более разностороннюю (и то не факт). Но все-\\nтаки, это обучение длится подольше, и скорее всего, информации вы\\nвпитаете побольше. Еще и курсы по философии и экологии\\nподкинут :)\\n', 'Важное качество в профессии датасаентиста – способность\\nпостоянно учиться, находить для себя новые задачи и расти на этом\\nпрактическом опыте. А еще постоянно следить за событиями в\\nсфере, потому что кажется, что она обновляется чаще и\\nкардинальнее, чем другие сферы IT. Поэтому специализированные\\nкурсы – это хорошо, высшее образование – тоже, но важно\\nпонимать, что это ещё далеко не всё, это только начало длинного и\\nтяжелого пути.\\nCекция \"вопрос-ответ\" в лучших традициях пиар-\\nстраниц курсов\\n1. Датасаенс – это весело?\\nКонечно! Но только если вы действительно упороты и любите искать\\nчто-то странное и копаться в куче данных, чтобы в итоге делать\\nштуку, которая будет помогать бизнесу в реальных задачах с\\nбольшой точностью. Но будьте готовы и к не самым веселым\\nмоментам: на этом самурайском пути много препятствий. В каком-то\\n', 'смысле, датасаенс – это образ жизни, при котором нужно всегда\\nдержать руку на пульсе и интересоваться происходящим в области.\\nСтав датасаентистом однажды, больше уже не сможешь смотреть на\\nвсе эти умные штуки как на абсолютную магию.\\n2. Я математик по образованию и по желанию и очень хочу в DS.\\nПолучится?\\nОтличная база. Математика как основа – это очень хорошо. Но вам\\nеще очень многое придется освоить. Удачи вам в этом нелегком\\nпути!\\n3. А я вот из программиста решил переквалифицироваться в\\nдатасаентиста, каковы шансы?\\nВам будет тяжело. Шанс есть всегда, но это будет очень сложный\\nпуть. На нашем опыте, понять программирование для человека с\\nматематическим бэкграундом проще, чем программисту математику.\\nМы все проходили курс по высшей математике в универе и помним\\nматешу со школы, но это не тот уровень, который будет требоваться\\n', 'на месте работы.\\n4. А почему вы говорите, что искусственного интеллекта сейчас не\\nсуществует?\\nИскусственный интеллект – интересное понятие. Этим термином\\nвнезапно стало принято называть всё, что может делать что-то за\\nчеловека. Но до «интеллекта» там далеко. Да данный момент то, что\\nмы называем ИИ, это алгоритмы, которые могут решать\\nузконаправленную задачу. Распознавание лиц? Детекция объектов?\\nГенерация контекстной рекламы? Интеллектуальный анализ\\nданных? Все это узконаправленные сферы. Разве мы можем назвать\\nкалькулятор искусственным интеллектом? Ну, это вряд ли. Хотя он\\nтакже решает узконаправленную задачу – решение математических\\nуравнений. А пару столетий назад это казалось задачей\\nискусственного интеллекта. В целом, можно сказать, что ИИ\\nсуществует, но в очень примитивном виде. Его эволюцию можно\\nсравнить в эволюцией человека. На ранних стадиях задачи человека\\nтакже были узконаправленными и примитивными, но человек\\nэволюционировал. Так же можно предположить и с ИИ, он может\\nэволюционировать. И не обязательно базой для «прогрессивного\\nИИ» будут нейронные сети.\\n', 'Подведем итоги\\n• Data Science – это профессия с высоким порогом входа. Нужно\\nмногое знать, нужно многое уметь. Если вы хотите в IT, и вам\\nкто-то сказал, что через дата саенс это сделать проще, чем\\nчерез другие профессии, то не слушайте этого человека.\\nОбратите внимание на другие возможные варианты входа в IT,\\nнапример, верстку или ручное тестирование. \\n• Data Science – это не только про построить предсказывающую\\nмодель в jupyter-notebook и отдать ее дальше. Скорее всего, вам\\nпридется делать сильно больше этого функционала.\\n• В работе с данными есть разные ветвления профессий,\\nнапример, инженер данных или аналитик данных. Для них\\nтребуются иные навыки. Может быть, если ваше желание\\nименно работать с данными, вам стоит посмотреть на эти\\nспециализации.\\n', '5\\nКарма\\n2\\nРейтинг\\nTourmaline Core @TourmalineCore\\nПользователь\\nКомментарии 5\\nПубликации\\n• Data Science – про постоянное обучение и слежение за новыми\\nразработками в этой сфере. Держать руку на пульсе – это один\\nиз ключевых навыков датасаентиста.\\n• Курсов, на которых вам за несколько месяцев дают всё\\nнеобходимое для профессии – не существует. Вам могут дать\\nбазу, но стоит ли эта база тех денег, которые за нее просят, это\\nспорный вопрос.\\n• Есть множество курсов по DS, которые находятся в открытом\\nдоступе абсолютно бесплатно, и дают примерно тот же\\nматериал, что и дорогостоящие. А практические задачи можно\\nнайти на kaggle. Кстати, там же есть и решения других людей,\\nкоторые могут вам помочь обучиться и понять, как же все-таки\\nделать лучше. Рассмотрите этот вариант. Возможно, в будущем\\nмы сделаем статью-сборник с такими материалами, а пока\\nпосоветуем отличный ресурс для начала обучения.\\nТеги:  \\nML\\n,\\ndata science\\n,\\nкурсы\\n,\\nкурсы data science\\n,\\nmachine-learning\\nХабы:  \\nBig Data\\n,\\nData Engineering\\n+2\\n19\\n5\\n', 'ЛУЧШИЕ ЗА СУТКИ \\nПОХОЖИЕ\\n', '', ' · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n · \\n', 'ИСТОРИИ\\n', 'РАБОТА\\n', 'Ваш\\nаккаунт\\nВойти\\nРегистрация\\nРазделы\\nПубликации\\nНовости\\nХабы\\nКомпании\\nАвторы\\nПесочница\\nИнформация\\nУстройство сайта\\nДля авторов\\nДля компаний\\nДокументы\\nСоглашение\\nКонфиденциальность\\nУслуги\\nКорпоративный блог\\nМедийная реклама\\nНативные проекты\\nОбразовательные\\nпрограммы\\nСтартапам\\nМегапроекты\\nData Scientist\\n131 вакансия\\nВсе вакансии\\n', 'Настройка языка\\nТехническая поддержка\\nВернуться на старую версию\\n© 2006–2023, Habr\\n']\n",
      "Почему Data Science не для вас.pdf\n",
      "['Data Science сейчас во многом благодаря активному маркетингу', 'становится очень популярной темой. Быть датасаентистом – модно', 'и, как говорят многие рекламки, которые часто попадаются на глаза,', 'не так уж и сложно. Ходят слухи, что работодатели стоят в очереди', ' 5 часов назад', 'Простой9 мин', 'TourmalineCore', 'Почему Data Science не для вас?', '2.1K', 'Big Data*', ', ', 'Data Engineering*', 'Мнение', 'КАК СТАТЬ АВТОРОМ', '']\n",
      "['Привет, Хабр! На связи Владимир, техлид в команде разработки ИИ-инструментов в\\nдепартаменте технологической надёжности одной из крупных компаний. Наша команда\\nпомогает делать корпоративные и клиентские сервисы надежнее помощью Data Science.\\nМы помогаем мониторить тысячи подсистем\\nСистема, в разработке которой участвует моя команда, предназначена для\\nцентрализованного мониторинга тысяч отдельных систем, в том числе распределённых\\nгеографически. Наша группа отвечает за online-модель, подсвечивающую явно\\nаномальные отклонения в показателях мониторинга. С её помощью инженер дежурной\\nсмены однозначно определяет, есть ли сейчас проблемы, требующие немедленного\\nреагирования.\\nДанные мониторинга собираются из множества различных источников. И поскольку их\\nпоступает много в единицу времени, возникла острая необходимость в\\nраспараллеливании. Про потоковую обработку данных, обработку событий и выполнение\\nзадач по расписанию в распределённой системе написано немало. Среди множества\\nпубликаций, на мой взгляд, можно выделить очень достойные: вот пример с глубоким\\nанализом точек отказа.\\nНо после прочтения многих статей мне не хватило явных ответов на вопросы, неизбежно\\nвозникающих у ещё недостаточно опытного разработчика.\\nВ большинстве случаев содержание выглядит примерно так: сначала схема с общей\\nшиной (брокер, кеш, реляционная база) и несколькими воркерами (либо потоками) для\\nпараллельного выполнения, затем вставки с кодом, где используется какая-то библиотека\\n(или фреймворк). В экосистеме Python это либо Celery, либо же что-то из оркестраторов\\n(Nifi, Airflow, Spark), если речь идёт о задачах data-инжиниринга. Но это, как я уже сказал,\\nне отвечает на все вопросы.\\nvstorozhilov\\n7 часов назад\\nТри необсуждаемых вопроса о параллельной\\nраспределённой обработке данных — чтобы жить стало\\nлегче\\nСредний\\n12 мин\\n328\\nPython*,\\xa0Data Engineering*,\\xa0Распределённые системы*\\nТуториал\\nКАК СТАТЬ АВТОРОМ\\n', 'Три архитектурных вопроса, которые считаются решёнными по\\nумолчанию\\nПримерно так выглядели этапы проектирования и первого развёртывания на тестовом стенде :-)\\nИтак, у неопытных разработчиков обязательно возникнут такие вопросы:\\nКак корректно распределять задачи между воркерами, чтобы не возникало различных\\nаномалий конкурентности? Например, когда воркеры дублируют друг друга,\\n', 'обрабатывая одни и те же данные, или, наоборот, блокируют один другого?\\nКак архитектурно грамотно гарантировать «довыполнение» задачи в случае ошибки\\nворкера?\\nКак гарантировать выполнение задачи не позднее требуемой задержки (актуально для\\nonline-систем)?\\nТут для полноты картины стоит привести примеры нескольких сценариев отказа, которые\\nмне предстояло решить:\\nВоркер взял задачу в работу и получил ошибку из-за сетевой недоступности, без\\nпоследующего перезапуска контейнера с приложением.\\nВоркер взял задачу в работу и завершился аварийно, с перезапуском контейнера с\\nприложением, например из-за проблем с выделением памяти.\\nВоркер взял задачу в работу, находился в работе больше допустимого времени и\\nзавершился успешно или с ошибкой слишком поздно (актуально при наличии\\nтребования по максимальной задержке).\\nА так выглядели первые попытки учесть все возможные точки отказа\\n', 'Почему мой пример может предложить ответы на эти вопросы\\nПредсказательную онлайн-модель, над которой работала наша команда, можно назвать\\nETL-сервисом, если не углубляться в детали алгоритма обработки данных. Необходимость\\nв подобных перекладчиках, либо же событийных обработчиках, возникает регулярно, когда\\nпроектируешь микросервисную систему или потоки данных внутри оркестраторов (data-\\nинженеры, это я про вас).\\nКроме того, система, компонентом которой является сервис онлайн-модели,\\nвысококритичная, из-за чего к нашему сервису предъявляются повышенные\\nнефункциональные требования:\\nОтсутствие недоступности: обработанные данные по каждому показателю\\nмониторинга должны появиться минута в минуту, потери в обработанных данных\\n(пропуски точек на графиках показателей) не допускаются.\\nВысокая пропускная способность (скорость обработки): в течение 60 секунд\\nнужно успеть загрузить из базы-источника значение каждого показателя за конкретную\\nминуту, и дополнительно 179 предыдущих (то есть суммарно за 180-минутное окно),\\nобработать и положить результат в базу-приёмник (для последующей отрисовки\\nграфиков на фронтенде). Всего более 100 тысяч регулярно обновляемых временных\\nрядов с данными по различным системам.\\nГеорезервирование: надо уметь пережить отказ целого ЦОДа, на котором запущен\\nотдельный k8s-кластер, где, в свою очередь, развёрнут сервис.\\nПроектирование архитектуры системы с подобными требованиями и первые попытки\\nтестирования ее прототипа привели меня, тогда ещё малоопытного, к перечисленным\\nвыше вопросам. Я сделаю упор на эти аспекты далее, при погружении в подробности\\nархитектуры сервиса онлайн-модели.\\nАрхитектура сервиса онлайн-модели с акцентом на три поставленных\\nвопроса\\nВ чём помогает механизм групп консьюмеров Kafka\\nПрежде чем перейдём к рассмотрению архитектуры, поговорим немного о том, почему в\\nкачестве шины данных использовали брокер Kafka, а также подробнее рассмотрим\\nмеханизм групп консьюмеров.\\nПро то, что такое Kafka и связанные с ней понятия — продьюсер, консьюмер, топик,\\nпартиция, оффсет — можно узнать из видео. Я рекомендую посмотреть его тем, кто пока\\nне знаком с Kafka, так как рассчитываю на понимание при дальнейшем прочтении.\\n', 'Во многом мне помог механизм групп консьюмеров, с помощью которого уже частично\\nможно реализовать распределённое отказоустойчивое распараллеливание, а именно:\\nКаждый консьюмер закрепляется только за своим набором партиций внутри группы,\\nчтение из других партиций группы невозможно. Это позволяет избегать различных\\nаномалий конкурентности при распределении задач между воркерами.\\nВнутри группы консьюмеров для каждой партиции отслеживается свой собственный\\nоффсет (номер позиции последнего сообщения в очереди, для которого сделан\\nкоммит). Например, если консьюмер будет делать коммиты прочитанных позиций\\nтолько после успешного выполнения и записи результатов, а не сразу после прочтения\\nсообщения из партиции, то в случае ошибки можно будет «довыполнить» задачу при\\nповторной попытке.\\nКонсьюмеры периодически отправляют «пульс» (heartbeat) для обозначения себя\\n«живыми».\\nВ случае, если какой-то консьюмер перестаёт подавать признаки жизни, или, наоборот,\\nв группе появляется новый, то запускается процедура ребалансировки партиций\\n(partition reassignment) между «живыми» консьюмерами по выбранной стратегии (round\\nrobin, range, sticky). Новый консьюмер может продолжить читать из партиции с того\\nоффсета, на котором остановился «умерший», это тоже помогает гарантировать\\nсогласованное «довыполнение» при падениях воркеров.\\nМеханизм групп консьюмеров Kafka    \\nКраткий итог про группы консьюмеров: на первый и часть второго вопроса отвечает\\nфункциональность самой Kafka, что существенно упростило мне жизнь при разработке.\\n', 'Верхнеуровневая архитектура сервиса онлайн-модели. Оба компонента написаны на Python, чтобы не \\nнарушать общий стек с коллегами-аналитиками и data scientist-ами.\\nПостановщик задач: бизнес-логика, точки отказа, георезервирование\\nСервис постановки задач (task manager) является вторым ключевым инструментом\\nраспараллеливания. Первый — это группы консьюмеров Kafka, его цель — поставить\\nзадачу на выполнение онлайн-корректировки по фиксированному набору показателей за\\nконкретный минутный таймстемп: например, 2025-01-01 10:01:00, 2025-01-01 10:02:00 и т.\\nд. Его цикл бизнес-логики повторяется постоянно в реальном времени, с фиксированной\\nвеличиной паузы между итерациями. Цикл содержит следующий набор действий:\\n1. Проверить, есть ли в топике уже поставленные на выполнение задачи за текущий и\\nдва следующих минутных таймстемпа (то есть в нормальном режиме работы в момент\\nвремени 12:05 будет поставлена задача на 12:07; при сбоях, во избежание пропусков в\\nобработанных данных, — дополнительно за 12:06 и 12:05 по необходимости) и\\nзавершить цикл работы, а если нет — то переходить на следующий этап.\\n2. Определить актуальный набор показателей, по которым поступают новые данные\\n(сделать select из таблицы с метаинформацией).\\n3. Разбить набор уникальных показателей на пакеты.\\n4. Сформировать задачи на выполнение для каждого пакета за недостающие минутные\\nтаймстемпы: в содержании указать таймстемп и набор идентификаторов показателей\\n(metric_id), сгенерировать UUID с использованием порядкового номера батча и\\nтаймстемпа.\\n5. Отправить задачи в топик Kafka, распределяя по партициям: каждая задача в\\nотдельную партицию.\\n', 'Цикл бизнес-логики постановщика задач\\nЦифры рядом с блоками соответствуют этапам, перечисленным выше. Ключевым шагом\\nдля обеспечения отказоустойчивости является предварительная проверка наличия в\\nтопике поставленных на выполнение задач за текущий и два следующих минутных\\nтаймстемпа. Такой «запас» необходим для «выживания» в случаях аварийного падения\\nконтейнера с приложением или ошибок сетевого взаимодействия.\\nРассмотрим возможные точки отказа (наглядная иллюстрация на рисунке ниже), не\\nсвязанные с падением контейнера с приложением:\\n1. Сетевая ошибка чтения из топика Kafka последних задач для определения\\nнедостающих таймстемпов.\\n2. Сетевая ошибка запроса к базе-хранилищу агрегированных данных на получение\\nполного списка актуальных показателей мониторинга.\\n3. Сетевая ошибка записи недостающих задач в топик Kafka.\\n', 'Цикл бизнес-логики постановщика задач с точками отказа при сетевом взаимодействии в виде блок-схемы\\nЦифры рядом с блоками соответствуют точкам отказа, перечисленным выше. Во всех\\nслучаях преодолением отказа будет повтор цикла на следующей итерации.\\nАварийное падение контейнера с приложением (на любом из этапов цикла бизнес-логики)\\nпреодолевается с помощью контроллера кластера виртуализации с одной стороны, и\\nгеорезервирования с другой. Достичь георезервирования, а вернее согласованной работы\\nэкземпляров приложения «постановщик задач» из разных геокластеров, помогает всё тот\\nже механизм групп консьюмеров Kafka.\\n', 'Архитектура сервиса постановщика задач c учётом георезервирования\\nВ данном случае внутри группы консьюмеров используется собственная стратегия\\nраспределения партиций: все партиции только одному консьюмеру, второй при этом\\nвыступает в качестве резервной standby (неактивной) копии на случай сбоя первого.\\nВ коде постановщика на этапе определения недостающих задач предусмотрена\\nдополнительная проверка на «пустые» данные (их получение возможно тогда, когда за\\nконсьюмером нет закреплённых партиций внутри группы, так как ему их не хватило) и, в\\nслучае положительного результата, переход сразу на этап паузы до следующей попытки\\n', '(итерации цикла бизнес-логики). То есть второй консьюмер будет постоянно получать\\n«пустые» данные, выполняя «холостой» цикл.\\nПри отказе в работе активного консьюмера (вызванного, например, падением целого\\nЦОДа, на котором развёрнут кластер) брокер Kafka перестанет получать от первого\\nheartbeat-сигналы и переназначит все партиции второму консьюмеру. Тем самым, второй\\nконсьюмер продолжит ставить задачи данных на выполнение без пропусков, а первый\\nперезапустится контроллером геокластера виртуализации и уже сам станет stanby-копией.\\nВоркер: бизнес-логика, точки отказа, георезервирование\\nОбработкой данных по показателям занимается компонент «воркер». Его цикл бизнес-\\nлогики повторяется постоянно в реальном времени, с фиксированной величиной паузы\\nмежду итерациями. Цикл содержит следующий набор действий:\\n1. Получить задачу на выполнение из топика Kafka.\\n2. Проверить, выполнена ли она, то есть посмотреть, присутствует ли её UUID в таблице\\nуспешно выполненных, если нет — переходить на следующий этап, если есть —\\nзавершить цикл работы.\\n3. Загрузить необходимые данные по набору показателей: получить по каждому\\nпоказателю значения за указанный таймстемп и предыдущие 179 минут.\\n4. Выполнить процедуру онлайн-корректировки по данным всего набора показателей из\\nполученной задачи.\\n5. Записать результаты онлайн-корректировки в базу с обработанными данными с\\nпомощью SQL-запроса вида INSERT ... ON CONFLICT ..., чтобы не допустить\\nповторную перезапись, если результирующие данные за таймстемп выполняемой\\nзадачи уже были сохранены ранее.\\n6. Отметить задачу как успешно выполненную: записать UUID в таблицу выполненных\\nзадач.\\n7. Закоммитить оффсет в соответствующую партицию топика (откуда была прочитана\\nзадача) в рамках используемой группы консьюмеров.\\n', 'Цикл бизнес-логики воркерa\\nЦифры рядом с блоками соответствуют этапам, перечисленным выше. Ключевыми шагами\\nдля обеспечения отказоустойчивости являются:\\nПредварительная проверка на то, выполнена ли уже задача.\\nФиксация факта успешного выполнения задачи в отдельной таблице.\\nСохранение результатов корректировки данных по показателям через INSERT ... ON\\nCONFLICT ... для исключения дублирования или перезаписи.\\nВсё перечисленное гарантирует согласованное «довыполнение» задачи при ошибке\\nворкера на любом из этапов цикла бизнес-логики и исключает повторное выполнение\\n«тяжёлого» расчёта другим воркером в случае, если ошибка произошла на этапе 7.\\n', 'Автор слишком часто видел, как воркер с первой попытки не хотел брать задачу в работу\\nРассмотрим возможные точки отказа, не связанные с аварийным падением контейнера с\\nприложением (иллюстрация на рисунке ниже):\\n1. Сетевая ошибка чтения из топика Kafka задачи на выполнение.\\n2. Сетевая ошибка запроса к базе для предварительной проверки, была ли данная\\nзадача уже выполнена ранее.\\n3. Сетевая ошибка запроса к базе с целью получения необходимых для выполнения\\nзадачи данных.\\n4. Сетевая ошибка запроса к базе для записи результатов корректировки и возможных\\nметаданных по новым метрикам.\\n', '5. Сетевая ошибка запроса к базе с целью добавления UUID задачи в таблицу успешно\\nвыполненных.\\n6. Сетевая ошибка выполнения коммита в топик Kafka.\\nЦикл бизнес-логики воркера с точками отказа при сетевом взаимодействии\\nЦифры рядом с блоками соответствуют точкам отказа, перечисленным выше. Во всех\\nслучаях преодолением отказа будет повтор цикла на следующей итерации.\\nАварийное падение контейнера с приложением на любом из этапов цикла бизнес-логики\\nпреодолевается с помощью контроллера кластера виртуализации с одной стороны, и\\nгеорезервирования с другой.\\n', 'Архитектура сервиса-воркера c учётом георезервирования\\nВспомним, что в нефункциональных требованиях к системе присутствует пункт про\\nмаксимальную задержку. По итогам тестов производительности воркера со всеми\\nвозможными оптимизациями (многопоточность, подбор оптимального размера пакета\\nпоказателей, переход на самописные реализации некоторых библиотечных функций) в\\nусловиях доступного корпоративного железа максимальное время выполнения составило\\n30 секунд. С такими результатами очень высока вероятность того, что суммарная задержка\\nв случае сбоя составит больше минуты.\\n', 'Для выполнения требования по максимальной задержке георезервирование было\\nреализовано так, что каждому геокластеру соответствует своя группа консьюмеров на\\nуровне брокера Kafka. Обе группы настроены на одни и те же партиции.\\nПо итогу, в штатном режиме задача за конкретный таймстемп будет выполняться\\nпараллельно (дублироваться) двумя воркерами из разных геокластеров, а результат,\\nвозможные метаданные по новым метрикам и UUID выполненной задачи будут записаны\\nтолько один раз (благодаря INSERT ... ON CONFLICT ...). Предполагается, что\\nодновременное «падение» двух воркеров, выполняющих одну и ту же задачу, но\\nнаходящихся в разных геокластерах (то есть физически в разных ЦОДах), маловероятно.\\nЕщё одна вещь, обеспечивающая согласованность работы консьюмеров из разных\\nгеокластеров, — общая таблица с UUID успешно выполненных задач. Она исключает\\nповторное выполнение в случае сбоя только одного из дублирующих воркеров.\\nЕщё раз о ключевых моментах обеспечения нефункциональных\\nтребований\\nМы подробно рассмотрели как архитектуру планировщика задач, так и архитектуру\\nворкера (в обоих случаях с обеспечением георезервирования). Все поставленные перед\\nсистемой нефункциональные требования обеспечены, а именно:\\nОтсутствие недоступности (появление обработанных данных минута в минуту,\\nотсутствие пропусков (потерь) в данных) гарантируется: со стороны планировщика\\nзадач — общей группой консьюмеров Kafka между единичными экземплярами\\nконтейнеров в разных геокластерах и «запасом» публикуемых в топик задач сразу на\\nтекущий и два последующих минутных таймстемпа; со стороны воркера —\\n«дублирующими» группами консьюмеров Kafka для множественных экземпляров\\nконтейнеров в разных геокластерах.\\nВысокая пропускная способность гарантируется распараллеливанием потока\\nобработки через множество независимых друг от друга воркеров, а также некоторыми\\nоптимизациями на уровне кода приложения.\\nГеорезервирование достигается размещением экземпляров приложений в k8s-\\nкластерах, развёрнутых на географически разнесенных ЦОДах. Согласованная работа\\nмежду контейнерами из разных геокластеров становится возможной: в случае\\nпланировщика задач — благодаря общей группе консьюмеров Kafka между\\nединичными экземплярами контейнеров из разных геокластеров; в случае воркера —\\nблагодаря фиксации UUID успешно выполненных задач в одну общую SQL-таблицу\\nконтейнерами из разных геокластеров.\\n', 'Когда всё-таки предусмотрел все возможные сценарии отказа\\nПочему архитектурные решения в нашем примере можно\\nпереиспользовать много где ещё\\nРассмотренный пример может повторять типичную реализацию распределённых систем,\\nсвязанных с обработкой событий, где в качестве события может быть что угодно:\\nсообщение в чате, электронное письмо, транзакция платёжной операции, клиентская\\nактивность в CRM. Во всех перечисленных случаях крайне важно гарантировать\\nобязательную доставку данных без потерь и с требованием по максимальной задержке,\\nчтобы не допустить ухудшения клиентского опыта.\\nРазобранный пример актуален и для потоковой обработки с помощью оркестраторов в\\nзадачах data-инжиниринга. Используемый оркестраторами directed acyclic graph (DAG)\\nслужит для преодоления различных точек отказа примерно такими же способами, как\\nописано в примере.\\nПоэтому в качестве ответов на вопросы, поставленные в начале, можно привести\\nнесколько обобщающих выводов, полезных при архитектурном проектировании подобных\\nрешений:\\nДля достижения согласованной обработки между распределёнными контейнерами с\\nприложением необходимо реализовать (или использовать возможности готовой\\nтехнологии) кооперативный механизм распределения задач (событий), чтобы\\nизбежать различных аномалий конкурентности вроде нежелательных дублирований\\nили блокировок.\\nТут, на самом деле, всё будет зависеть от решаемых системой задач и требований. В\\nкаких-то случаях может быть достаточно функциональности SQL с транзакциями и\\n', '3\\nКарма\\n1\\nРейтинг\\nВладимир Сторожилов @vstorozhilov\\nbackend, devops и когда нужно data scientist ;-)\\nблокировками. В моём случае помогли группы консьюмеров Kafka. Нечто аналогичное\\nможно увидеть и в RabbitMQ, где сообщения из одной и той же очереди\\nраспределяются между консьюмерами через round robin. Даже Redis имеет\\nфункциональность каких-никаких транзакций. Но если всё-таки обойтись готовыми\\nинструментами никак не получается, то придётся реализовывать ещё один компонент\\n— менеджер задач (событий). Пример можно посмотреть всё в той же статье,\\nупомянутой в предисловии.\\nЧтобы однозначно гарантировать исполнение события (задачи), может быть\\nнеобходима фиксация факта успешного выполнения в общее хранилище с\\nпомощью сохранения туда уникального идентификатора.\\nС другой стороны, это позволит избежать повторного запуска «тяжёлого» алгоритма\\nобработки данных и пустой траты вычислительных ресурсов, если воркер упал на\\nодном из последних этапов. В моём примере это была отдельная SQL-таблица. Из\\nготовых инструментов можно снова вспомнить пример библиотеки Celery в экосистеме\\nPython, которая тоже сохраняет идентификаторы завершённых задач в отдельную\\nочередь, таблицу или массив (в зависимости от того, что используется в качестве\\nшины и хранилища).\\nЕсли к системе есть требование по максимально допустимой задержке, то\\nхорошим способом этого достичь будет дублирование потока обработки, то есть\\nобработка одних и тех же событий (задач) через параллельно запущенные воркеры\\n(возможно, из разных кластеров виртуализации, если есть требование по\\nгеорезервированию).\\nТеги: конкурентность, распределенные системы, распределённая обработка данных, \\nгеорезервирование, таск-менеджер, таск-раннеры, дата-инжиниринг, параллелизм\\nХабы: Python, Data Engineering, Распределённые системы\\nРедакторский дайджест\\nПрисылаем лучшие статьи раз\\xa0в\\xa0месяц\\nЭлектропочта\\n+1\\n11\\n0\\n']\n",
      "Три необсуждаемых вопроса о параллельной распределённой обработке данных — чтобы жить стало легче _ Хабр.pdf\n",
      "['Привет, Хабр! На связи Владимир, техлид в команде разработки ИИ-инструментов в', 'департаменте технологической надёжности одной из крупных компаний. Наша команда', 'помогает делать корпоративные и клиентские сервисы надежнее помощью Data Science.', 'Мы помогаем мониторить тысячи подсистем', 'Система, в разработке которой участвует моя команда, предназначена для', 'централизованного мониторинга тысяч отдельных систем, в том числе распределённых', 'географически. Наша группа отвечает за online-модель, подсвечивающую явно', 'аномальные отклонения в показателях мониторинга. С её помощью инженер дежурной', 'смены однозначно определяет, есть ли сейчас проблемы, требующие немедленного', 'реагирования.', 'Данные мониторинга собираются из множества различных источников. И поскольку их', 'поступает много в единицу времени, возникла острая необходимость в', 'распараллеливании. Про потоковую обработку данных, обработку событий и выполнение', 'задач по расписанию в распределённой системе написано немало. Среди множества', 'публикаций, на мой взгляд, можно выделить очень достойные: вот пример с глубоким', 'анализом точек отказа.', 'Но после прочтения многих статей мне не хватило явных ответов на вопросы, неизбежно', 'возникающих у ещё недостаточно опытного разработчика.', 'В большинстве случаев содержание выглядит примерно так: сначала схема с общей', 'шиной (брокер, кеш, реляционная база) и несколькими воркерами (либо потоками) для', 'параллельного выполнения, затем вставки с кодом, где используется какая-то библиотека', '(или фреймворк). В экосистеме Python это либо Celery, либо же что-то из оркестраторов', '(Nifi, Airflow, Spark), если речь идёт о задачах data-инжиниринга. Но это, как я уже сказал,', 'не отвечает на все вопросы.', 'vstorozhilov', '7 часов назад', 'Три необсуждаемых вопроса о параллельной', 'распределённой обработке данных — чтобы жить стало', 'легче', 'Средний', '12 мин', '328', 'Python*,\\xa0Data Engineering*,\\xa0Распределённые системы*', 'Туториал', 'КАК СТАТЬ АВТОРОМ', '']\n"
     ]
    }
   ],
   "source": [
    "# Перебор текста из pdf файла, парсинг\n",
    "Brak = []\n",
    "NameCompany = []\n",
    "Description = []\n",
    "Raiting = []\n",
    "DataPublish = []\n",
    "Activity = []\n",
    "TextArticle = []\n",
    "for pdf in all_pdf:\n",
    "    try:  \n",
    "        x = extract_text_from_pdf(pdf)\n",
    "        print(pdf.split('\\\\')[-1])\n",
    "        N, R, A, D = pars_pdf(x[0])\n",
    "        T = ''.join(x).replace('\\n',' ')\n",
    "        NameCompany.append(N)\n",
    "        Raiting.append(R)\n",
    "        DataPublish.append(D)\n",
    "        Activity.append(A)\n",
    "        TextArticle.append(T)\n",
    "    except:\n",
    "        Brak.append(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e5b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PDF2\\\\ASML_ Создание самых сложных машин на Земле _ Хабр.pdf',\n",
       " 'PDF2\\\\Deutsche Telekom и Perplexity объявили о новом «AI Phone» стоимостью менее 1 000 долларов _ Хабр.pdf',\n",
       " 'PDF2\\\\Yandex N.V. запретили в\\xa0течение пяти лет создавать аналогичные «Яндексу» сервисы _ Хабр.pdf',\n",
       " 'PDF2\\\\Генеральный директор Mozilla покинула свой пост _ Хабр.pdf',\n",
       " 'PDF2\\\\За что безопасники будут гореть в аду_ _ Хабр.pdf',\n",
       " 'PDF2\\\\Заезжаем в Kotlin Multiplatform. Но какой ценой_ _ Хабр.pdf',\n",
       " 'PDF2\\\\Как быстро найти цены на совместимую с Home Assistant бытовую технику_ мой опыт автоматизации поиска цен _ Хабр.pdf',\n",
       " 'PDF2\\\\Как системному аналитику написать хорошее резюме —\\xa011 рекомендаций _ Хабр.pdf',\n",
       " 'PDF2\\\\Компании _ Хабр.pdf',\n",
       " 'PDF2\\\\Может ли chatGPT забронировать столик в ресторане через WhatsApp.pdf',\n",
       " 'PDF2\\\\Почему Data Science не для вас.pdf',\n",
       " 'PDF2\\\\Три необсуждаемых вопроса о параллельной распределённой обработке данных — чтобы жить стало легче _ Хабр.pdf']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Brak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e643fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataFrame\n",
    "df = pd.DataFrame({\n",
    "'NameCompany': [],\n",
    "'Description': [],\n",
    "'Raiting': [],\n",
    "'DataPublish': [],\n",
    "'Activity': [],\n",
    "'TextArticle': []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6a045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NameCompany",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Description",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Raiting",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DataPublish",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Activity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TextArticle",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bfb3c0b1-d9d0-4ce3-8304-cdf427de162e",
       "rows": [],
       "shape": {
        "columns": 6,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NameCompany</th>\n",
       "      <th>Description</th>\n",
       "      <th>Raiting</th>\n",
       "      <th>DataPublish</th>\n",
       "      <th>Activity</th>\n",
       "      <th>TextArticle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NameCompany, Description, Raiting, DataPublish, Activity, TextArticle]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(NameCompany), len(Description), len(Raiting), len(DataPublish),len(Activity), len(TextArticle) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87c281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red_mad_robot',\n",
       " 'Cloud4Y',\n",
       " 'Cloud4Y',\n",
       " 'Southbridge',\n",
       " 'Альфа-Банк',\n",
       " 'RUVDS.com',\n",
       " 'Хабр',\n",
       " 'RUVDS.com',\n",
       " 'RUVDS.com',\n",
       " 'getmatch']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NameCompany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NameCompany'] = NameCompany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Raiting'] = Raiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f43976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DataPublish'] = DataPublish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e06686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Activity'] = Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TextArticle'] = TextArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011dfe46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NameCompany",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Description",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Raiting",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DataPublish",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Activity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TextArticle",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "53040f31-31a3-40d7-81a1-3529510b52bc",
       "rows": [
        [
         "0",
         "red_mad_robot",
         null,
         "117.94",
         " 6 часов назад",
         "№1 в разработке цифровых решений для бизнеса",
         "117.94 Рейтинг red_mad_robot №1 в разработке цифровых решений для бизнеса  6 часов назад Средний18 мин redmadrobot Cocoapods, Carthage, SPM: как выбрать менеджер зависимостей в iOS 179 Блог компании red_mad_robot ,  Разработка под iOS* FAQ КАК СТАТЬ АВТОРОМ Старший iOS-разработчик red_mad_robot Аня Кочешкова рассказывает, чем отличаются три менеджера зависимостей, в каких случаях и для каких задач подойдёт тот или иной. Материал будет полезен джун-специалистам, которые только начали погружаться в разработку: специально для них подробно объясняем, что такое семантическое версионирование, как устроены модули кода и в чём разница между динамическими и статическими библиотеками. Что такое менеджер зависимостей и зачем он нужен В современной разработке зависимость — это написанный кем-то другим код, который используется в вашей программе. Добавление зависимости позволяет воспользоваться уже сделанной кем-то хорошей работой и избежать «изобретения велосипеда», а также лишних действий: проектирования, написания, тестирования, отладки и поддержки определённой логики. Такие «куски логики» называют пакетами, библиотеками или модулями. Чем сложнее проект, тем больше сторонних библиотек (правда, некоторые приложения могут вообще отказываться от их использования), а у этих библиотек много разных версий. Они могут ссылаться друг на друга и накладывать ограничения на эти версии. Эту работу можно проиллюстрировать таким бытовым примером. Чтобы телефон показал погоду на улице, не нужно встраивать в него термометр и вытаскивать за окно. В нём уже зашита специальная технология, которая позволяет ему связаться с «библиотекой», где хранятся данные гидрометцентра, которые, как и погода, всё время обновляются. Иногда с новой версией добавляется какая-нибудь новая функциональность. В примере с термометром библиотека с новой версией умеет ещё и выдавать карту дождей, а до этого могла показывать только температуру. Менеджеры зависимостей позволяют избежать долгого и муторного решения такого набора «уравнений» — то есть берут на себя часть работы, которую без них разработчик делал бы руками. Это система, которая позволяет управлять зависимостями в вашем проекте. А ещё у сторонних библиотек часто могут выходить новые версии с исправленными ошибками, и благодаря менеджеру зависимостей вам не нужно следить за их выходом — он всё обновит за вас. Предположим, вы решили подключить в ваш проект Firebase Crashlytics и GoogleUtilities. Обе эти библиотеки ссылаются на библиотеку PromisesObjC. Firebase хочет, чтобы она была любой версии, начинающейся с 2.1, а GoogleUtilities — чтобы она была от 1.2 до 3.0. Такое уравнение, конечно, можно решить вручную, но что делать, когда этих уравнений несколько десятков, а уровней зависимостей — два, три или больше? Для таких задач в распоряжении iOS-разработчика существуют три основных менеджера зависимостей: 1. Cocoapods. Пожалуй, самый распространённый сегодня, он требует лишь указывать список зависимостей в определённом формате в текстовом файле. 2. Carthage. Менее удобен в этом плане: вы также указываете список зависимостей в текстовом файле, но помимо этого нужно ещё немного «самостоятельной работы», о которой расскажем ниже. 3. SPM, или Swift Package Manager. Официальный менеджер зависимостей от Apple. Появился недавно, и многие команды переходят на него с Cocoapods. Позволяет управлять зависимостями как через графический, так и через текстовый интерфейс прямо из Xcode. Случается, что нужная библиотека не поддерживает необходимый менеджер зависимостей, потому что разработчики каждой библиотеки сами выбирают, какой менеджер поддержать. Поддержка каждого менеджера — это работа, которая занимает определённое время, поэтому часто выбирают работать только с самым популярным менеджером, Сocoapods. А если в проекте, например, SPM, а библиотека есть только в Cocoapods или просто нужна в одной конкретной версии, приходится добавлять вручную как framework / xcframework, о чём мы также поговорим позже. Но прежде чем начать говорить об управлении зависимостями, нужно разобрать несколько связанных с ним терминов. Фантастическое Semantic Versioning, и где оно обитает Semantic Versioning (SemVer) — это стандарт, который говорит о том, как работать с версиями пакетов/библиотек/приложений, — как их указывать и накладывать на них ограничения. Зачем накладывать ограничения? Возьмём такой пример. Разработчик создаёт приложение и использует некоторый набор библиотек для упрощения работы — например, библиотек для работы с сетью Alamofire. Допустим, начиная с определённой версии в библиотеке появляется поддержка какой-то новой функциональности, например Modern Concurrency. Эта функциональность доступна только начиная с определенной версии библиотеки. В таком случае от разработчика потребуется всего лишь поднять версию Alamofire в манифесте, где он описывает используемые библиотеки. Если начиная с какой-то версии библиотеки исправляется некий важный баг, но не сильно меняется функциональность самой библиотеки, то обновление до свежей исправленной версии произойдёт без лишнего вмешательства разработчика, в следующий раз при обновлении текущих библиотек проекта. Разберём, что такое SemVer, чтобы понимать формат версий, с которыми придётся работать, а также узнаем, как можно указать версию зависимости и наложить на неё ограничения. Этот набор правил — глобальный стандарт, который используется повсеместно, начиная от пакетов Node.js и заканчивая версиями приложений в App Store. По этому стандарту номер версии — это строка, которая состоит из чисел, разделённых точкой, и имеет формат A.B.C, где A — мажорная версия, B — минорная версия и C — патч-версия. Версии сравниваются слева направо, например: 1.0.0 < 2.0.0 < 2.1.0 < 2.1.1. Но зачастую мажорная версия увеличивается или при выпуске каких- либо масштабных фич, или при редизайне приложения, или просто при желании обозначить принципиальное обновление приложения/библиотеки. Синтаксис SemVer в Cocoapods, Carthage и SPM Cocoapods В Cocoapods можно указать одну из стратегий выбора версии: 1. Если необходима конкретная версия, просто укажите её без дополнительных символов, как есть: '5.0.0'. 2. Если необходима точная версия, но при этом вы также хотите обновляться, если придут исправления багов в патч-версии, используйте операнд ~>: '~> 5.0.0'. 3. Можно указывать неравенства: '> 5.0.0', '≥ 5.0.0' или '< 5.0.0', '≤ 5.0.0'. В этом случае Cocoapods обновит библиотеку до 5.0.1, 5.0.2 и т. д., но не возьмёт обновление 5.1.0, так как в нём уже повышена минорная, а не патч-версия. Если вы также хотите завязаться на минорную версию, достаточно просто указать '~> 5.0'. Carthage В Carthage используется тот же формат, что и в Cocoapods, с тем только исключением, что точная версия указывается через равенство: == 5.0.0. SPM SPM позволяет указать стратегию выбора версии прямо в интерфейсе, доступны следующие: Аналогичный синтаксис и у Package.swift файла. Модули — что к чему: библиотеки, фреймворки, XCFramework и Swift-пакеты Организация кода в iOS построена на понятии модулей. Модуль — это отдельный кусок кода или функциональности, который можно распространять разными способами и переиспользовать. Существует четыре вида модулей. Разберёмся, в чём разница: 1. Библиотека — это исполняемый код, скомпилированный под конкретную архитектуру. 2. Фреймворк — папка, содержащая в себе библиотеку и вспомогательные файлы. 3. XCFramework — набор фреймворков под разные архитектуры. 4. Swift-пакет — фреймворк, который используется для работы со Swift Package Manager. Если говорить на языке бытовых примеров, то код — это комната, а модули — разные предметы в ней. Торшер, например, позволяет включать и выключать свет, но как он работает внутри, мы не знаем. И этим торшером могут пользоваться в разных квартирах и комнатах разные люди. Библиотеки Библиотека — это исполняемый код, скомпилированный под конкретную архитектуру, например arm64. Архитектуры могут различаться для разных типов устройств и симуляторов: • iOS-устройства — архитектуры armv7, armv7s, arm64, • iOS-симулятор — архитектуры i386 и x86_64. Исходный код — текст программы, который написал разработчик. Исполняемый код — код, который получился в результате компиляции. Компилятор переводит исходный код в машинный, который сможет исполнить операционная система, — на выходе получается исполняемый код. Библиотеки бывают статическими и динамическими (об этом позже), и представляют собой один бинарный файл. Если представить, что библиотека — это настоящая библиотека со стеллажами, в которых хранятся книги, то эти книги — справочники о том, как что-то сделать (методы для выполнения разных задач). По сути, это скомпилированный набор объектных файлов, которые программа может использовать для выполнения любой задачи. То есть все файлы, которые библиотека собирается использовать, были скомпилированы до этапа компоновки. Фреймворки Фреймворк (.framework) представляет собой папку с файлами, в которой лежит сама библиотека, хедеры и ресурсы. Эта папка должна иметь определённую структуру, но нет нужды запоминать её, так как фреймворки, как правило, собираются самим Xcode. Поддержка фреймворков добавилась в iOS 8. Фреймворки служат той же цели, что и библиотеки, но могут включать в себя различные ресурсы — картинки, документацию, строковые файлы. Они также могут быть статическими или динамическими. Что использовать: фреймворк или библиотеку В чём же разница между фреймворком и библиотекой? И когда их использовать? Принципиальное различие — в архитектуре. XCFrameworks Для начала стоит рассказать, что такое fat-фреймворки. Это такие фреймворки, которые содержат библиотеки для разных архитектур и платформ, соединённые в одну. С появлением Xcode 11 на замену им пришли XCFrameworks, которые ввели структуру, позволяющую разнести библиотеки для разных платформ и архитектур по папкам. XCFramework — это, по сути, такой усовершенствованный fat- фреймворк. Он также содержит различные библиотеки для разных архитектур и платформ, но теперь нет необходимости трудиться над тем, чтобы объединить их в одну. В одном XCFramework может лежать несколько разных фреймворков — для iOS, macOS, tvOS, watchOS, а также для разных архитектур — armv7, x86_64 и т. д., каждый в своей папке. В каком-то смысле фреймворк — тоже разновидность библиотеки, а XCFramework — разновидность фреймворка, но всё же эти понятия принято разделять. Создатель зависимости сам выбирает, в каком виде её распространять. Нам важно понимать различие между этими видами, выбор можно сделать, только создавая свою библиотеку. Swift Package Swift-пакет используется для распространения только через SPM. Содержит исходный код, а также Package.swift файл, который описывает его конфигурацию. Доступен начиная со Swift 3. Static vs Dynamic Прежде чем начать, наконец, говорить об управлении зависимостями, важно разобраться, в чём различие между статическими и динамическими библиотеками и фреймворками. Первое, что здесь важно понимать: системные iOS- и macOS- библиотеки — динамические. Приложения могут хранить ссылку на библиотеку. Например, системная библиотека Foundation — динамическая. Она лежит на айфоне в единственном экземпляре, а все приложения лишь ссылаются на неё. Фреймворки, как и библиотеки, бывают динамические и статические и представляют собой обёртку библиотеки. Так, динамические библиотеки вне фреймворков не поддерживаются на iOS, watchOS и tvOS, и здесь важно отметить, что во всех дальнейших сравнениях, говоря «динамическая библиотека», мы будем иметь в виду её использование в рамках динамического фреймворка. Фреймворки в Cocoapods и Carthage Зависимости, распространяемые через Cocoapods, называются кокоаподами, или подами. До iOS 8 под представлял собой fat- статическую библиотеку. Есть специальная инструкция — по ней Cocoapods использует фреймворки вместо статичных библиотек, которые не поддерживались Swift до Xcode 9 и CocoaPods 1.5.0. Так что раньше эта директива была обязательна, сейчас — опциональна. Если нужно более быстрое время запуска, можно её убрать (см. предыдущий раздел). Carthage поддерживает только фреймворки. target 'TargetName' do   use_frameworks! end Три менеджера зависимостей: Cocoapods, Carthage, SPM Наконец, то, ради чего мы тут собрались, — менеджеры зависимостей. Посмотрим, какие они бывают, чем отличаются и как выбрать между ними. Cocoapods Это один из самых популярных сегодня менеджеров зависимостей. Позволяет указать список зависимостей в текстовом файле, а затем генерирует на основе его .xcworkspace, который будет содержать два проекта — ваш проект и проект с подами. Как правило, почти все библиотеки поддерживают Cocoapods, хотя с развитием SPM стали появляться и такие, которые поддерживают исключительно SPM (в основном это библиотеки Apple). В таком случае, если ваш менеджер зависимостей Cocoapods, единственный вариант воспользоваться библиотекой — вручную её собрать и прилинковать к проекту. Чтобы понять, поддерживает ли библиотека Cocoapods, достаточно заглянуть в readme, либо обратить внимание на наличие .podspec- файла в корне репозитория. Все библиотеки, опубликованные в Cocoapods, можно посмотреть на их официальном сайте. Cocoapods в вашем проекте Для понимания работы Cocoapods важны два термина — подфайл и спека. Подфайл описывает все зависимости приложения, а спека — библиотеку (её название и набор её зависимостей) для Cocoapods. Спеки хранятся в общем репозитории спек Cocoapods. Ещё их можно хранить в приватных репозиториях. Для описания конфигурации зависимостей используется подфайл. Этот файл написан на языке Ruby. 1. В файле опционально укажите источник спек. Источников может быть несколько, даже приватные. source 'https://github.com/CocoaPods/Specs.git' — официальный источник спек, который указан по умолчанию. source 'https://github.com/Artsy/Specs.gi — можно указать свои источники спек, в том числе приватные. 2. Укажите минимальную версию iOS: platform: ios, '9.0'. 3. Укажите таргет и набор зависимостей для него. Если таргетов несколько, укажите все, так как зависимости для них могут различаться. 4. Декларируйте зависимость. Первым идёт её официальное название (то самое, которое указано в спеке), затем версия. Используйте в названии правила из блока про SemVer. Есть и другие способы указать зависимости. Это может понадобиться в случаях, когда нужно использовать какую-то конкретную версию библиотеки. Например, из определённой ветки репозитория, где она хранится, или даже из определённого коммита. Либо указать путь локально, если есть только её файл.     target 'MyApp' do     pod 'GoogleAnalytics', '~> 3.1'     ...     end 1. Можно указать ресурс репозитория, с которого будем загружать библиотеку: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git' 2. Можно указать нужную ветку в репозитории: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :branch => 'dev' 3. Или тег: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :tag => '0.7.0' 4. Или коммит: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :commit => '082f8310af' 5. Можно указать на локальный под (путь должен вести на папку, где лежит .podspec): pod 'AFNetworking', :path => '~/Documents/AFNetworking' Рядом с подфайлом обычно лежит Podfile.lock. Этот файл описывает конечные версии зависимостей после их установки. Он обычно так же заливается в GitHub, как и сам Podfile, чтобы избежать ошибок и рассинхрона при установке зависимостей у разных разработчиков. Ещё он ускоряет выполнение pod install. Чтобы приступить к работе, запустите pod install. Можно выполнить pod update, если вы хотите обновить все библиотеки в проекте, иначе же будут использоваться фиксированные версии зависимостей из Podfile.lock. Как Cocoapods работает под капотом Pod install генерирует workspace — файл .xcworkspace, состоящий из двух проектов — проекта с вашим приложением и отдельного проекта, в котором хранятся зависимости (поды). Чтобы всё работало корректно, всегда открывайте workspace-файл. Если внимательнее посмотреть, что происходит, можно увидеть, что продукт этого проекта линкуется в основной проект: Также Cocoapods добавляет пару скриптов в Build Phases: Поддержка Cocoapods в вашей библиотеке Чтобы библиотека была доступна через Cocoapods, нужно сначала описать библиотеку или под при помощи спеки, собрать её и отправить спеку в приватный либо глобальный репозиторий Cocoapods. Разберёмся, как это сделать. 1. Создайте спеку и файл лицензии. Лицензию нужно добавлять для любой библиотеки, и чаще всего используется лицензия MIT — самая популярная лицензия для программ с открытым исходным кодом. Её текст можно найти в интернете. Здесь нужно описать вашу библиотеку. Файл принято форматировать так, чтобы вторая часть строки была выровнена.     Pod::Spec.new do |spec|     # Название библиотеки. Именно его будут указывать разраб     spec.name                    = 'NAME'      # Версия     spec.version                 = '1.2.1'      # Описание     spec.summary                 = 'Describe your framework.     # Здесь можно указать путь на вебсайт или просто GitHub-     spec.homepage                = 'https://xxx'      # Путь к лицензии. Да, её тоже нужно создать     spec.license                 = { type: 'MIT', file: 'LIC     # Автор библиотеки     spec.author                  = { \"Your Name\" => 'your-em     # Путь к исходным файлам (чаще всего это ваш репозитоий)     spec.source                  = { :git => 'https://github     # Минимальная поддерживаемая версия iOS и Swift     spec.ios.deployment_target   = '13.0'      spec.swift_version           = '5.0'     # Если библиотека использует локальный фреймворк, его ну     spec.vendored_frameworks     = 'Path/To/Files/NameOfVend 2. Проверьте, что всё корректно. Следующая команда валидирует спеку — проверяет, что она не имеет ошибок, а проект билдится — pod lib lint. 3. Отправьте спеку в репозиторий спек. Как правило, это делается уже на финальном этапе разработки — после того, как прошла проверка и вы готовы публиковать работу, — pod trunk push NAME.podspec. Carthage     # Указываем вайлдкард-пути к файлам, которые необходимо      spec.source_files            = 'Path/To/Files/**/*.{swif     # Путь к ресурсам     spec.resources               = 'Path/To/Files/Resources/     # Укажите здесь все ассеты, которые вам нужны, включая .     spec.resource_bundles        = {'BundleName' => ['Path/T     # Указываем зависимости     spec.dependency           \"Alamofire\", \"~> 5.0.0\"      # И системные зависимости     spec.frameworks           = 'Foundation'      spec.ios.frameworks       = 'UIKit'     end Carthagе — это децентрализованный менеджер зависимостей, которые не нужно «поддерживать» разработчику. Carthage сам выкачивает библиотеку из репозитория, компилирует её и предоставляет в готовом виде. Поэтому, если хотите использовать Carthage, не нужно искать в репозитории с библиотекой определённые инструкции или слова о том, что он поддерживается, — просто попробуйте. Нет у Carthage и централизованного источника, как у Cocoapods. Этим источником, по сути, является сам GitHub. Конечно, этого не всегда бывает достаточно. Некоторые библиотеки Carthage просто не может «переварить» — что-то не собирается, а что-то отваливается. Задача разработчика — проверить, всё ли работает корректно. Наконец, Carthage может автоматически подхватить уже собранный фреймворк, если он прикреплён к соответствующему релизу на GitHub. Carthage в вашем проекте Зависимости в Carthage описываются в текстовом файле Cartfile. Он очень похож на Podfile, только гораздо проще по структуре. Мы указываем источник, имя зависимости и версию. 1. Источник зависимости. Как правило, GitHub, либо git для универсальных репозиториев Git, размещённых в другом месте. Ключевое слово git сопровождается путём к репозиторию, будь то удалённый URL-адрес, используя git://, http://, или ssh://, или локальный — путём к репозиторию git на компьютере разработчика. 2. Название. Через слеш указывается владелец и название репозитория, которые можно подсмотреть в ссылке на репозиторий. 3. Версия. Особенности указания версий мы уже рассмотрели в блоке про семантическое версионирование, но можно также указывать и ветки. Примеры: 1. Версия — github \"Alamofire/Alamofire\" == 2.0 2. Ветка — github \"username/project\" \"branch\" 3. Локальный проект — git \"file:///directory/to/project\" \"branch\" 4. Бинарник (путь указывается в json) — binary \"https://my.domain.com/release/MyFramework.json\" ~> 2.3 После описания зависимостей потребуется ещё несколько дополнительных шагов. 1. Вызовите carthage update. Carthage склонирует репозитории с указанных в Cartfile путей и затем сбилдит для каждой зависимости фреймворк. После окончания работы вы увидите несколько файлов и каталогов: 2. Полученные фреймворки затем вручную перетащите из папки Build в проект в секцию Frameworks, Libraries и Embedded Content. 3. Carthage требует добавления нового скрипта в Build Phases в качестве workaround бага Apple. Это позволяет dSYM- файлам корректно подтягиваться при архивации. 4. Перейдите в Build Phases и добавьте новый Run Script. Добавьте следующую команду: /usr/local/bin/carthage copy- frameworks. 5. Нажмите на плюс под Input Files и добавьте запись для каждой платформы: $(SRCROOT)/Carthage/Build/iOS/Alamofire.framework. Как Carthage работает под капотом Как и в случае с Cocoapods, создаётся лок-файл Cartfile.resolved, который фиксирует конечные версии зависимостей. Каталог Carthage содержит два подкаталога: 1. Build. Содержит собранный фреймворк для каждой зависимости. 2. Checkouts. Содержит клонированные исходники, которые Carthage потом компилирует. Это своего рода кэш, который ускоряет работу. Поддержка Carthage в вашей библиотеке Для поддержки Carthage достаточно сделать релиз на GitHub. Но никогда не помешает проверить, всё ли билдится корректно в вашем случае и, если есть какие-то проблемы, решить их. Для этого, выполните carthage build --no-skip-current. Carthage попытается сбилдить ваш проект и создаст в нём папку Carthage. Также стоит обратить внимание, что он видит только те схемы, которые помечены в проекте как Shared. SPM Или Swift Package Manager — официальный менеджер зависимостей от Apple. Работа с ним происходит прямо в Xcode, а .Xcodeproj файлы больше не нужны (хотя при желании есть возможность сгенерировать файл проекта из SPM-пакета). Если заглянуть в репозиторий библиотеки, то первое, что будет говорить о поддержке SPM, — это наличие Package.swift-файла. SPM в вашем проекте Добавление зависимостей в проект происходит в секции Package Dependencies. 1. Для начала выберите File > Swift Packages > Add package dependecy. 2. Найдите модуль по поиску, либо введите URL репозитория. 3. Укажите версию: Как и во всех предыдущих случаях, SPM создает Package.resolved, назначение которого то же, что и у других менеджеров зависимостей. В меню File > Swift Packages доступно несколько действий: • Reset Package Caches удаляет кэши из Derived Data. • Resolve Package Versions резолвит версии зависимостей. Создаёт Package.resolved, если его не существует, но не меняет его, если он уже есть. Может потребоваться, если Package.resolved был, например, обновлён извне. • Update to Latest Packages Versions обновляет версии пакетов, может модифицировать Package.resolved. Поддержка SPM в вашей библиотеке Если вы разрабатываете библиотеку и хотите распространять её через SPM, нужно будет создать Package.swift файл. Таким образом может быть сконфигурирован и обычный проект, что позволяет отойти от использования project-файлов. Конфигурация через SPM выглядит чище и более проста для понимания и редактирования, чем стандартные project-файлы, которые зачастую нечитаемы. Если вы пишете библиотеку и хотите распространять её через SPM, обязательно убедитесь, что потенциальные потребители библиотеки не используют Cocoapods. Одновременная поддержка Cocoapods и SPM — часто непростая задача. import PackageDescription   let package = Package(     // Название нашего пакета     name: \"Resources\",     // Платформы, которые поддерживаются нашим пакетом     platforms: [         .iOS(.v11),     ],     // То, что будут использовать сторонние приложения     products: [         .library(             name: \"Resources\",             // Динамический или статический продукт             // по дефолту значение nil - SPM сам будет поним             // преференция, скорее всего, будет отдаваться .             type: .dynamic,             targets: [\"Resources\"]),     ],         // Зависимости, необходимые для работы нашего пакета   // здесь они просто загружаются, добавляются они в target     dependencies: [         // Название пакета, путь к нему и источник (ветка, в         .package(             name: \"R.swift.Library\",              url: \"https://github.com/mac-cain13/R.swift.Libr             branch: \"master\"),         .package(             name: \"SVGKit\",             url: \"https://github.com/SVGKit/SVGKit.git\", И как выбрать подходящий менеджер             .upToNextMajor(from: \"3.0.0\")),         .package(             name: \"PhoneNumberKit\",             url: \"https://github.com/marmelroy/PhoneNumberKi             from: \"3.3.4\"),         // Пример подключения локального пакета         .package(path: \"../Core\")     ],     targets: [         // Это то, из чего мы будем складывать наш продукт         .target(             name: \"Resources\",             dependencies: [                 // Здесь мы указываем зависимости, которые м                 .product(name: \"RswiftDynamic\", package: \"R.             ],             resources: [                 // Все ресурсы, которые мы хотим использоват                 // Путь к ним относительный от Sources/имя_п                 .process(\"Resources\")             ])     ] ) Универсального ответа нет — выбор менеджера зависимостей зависит исключительно от нужд и особенностей вашего проекта. SPM чаще всего не будут поддерживать старые библиотеки, в то время как Cocoapods вполне могут не поддерживать новые (хотя это и большая редкость, чем первое). Carthage в этом плане может показаться универсальнее. Хоть он более громоздкий и сложный во взаимодействии, но он единственный разделяет шаги управления зависимостями и добавления их в проект. Это даёт контроль над ситуацией и ускоряет разработку — не будет лишних пересборок или заморозки интерфейса, потому что нужно зарезолвить зависимости. Над материалом работали: • текст — Аня Кочешкова, Ника Черникова, • редактура — Виталик Балашов, • иллюстрации — Юля Ефимова. Делимся железной экспертизой от практик в нашем телеграм-канале red_mad_dev. А полезные видео складываем на одноимённом YouTube-канале. Присоединяйся! Теги:   red_mad_robot , ios , cocoapods , carthage , spm , swift package manager , зависимости , управление зависимостями Хабы:   Блог компании red_mad_robot , Разработка под iOS +1 3 1 red_mad_robot №1 в разработке цифровых решений для бизнеса 57 Карма 13 Рейтинг @redmadrobot Пользователь Комментарии 1 Публикации ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты ИНФОРМАЦИЯ Сайт redmadrobot.ru Дата регистрации 16 августа 2009 Дата основания 16 ноября 2008 Численность 1 001–5 000 человек Местоположение Россия Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "1",
         "Cloud4Y",
         null,
         "71.07",
         "Автор оригинала: Yashwant Singh",
         "#1 Корпоративный облачный провайдер",
         "71.07 Рейтинг Cloud4Y #1 Корпоративный облачный провайдер Автор оригинала: Yashwant Singh Взлом Instagram*‑аккаунта — популярный запрос в поисковиках. Поэтому есть смысл рассказать о том, как это обычно работает. Просто для того, чтобы вы знали, откуда может пойти атака.  Cloud4Y 5 часов назад OSINT & Hacking — как работает фишинг для нельзяграма  Простой  3 мин  1.1K Блог компании Cloud4Y , Информационная безопасность *, Социальные сети и сообщества Обзор  Перевод КАК СТАТЬ АВТОРОМ Чтобы начать попытки заполучить доступ к аккаунту, вы должны знать ник человека, которого вы пытаетесь взломать. Так что небольшая разведка будет очень кстати. Только не увлекайтесь. Существуют различные инструменты для разведки, в первую очередь, поиск пользователя в конкретной соцсети с целью узнать его ник. Я нашёл отличный инструмент под названием «Slash», который можно использовать для поиска любых учётных записей пользователя, если он везде регистрируется под одним ником. Ставим Slash git clone https://github.com/theahmadov/slash  cd slash  pip install -r requirements.txt  python slash.py help Я проверил Slash на себе, и посмотрите на эти результаты. Некоторые из учетных записей, перечисленных здесь, были созданы много лет назад. Slash — это простой консольный инструмент. Но вы также можете использовать такие инструменты, как WhatsMyName Web, который совершенно бесплатен. Вот, посмотрите. Я проверил WhatsMyName на себе. Мой ник «earthtoyash». Теперь, когда мы знаем больше о нашем пользователе, можно использовать эти знания. Например, через отправку фишинговых ссылок. Для этого создадим полезную нагрузку с помощью Zphisher. Ставим Zphisher с GitHub Клонируем репозиторий: git clone --depth=1 https://github.com/htr-tech/zphisher.git Запускаем файл zphisher.sh: cd zphisher && ./zphisher.sh При первом запуске он установит зависимости и на этом всё. Система скажет, что Zphisher установлен. После установки вам нужно будет снова запустить zphisher.sh в каталоге zphisher командой ./zphisher , и тогда вы получите что-то вроде этого: Как вы можете видеть, есть много вариантов и шаблонов, благодаря чему любой может заниматься фишингом. Мы сосредоточимся на нельзяграме. Итак, введите «2» и нажмите Enter. Следующий шаг полностью зависит от вас, выберите любой из них. Затем появится окно с выбором. Я выбрал третий вариант, так как он минималистичный и удобен для того, чтобы показать возможности инструмента. Опять же, чтобы все было просто, я пропущу пользовательский порт, но если вы уже используете порт 8080, то можете изменить его на 8000. Если нет, оставляйте всё как есть. Также важно маскировать URL, ну просто в целях безопасности. Можно использовать что‑то вроде этого: Всё, Zphisher создал фишинговую ссылку, которую можно отправить жертве. Как только она нажмёт на ссылку, вы начнёте получать информацию о ней. Например, IP‑адреса, имена пользователей, пароли и т. д. Ещё можно использовать обратный поиск IP, чтобы определить местоположение вашей цели и многое другое. Итак, вот эти фишинговые ссылки. При нажатии открывается страница, похожая на официальную страницу входа в запрещённую соцсеть. Вот она, нехорошая После ввода учётных данных можно получить много информации на «хакерской» стороне терминала. Вот так, господа и дамы, можно без особого труда взломать учетную запись в нельзяграме. Поэтому в очередной раз напоминаем: нельзя нажимать на ссылки, которым вы не доверяете. Само собой разумеется, не используйте информацию из этой статьи с намерением причинить кому-либо вред. OSINT законен, но фишинг и кража личных данных даже в запрещённой соцсети является уголовным преступлением. И да, * Организация Meta, а также её продукт Instagram, на которые мы ссылаемся в этой статье, признаны экстремистскими и запрещены на территории РФ. Спасибо за внимание! Cloud4Y #1 Корпоративный облачный провайдер Сайт Facebook Twitter ВКонтакте Telegram Что ещё интересного есть в блоге Cloud4Y → Информационная безопасность и глупость: необычные примеры → NAS за шапку сухарей → Взлом Hyundai Tucson, часть 1, часть 2 → Столетний язык программирования — какой он → 50 самых интересных клавиатур из частной коллекции Теги:  взлом, соцсети, osint Хабы:  Блог компании Cloud4Y, Информационная безопасность,  Социальные сети и сообщества  0  19  1 149 Карма 53.5 Рейтинг Cloud4Y @Cloud4Y Корпоративный облачный провайдер Сайт ВКонтакте Telegram  Комментарии 1 Публикации ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  И Н Ф О Р М А Ц И Я Сайт www.cloud4y.ru Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Дата регистрации 29 июля 2011 Дата основания 2009 Численность 51–100 человек Местоположение Россия Представитель Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "2",
         "Cloud4Y",
         null,
         "71.07",
         "Автор оригинала: Yashwant Singh",
         "#1 Корпоративный облачный провайдер",
         "71.07 Рейтинг Cloud4Y #1 Корпоративный облачный провайдер Автор оригинала: Yashwant Singh Взлом Instagram*аккаунта — популярный запрос в поисковиках. Поэтому есть смысл рассказать о том, как это обычно работает. Просто для того, чтобы вы знали, откуда может пойти атака.  5 часов назад Простой3 мин Cloud4Y OSINT & Hacking — как работает фишинг для нельзяграма 1.1K Блог компании Cloud4Y ,  Информационная безопасность* ,  Социальные сети и сообщест Обзор Перевод КАК СТАТЬ АВТОРОМ Чтобы начать попытки заполучить доступ к аккаунту, вы должны знать ник человека, которого вы пытаетесь взломать. Так что небольшая разведка будет очень кстати. Только не увлекайтесь. Существуют различные инструменты для разведки, в первую очередь, поиск пользователя в конкретной соцсети с целью узнать его ник. Я нашёл отличный инструмент под названием «Slash», который можно использовать для поиска любых учётных записей пользователя, если он везде регистрируется под одним ником. Ставим Slash Я проверил Slash на себе, и посмотрите на эти результаты. Некоторые из учетных записей, перечисленных здесь, были созданы много лет назад. git clone https://github.com/theahmadov/slash  cd slash  pip install -r requirements.txt  python slash.py help Slash — это простой консольный инструмент. Но вы также можете использовать такие инструменты, как WhatsMyName Web, который совершенно бесплатен. Вот, посмотрите. Я проверил WhatsMyName на себе. Мой ник «earthtoyash». Теперь, когда мы знаем больше о нашем пользователе, можно использовать эти знания. Например, через отправку фишинговых ссылок. Для этого создадим полезную нагрузку с помощью Zphisher. Ставим Zphisher с GitHub Клонируем репозиторий: Запускаем файл zphisher.sh: При первом запуске он установит зависимости и на этом всё. Система скажет, что Zphisher установлен. После установки вам нужно будет снова запустить zphisher.sh в каталоге zphisher командой ./zphisher , и тогда вы получите что-то вроде этого: git clone --depth=1 https://github.com/htr-tech/zphisher.git cd zphisher && ./zphisher.sh Как вы можете видеть, есть много вариантов и шаблонов, благодаря чему любой может заниматься фишингом. Мы сосредоточимся на нельзяграме. Итак, введите «2» и нажмите Enter. Следующий шаг полностью зависит от вас, выберите любой из них. Затем появится окно с выбором. Я выбрал третий вариант, так как он минималистичный и удобен для того, чтобы показать возможности инструмента. Опять же, чтобы все было просто, я пропущу пользовательский порт, но если вы уже используете порт 8080, то можете изменить его на 8000. Если нет, оставляйте всё как есть. Также важно маскировать URL, ну просто в целях безопасности. Можно использовать чтото вроде этого: Всё, Zphisher создал фишинговую ссылку, которую можно отправить жертве. Как только она нажмёт на ссылку, вы начнёте получать информацию о ней. Например, IPадреса, имена пользователей, пароли и т. д. Ещё можно использовать обратный поиск IP, чтобы определить местоположение вашей цели и многое другое. Итак, вот эти фишинговые ссылки. При нажатии открывается страница, похожая на официальную страницу входа в запрещённую соцсеть. Вот она, нехорошая После ввода учётных данных можно получить много информации на «хакерской» стороне терминала. Вот так, господа и дамы, можно без особого труда взломать учетную запись в нельзяграме. Поэтому в очередной раз напоминаем: нельзя нажимать на ссылки, которым вы не доверяете. Само собой разумеется, не используйте информацию из этой статьи с намерением причинить кому-либо вред. OSINT законен, но фишинг и кража личных данных даже в запрещённой соцсети является уголовным преступлением. И да, * Организация Meta, а также её продукт Instagram, на которые мы ссылаемся в этой статье, признаны экстремистскими и запрещены на территории РФ. Спасибо за внимание! Что ещё интересного есть в блоге Cloud4Y → Информационная безопасность и глупость: необычные примеры → NAS за шапку сухарей → Взлом Hyundai Tucson, часть 1, часть 2 → Столетний язык программирования — какой он → 50 самых интересных клавиатур из частной коллекции Теги:   взлом , соцсети , osint Хабы:   Блог компании Cloud4Y , Информационная безопасность , Социальные сети и сообщества 0 19 1 Cloud4Y #1 Корпоративный облачный провайдер Сайт Facebook Twitter ВКонтакте Telegram 149 Карма 53.5 Рейтинг Cloud4Y @Cloud4Y Корпоративный облачный провайдер Сайт ВКонтакте Telegram Комментарии 1 Публикации ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты ИНФОРМАЦИЯ Сайт www.cloud4y.ru Дата регистрации 29 июля 2011 Дата основания 2009 Численность 51–100 человек Местоположение Россия Представитель Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "3",
         "Southbridge",
         null,
         "280.79",
         "Автор оригинала: Seifeldin Mahjoub",
         "Обеспечиваем стабильную работу highload-проектов",
         "4.29 Оценка 280.79 Рейтинг Southbridge Обеспечиваем стабильную работу highload-проектов Автор оригинала: Seifeldin Mahjoub Перевели статью о создании пайплайна для развертывания статического веб-сайта на AWS S3 Bucket на примере Gitlab CI/CD, чтобы быстро вникнуть в основы технологии и начать применять ее в работе. В статье рассматриваются базовые концепции CI и CD, а также этапы CI/CD-пайплайна.  2 часа назад 5 мин zubarek Быстрое начало работы с Gitlab CI/CD: пайплайн для веб-сайта на AWS S3 Bucket 362 Блог компании Southbridge ,  Тестирование IT-систем* ,  Системное администрирование* ,  Туториал Перевод КАК СТАТЬ АВТОРОМ От автора Мне повезло быть частью некоторых профессиональных команд, каждая из которых применяла несколько DevOps практик. И меня поразило то, как качество кода, скорость разработки и позитивный настрой команды коррелируют с CI/CD-пайплайном. По моему мнению, зрелость пайплайна может служить прекрасным показателем опытности разработчика, качества кода и эффективности всей команды. Во многих случаях, которые я наблюдал, пайплайны были выстроены либо DevOps-инженером, либо отдельной DevOps- командой. Да и последний отчет State of CD 2022 продемонстрировал, что только 22% разработчиков создают пайплайны. Моя цель — увеличить это число: помочь разработчикам взять на себя ответственность за пайплайны, выстраивать непрерывный процесс доставки и создавать качественный код. В статье рассматриваются фундаментальные концепции CI и CD. Что такое CI/CD? Многие бизнесы применяют фреймворки Agile, так как они позволяют менять приоритеты и повышать скорость доставки. Кроме всего прочего, такой подход улучшает атмосферу в команде и помогает увеличить прибыль. Если ваша компания следует по пути Agile, то принятие культуры, философии и практик DevOps станет ее большим преимуществом. Модное словечко последних десятилетий, DevOps сегодня считается настоящим стандартом индустрии. CI/CD — это практика DevOps, которая помогает разработчикам ПО доставлять изменения в коде с высокой частотой и надежностью. «Быстрый билд, быстрый тест, быстрый фейл» При наличии автоматизированных тестов команды тяготеют к общей автоматизации задач и частым, надежным поставкам кода. Создание CI/CD-пайплайна в этом случае может привести к нескольким преимуществам. Бизнес выигрывает от снижения затрат и повышения производительности, ускорения Time to Market и адаптации к изменяющимся требованиям рынка. Команда выигрывает от быстрой обратной связи, улучшения эффективности разработки, уменьшения количества бутылочных горлышек и повышения уровня вовлеченности и удовлетворенности сотрудников. Фазы CI и CD CI — непрерывная интеграция. Непрерывная интеграция позволяет по много раз в день коммитить изменения в основную ветку вашей кодовой базы. Учитывая ограниченные когнитивные способности человека, CI стимулирует разработчиков вносить в код небольшие изменения, которые легче рассмотреть, покрыть автоматическими тестами и часто релизить. Это позволяет избежать напряженных и переполненных merge conflict-ами дней подготовки к релизу с тоннами ручного тестирования. CD — непрерывная доставка. Следующий шаг после CI позволяет гарантировать, что кодовая база постоянно готова к деплою, а задеплоить ее можно одним нажатием кнопки. При этом неважно, с чем вы работаете: с масштабной распределенной системой, сложной производственной средой и т. д. Ключевой момент — автоматизация. CD — непрерывное развертывание. Последний этап зрелого CI/CD-пайплайна, где все изменения в коде автоматически развертываются в продакшн без ручного вмешательства. Само собой, для этого требуется большое количество хорошо продуманных автоматических тестов. State of CD 2022 утверждает, что «47% разработчиков применяют CI или СD, но только один из пяти использует оба подхода для автоматизации сборки, тестирования и развертывания кода». Книга Accelerate подводит итоги многолетнего исследования с использованием отчетов State of DevOps, основанных на 23 000 наборов данных компаний по всему миру. Как видите, высокопроизводительные команды могут деплоить по требованию (или несколько раз в день). Этапы CI/CD-пайплайна Стадия исходного кода — здесь запускается пайплайн. Обычно это происходит после изменений в Git-репозитории, которые проявляются в открытии нового Pull Request-а или в пуше в ветку. Другой способ заключается в настройке инструментария CI/CD для запуска пайплайна через автоматическое расписание или после запуска другого пайплайна. Стадия сборки — этап, в процессе которого происходит проверка и сборка кода. Здесь особенно полезны такие инструменты, как Docker: они обеспечивают однородную среду. Стадия тестирования — CI/CD невозможно представить без автоматизированных тестов. В конце концов, все хотят быть уверены, что изменения в коде не сломают продакшн. Стадия развертывания — на последнем этапе (после успешного прохождения всех предыдущих стадий) код можно развернуть в выбранной среде. Пример с Gitlab В этом примере будет использован Gitlab CI/CD, однако концепции аналогичны и для остальных инструментов, поэтому их можно применить к другим сервисам хостинга репозиториев. Существует несколько инструментов CI/CD, например всемирно известный Jenkins. Этот инструмент требует некоторой настройки и конфигурации, в то время как другие поставляются сервисами хостинга репозиториев (такими как GitHub Actions и Bitbucket Pipelines) с предварительной настройкой. Поэтому если ваш код размещен на Gitlab, то легче всего использовать Gitlab CI/CD, поскольку код и управление CI/CD находятся на одной платформе. Как все это может работать без настроек? Для ответа на этот вопрос стоит немного погрузиться в архитектуру Gitlab, а именно — в инстансы и раннеры. Инстансы хранят код приложения и конфигурации пайплайна. Раннеры выступают в качестве агентов, выполняющих операции в пайплайнах. В Gitlab каждый инстанс может быть подключен к одному или нескольким раннерам. Gitlab.com — это управляемый инстанс с несколькими раннерами, которые сам Gitlab и поддерживает. Следовательно, если вы используете этот инстанс, то получаете все необходимое из коробки. Приступим к работе Gitlab предлагает несколько шаблонов при создании нового проекта. Конфигурация пайплайна Gitlab CI/CD по умолчанию находится в файле .gitlab-ci.yml в корневом каталоге. Предположим, мы хотим создать простой пайплайн, который проверяет: написан, протестирован и развернут ли код. Вот несколько концепций и терминов для ознакомления перед началом работы. Пайплайн (Pipeline) Пайплайн — это набор заданий, разделенных на этапы. Gitlab предлагает различные типы пайплайнов, например parent-child или multi-project. Полный список см. здесь. Этап (Stage) Этап — это шаг в пайплайне, предоставляющий информацию о том, какие задания запускать (сборка, тестирование и т. д.). Один этап может включать одно или несколько заданий. Задание (Job) Задание — основной блок пайплайна (компиляция, линтинг и т. д.). Для каждого задания должны быть определены name и script. После выполнения всех заданий на этапе пайплайн переходит к следующему. Теперь — к коду Выстраиваем пайплайн Gitlab CI/CD, который собирает, тестирует и разворачивает статический веб-сайт в AWS S3 Bucket. Для начала создадим новый .gitlab-ci.yml 1. Определим переменные 2. Определим этапы 3. Определим задания на каждом этапе variables: # variabiles definitions for easier reuse of valu  CI_NODE_IMAGE: \"node:16.13.2\" # Pipeline stages stages:  - install  - build  - test  - deploy #install job definition install:  stage: install  image: \"$CI_NODE_IMAGE\" # variable reference  script: # Shell script that is executed by the runner.    - npm ci  cache: # List of files that should be cached between subseq    key:      files:        - package.json        - package-lock.json    paths: # directories to cache      - node_modules # Build Job definition На этом все, спасибо за внимание. Научиться работать с пайплайнами, билдами и артефактами можно на курсе Gitlab CI/CD в Слёрм. Вы узнаете, из чего состоит Gitlab и какие у него возможности и настройки, а также разберете лучшие практики построения пайплайна, особенности шаблонизации и работы с переменными. build:  stage: build  image: $CI_NODE_IMAGE  script:    - npm run build  artifacts: # list of files and directories that are attache    paths:      - dist/  cache:    key:      files:        - package.json        - package-lock.json    paths:      - node_modules    policy: pull # Test Job definition test:  stage: test  image: $CI_NODE_IMAGE  script:    - npm run test # Deploy Job definition deploy:  stage: deploy  image: registry.gitlab.com/gitlab-org/cloud-deploy/aws-base  script:    - aws s3 cp --recursive dist s3://bucket-name # copies th Southbridge Обеспечиваем стабильную работу highload-проектов Сайт Сайт 14 Карма 16.9 Рейтинг Лиза Зубарькова @zubarek Пользователь Комментировать Публикации Видеокурс доступен всегда. Посмотреть программу: https://slurm.club/3JUKdzT Теги:   ci/cd , gitlab-ci , aws , gitlab , pipeline Хабы:   Блог компании Southbridge , Тестирование IT-систем , Системное администрирование , Программирование , DevOps +7 14 0 ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ Ваш аккаунт Разделы Информация Услуги  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИНФОРМАЦИЯ Сайт southbridge.io Дата регистрации 15 ноября 2012 Дата основания 22 февраля 2008 Численность 51–100 человек Местоположение Россия Представитель Антон Скобин Войти Регистрация Публикации Новости Хабы Компании Авторы Песочница Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "4",
         "Альфа-Банк",
         null,
         "404.32",
         "Подписаться",
         "Лучший мобильный банк по версии Markswebb",
         "404.32 Рейтинг Альфа-Банк Лучший мобильный банк по версии Markswebb Подписаться В Альфа-Банке мы внедрили ИИ-агентов, которые проектируют, разрабатывают и проверяют автотесты. При этом полностью автономно, как QA-инженеры, но в разы быстрее и точнее. Подобных примеров, когда ИИ разрабатывает автотесты от анализа требований до пул-реквеста, в нашей стране, пожалуй, ещё не было. Что умеют агенты? Анализировать контекст из Jira и Confluence, вычленяя суть задачи. Прогнозировать риски, зависимости и даже «пограничные» сценарии. Генерировать DTO для REST API и превращать ручные сценарии в Java-тесты за минуты. Сверять код с бизнес-логикой и техстандартами Альфы, защищая прод от случайных ошибок. Создавать вариативные проверки — от позитивных кейсов до сложных негативных условий. Автоматизировать рутину. В ИИ-команде QA есть несколько агентов, каждый работает над своей частью из перечная выше. Сейчас решение пилотируется в нескольких продуктовых командах, но результаты уже видны: AlfaTeam 2 часа назад ИИ-агенты в Альфа-Банке: нейросети создают автотесты без участия человека 1 мин 717 Блог компании Альфа-Банк, Искусственный интеллект, Тестирование IT-систем* Войти Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп КАК СТАТЬ АВТОРОМ Зарплаты айтишников Миграция за ночь? Легко! Без сна и с адре… Альфа-Банк Лучший мобильный банк по версии Markswebb Сайт Хабр Карьера 14 Карма 0.3 Рейтинг @AlfaTeam Пользователь Сайт Facebook ВКонтакте Другие новости mefdayy 19 минут назад Nvidia и Broadcom тестируют передовой процесс производства чипов Intel 2 мин меньше ошибок в проде, предсказуемые дедлайны и высвобожденные ресурсы для творческих задач. «Одна команда ИИ-агентов экономит десятки часов работы, увеличивает скорость релизов и находит на 30% больше багов» Святослав Соловьев, Директор по генеративному ИИ в ИТ Альфа-Банка. В ближайшее время подробнее опишем как устроены агенты, какие технологии используем и как мы измеряем их эффективность. Теги: автоматизация, искусственный интеллект, тестирование, автотесты Хабы: Блог компании Альфа-Банк, Искусственный интеллект, Тестирование IT-систем Редакторский дайджест Присылаем лучшие статьи раз в месяц Электропочта Комментарии 11 123 Подписаться "
        ],
        [
         "5",
         "RUVDS.com",
         null,
         "2376.9",
         "Автор оригинала: Lahiru Hewawasam",
         "VDS/VPS-хостинг. Скидка 15% по коду HABR15",
         "2376.9 Рейтинг RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Автор оригинала: Lahiru Hewawasam  4 часа назад Простой9 мин ru_vds Инструменты наблюдаемости, о которых нужно знать в 2023 году 752 Блог компании RUVDS.com ,  Open source* ,  Хранение данных* ,  Облачные сервисы* ,  Мик Перевод КАК СТАТЬ АВТОРОМ Когда организации переходят в облако, их системы тоже начинают стремиться к распределённым архитектурам. Один из самых распространённых примеров этого — использование микросервисов. Однако это также создаёт новые сложности с точки зрения наблюдаемости. Необходимо подбирать подходящие инструменты для мониторинга, отслеживания и трассировки этих систем при помощи анализа выходных результатов посредством метрик, логов и трассировок. Это позволяет командам разработчиков быстро выявлять первопричины проблем, устранять их и оптимизировать производительность приложений, ускоряя выпуск кода. В этой статье мы рассмотрим возможности, ограничения и важные особенности одиннадцати популярных инструментов наблюдаемости, что позволит вам выбрать наиболее подходящий для вашего проекта. Helios Helios — это решение по обеспечению наблюдаемости для разработчиков, предоставляющее информацию по всему потоку приложений. Оно включает в себя фреймворк распространения контекста OpenTelemetry и обеспечивает наблюдение за микросервисами, serverless-функциями, базами данных и сторонними API. Можно протестировать песочницу продукта или использовать его бесплатно, зарегистрировавшись здесь. ▍ Основные возможности • Обеспечение полного контроля: Helios предоставляет информацию о распределённой трассировке в полном контексте, показывает, как передаются данные через всё приложение в любом окружении. • Визуализация: позволяет пользователям собирать и визуализировать данные трассировок из множественных источников данных, чтобы исследовать и устранять потенциальные проблемы. • Многоязыковая поддержка: поддерживает множество языков и фреймворков, в том числе Python, JavaScript, Node.js, Java, Ruby, .NET, Go, C++ и Collector. • Обмен и многократное использование: вы с лёгкостью можете сотрудничать с участниками команды, обмениваясь через Helios трассировками, тестами и триггерами. Кроме того, Helios позволяет многократно использовать запросы и полезные нагрузки между участниками команды. • Автоматическая генерация тестов: автоматически генерирует тесты на основании данных трассировок. • Простота интеграций: интегрируется в существующую экосистему, включая логи, тесты, мониторинг ошибок и многое другое. • Воссоздание процессов: Helios всего за несколько кликов позволяет в точности воссоздавать рабочие процессы, в том числе HTTP-запросы, сообщения Kafka и RabbitMQ, а также вызовы Lambda. ▍ Популярные способы использования • Распределённая трассировка • Интеграция трассировок в многоязыковое приложение • Наблюдаемость serverless-приложения • Устранение неполадок в тестах • Информация о вызовах API • Анализ и выявление узких мест Prometheus Prometheus — это опенсорсный инструмент, широко используемый для обеспечения наблюдаемости в нативных облачных окружениях. Он может собирать и хранить данные временных последовательностей и предоставляет инструменты визуализации для анализа собранных данных. ▍ Основные возможности • Сбор данных: он может скрейпить метрики из различных источников, в том числе из приложений, сервисов и систем. Также он «из коробки» поддерживает множество форматов данных, в том числе логи, трассировки и метрики. • Хранилище данных: он сохраняет собранные данные в базе данных временных последовательностей, позволяя эффективно запрашивать и агрегировать данные с течением времени. • Система алертов: инструмент включает в себя встроенную систему алертов, которая может запускать алерты на основании запросов. • Исследование сервисов: он может автоматически распознавать и скрейпить метрики сервисов, работающих в различных окружениях, например, Kubernetes и в других системах управления контейнерами. • Интеграция с Grafana: инструмент имеет гибкую интеграцию с Grafana, позволяющей создавать дэшборды для отображения и анализа метрик Prometheus. ▍ Ограничения • Ограниченные возможности анализа первопричин: инструмент в первую очередь предназначен для мониторинга и алертов. Поэтому он не предоставляет встроенных возможностей аналитики первопричин проблем. • Масштабирование: хотя инструмент может обрабатывать множество метрик, это может привести к большой трате ресурсов, поскольку Prometheus хранит все данные в памяти. • Моделирование данных: содержит модель данных на основе пар «ключ-значение» и не поддерживает вложенных полей и join. ▍ Популярные способы применения • Сбор и хранение метрик • Система алертов • Исследование сервисов Grafana Grafana — это опенсорсный инструмент, в первую очередь используемый для визуализации и мониторинга данных. Он позволяет с лёгкостью создавать интерактивные дэшборды для визуализации и анализа данных из различных источников. ▍ Основные возможности • Визуализация данных: создаёт настраиваемые и интерактивные дэшборды для визуализации метрик и логов из различных источников данных. • Система алертов: позволяет настраивать алерты на основании состония метрик для информирования о потенциальных проблемах. • Выявление аномалий: позволяет настроить выявление аномалий для автоматического определения и отправки алертов в случае аномального поведения в метриках. • Анализ первопричин: позволяет углубиться в метрики для анализа первопричин, предоставляя подробную информацию с историческим контекстом. ▍ Ограничения • Хранение данных: архитектура инструмента не поддерживает долговременное хранение и для сохранения метрик и логов требует дополнительных инструментов наподобие Prometheus или Elasticsearch. • Моделирование данных: Grafana не предоставляет расширенных возможностей моделирования данных. То есть она не позволяет моделировать конкретные типы данных и выполнять сложные запросы. • Агрегирование данных: Grafana не содержит встроенных функций агрегирования. ▍ Популярные способы применения • Визуализация метрик • Система алертов • Выявление аномалий Elasticsearch, Logstash и Kibana (ELK) Стек ELK — это популярное опенсорсное решение, помогающее управлять логами и анализировать данные. Оно состоит из трёх компонентов: Elasticsearch, Logstash и Kibana. Elasticsearch — это движок распределённого поиска и аналитики, способный обрабатывать большие объёмы структурированных и неструктурированных данных; он позволяет хранить и индексировать большие массивы данных, а также выполнять поиск по ним. Logstash — это конвейер сбора и обработки данных, позволяющий собирать, обрабатывать и обогащать данные из множества источников, например, файлов логов. Kibana — это инструмент визуализации и исследования данных, позволяющий создавать интерактивные дэшборды и визуализации на основе данных, находящихся в Elasticsearch. ▍ Основные возможности • Управление логами: ELK позволяет собирать, обрабатывать, хранить и анализировать данные логов и метрики из множества источников, предоставляя централизованную консоль для поиска по логам. • Поиск и анализ: позволяет выполнять поиск и анализ релевантных данных логов, что критически важно для выявления и устранения первопричин проблем. • Визуализация данных: Kibana позволяет создавать настраиваемые дэшборды, которые способны визуализировать данные логов и метрики из множества источников данных. • Выявление аномалий: Kibana позволяет создавать алерты для аномальной активности в данных логов. • Анализ первопричин: стек ELK позволяет глубоко изучать данные логов, чтобы лучше понять первопричины, предоставляя подробные логи и исторический контекст. ▍ Ограничения • Трассировка: ELK нативно не поддерживает распределённую трассировку. Поэтому может понадобиться применение дополнительных инструментов наподобие Jaeger. • Мониторинг в реальном времени: архитектура ELK позволяет ему качественно выполнять задачи платформы управления логами и анализа данных. Однако в отчётности логов присутствует незначительная задержка, и пользователям приходится ждать. • Сложная настройка и поддержка: для платформы требуется сложный процесс настройки и поддержки. Кроме того, для управления большими объёмами данных и множественными источниками данных требуются специфические знания. ▍ Популярные способы применения • Управление логами • Визуализация данных • Комплаенс и безопасность InfluxDB и Telegraf InfluxDB и Telegraf — это опенсорсные инструменты, популярные благодаря своим возможностям по хранению и мониторингу данных временных последовательностей. InfluxDB — это база данных временных последовательностей, хранящая большие объёмы данных временных последовательностей и выполняющая запросы к ним при помощи своего языка запросов, напоминающего SQL. Telegraf — это хорошо известный агент сбора данных, способный собирать и отправлять метрики широкому выбору получателей, например, InfluxDB. Также он поддерживает многие источники данных. ▍ Основные возможности Комбинация из InfluxDB и Telegraf предоставляет множество возможностей, повышающих наблюдаемость приложений. • Сбор и хранение метрик: Telegraf позволяет собирать метрики из множества источников и отправлять их в InfluxDB для хранения и анализа. • Визуализация данных: InfluxDB можно интегрировать со сторонними инструментами визуализации наподобие Grafana для создания интерактивных дэшбордов. • Масштабируемость: архитектура InfluxDB позволяет обрабатывать большие объёмы данных временных последовательностей и выполнять горизонтальное масштабирование. • Поддержка множества источников данных: Telegraf поддерживает более двухсот плагинов ввода для сбора метрик. ▍ Ограничения • Ограниченные возможности алертинга: в обоих инструментах отсутствуют возможности алертинга и для его подключения необходима интеграция сторонних сервисов. • Ограниченный анализ первопроичин: в этих инструментах отсутствуют нативные возможности анализа первопричин и необходима интеграция сторонних сервисов. ▍ Популярные способы применения • Сбор и хранение метрик • Мониторинг Datadog Datadog — это популярная облачная платформа для мониторинга и аналитики. Она широко используется для получения информации о здоровье и производительности распределённых систем с целью заблаговременного устранения проблем. ▍ Основные возможности • Многооблачная поддержка: пользователи могут выполнять мониторинг приложений, работающих на облачных платформах нескольких поставщиков, например, AWS, Azure, GCP и так далее. • Карты сервисов: позволяют выполнять визуализацию зависимостей сервисов, местоположений, сервисов и контейнеров. • Аналитика трассировок: пользователи могут анализировать трассировки, предоставляя подробную информацию о производительности приложений. • Анализ первопричин: позволяет глубоко изучать метрики и трассировки, чтобы понять первопричину проблем, предоставляя подробную информацию с историческим контекстом. • Выявление аномалий: может настраивать систему выявления аномалий, которая автоматически выявляет аномальное поведение в метриках и создаёт алерты о нём. ▍ Ограничения • Затраты: Datadog — это облачный платный сервис, стоимость которого увеличивается при развёртывании крупномасштабных систем. • Ограниченная поддержка потребления, хранения и индексации логов: Datadog по умолчанию не предоставляет поддержку анализа логов. Необходимо отдельно приобретать поддержку потребления и индексации логов. Поэтому большинство организаций принимает решение хранить ограниченное количество логов, что может вызвать неудобства при устранении проблем, поскольку отсутствует доступ к полной истории проблемы. • Нехватка контроля за хранением данных: Datadog хранит данные на собственных серверах и не позволяет пользователям хранить данные локально или в дата-центрах компании. ▍ Популярные способы применения • Конвейеры наблюдаемости • Распределённая трассировка • Мониторинг контейнеров New Relic New Relic — это облачная платформа мониторинга и аналитики, позволяющая выполнять мониторинг приложений и систем в распределённом окружении. Она использует сервис «New Relic Edge» для распределённой трассировки и способна выполнять наблюдение за 100% трассировок приложения. ▍ Основные возможности • Мониторинг производительности приложений: предоставляет комплексное решение APM для мониторинга производительности приложений и устранения проблем. • Многооблачная поддержка: поддерживает мониторинг приложений на облачных платформах нескольких поставщиков, например, AWS, Azure, GCP и так далее. • Аналитика трассировок: позволяет анализировать трассировки, предоставляя подробную информацию о производительности системы и приложений. • Анализ первопричин: позволяет глубоко изучать матрики и трассировки для анализа первопричин проблем. • Управление логами: собирает, обрабатывает и анализирует данные логов из различных источников, обеспечивая всеобъемлющую картину логов. ▍ Ограничения • Ограниченная опенсорсная интеграция: New Relic — это платформа с закрытыми исходниками, поэтому её интеграция с опенсорсными инструментами может быть ограниченной. • Затраты: New Relic может быть более дорогим по сравнению с другими решениями при работе с крупномасштабными системами. ▍ Популярные способы применения • Мониторинг производительности приложений • Многооблачный мониторинг • Аналитика трассировок AppDynamics AppDynamics — это платформа мониторинга и аналитики, позволяющая отслеживать и визуализировать каждый компонент приложения, а также управлять ими. Кроме того, она позволяет выполнять анализ первопричин для выявления внутренних проблем, которые могут влиять на производительность приложения. ▍ Основные возможности • Сбор данных: пользователи могут собирать метрики и трассировки из множества источников: хостов, контейнеров, облачных сервисов и приложений. • Выявление аномалий: позволяет настраивать систему выявления аномалий, которая выявляет аномальное поведение и сообщает о нём при помощи алертов. • Аналитика трассировок: пользователи могут анализировать трассировки и получать подробную информацию о производительности. Мониторинг производительности приложений: предоставляет комплексное решение APM, позволяющее выполнять мониторинг и устранение проблем производительности приложения. • Ограниченные возможности настройки: по сравнению с другими инструментами, опции настройки не очень гибки, потому что пользователи не могут настраивать решение самостоятельно. ▍ Популярные способы применения • Мониторинг производительности приложений • Многооблачный мониторинг • Управление бизнес-транзакциями Выбор лучшего инструмента наблюдаемости Наблюдаемость — неотъемлемая часть разработки и эксплуатации современного ПО. Она помогает компаниям выполнять мониторинг здоровья и производительности систем и быстро решать проблемы ещё до того, как они станут критичными. В этой статье мы рассказали об одиннадцати лучших инструментах наблюдаемости, о которых должны знать разработчики при работе с распределёнными системами. Как видите, каждый инструмент имеет свои сильные стороны и ограничения. Поэтому чтобы найти подходящий для вас инструмент, следует сравнить его с требованиями вашей системы. Выбор наилучшего инструмента наблюдаемости для вашей организации зависит от конкретных потребностей, таких, как окружение, технологический стек, опыт разработчиков, профили пользователей, требования к мониторингу и устранению проблем, а также рабочие процессы. Надеюсь, эта статья была для вас полезной. Telegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх  ️ RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 327 Карма 576 Рейтинг @ru_vds Пользователь Комментарии 3 Публикации Теги:   ruvds_перевод , наблюдаемость , observability , graphana , prometheus , helios , логи , метрики , трассировка Хабы:   Блог компании RUVDS.com , Open source , Хранение данных , Облачные сервисы , Микросервисы +9 21 3 ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ Ваш аккаунт Разделы Информация Услуги  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИНФОРМАЦИЯ Сайт ruvds.com Дата регистрации 18 марта 2016 Дата основания 27 июля 2015 Численность 11–30 человек Местоположение Россия Представитель ruvds Войти Регистрация Публикации Новости Хабы Компании Авторы Песочница Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "6",
         "Хабр",
         null,
         "353.18",
         "8 февраля — день российской науки. Этот праздник отмечается в Академии Наук, в НИИ,",
         "Экосистема для развития людей, вовлеченных в IT",
         "4.58 Оценка 353.18 Рейтинг Хабр Экосистема для развития людей, вовлеченных в IT 8 февраля — день российской науки. Этот праздник отмечается в Академии Наук, в НИИ, вузах, исследовательских лабораториях по всей стране. Судьба российской науки непростая, всегда переплетённая с историей страны и ею же обусловленная: непростой путь к открытию МГУ, талантливые кулибины (и И.Кулибин) из глубинки, учёные в изгнании, Туполевская шарага, достижения института Гамалеи… Всё смешано, переплетено и влияло и влияет на весь мир. Величие российской науки прежде всего в людях, которые её создают, которые разрабатывают, изобретают, экспериментирую и точно знают, что делают.  В этот день всегда спорят медики, биологи, инженеры, программисты, филологи, физики, химики — чьи учёные научнее и главнее, кто определял и создавал будущее? Ответ простой: все, каждый — в своей сфере. Предлагаем сегодня собрать российские  открытия и учёных, которые вас вдохновляют, впечатляют, будоражат воображение и влияют (или повлияли) на вас. Напишем эту статью вместе? Exosphere 10 часов назад История российской науки: напишем вместе? Простой 5 мин 1.3K Блог компании Хабр, Научно-популярное Ретроспектива КАК СТАТЬ АВТОРОМ Технотекст Добавь свой раздел в статью про российскую науку Войти Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп Правила создания статьи Вы в комментариях рассказываете о том самом учёном и его изобретении, которые важны именно для вас, а мы добавляем ваши истории в общую статью и формируем самый крутой лонгрид об истории российской науки. Формат свободный, желательно (не обязательно!) отразить: ФИО учёного, его годы жизни основные изобретения краткая биография наиболее значимое изобретение (на ваш взгляд) чем вас вдохновил учёный, как повлиял на вас изображения приветствуются. Комментарии будут сохраняться и вы сможете их обсуждать — так, как и всегда. Автор самого заплюсованного описания получит приятный сюрприз от команды Хабра. Чур, википедию не копировать! Итак, поехали.  Николай Иванович Пирогов (1810 — 1881) великий российский врач, хирург Николай Пирогов в 14 лет поступил на первый курс медицинского факультета МГУ (тогда Московского университета). По окончании вуза он в числе семи лучших студентов отправился в Депт (Тарту) заниматься анатомическими исследованиями и готовиться к профессорской деятельности. Считается одним из лучших анатомов мира, родоначальник топографической анатомии. Именно он разрезал замороженное человеческое тело в четырёх плоскостях и создал атлас топографической анатомии, который стал незаменимым пособием для хирургов. Это был не просто учебный материал, но прежде всего возможность оперировать с минимальным травматизмом для пациента. Пирогов брался за любые операции, от ампутаций и извлечения камней до глазных манипуляций и сосудистых вмешательств. Именно он провёл в 1847 году первую свою операцию под наркозом, тщательно изучив свойства эфира и хлороформа. Затем в течение года прошло 300 подобных операций. Николай Пирогов — автор алгоритма сортировки раненных на поле брани, благодаря чему удалось спасти огромное количество жизней, а сам алгоритм до сих пор служит организации военной медицины. Именно он ввёл в практику российской медицины обеззараживание хлорной водой и настойкой йода. До сих пор хлорка, йодопирон, йодинол и йод служат хирургам и всем медикам верную службу. Николай Пирогов был первым учёным от медицины, о котором я узнала в возрасте 7 или 8 лет, уже будучи немного горящей «стать доктором». Жизнь повернулась самым причудливым образом, но изучение истории медицины сделало моё мышление структурным, комплексным и навсегда привило любовь к людям. Какие бы они ни были, они — люди, с болью, со стрессом, с заболеваниями и радостью выздоровления. Совершенно особенное ощущение. Художественный фильм «Пирогов» Гааз и Пирогов — лекторий «Достоевский». Хотя конкретно в этом фильме история Гааза потрясает гораздо больше. Семён Николаевич Корсаков (1787-1853) создатель первой в мире перфокарточной машины (табулятора) «для сравнения идей» Рассказывает @Motanto Его история меня когда-то поразила: только подумайте, кто-то что-то делал в программировании до Бэббиджа, да ещё и в Российской Империи. Ещё больше поразил тот факт, что Корсаков был обвинён современниками в пустой трате времени на изобретения, а его славные дела дошли до мирового научного сообщества и до нас с вами благодаря \"раскопкам\" учёных XX века, в частности благодаря математику и кибернетику Г.Н. Поварову. Потомственный дворянин, Семён Корсаков стал прародителем отечественной кибернетики. Он всегда видел своей задачей усиление работы разума с помочью различных механических устройств (чем вам не первобытное программирование?!). Кроме изобретённых им гомеоскопов и идеоскопа, он подарил миру идею перфокарт. Именно гомеоскопические перфорированные таблицы стали прототипом тех самых картонных карт. Устройства Корсакова помогали сравнивать таблицы и обрабатывать большие массивы данных, с помощью его устройств можно было решать задачи классификации. Корсаков видел бытовое применение своим изобретениям (реестры, каталоги, регистрации), ничего не патентовал и просто представил машины общественности. С.Н.Корсаков и машина для сравнения идей Однако комиссия из пяти ученых, возглавляемая молодым математиком академиком М. В. Остроградским, вынесла вердикт: «Господин Корсаков потратил чересчур много своих интеллектуальных сил, пытаясь научить других, как вовсе без оного интеллекта прожить». Кажется, это лучшее определение ИТ-сферы целиком ;-) Такая вот биг дата доэлектрической эры. Павел Петрович Аносов (10 июля 1796 — 25 мая 1851) горный инженер, учёный-металлург Рассказывает @Mimizavr Талантами Русь богата И вот, старики говорят, Что сталь дамасских булатов Рассек уральский булат. Павел Петрович Аносов - на мой взгляд, совершенно фантастическая фигура, благодаря которой в 19 веке очень сильно развилась металлургия на Урале. Будучи горным инженером по специальности, он занимался научной работой в сфере металлургии, стал крупным организатором горнозаводской промышленности на Урале, исследовал природу Южного Урала, вел геологическую разведку месторождений россыпного золота и железных руд, изобрел ряд устройств и механизмов (в том числе и эффективную золотопромывную машину), разработал технологию производства огнеупорных тиглей (первый в России), стал десятым губернатором Томской губернии. Павел Петрович известен, в первую очередь, как создатель русского булата. Булат — особый вид стали, очень прочный и упругий, с узором на поверхности, который является не украшением, а показателем качества металла. Впервые оружие из такой стали начали использовать воины Древней Индии. Такими мечами разрубали камни и легкие воздушные ткани. Несколько веков спустя центром по изготовлению булатных клинков стал Дамаск. Как мастера 12-15 веков могли варить такую удивительную сталь, оставалось загадкой, разгадать которую сумел лишь Павел Аносов. До Павла Аносова создать булат пытались многие, но никому это не удавалось. За несколько лет работы мастер испробовал 189 способов сталеварения, но ни один из них не не дал булат. И лишь в 190-ый раз смешивая компоненты, создавая необходимые температурные условия мастер открыл тайну. Игорь Васильевич Курчатов (1903–1960) физик, «отец» атомной бомбы СССР Говорит и показывает @Boomburum Игорь Васильевич Курчатов (1903–1960) — один из первопроходцев в области ядерной физики, «отец» атомной бомбы СССР. В числе его заслуг — создание первого в Европе атомного реактора, первой в СССР атомной бомбы и первой в мире термоядерной бомбы. Но были заслуги и в развитии мирного атома — под его руководством была сооружена первая в мире атомная электростанция (Обнинская АЭС).  Самый засекреченный ученый Самый засекреченный ученый Теги:  российская наука, российские учёные Хабы:  Блог компании Хабр, Научно-популярное Редакторский дайджест Присылаем лучшие статьи раз в месяц Хабр Экосистема для развития людей, вовлеченных в IT Хабр Карьера Facebook Twitter ВКонтакте Instagram 244 Карма 128.3 Рейтинг @Exosphere модератор «Хабра», куратор-эксперт ОПРОС Вы знаете, что такое IT-менторство и как оно работает? Знаю, я — ментор или у меня была консультация с ментором Знаю и хочу попробовать, но пока опыта нет Знаю и не хочу пробовать, менторство не решает мой запрос Не знаю, но мне интересно Не знаю и мне неинтересно Голосовать Воздержаться Проголосовал 1061 пользователь. Воздержались 219 пользователей. Электропочта Комментарии 8 ИНФОРМАЦИЯ Сайт habr.com Дата регистрации 9 августа 2008 Дата основания 26 мая 2006 Ваш аккаунт Войти Регистрация Разделы Статьи Новости Информация Устройство сайта Для авторов Услуги Корпоративный блог Медийная реклама Численность 51–100 человек Местоположение Россия Представитель Алексей ССЫЛКИ Хабр Карьера career.habr.com Хабр Q&A qna.habr.com Хабр Фриланс freelance.habr.com ВИДЖЕТ ВКОНТАКТЕ Хабы Компании Авторы Песочница Для компаний Документы Соглашение Конфиденциальность Нативные проекты Образовательные программы Стартапам Настройка языка Техническая поддержка © 2006–2024, Habr Хабр Подписан 1 друг Подписаться на новости БЛОГ НА ХАБРЕ 10 часов назад История российской науки: напишем вместе? 31 янв в 15:03 10 «золотых» советов авторам любых текстов 28 янв в 12:22 Истории, достойные экранизации, или День защиты персональных данных 31 дек 2023 в 02:19 Поздравление-загадка от Хабра 29 дек 2023 в 18:31 Анонимный Дед Мороз на Хабре: хвастаемся подарками 1.3K 8 8.5K 41 +41 7.6K 16 +16 4K 44 +44 4.1K 97 +97 "
        ],
        [
         "7",
         "RUVDS.com",
         null,
         "2394.92",
         "Несмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO-",
         "VDS/VPS-хостинг. Скидка 15% по коду HABR15",
         "2394.92 Рейтинг RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Несмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO- образов этих дисков остаётся актуальным. Многие операционные системы поставляются в виде ISO-образов, а администраторам необходимо поддерживать разношёрстный парк старых персональных компьютеров. artyomsoft 20 мар в 14:00 Как создать аппаратный эмулятор CD-ROM без паяльника Средний 19 мин 9.2K Блог компании RUVDS.com, Системное администрирование*, Разработка под Linux*, Разработка на Raspberry Pi*,  КАК СТАТЬ АВТОРОМ Вызовы для k8s. Чего ждать операторам в будущем Существует множество решений, как можно установить операционную систему с ISO- образа без записи его на оптический носитель. Я уже затрагивал тему ISO-образов в моих статьях: «Раскрываем секреты загрузочных ISO-образов» и «Что вам нужно знать о внешних загрузочных дисках». В этой статье я хочу рассказать о ещё одном способе, который, как оказывается, вшит в ядро Linux. Если ваш одноплатный компьютер имеет USB OTG-разъём, и на него возможна установка Linux, то вы c большой долей вероятности можете сделать из одноплатника аппаратный эмулятор привода оптических дисков. Меня этот способ заинтересовал. Я проверил его сам и, получив положительный результат у себя, решил поделиться с вами. Я сам узнал много интересного, систематизировал свои знания, поэтому надеюсь, что чтение будет познавательно и интересно для вас. Как всегда, если вы хотите посмотреть, что получится в итоге, уточнить детали, вы всегда можете найти исходный код в моём репозитории на GitHub. При написании статьи я поставил себе следующие цели: 1. Аппаратный эмулятор CD-ROM должен быть реализован без использования паяльника и макетных плат. 2. Реализация должна быть понятна человеку, имеющему лишь базовые представления о Linux, USB и Bluetooth. 3. Решение должно быть таким, чтобы его можно было с небольшими изменениями реализовать на различных одноплатных компьютерах. 4. Побудить интерес читателя к изучению используемых в статье технологий. 5. Изложить материал, необходимый для решения задачи лаконично и просто. Не уверен, что у меня это получилось из-за большого объёма темы. Буду признателен, если вы в отзывах напишете своё мнение. Оглавление Суть решения Проверка решения на практике От проверки идеи до реализации Операционная система Linux USB Bluetooth Сборка и модификация дистрибутива Raspberry Pi OS Реализация Как пользоваться эмулятором Особенности моего эмулятора Выводы Суть решения Решение заключается в том, что, модифицируя операционную систему Linux на одноплатном компьютере (встраиваемой системе), можно получить из него устройство, которое будет распознаваться компьютером как внешний оптический привод USB или флеш-накопитель. В ядро Linux включена поддержка эмуляции CD-ROM и эмуляции флеш-накопителя. Но это не значит, что любую встраиваемую систему можно превратить в них. Для этого ещё необходимо, чтобы встраиваемая система имела USB OTG-контроллер или USB- контроллер периферийного устройства. Проверка решения на практике Я делал эмулятор оптических дисков, используя Raspberry Pi Zero 2 W. Но вы можете использовать и другие одноплатные компьютеры. Естественно, вам тогда придётся самим разбираться с некоторыми проблемами, которые с большой долей вероятностью у вас возникнут. У меня других одноплатных компьютеров кроме Raspberry Pi не было, поэтому привожу алгоритм, как делал я. 1. Скачать образ Raspberry OS Light с сайта raspberrypi.org. 2. Записать образ на SD-карту. Я использовал программу balenaEtcher. 3. Добавить строку dtoverlay=dwc2  в файле config.txt на SD-карте. 4. Записать файлы ssh и wpa_supplicant.conf на SD-карту. Файл userconf.txt нужен, чтобы установить пароль для пользователя, ssh — чтобы включить SSH, wpa_supplicant.conf — чтобы указать точку доступа и пароль для Wi-Fi. 5. Вставить SD-карту в Raspberry Pi Zero 2 W. 6. Подключить USB-разъём Raspberry Pi Zero 2 W к USB-разъёму компьютера. 7. Подождать, пока выполнится первая загрузка и Raspberry Pi Zero 2 W подключится к Wi-Fi-сети. 8. Найти IP-адрес Raspberry Pi Zero 2 W в Wi-Fi-сети и подключиться к нему по протоколу SFTP. Я использовал приложение WinSCP. 9. Записать ISO-образы, которые вы хотите эмулировать, в файловую систему Raspberry Pi. 10. Подключиться через SSH к Raspberry Pi W 2. Это можно сделать при помощи приложения PuTTY. 11. Для того, чтобы ваш Raspberry Pi Zero 2 W стал внешним USB CD-ROM, ввести команду: $ sudo modprobe g_mass_storage cdrom=y removable=y stall=n file=/full/filename/w После чего у вас на компьютере распознается внешний USB CD-ROM, в который вставлен диск. 12. Для прекращения эмуляции ввести команду: $ sudo modprobe -r g_mass_storage Приведённый файл userconf.txt устанавливает для пользователя pi пароль «raspberry». Файл ssh — это пустой файл, который не содержит никаких данных. От проверки идеи до реализации Приведённая выше последовательность шагов позволяет вам посмотреть работу эмуляции в действии. Однако это решение обладает рядом недостатков. 1. Чтобы загрузить и выбрать образ для эмуляции, необходимо наличие Wi-Fi. Файл userconf.txt Файл wpa_supplicant.conf 2. При перезагрузке Raspberry Pi необходимо заново монтировать образ. 3. Если нужно эмулировать образ для загрузки с него операционной системы, понадобится ещё один компьютер для управления Raspberry Pi. 4. Эмулировать можно образ размером максимум 2 Gib. Если вам интересно, как избавиться от этих недостатков, у вас есть время и интерес разобраться в этом вопросе, то предлагаю продолжить чтение. Краткое содержание моей реализации следующее: 1. Для общения с Raspberry Pi будет использоваться Bluetooth. 2. Чтобы работа устройства была возможна в автономном режиме (без подключений Wi- Fi и Bluetooth), управляющий скрипт оформляется в виде службы Systemd. 3. Для управления по Bluetooth будет использоваться приложение Serial Bluetooth Terminal c Google Play. 4. Для эмуляции оптических дисков с образов размером больше 2Gib необходимо внести небольшие изменения в модуль ядра Linux и выполнить перекомпиляцию. Приведу кратко, что вам нужно знать, чтобы лучше понять суть того, что мы будем делать. Операционная система Linux ▍ Ядро Linux Ядро Linux содержит в себе абстракции для работы с устройствами там, где оно запускается. Реализуются эти абстракции в специальных программах, называемых драйверами. В ОС Linux драйвер может находиться непосредственно в файле ядра, а может быть оформлен в виде отдельного модуля. В большинстве случаев предпочтителен второй способ, так как модули можно динамически удалять и добавлять. Например, если устройство не подключено к системе или с ним не осуществляется работа, драйвер нам не нужен, и его можно выгрузить из памяти или не загружать вообще. При загрузке ядра ему необходима информация об устройствах, которые присутствуют в системе, чтобы корректно загрузить драйверы (модули) для них. Эта информация может передаваться из различных источников. Например, в архитектуре x86 это будет ACPI. В архитектуре ARM это Device Tree. ▍ Device Tree и Overlays Иногда Device Tree нужно модифицировать, чтобы можно было загрузить корректно драйвера для устройств. Делается это при помощи подключения overlays. Они содержат информацию, что необходимо изменить в исходном Device Tree. ▍ Headless-режим работы Raspberry Pi Очень часто в различных статьях и самоучителях по работе с Raspberry Pi необходимо подключение монитора, клавиатуры и мыши. Но на самом деле есть возможность работать с ним в так называемом headless-режиме. В этом режиме вы работаете с Raspberry Pi при помощи эмулятора терминала. Соединение его с Raspberry Pi может быть UART, USB, Bluetooth, Ethernet, Wi-Fi. Главная сложность заключается в том, как можно работать в этом режиме с самого начала, сразу после записи образа операционной системы на SD-карту, если у вас нет лишнего монитора и клавиатуры. Как активировать SSH, настроить Wi-Fi на использование определённой точки доступа? В Raspberry Pi OS такая возможность есть. Достаточно разместить определённые файлы в разделе FAT32 на SD-карте и загрузиться с неё. Raspberry Pi OS сделает необходимые настройки сама. ▍ Файловые системы, блочные устройства, разделы, монтирование Меня всегда восхищала идея, что в Linux всё является файлом. Правильно используя средства Linux, можно практически без программирования выполнять сложные задачи. Блочное устройство — это некоторый файл, в который можно записывать и считывать данные блоками байтов различной длины. Что будет происходить при этом, зависит от того, с чем реально ассоциирован этот файл. Например, если он ассоциирован с жёстким диском, то тогда будут читаться/записываться данные на жёсткий диск, не обращая внимание на разделы и файловые системы. Если он ассоциирован с разделом жёсткого диска, то будут читаться/записываться данные, не обращая внимание на файловую систему. При помощи команды losetup можно добиться того, что он будет ассоциироваться с обычным файлом на диске, что позволит создавать образы разделов и дисков. Ещё полезной командой Linux является команда kpartx, которая создаёт блочные устройства из файла образа диска. Каждое из устройств будет ассоциировано с образом раздела, который хранится в этом файле. Форматирование раздела в Linux выполняется одной командой. В качестве параметра необходимо передать имя файла блочного устройства. Например, для создания файловой системы exFAT на блочном устройстве /dev/mmcblk0p3: $ mkfs.exfat /dev/mmcblk0p3 Чтобы можно было работать с файлами файловой системы, размещённой на блочном устройстве, нужно примонтировать файловую систему к корневой файловой системе при помощи команды mount. $ mkdir -p /mnt/data $ mount -t auto /dev/mmcblk0p3 /mnt/data Обратите внимание, что директория, куда будет производиться монтирование, должна существовать до того, как вы будете монтировать. Если нужно размонтировать файловую систему, используется команда umount. $umount /mnt/data Чтобы посмотреть, какие у вас есть блочные устройства, и куда они примонтированы, можно использовать команду: $ lsblk ▍ Systemd Загрузка Linux происходит в несколько этапов. Сначала загружается ядро операционной системы, затем ядро запускает процесс init. Задача процесса init загрузить и инициализировать процессы пространства пользователя и находиться в памяти до перезагрузки или выключения компьютера (устройства). За долгие годы существования Linux было написано множество реализаций init. На данный момент во многих Linux- дистрибутивах используется реализация, называемая systemd. Её мы и будем использовать. Минимум команд, которые необходимо знать для работы с systemd. Команда Назначение systemctl start sn.service запустить службу systemctl stop sn.service остановить службу systemctl status sn.service посмотреть статус службы systemctl enable sn.service включить службу (служба будет автоматически запущена при следующей загрузке Linux) systemctl disable sn.service выключить службу journalctl -u sn.service -b посмотреть логи службы, начиная с момента последней загрузки Linux ▍ Терминалы, PuTTY, sshd, agetty Для администрирования ОС Linux из Windows часто используют эмулятор терминала PuTTY. Он позволяет подключаться к компьютеру или устройству c ОС Linux с помощью различных соединений (Ethernet, Wi-Fi, эмулируемого последовательного порта на Bluetooth или USB) и работать удалённо с консолью Linux в Windows. Чтобы такое было возможно, в ОС Linux должна быть запущена специальная программа, которая будет взаимодействовать с PuTTY. Это может быть sshd в случае SSH-соединения или agetty в случае последовательного порта. При подключении через последовательный порт по умолчанию вы увидите чёрно-белый экран без поддержки манипулятора мышь. Чтобы добавить поддержку мыши и цветного экрана, необходимо изменить значение переменной окружения TERM в файле /usr/lib/systemd/system/serial-getty@.service. [Service] Environment=TERM=xterm USB Чтобы два USB-устройства могли работать друг с другом, необходимо наличие у каждого из них USB-контроллера. USB-контроллер в конкретный момент времени может работать в режиме хоста (Host) или режиме периферийного устройства (Device). Если одно из взаимодействующих устройств работает в режиме Host, то другое должно работать в режиме Device. Существуют следующие виды USB-контроллеров: Host — всегда работает в режиме Host. Device — всегда работает в режиме Device. OTG — может работать или в режиме хоста или в режиме периферийного устройства. Переключение режимов может быть аппаратным (при помощи особой распайки кабеля OTG кабель переводит в контроллер режим хоста) или программным Режим хоста подразумевает посылку команд, а режим периферийного устройства — их обработку. ▍ OTG USB-контроллер Возьмём Android-телефон с OTG-контроллером. Это означает, что при сопряжении по USB с компьютером (для записи файлов с компьютера на телефон), он будет играть роль периферийного устройства, а при сопряжении по USB с периферийным устройством (мышью, клавиатурой, сетевой картой, флэш-накопителем, монитором, принтером) телефон будет играть роль хоста. Обычно USB-контроллер периферийного устройства или USB OTG-контроллер присутствуют во встраиваемых устройствах. Также они могут быть интегрированы в однокристальную систему (SoC). Но по факту на устройстве может отсутствовать физический USB-разъём для подключения. Например, на всех Raspberry Pi установлена SoC, которая имеет OTG-контроллер, но фактически физический разъём для него есть только в Raspberry Pi Zero (Zero W, Zero 2 W) и в Raspberry Pi 4. ▍ Дескрипторы USB Каждое USB-устройство имеет дескрипторы. Дескрипторы — это информация о USB- устройстве, которая используется операционной системой для корректного выбора драйвера для устройства. Мне понравилось описание, которое приведено на сайте Microsoft. ▍ Создание USB-устройств в Linux Ядро Linux содержит модули, которые позволяют создавать виртуальные USB-устройства. Это может быть Mass Storage, последовательный порт, сетевая карта. Загрузив и настроив эти модули, вы можете сделать так, чтобы компьютером ваш одноплатник распознавался одним или несколькими такими устройствами. Если вам достаточно одного устройства, то вы можете загрузить модуль для этого устройства, опционально передав ему параметры для конфигурации при помощи команды modprobe. Когда отпадёт необходимость в этом устройстве, его можно выгрузить при помощи команды modprobe -r. Чтобы на одном физическом порту у вас распознавалось несколько устройств одновременно, нужно использовать модуль libcomposite и сконфигурировать эти устройства при помощи создания структур в файловой системе ConfigFS в директории /sys/kernel/config/usb_gadget. Такие устройства называются композитными USB-устройствами. Вы, скорее всего, встречались с такими, например, если у вас беспроводная клавиатура и мышь, а для них используется один приёмопередатчик. В нашем случае мы создадим композитное USB-устройство, которое будет последовательным портом и устройством хранения. Последовательный порт мы будем использовать для подключения к нашему эмулятору оптических дисков через PuTTY. Изначально я хотел, что бы это была сетевая карта и SSH, но карта требует настройки в операционной системе компьютера, поэтому для простоты отказался от этой идеи в пользу последовательного порта. ▍ Создание композитного USB-устройства при помощи ConfigFS 1. Загружаем модуль libcomposite. modprobe libcomposite 2. Заполняем дескрипторы для устройства. $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ mkdir -p $usb_dev $ echo 0x0137 > $usb_dev/idProduct $ echo 0x0100 > $usb_dev/bcdDevice $ echo 0x0200 > $usb_dev/bcdUSB $ echo 0xEF  > $usb_dev/bDeviceClass $ echo 0x02 > $usb_dev/bDeviceSubClass $ echo 0x01 > $usb_dev/bDeviceProtocol $ mkdir -p $usb_dev/strings/0x409 $ echo \"abababababababa\" > $usb_dev/strings/0x409/serialnumber $ echo \"Linux Foundation\" > $usb_dev/strings/0x409/manufacturer $ echo \"USB CD-ROM Emulator\" > $usb_dev/strings/0x409/product 3. Создаём конфигурацию. mkdir -p $usb_dev/configs/c.1 mkdir -p $usb_dev/configs/c.1/strings/0x409 echo \"acm+usb\" > $usb_dev/configs/c.1/strings/0x409/configuration echo \"0x80\" > $usb_dev/configs/c.1/bmAttributes echo 250 > $usb_dev/configs/c.1/MaxPower 4.Создаём и подключаем функцию acm (последовательный порт через USB). $ mkdir -p $usb_dev/functions/acm.usb0 $ ln -s $usb_dev/functions/acm.usb0 $usb_dev/configs/c.1 5. Создаём и подключаем функцию mass_storage. Mass_storage в данном случае — это эмуляция CD-ROM для ISO-образа /home/pi/1.iso. $ mkdir -p $usb_dev/functions/mass_storage.usb0/lun.0 $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/cdrom $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/removable $ echo 0 > $usb_dev/functions/mass_storage.usb0/lun.0/nofua $ echo 0 > $usb_dev/functions/mass_storage.usb0/stall $ echo \"/home/pi/1.iso\" > $usb_dev/functions/mass_storage.usb0/lun.0/file $ ln -s $usb_dev/functions/mass_storage.usb0 $usb_dev/configs/c.1 6. Активируем созданное устройство. $ ls /sys/class/udc > $usb_dev/UDC ▍ Удаление композитного USB-устройства при помощи ConfigFS 1. Деактивируем устройство. $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ echo \"\"> $usb_dev/UDC 2. Удаляем функцию mass_storage. $ rm $usb_dev/configs/c.1/mass_storage.usb0 $ rmdir $usb_dev/functions/mass_storage.usb0 3. Удаляем функцию acm. $ rm $usb_dev/configs/c.1/acm.usb0 $ rmdir $usb_dev/functions/acm.usb0 4. Удаляем конфигурацию. $ rmdir $usb_dev/configs/c.1/strings/0x409 $ rmdir $usb_dev/configs/c.1 5. Удаляем устройство. $ rmdir $usb_dev/strings/0x409 $ rmdir $usb_dev 6. Выгружаем загруженные устройством модули. $ modprobe -r usb_f_mass_storage $ modprobe -r usb_f_acm $ modprobe -r libcomposite Cтруктура файловой системы для созданного эмулятора CD-ROM Bluetooth Тема Bluetooth очень объёмная, и её невозможно изложить в одной статье, поэтому приведу только тот минимум, который позволяет понять, как мы будем использовать Bluetooth. Bluetooth — технология, которая позволяет связывать устройства без проводов по радиоканалу. На данный момент существует множество версий спецификации Bluetooth. Спецификация Bluetooth освещает множество вопросов. Чтобы передать данные с одного устройства на другое, необходимо наличие на обоих устройствах контроллеров и стеков Bluetooth. Bluetooth-контроллер — аппаратное устройство, обычно выполненное в виде микросхемы или части более сложной микросхемы, позволяющее получать/передавать данные по радиоканалу в соответствии со спецификацией Bluetooth. Bluetooth-стек — программная реализация протоколов, описанных в спецификации Bluetooth. Протоколы Bluetooth, предназначенные для решения определённых задач, группируются в профили Bluetooth. Мы будем использовать два профиля Bluetooth: 1. Generic Access Profile (GAP), который поддерживается всеми Bluetooth-устройствами. 2. Serial Port Profile (SPP), который подразумевает использование последовательного порта поверх соединения Bluetooth. ▍ Поддержка Bluetooth операционными системами Bluetooth-контроллеры могут иметь различные аппаратные интерфейсы для доступа. Это может быть UART, USB, PCIe. В случае операционной системы многие детали скрываются, и можно о них не думать. С контроллером можно работать на низком уровне через драйвер или уже используя высокоуровневые библиотеки и приложения, предоставляемые стеком Bluetooth, например, в Linux широко распространён стек BlueZ. ▍ BlueZ Стек Bluetooth BlueZ состоит из двух частей. Одна часть представлена модулями ядра Linux, уже включена в ядро. Если она отсутствует, то её нужно включить и перекомпилировать ядро. Вторая часть представлена приложениями для пространства пользователя. Приложения позволяют конфигурировать и работать со стеком Bluetooth. На данный момент многие приложения считаются устаревшими, и разработчики BlueZ рекомендуют использовать более новые приложения и интерфейс D-Bus для работы со стеком. Но, как мне кажется, именно те старые, устаревшие приложения позволяют лучше понять работу Bluetooth, поэтому в учебных целях я буду использовать их, для чего нужно будет инициализировать BlueZ в режиме совместимости. $ bluetoothd --noplugin=sap -C ▍ Протоколы Bluetooth Я не буду утомлять вас различными схемами, диаграммами, которые вы легко можете найти в интернете. Расскажу только о тех протоколах, с которыми нам предстоит столкнуться и нужно будет сконфигурировать. ▍ Service Discovery Protocol (SDP) При помощи протокола SDP можно определить, какие приложения (сервисы) находятся на хосте, и с ними возможен обмен данными через Bluetooth Чтобы можно было увидеть сервис с другого устройства, его необходимо зарегистрировать в SDP database. Например, если мы хотим зарегистрировать службу, представляющую эмуляцию последовательного порта в Bluetooth, это можно сделать следующей командой: $ sdptool add SP Чтобы можно было посмотреть службы, зарегистрированные у вас на хосте, нужно ввести команду: $ sdptool browse local ▍ Radio Frequency Communications (RFCOMM) Протокол RFCOMM позволяет создавать виртуальное соединение по последовательному порту между двумя хостами. На одном из хостов создаётся сервер, которому выделяется канал RFCOMM, второй из хостов подключается к нему, указывая MAC-адрес и номер канала Канал RFCOMM немного напоминает порт в UDP или TCP, но если у них и у источника и у получателя есть порты, то у RFCOMM для источника и получателя один и тот же канал. Поэтому невозможно создать несколько подключений на один и тот же канал. В Linux можно использовать команду rfcomm для создания процесса, который будет слушать определённый канал RFCOMM и при соединении запускать другой процесс. $ rfcomm -r watch hci0 1 /usr/local/bin/cdemu-cmd /dev/rfcomm0 /dev/rfcomm0 В данном случае на Bluetooth-контроллере hci0 RFCOMM будет прослушиваться канал 1 и запускаться процесс cdemu-cmd с двумя параметрами командной строки /dev/rfcomm0 и /dev/rfcomm0. ▍ Утилита bluetoothctl Утилита Bluetoothctl позволяет сопрягать устройство, на котором вы её запустили с другим устройством. Вы можете сделать устройство видимым для обнаружения другими устройствами, а также найти другое устройство и выполнить с ним сопряжение. Более подробно расписано в документации к утилите, которая доступна по команде: $ man bluetoothctl ▍ Serial Bluetooth Terminal Для отладки приложений, использующих Bluetooth, удобно использовать приложение для Android Serial Bluetooth Terminal. Это приложение позволяет работать с Bluetooth- устройствами, у которых доступен профиль SPP. В нашем случае мы будем использовать его как визуальный интерфейс для работы с нашим эмулятором оптических дисков. Сборка и модификация дистрибутива Raspberry Pi OS Чтобы сделать полноценный аппаратный эмулятор оптических дисков, нам придётся немного модифицировать исходный дистрибутив Linux. Это подразумевает перекомпиляцию ядра, изменение нескольких конфигурационных файлов и добавление своего программного кода. Для меня это было удобно сделать при помощи Docker. ▍ Кросс-компиляция ядра Linux Кросс-компиляция позволяет на компьютере с одной архитектурой получать исполняемые файлы для другой архитектуры. Мы можем компилировать ядро Linux для Raspberry Pi на Raspberry Pi, а можем, используя кросс-компиляцию, сделать это на обычном компьютере с архитектурой x86, что существенно сократит время компиляции из-за большего быстродействия компьютера. Подробно о том, как выполнять кросс-компиляцию Raspberry Pi OS, можно почитать тут. ▍ Chroot и запуск бинарных файлов другой архитектуры Команда Linux chroot позволяет запускать процессы с изменённой корневой системой. Это кажется немного запутанным, но суть в следующем. В качестве параметра команде передаётся путь к корневой директории. В результате запуска команды через chroot запущенный процесс будет считать, что корнем файловой системы является та директория, которую передали в качестве параметра. Применений у команды chroot несколько, например, её можно использовать, чтобы запустить команду apt для Raspberry Pi в Docker-контейнере. Интересно, что Docker Desktop для Windows позволяет запускать исполняемые файлы для архитектуры ARM. В Linux-версии Docker такое сделать можно, но нужна дополнительная настройка. Реализация Созданный мной проект состоит из следующих файлов: 1. Dockerfile и скрипт, который выполняется в Docker-контейнере. 2. Файлы, которые необходимо добавить или обновить в исходном дистрибутиве: cdemu — основная логика работы эмулятора оптических дисков, написанная на языке bash; cdemu-cmd — bash-скрипт для обработки команд от пользователя и передачи их эмулятору; bash-utils.sh — bash-скрипт co вспомогательными функциями; cdemu-bluetooth-ui.service — systemd-служба, которая запускает интерпретатор команд на создаваемом RFCOMM-соединении телефона и Raspberry Pi; cdemu.service — systemd-служба, которая запускает эмулятор оптических дисков при загрузке; bluetooth.service — изменённая служба systemd для инициализации bluetooth; serial-getty@.service — изменённая служба systemd для запуска agetty на создаваемом соединении на последовательном порту; firstboot.service — служба systemd для запуска скрипта при первой загрузке операционной системы. Я её позаимствовал из проекта raspberian-firstboot; config.txt — изменённый файл конфигурации для загрузки Raspberry Pi. Содержит подключение overlay dwc. Это необходимо, чтобы USB-контроллер мог работать в device mode; fistboot.sh — скрипт, который запускается службой systemd firstboot.service; userconf.txt — файл, который необходим, чтобы установить пароль для пользователя pi. В последних версиях Raspberry Pi OS пользователь pi не активирован по умолчанию, поэтому необходимо наличие этого файла; ssh — файл необходим, чтобы активировать ssh, который отключён по умолчанию; wpa_supplicant.conf — файл, необходимый, если вы хотите настроить Raspberry Pi на работу с вашей точкой доступа. Листинги файлов не привожу, так как это ещё больше раздует и так большую статью. Ознакомиться вы с ними можете здесь. Как пользоваться эмулятором 1. Собираем Docker-образ. docker build -t raspi-image .  2. Собираем образ RaspberryPi OS. docker run --privileged -v c:\\temp:/build --name raspi-image -it --rm raspi-image 3. Записываем образ на SD-карту. Вставляем её в Raspberry Pi. 4. Подключаем Rapsberry Pi Zero 2 W к компьютеру. 5. Через некоторое время у вас появится съёмный накопитель. 6. На этот съёмный накопитель, содержащий файл Readme.txt, копируем образы, которые хотим эмулировать. 7. Находим виртуальный COM-порт, созданный после подключения Raspberry Pi к компьютеру. 8. Подключаемся к Raspberry Pi через с помощью Putty через виртуальный COM-порт. 9. Запускаем интерактивное приложение для управления эмулятором. $ sudo cdemu-cmd 10. Если хотим сделать управление с телефона, то выполняем сопряжение телефона и Raspberry Pi. Для чего вводим в эмуляторе терминала команду: $ sudo bluetoothctl 11. Делаем Raspberry Pi доступным для обнаружения: discoverable on 12. Находим его на телефоне и выполняем полключение. После чего соглашаемся с PIN- кодом на телефоне и Raspberry Pi. yes 13. Выходим из bluetoothctl. exit 14. Запускаем на телефоне Serial Bluetooth Terminal и выполняем подключение к Raspberry Pi из него. Теперь можно посылать команды созданному эмулятору CD-ROM. Команды, которые можно посылать эмулятору: 1. hdd — переключение в режим эмуляции внешних жёстких дисков. 2. cdrom — переключение в режим эмуляции внешних приводов оптических дисков. 3. list — вывести список доступных ISO-образов, которые можно эмулировать. 4. insert <порядковый номер> — поместить ISO-образ для эмуляции. 5. eject — извлечь ISO-образ из эмулятора. 6. help — показать список доступных команд в текущем режиме. Особенности моего эмулятора Интересно, но в Linux по умолчанию нельзя эмулировать ISO-образы размером больше 2 Gib. Я просмотрел исходный код драйвера в файле drivers/usb/gadget/function/storage_common.c и предположил, что нет оснований не применять патч к ядру Linux от Adam Bambuch, который просто удаляет одно условие. Образы эмулировались нормально и при снятии ISO-образа с эмулируемого CD-ROM он был идентичен исходному. Поверял по хеш-коду для файла ISO-образа. Не пойму, почему есть это ограничение в Linux и почему его до сих пор не убрали? Если вы знаете ответ, ответьте в комментариях. Моя реализация не требует никаких дополнительных деталей. Нужен только Raspberry Pi Zero 2 W, один или два кабеля USB и адаптер питания, если будете использовать два кабеля USB. Один для питания, второй для передачи данных. Хоть и использование дополнительного кабеля и адаптера добавляет громоздкости, это решает проблему перезагрузки Raspberry Pi, если компьютер или ноутбук отключает ненадолго питание при перезагрузке. Кроме того, я не использую Python, только bash. Выводы Полученное программно-аппаратное решение, хоть и обладает рядом недостатков по сравнению с карманом Zalman (не поддерживается USB 3.0, нет интерактивного меню на самом устройстве), позволит вам установить практически любую операционную систему на широкий спектр компьютеров путём простого копирования ISO-образа. Решение является прототипом, но вместе с тем позволяет углубить знания по многим темам, или получить, если вы были с ними не знакомы. Так как основной целью была разработка прототипа, я запускал bluetoothd в режиме совместимости, и почти всю логику написал на bash. Я хотел показать возможность превратить встраиваемое устройство с операционной системой Linux в аппаратный эмулятор флеш-накопителя или привода оптических дисков, приложив минимум усилий. Надеюсь, что это удалось. Чтобы уместить всё в одной статье, я лишь поверхностно коснулся тех тем, которые необходимы для понимания. Если вас заинтересовало, вы можете самостоятельно изучить их углублённо. Объём статьи не позволяет осветить все интересности, с которыми я столкнулся при разработке эмулятора, и решения, которые применял и проверял. Приведу лишь несколько из них. Например, я долго боролся с зависанием при удалении составного устройства. Помогло использование службы serial-getty вместо getty, хотя во многих статьях упоминалась getty. Я долго разбирался, как можно сделать сопряжение через Bluetooth между Raspberry и телефоном, использовал команду bt-agent, но потом всё-таки отказался от неё в пользу bluetoothctl. При переключении эмулятора в режим HDD для записи ISO-образов изначально я открывал для доступа всю SD-карту и хранил ISO-образы в отдельном разделе, но потом посчитал, что для безопасности лучше хранить образ диска с ISO-образами в отдельном файле и открывать доступ только к нему, хоть это и снизило скорость записи, но пользователя не обескураживают появляющиеся несколько дисков. Разработанный прототип есть куда улучшать. Можно, например, создать более минималистичный дистрибутив Linux, который будет содержать только то, что реально используется для эмуляции, или создать более удобное графическое приложение для Android для работы с эмулятором. А можно упростить работу с Bluetooth, напрямую работая с драйверами bluetooth или используя интерфейс D-Bus для работы с Bluetooth- устройствами. Или вообще всё-таки взяться за паяльник и сделать устройство, более похожее по функционалу на карман Zalman. Но главное, вы увидели, что это реально сделать, а когда видишь положительный результат, это вдохновляет на большее творчество. В процессе тестирования и отладки программного кода было замечено, что на Lenovo X1 Extreme Gen 2 эмулятор CD-ROM дисков великолепно определялся в Windows 10, но отказывался определяться в BIOS. Эмпирически было определено, что помогает отключение режима экономии энергии процессора в BIOS. Также ноутбук отключал питание на usb при перезагрузке, поэтому понадобилось дополнительное питание Raspberry Pi. Интересно, но на ASUS K53E и Gigabyte BRIX всё работает без проблем. Решение с небольшими модификациями можно реализовать на Raspberry Pi 4. Но если вы поняли суть решения, вы его сможете повторить и на других одноплатных компьютерах, которые имеют выведенные USB-порты для OTG или USB-контроллеры периферийных устройств. RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 84 Карма 82.1 Рейтинг @artyomsoft Пользователь Комментарии 17 Dockerfile на данный момент только выполняется в Docker Desktop для Windows. В Linux он работать не будет. В заключение хочу сказать, что существует ещё один способ эмулировать оптические диски, который я не пробовал, но знаю о его существовании из ваших комментариев к одной из моих статей — это программа DriveDroid для Android. Я ей не пользовался, так как для её работы нужно получать права root на телефоне. Но, скорее всего, из-за ограничений в ядре Linux программа поддерживает ISO-образы до 2 Gib и/или работает только с гибридными ISO-образами. Если я не прав, буду рад увидеть ваши опровержения в комментариях. Telegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх 🕹️ Теги:  linux kernel, usb, bluetooth, bluez, linux modules, эмуляция, iso, cd-rom, systemd,  agetty, ruvds_статьи Хабы:  Блог компании RUVDS.com, Системное администрирование,  Разработка под Linux, Разработка на Raspberry Pi, DIY или Сделай сам Редакторский дайджест Присылаем лучшие статьи раз в месяц Электропочта Публикации RationalAnswer 9 часов назад Как обнулялся Credit Suisse: разбираем траекторию погружения на дно швейцарского гига-банка 12 мин kesn 10 часов назад Байки погромиста. Если кто-то скажет, что программирование — это скучно Простой 12 мин Tutelka 9 часов назад Обещания — настоящие и не очень Простой 7 мин alizar 9 часов назад Удивительное рядом. Как устроен буфер обмена в Windows и Linux Простой 6 мин ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 12K Обзор +75 26 49 11K Мнение +75 33 9 2.7K Мнение +34 24 10 4.7K +31 46 10 unxed 3 часа назад Linux-порт Far Manager: новости весны 2023 Средний 4 мин Показать еще 1.8K Дайджест +29 14 7 ИНФОРМАЦИЯ Сайт ruvds.com Дата регистрации 18 марта 2016 Дата основания 27 июля 2015 Численность 11–30 человек Местоположение Россия Представитель ruvds ССЫЛКИ VPS / VDS сервер от 130 рублей в месяц. ruvds.com Дата-центры RUVDS в Москве, Санкт-Петербурге, Казани, Екатеринбурге, Новосибирске, Лондоне, Франкфурте, Цюрихе, Амстердаме ruvds.com Помощь и вопросы ruvds.com Партнерская программа RUVDS ruvds.com VPS (CPU 1x2ГГц, RAM 512Mb, SSD 10 Gb) — 190 рублей в месяц ruvds.com VPS Windows от 523 рублей в месяц. Бесплатный тестовый период 3 дня. ruvds.com VDS в Цюрихе. Дата-центр TIER III — швейцарское качество по низкой цене. Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr ruvds.com Антивирусная защита виртуального сервера. Легкий агент для VPS. ruvds.com VPS в Лондоне. Дата-центр TIER III — английская точность за рубли. ruvds.com VPS с видеокартой на мощных серверах 3,4ГГц ruvds.com ПРИЛОЖЕНИЯ RUVDS Client Приложение для мониторинга и управления виртуальными серверами RUVDS с мобильных устройств. Android iOS ВИДЖЕТ БЛОГ НА ХАБРЕ 5 часов назад Как устроено индексирование баз данных 1.7K 0 9 часов назад Удивительное рядом. Как устроен буфер обмена в Windows и Linux вчера в 14:00 Будни техпода. Размещение игрового сервера на VDS вчера в 08:45 Ностальгические игры: Parasite Eve 20 мар в 18:00 Инструменты наблюдаемости, о которых нужно знать в 2023 году 4.7K 10 2.9K 12 3.4K 4 3.2K 4 "
        ],
        [
         "8",
         "RUVDS.com",
         null,
         "2394.92",
         " 20 мар в 12:00",
         "VDS/VPS-хостинг. Скидка 15% по коду HABR15",
         "2394.92 Рейтинг RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15  20 мар в 12:00 Средний19 мин artyomsoft Как создать аппаратный эмулятор CD-ROM без паяльника 9.2K Блог компании RUVDS.com ,  Системное администрирование* ,  Разработка под Linux* ,  Ра КАК СТАТЬ АВТОРОМ Несмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO-образов этих дисков остаётся актуальным. Многие операционные системы поставляются в виде ISO-образов, а администраторам необходимо поддерживать разношёрстный парк старых персональных компьютеров. Существует множество решений, как можно установить операционную систему с ISO-образа без записи его на оптический носитель. Я уже затрагивал тему ISO-образов в моих статьях: «Раскрываем секреты загрузочных ISO-образов» и «Что вам нужно знать о внешних загрузочных дисках». В этой статье я хочу рассказать о ещё одном способе, который, как оказывается, вшит в ядро Linux. Если ваш одноплатный компьютер имеет USB OTG-разъём, и на него возможна установка Linux, то вы c большой долей вероятности можете сделать из одноплатника аппаратный эмулятор привода оптических дисков.  Меня этот способ заинтересовал. Я проверил его сам и, получив положительный результат у себя, решил поделиться с вами.  Я сам узнал много интересного, систематизировал свои знания, поэтому надеюсь, что чтение будет познавательно и интересно для вас. Как всегда, если вы хотите посмотреть, что получится в итоге, уточнить детали, вы всегда можете найти исходный код в моём репозитории на GitHub. При написании статьи я поставил себе следующие цели: 1. Аппаратный эмулятор CD-ROM должен быть реализован без использования паяльника и макетных плат. 2. Реализация должна быть понятна человеку, имеющему лишь базовые представления о Linux, USB и Bluetooth. 3. Решение должно быть таким, чтобы его можно было с небольшими изменениями реализовать на различных одноплатных компьютерах. 4. Побудить интерес читателя к изучению используемых в статье технологий. 5. Изложить материал, необходимый для решения задачи лаконично и просто. Не уверен, что у меня это получилось из- за большого объёма темы. Буду признателен, если вы в отзывах напишете своё мнение. Оглавление • Суть решения • Проверка решения на практике • От проверки идеи до реализации • Операционная система Linux • USB • Bluetooth • Сборка и модификация дистрибутива Raspberry Pi OS • Реализация • Как пользоваться эмулятором • Особенности моего эмулятора • Выводы Суть решения Решение заключается в том, что, модифицируя операционную систему Linux на одноплатном компьютере (встраиваемой системе), можно получить из него устройство, которое будет распознаваться компьютером как внешний оптический привод USB или флеш- накопитель. В ядро Linux включена поддержка эмуляции CD-ROM и эмуляции флеш-накопителя. Но это не значит, что любую встраиваемую систему можно превратить в них. Для этого ещё необходимо, чтобы встраиваемая система имела USB OTG-контроллер или USB- контроллер периферийного устройства. Проверка решения на практике Я делал эмулятор оптических дисков, используя Raspberry Pi Zero 2 W. Но вы можете использовать и другие одноплатные компьютеры. Естественно, вам тогда придётся самим разбираться с некоторыми проблемами, которые с большой долей вероятностью у вас возникнут. У меня других одноплатных компьютеров кроме Raspberry Pi не было, поэтому привожу алгоритм, как делал я. 1. Скачать образ Raspberry OS Light с сайта raspberrypi.org. 2. Записать образ на SD-карту. Я использовал программу balenaEtcher. 3. Добавить строку dtoverlay=dwc2  в файле config.txt на SD- карте. 4. Записать файлы ssh и wpa_supplicant.conf на SD-карту. Файл userconf.txt нужен, чтобы установить пароль для пользователя, ssh — чтобы включить SSH, wpa_supplicant.conf — чтобы указать точку доступа и пароль для Wi-Fi. 5. Вставить SD-карту в Raspberry Pi Zero 2 W. 6. Подключить USB-разъём Raspberry Pi Zero 2 W к USB-разъёму компьютера. 7. Подождать, пока выполнится первая загрузка и Raspberry Pi Zero 2 W подключится к Wi-Fi-сети. 8. Найти IP-адрес Raspberry Pi Zero 2 W в Wi-Fi-сети и подключиться к нему по протоколу SFTP. Я использовал приложение WinSCP. 9. Записать ISO-образы, которые вы хотите эмулировать, в файловую систему Raspberry Pi. 10. Подключиться через SSH к Raspberry Pi W 2. Это можно сделать при помощи приложения PuTTY. 11. Для того, чтобы ваш Raspberry Pi Zero 2 W стал внешним USB CD-ROM, ввести команду: После чего у вас на компьютере распознается внешний USB CD- ROM, в который вставлен диск. 12. Для прекращения эмуляции ввести команду: Приведённый файл userconf.txt устанавливает для пользователя pi пароль «raspberry». Файл ssh — это пустой файл, который не содержит никаких данных. От проверки идеи до реализации Приведённая выше последовательность шагов позволяет вам посмотреть работу эмуляции в действии. Однако это решение обладает рядом недостатков. $ sudo modprobe g_mass_storage cdrom=y removable=y stall $ sudo modprobe -r g_mass_storage Файл userconf.txt Файл wpa_supplicant.conf 1. Чтобы загрузить и выбрать образ для эмуляции, необходимо наличие Wi-Fi. 2. При перезагрузке Raspberry Pi необходимо заново монтировать образ. 3. Если нужно эмулировать образ для загрузки с него операционной системы, понадобится ещё один компьютер для управления Raspberry Pi. 4. Эмулировать можно образ размером максимум 2 Gib. Если вам интересно, как избавиться от этих недостатков, у вас есть время и интерес разобраться в этом вопросе, то предлагаю продолжить чтение. Краткое содержание моей реализации следующее: 1. Для общения с Raspberry Pi будет использоваться Bluetooth. 2. Чтобы работа устройства была возможна в автономном режиме (без подключений Wi-Fi и Bluetooth), управляющий скрипт оформляется в виде службы Systemd. 3. Для управления по Bluetooth будет использоваться приложение Serial Bluetooth Terminal c Google Play. 4. Для эмуляции оптических дисков с образов размером больше 2Gib необходимо внести небольшие изменения в модуль ядра Linux и выполнить перекомпиляцию. Приведу кратко, что вам нужно знать, чтобы лучше понять суть того, что мы будем делать. Операционная система Linux ▍ Ядро Linux Ядро Linux содержит в себе абстракции для работы с устройствами там, где оно запускается. Реализуются эти абстракции в специальных программах, называемых драйверами. В ОС Linux драйвер может находиться непосредственно в файле ядра, а может быть оформлен в виде отдельного модуля. В большинстве случаев предпочтителен второй способ, так как модули можно динамически удалять и добавлять. Например, если устройство не подключено к системе или с ним не осуществляется работа, драйвер нам не нужен, и его можно выгрузить из памяти или не загружать вообще. При загрузке ядра ему необходима информация об устройствах, которые присутствуют в системе, чтобы корректно загрузить драйверы (модули) для них. Эта информация может передаваться из различных источников. Например, в архитектуре x86 это будет ACPI. В архитектуре ARM это Device Tree. ▍ Device Tree и Overlays Иногда Device Tree нужно модифицировать, чтобы можно было загрузить корректно драйвера для устройств. Делается это при помощи подключения overlays. Они содержат информацию, что необходимо изменить в исходном Device Tree. ▍ Headless-режим работы Raspberry Pi Очень часто в различных статьях и самоучителях по работе с Raspberry Pi необходимо подключение монитора, клавиатуры и мыши. Но на самом деле есть возможность работать с ним в так называемом headless-режиме. В этом режиме вы работаете с Raspberry Pi при помощи эмулятора терминала. Соединение его с Raspberry Pi может быть UART, USB, Bluetooth, Ethernet, Wi-Fi. Главная сложность заключается в том, как можно работать в этом режиме с самого начала, сразу после записи образа операционной системы на SD-карту, если у вас нет лишнего монитора и клавиатуры. Как активировать SSH, настроить Wi-Fi на использование определённой точки доступа? В Raspberry Pi OS такая возможность есть. Достаточно разместить определённые файлы в разделе FAT32 на SD-карте и загрузиться с неё. Raspberry Pi OS сделает необходимые настройки сама. ▍ Файловые системы, блочные устройства, разделы, монтирование Меня всегда восхищала идея, что в Linux всё является файлом. Правильно используя средства Linux, можно практически без программирования выполнять сложные задачи. Блочное устройство — это некоторый файл, в который можно записывать и считывать данные блоками байтов различной длины. Что будет происходить при этом, зависит от того, с чем реально ассоциирован этот файл. Например, если он ассоциирован с жёстким диском, то тогда будут читаться/записываться данные на жёсткий диск, не обращая внимание на разделы и файловые системы. Если он ассоциирован с разделом жёсткого диска, то будут читаться/записываться данные, не обращая внимание на файловую систему. При помощи команды losetup можно добиться того, что он будет ассоциироваться с обычным файлом на диске, что позволит создавать образы разделов и дисков.  Ещё полезной командой Linux является команда kpartx, которая создаёт блочные устройства из файла образа диска. Каждое из устройств будет ассоциировано с образом раздела, который хранится в этом файле. Форматирование раздела в Linux выполняется одной командой. В качестве параметра необходимо передать имя файла блочного устройства. Например, для создания файловой системы exFAT на блочном устройстве /dev/mmcblk0p3: Чтобы можно было работать с файлами файловой системы, размещённой на блочном устройстве, нужно примонтировать файловую систему к корневой файловой системе при помощи команды mount. Обратите внимание, что директория, куда будет производиться монтирование, должна существовать до того, как вы будете монтировать. Если нужно размонтировать файловую систему, используется команда umount. Чтобы посмотреть, какие у вас есть блочные устройства, и куда они примонтированы, можно использовать команду: ▍ Systemd Загрузка Linux происходит в несколько этапов. Сначала загружается ядро операционной системы, затем ядро запускает процесс init. Задача процесса init загрузить и инициализировать процессы пространства пользователя и находиться в памяти до перезагрузки или выключения компьютера (устройства). За долгие годы существования Linux было написано множество реализаций init. На данный момент во многих Linux-дистрибутивах используется реализация, называемая systemd. Её мы и будем использовать. $ mkfs.exfat /dev/mmcblk0p3 $ mkdir -p /mnt/data $ mount -t auto /dev/mmcblk0p3 /mnt/data $umount /mnt/data $ lsblk Минимум команд, которые необходимо знать для работы с systemd. Команда Назначение systemctl start sn.service запустить службу systemctl stop sn.service остановить службу systemctl status sn.service посмотреть статус службы systemctl enable sn.service включить службу (служба будет автоматически запущена при следующей загрузке Linux) systemctl disable sn.service выключить службу journalctl -u sn.service -b посмотреть логи службы, начиная с момента последней загрузки Linux ▍ Терминалы, PuTTY, sshd, agetty Для администрирования ОС Linux из Windows часто используют эмулятор терминала PuTTY. Он позволяет подключаться к компьютеру или устройству c ОС Linux с помощью различных соединений (Ethernet, Wi-Fi, эмулируемого последовательного порта на Bluetooth или USB) и работать удалённо с консолью Linux в Windows. Чтобы такое было возможно, в ОС Linux должна быть запущена специальная программа, которая будет взаимодействовать с PuTTY. Это может быть sshd в случае SSH- соединения или agetty в случае последовательного порта. При подключении через последовательный порт по умолчанию вы увидите чёрно-белый экран без поддержки манипулятора мышь. Чтобы добавить поддержку мыши и цветного экрана, необходимо изменить значение переменной окружения TERM в файле /usr/lib/systemd/system/serial-getty@.service.  USB Чтобы два USB-устройства могли работать друг с другом, необходимо наличие у каждого из них USB-контроллера. USB- контроллер в конкретный момент времени может работать в режиме хоста (Host) или режиме периферийного устройства (Device). Если одно из взаимодействующих устройств работает в режиме Host, то другое должно работать в режиме Device. Существуют следующие виды USB-контроллеров: • Host — всегда работает в режиме Host. • Device — всегда работает в режиме Device. • OTG — может работать или в режиме хоста или в режиме периферийного устройства. Переключение режимов может быть аппаратным (при помощи особой распайки кабеля OTG кабель переводит в контроллер режим хоста) или программным Режим хоста подразумевает посылку команд, а режим периферийного устройства — их обработку.  ▍ OTG USB-контроллер Возьмём Android-телефон с OTG-контроллером. Это означает, что при сопряжении по USB с компьютером (для записи файлов с компьютера на телефон), он будет играть роль периферийного устройства, а при сопряжении по USB с периферийным устройством (мышью, клавиатурой, сетевой картой, флэш-накопителем, монитором, принтером) телефон будет играть роль хоста. [Service] Environment=TERM=xterm Обычно USB-контроллер периферийного устройства или USB OTG- контроллер присутствуют во встраиваемых устройствах. Также они могут быть интегрированы в однокристальную систему (SoC). Но по факту на устройстве может отсутствовать физический USB-разъём для подключения. Например, на всех Raspberry Pi установлена SoC, которая имеет OTG-контроллер, но фактически физический разъём для него есть только в Raspberry Pi Zero (Zero W, Zero 2 W) и в Raspberry Pi 4. ▍ Дескрипторы USB Каждое USB-устройство имеет дескрипторы. Дескрипторы — это информация о USB-устройстве, которая используется операционной системой для корректного выбора драйвера для устройства. Мне понравилось описание, которое приведено на сайте Microsoft. ▍ Создание USB-устройств в Linux Ядро Linux содержит модули, которые позволяют создавать виртуальные USB-устройства. Это может быть Mass Storage, последовательный порт, сетевая карта. Загрузив и настроив эти модули, вы можете сделать так, чтобы компьютером ваш одноплатник распознавался одним или несколькими такими устройствами.  Если вам достаточно одного устройства, то вы можете загрузить модуль для этого устройства, опционально передав ему параметры для конфигурации при помощи команды modprobe. Когда отпадёт необходимость в этом устройстве, его можно выгрузить при помощи команды modprobe -r. Чтобы на одном физическом порту у вас распознавалось несколько устройств одновременно, нужно использовать модуль libcomposite и сконфигурировать эти устройства при помощи создания структур в файловой системе ConfigFS в директории /sys/kernel/config/usb_gadget. Такие устройства называются композитными USB-устройствами. Вы, скорее всего, встречались с такими, например, если у вас беспроводная клавиатура и мышь, а для них используется один приёмопередатчик. В нашем случае мы создадим композитное USB-устройство, которое будет последовательным портом и устройством хранения. Последовательный порт мы будем использовать для подключения к нашему эмулятору оптических дисков через PuTTY. Изначально я хотел, что бы это была сетевая карта и SSH, но карта требует настройки в операционной системе компьютера, поэтому для простоты отказался от этой идеи в пользу последовательного порта. ▍ Создание композитного USB-устройства при помощи ConfigFS 1. Загружаем модуль libcomposite. 2. Заполняем дескрипторы для устройства. modprobe libcomposite $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ mkdir -p $usb_dev $ echo 0x0137 > $usb_dev/idProduct $ echo 0x0100 > $usb_dev/bcdDevice $ echo 0x0200 > $usb_dev/bcdUSB $ echo 0xEF  > $usb_dev/bDeviceClass $ echo 0x02 > $usb_dev/bDeviceSubClass $ echo 0x01 > $usb_dev/bDeviceProtocol 3. Создаём конфигурацию. 4.Создаём и подключаем функцию acm (последовательный порт через USB). 5. Создаём и подключаем функцию mass_storage. Mass_storage в данном случае — это эмуляция CD-ROM для ISO-образа /home/pi/1.iso. 6. Активируем созданное устройство. $ mkdir -p $usb_dev/strings/0x409 $ echo \"abababababababa\" > $usb_dev/strings/0x409/serialnumb $ echo \"Linux Foundation\" > $usb_dev/strings/0x409/manufactu $ echo \"USB CD-ROM Emulator\" > $usb_dev/strings/0x409/produc mkdir -p $usb_dev/configs/c.1 mkdir -p $usb_dev/configs/c.1/strings/0x409 echo \"acm+usb\" > $usb_dev/configs/c.1/strings/0x409/configur echo \"0x80\" > $usb_dev/configs/c.1/bmAttributes echo 250 > $usb_dev/configs/c.1/MaxPower $ mkdir -p $usb_dev/functions/acm.usb0 $ ln -s $usb_dev/functions/acm.usb0 $usb_dev/configs/c.1 $ mkdir -p $usb_dev/functions/mass_storage.usb0/lun.0 $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/cdrom $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/remova $ echo 0 > $usb_dev/functions/mass_storage.usb0/lun.0/nofua $ echo 0 > $usb_dev/functions/mass_storage.usb0/stall $ echo \"/home/pi/1.iso\" > $usb_dev/functions/mass_storage.us $ ln -s $usb_dev/functions/mass_storage.usb0 $usb_dev/config ▍ Удаление композитного USB-устройства при помощи ConfigFS 1. Деактивируем устройство. 2. Удаляем функцию mass_storage. 3. Удаляем функцию acm. 4. Удаляем конфигурацию. 5. Удаляем устройство. $ ls /sys/class/udc > $usb_dev/UDC $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ echo \"\"> $usb_dev/UDC $ rm $usb_dev/configs/c.1/mass_storage.usb0 $ rmdir $usb_dev/functions/mass_storage.usb0 $ rm $usb_dev/configs/c.1/acm.usb0 $ rmdir $usb_dev/functions/acm.usb0 $ rmdir $usb_dev/configs/c.1/strings/0x409 $ rmdir $usb_dev/configs/c.1 $ rmdir $usb_dev/strings/0x409 $ rmdir $usb_dev 6. Выгружаем загруженные устройством модули. Cтруктура файловой системы для созданного эмулятора CD-ROM Bluetooth Тема Bluetooth очень объёмная, и её невозможно изложить в одной статье, поэтому приведу только тот минимум, который позволяет понять, как мы будем использовать Bluetooth. Bluetooth — технология, которая позволяет связывать устройства без проводов по радиоканалу. На данный момент существует множество версий спецификации Bluetooth. Спецификация Bluetooth освещает $ modprobe -r usb_f_mass_storage $ modprobe -r usb_f_acm $ modprobe -r libcomposite множество вопросов. Чтобы передать данные с одного устройства на другое, необходимо наличие на обоих устройствах контроллеров и стеков Bluetooth. Bluetooth-контроллер — аппаратное устройство, обычно выполненное в виде микросхемы или части более сложной микросхемы, позволяющее получать/передавать данные по радиоканалу в соответствии со спецификацией Bluetooth. Bluetooth-стек — программная реализация протоколов, описанных в спецификации Bluetooth. Протоколы Bluetooth, предназначенные для решения определённых задач, группируются в профили Bluetooth. Мы будем использовать два профиля Bluetooth: 1. Generic Access Profile (GAP), который поддерживается всеми Bluetooth-устройствами. 2. Serial Port Profile (SPP), который подразумевает использование последовательного порта поверх соединения Bluetooth. ▍ Поддержка Bluetooth операционными системами Bluetooth-контроллеры могут иметь различные аппаратные интерфейсы для доступа. Это может быть UART, USB, PCIe. В случае операционной системы многие детали скрываются, и можно о них не думать. С контроллером можно работать на низком уровне через драйвер или уже используя высокоуровневые библиотеки и приложения, предоставляемые стеком Bluetooth, например, в Linux широко распространён стек BlueZ.  ▍ BlueZ Стек Bluetooth BlueZ состоит из двух частей. Одна часть представлена модулями ядра Linux, уже включена в ядро. Если она отсутствует, то её нужно включить и перекомпилировать ядро. Вторая часть представлена приложениями для пространства пользователя. Приложения позволяют конфигурировать и работать со стеком Bluetooth. На данный момент многие приложения считаются устаревшими, и разработчики BlueZ рекомендуют использовать более новые приложения и интерфейс D-Bus для работы со стеком. Но, как мне кажется, именно те старые, устаревшие приложения позволяют лучше понять работу Bluetooth, поэтому в учебных целях я буду использовать их, для чего нужно будет инициализировать BlueZ в режиме совместимости.  ▍ Протоколы Bluetooth Я не буду утомлять вас различными схемами, диаграммами, которые вы легко можете найти в интернете. Расскажу только о тех протоколах, с которыми нам предстоит столкнуться и нужно будет сконфигурировать.  ▍ Service Discovery Protocol (SDP) При помощи протокола SDP можно определить, какие приложения (сервисы) находятся на хосте, и с ними возможен обмен данными через Bluetooth Чтобы можно было увидеть сервис с другого устройства, его $ bluetoothd --noplugin=sap -C необходимо зарегистрировать в SDP database. Например, если мы хотим зарегистрировать службу, представляющую эмуляцию последовательного порта в Bluetooth, это можно сделать следующей командой: Чтобы можно было посмотреть службы, зарегистрированные у вас на хосте, нужно ввести команду: ▍ Radio Frequency Communications (RFCOMM) Протокол RFCOMM позволяет создавать виртуальное соединение по последовательному порту между двумя хостами. На одном из хостов создаётся сервер, которому выделяется канал RFCOMM, второй из хостов подключается к нему, указывая MAC- адрес и номер канала Канал RFCOMM немного напоминает порт в UDP или TCP, но если у них и у источника и у получателя есть порты, то у RFCOMM для источника и получателя один и тот же канал. Поэтому невозможно создать несколько подключений на один и тот же канал. В Linux можно использовать команду rfcomm для создания процесса, который будет слушать определённый канал RFCOMM и при соединении запускать другой процесс. В данном случае на Bluetooth-контроллере hci0 RFCOMM будет $ sdptool add SP $ sdptool browse local $ rfcomm -r watch hci0 1 /usr/local/bin/cdemu-cmd /dev/rfcom прослушиваться канал 1 и запускаться процесс cdemu-cmd с двумя параметрами командной строки /dev/rfcomm0 и /dev/rfcomm0. ▍ Утилита bluetoothctl Утилита Bluetoothctl позволяет сопрягать устройство, на котором вы её запустили с другим устройством. Вы можете сделать устройство видимым для обнаружения другими устройствами, а также найти другое устройство и выполнить с ним сопряжение. Более подробно расписано в документации к утилите, которая доступна по команде:  ▍ Serial Bluetooth Terminal Для отладки приложений, использующих Bluetooth, удобно использовать приложение для Android Serial Bluetooth Terminal. Это приложение позволяет работать с Bluetooth-устройствами, у которых доступен профиль SPP. В нашем случае мы будем использовать его как визуальный интерфейс для работы с нашим эмулятором оптических дисков. Сборка и модификация дистрибутива Raspberry Pi OS Чтобы сделать полноценный аппаратный эмулятор оптических дисков, нам придётся немного модифицировать исходный дистрибутив Linux. Это подразумевает перекомпиляцию ядра, изменение нескольких конфигурационных файлов и добавление своего программного кода. Для меня это было удобно сделать при помощи Docker. ▍ Кросс-компиляция ядра Linux Кросс-компиляция позволяет на компьютере с одной архитектурой $ man bluetoothctl получать исполняемые файлы для другой архитектуры. Мы можем компилировать ядро Linux для Raspberry Pi на Raspberry Pi, а можем, используя кросс-компиляцию, сделать это на обычном компьютере с архитектурой x86, что существенно сократит время компиляции из-за большего быстродействия компьютера. Подробно о том, как выполнять кросс-компиляцию Raspberry Pi OS, можно почитать тут. ▍ Chroot и запуск бинарных файлов другой архитектуры Команда Linux chroot позволяет запускать процессы с изменённой корневой системой. Это кажется немного запутанным, но суть в следующем. В качестве параметра команде передаётся путь к корневой директории. В результате запуска команды через chroot запущенный процесс будет считать, что корнем файловой системы является та директория, которую передали в качестве параметра.  Применений у команды chroot несколько, например, её можно использовать, чтобы запустить команду apt для Raspberry Pi в Docker-контейнере. Интересно, что Docker Desktop для Windows позволяет запускать исполняемые файлы для архитектуры ARM. В Linux-версии Docker такое сделать можно, но нужна дополнительная настройка. Реализация Созданный мной проект состоит из следующих файлов: 1. Dockerfile и скрипт, который выполняется в Docker-контейнере. 2. Файлы, которые необходимо добавить или обновить в исходном дистрибутиве: • cdemu — основная логика работы эмулятора оптических дисков, написанная на языке bash; • cdemu-cmd — bash-скрипт для обработки команд от пользователя и передачи их эмулятору; • bash-utils.sh — bash-скрипт co вспомогательными функциями; • cdemu-bluetooth-ui.service — systemd-служба, которая запускает интерпретатор команд на создаваемом RFCOMM- соединении телефона и Raspberry Pi; • cdemu.service — systemd-служба, которая запускает эмулятор оптических дисков при загрузке; • bluetooth.service — изменённая служба systemd для инициализации bluetooth; • serial-getty@.service — изменённая служба systemd для запуска agetty на создаваемом соединении на последовательном порту; • firstboot.service — служба systemd для запуска скрипта при первой загрузке операционной системы. Я её позаимствовал из проекта raspberian-firstboot; • config.txt — изменённый файл конфигурации для загрузки Raspberry Pi. Содержит подключение overlay dwc. Это необходимо, чтобы USB-контроллер мог работать в device mode; • fistboot.sh — скрипт, который запускается службой systemd firstboot.service; • userconf.txt — файл, который необходим, чтобы установить пароль для пользователя pi. В последних версиях Raspberry Pi OS пользователь pi не активирован по умолчанию, поэтому необходимо наличие этого файла; • ssh — файл необходим, чтобы активировать ssh, который отключён по умолчанию; • wpa_supplicant.conf — файл, необходимый, если вы хотите настроить Raspberry Pi на работу с вашей точкой доступа. Листинги файлов не привожу, так как это ещё больше раздует и так большую статью.  Ознакомиться вы с ними можете здесь. Как пользоваться эмулятором 1. Собираем Docker-образ. 2. Собираем образ RaspberryPi OS. 3. Записываем образ на SD-карту. Вставляем её в Raspberry Pi. 4. Подключаем Rapsberry Pi Zero 2 W к компьютеру. 5. Через некоторое время у вас появится съёмный накопитель. 6. На этот съёмный накопитель, содержащий файл Readme.txt, копируем образы, которые хотим эмулировать. 7. Находим виртуальный COM-порт, созданный после подключения Raspberry Pi к компьютеру. 8. Подключаемся к Raspberry Pi через с помощью Putty через виртуальный COM-порт. 9. Запускаем интерактивное приложение для управления эмулятором. 10. Если хотим сделать управление с телефона, то выполняем сопряжение телефона и Raspberry Pi. Для чего вводим в эмуляторе терминала команду: docker build -t raspi-image . docker run --privileged -v c:\\temp:/build --name raspi-image $ sudo cdemu-cmd $ sudo bluetoothctl 11. Делаем Raspberry Pi доступным для обнаружения: 12. Находим его на телефоне и выполняем полключение. После чего соглашаемся с PIN-кодом на телефоне и Raspberry Pi. 13. Выходим из bluetoothctl. 14. Запускаем на телефоне Serial Bluetooth Terminal и выполняем подключение к Raspberry Pi из него. Теперь можно посылать команды созданному эмулятору CD-ROM. Команды, которые можно посылать эмулятору: 1. hdd — переключение в режим эмуляции внешних жёстких дисков. 2. cdrom — переключение в режим эмуляции внешних приводов оптических дисков. 3. list — вывести список доступных ISO-образов, которые можно эмулировать. 4. insert <порядковый номер> — поместить ISO-образ для эмуляции. 5. eject — извлечь ISO-образ из эмулятора. 6. help — показать список доступных команд в текущем режиме. Особенности моего эмулятора discoverable on yes exit Интересно, но в Linux по умолчанию нельзя эмулировать ISO-образы размером больше 2 Gib. Я просмотрел исходный код драйвера в файле drivers/usb/gadget/function/storage_common.c и предположил, что нет оснований не применять патч к ядру Linux от Adam Bambuch, который просто удаляет одно условие. Образы эмулировались нормально и при снятии ISO-образа с эмулируемого CD-ROM он был идентичен исходному. Поверял по хеш-коду для файла ISO-образа. Не пойму, почему есть это ограничение в Linux и почему его до сих пор не убрали? Если вы знаете ответ, ответьте в комментариях. Моя реализация не требует никаких дополнительных деталей. Нужен только Raspberry Pi Zero 2 W, один или два кабеля USB и адаптер питания, если будете использовать два кабеля USB. Один для питания, второй для передачи данных. Хоть и использование дополнительного кабеля и адаптера добавляет громоздкости, это решает проблему перезагрузки Raspberry Pi, если компьютер или ноутбук отключает ненадолго питание при перезагрузке. Кроме того, я не использую Python, только bash. Выводы Полученное программно-аппаратное решение, хоть и обладает рядом недостатков по сравнению с карманом Zalman (не поддерживается USB 3.0, нет интерактивного меню на самом устройстве), позволит вам установить практически любую операционную систему на широкий спектр компьютеров путём простого копирования ISO-образа.  Решение является прототипом, но вместе с тем позволяет углубить знания по многим темам, или получить, если вы были с ними не знакомы. Так как основной целью была разработка прототипа, я запускал bluetoothd в режиме совместимости, и почти всю логику написал на bash. Я хотел показать возможность превратить встраиваемое устройство с операционной системой Linux в аппаратный эмулятор флеш- накопителя или привода оптических дисков, приложив минимум усилий. Надеюсь, что это удалось. Чтобы уместить всё в одной статье, я лишь поверхностно коснулся тех тем, которые необходимы для понимания. Если вас заинтересовало, вы можете самостоятельно изучить их углублённо. Объём статьи не позволяет осветить все интересности, с которыми я столкнулся при разработке эмулятора, и решения, которые применял и проверял. Приведу лишь несколько из них. Например, я долго боролся с зависанием при удалении составного устройства. Помогло использование службы serial-getty вместо getty, хотя во многих статьях упоминалась getty. Я долго разбирался, как можно сделать сопряжение через Bluetooth между Raspberry и телефоном, использовал команду bt-agent, но потом всё-таки отказался от неё в пользу bluetoothctl.  При переключении эмулятора в режим HDD для записи ISO-образов изначально я открывал для доступа всю SD-карту и хранил ISO- образы в отдельном разделе, но потом посчитал, что для безопасности лучше хранить образ диска с ISO-образами в отдельном файле и открывать доступ только к нему, хоть это и снизило скорость записи, но пользователя не обескураживают появляющиеся несколько дисков. Разработанный прототип есть куда улучшать. Можно, например, создать более минималистичный дистрибутив Linux, который будет содержать только то, что реально используется для эмуляции, или создать более удобное графическое приложение для Android для работы с эмулятором. А можно упростить работу с Bluetooth, напрямую работая с драйверами bluetooth или используя интерфейс D-Bus для работы с Bluetooth-устройствами. Или вообще всё-таки взяться за паяльник и сделать устройство, более похожее по функционалу на карман Zalman. Но главное, вы увидели, что это реально сделать, а когда видишь положительный результат, это вдохновляет на большее творчество. В процессе тестирования и отладки программного кода было замечено, что на Lenovo X1 Extreme Gen 2 эмулятор CD-ROM дисков великолепно определялся в Windows 10, но отказывался определяться в BIOS. Эмпирически было определено, что помогает отключение режима экономии энергии процессора в BIOS. Также ноутбук отключал питание на usb при перезагрузке, поэтому понадобилось дополнительное питание Raspberry Pi. Интересно, но на ASUS K53E и Gigabyte BRIX всё работает без проблем. Решение с небольшими модификациями можно реализовать на Raspberry Pi 4. Но если вы поняли суть решения, вы его сможете повторить и на других одноплатных компьютерах, которые имеют выведенные USB-порты для OTG или USB-контроллеры периферийных устройств. Dockerfile на данный момент только выполняется в Docker Desktop для Windows. В Linux он работать не будет. В заключение хочу сказать, что существует ещё один способ эмулировать оптические диски, который я не пробовал, но знаю о его существовании из ваших комментариев к одной из моих статей — это программа DriveDroid для Android. Я ей не пользовался, так как для её работы нужно получать права root на телефоне. Но, скорее всего, из-за ограничений в ядре Linux программа поддерживает ISO- образы до 2 Gib и/или работает только с гибридными ISO-образами. Если я не прав, буду рад увидеть ваши опровержения в комментариях. RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 84 Карма 82.1 Рейтинг @artyomsoft Пользователь Комментарии 17 Публикации Telegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх  ️ Теги:   linux kernel , usb , bluetooth , bluez , linux modules , эмуляция , iso , cd-rom , systemd , agetty , ruvds_статьи Хабы:   Блог компании RUVDS.com , Системное администрирование , Разработка под Linux , Разработка на Raspberry Pi , DIY или Сделай сам +84 98 17 ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИНФОРМАЦИЯ Сайт ruvds.com Дата регистрации 18 марта 2016 Дата основания 27 июля 2015 Численность 11–30 человек Местоположение Россия Представитель ruvds Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "9",
         "getmatch",
         null,
         "414.35",
         "Весь интернет уже готовится к выходу Windows 12. Вероятно,",
         "Рассказываем о том, как строить карьеру в IT",
         "4.74 Оценка 414.35 Рейтинг getmatch Рассказываем о том, как строить карьеру в IT Весь интернет уже готовится к выходу Windows 12. Вероятно, следующая версия будет во многом сосредоточена на поддержке работы с ИИ. На это намекает в том числе недавняя утечка от Intel, а  4 часа назад 5 мин Arnak Новые утечки. Что мы знаем о выходе Windows 12 6.3K Блог компании getmatch ,  Разработка под Windows* ,  Софт ,  Искусственный интеллект КАК СТАТЬ АВТОРОМ также последние действия AMD и направление развития Bing. Мы знаем, что и Intel, и Microsoft готовятся к поддержке работы Windows 12 на новых процессорах. Об этом нам говорит утечка от leaf_hobby, который известен тем, что раскрывает полные спецификации чипов Intel Xeon перед их запуском. На этот раз leaf_hobby опубликовали подробности аппаратных показателей десктопной платформы Intel Meteor Lake, которую планируется выпустить в этом или в следующем году. Сейчас их твиты уже защищены, но интернет помнит™, а у журналистов отдельных изданий, вроде Insider и The Verge, к ним есть доступ. Сообщается, что Intel в имейле для своих сотрудников упоминает, что ее процессоры следующего поколения будут поддерживать Windows 12. Для этого Meteor Lake должен содержать 20 линий PCIe Gen5. Хотя Microsoft не объявляла о каких-либо планах в отношении Windows 12, уже есть признаки того, что компания планирует будущие версии Windows с расчетом на глубокую интеграцию функций ИИ. Даже последнее обновление Windows 11 уже интегрирует новую версию Bing с искусственным интеллектом прямо в панель задач. А ещё в этом обновлении бизнес-пользователи получают рекомендованный ИИ контент в меню «Пуск». Скоро искусственный интеллект придет в Edge, а потом — и во все остальные аспекты продуктов Microsoft. Компания очень воодушевлена успехом своей инвестиции в ChatGPT, и собирается сделать на него очень сильную ставку в надежде победить конкурентов, в том числе Google, у которой дело с инвестициями в ИИ пошло не так хорошо. Один из слайдов от Intel В начале января руководитель разработки Windows Паноса Паная на CES заявил, что «ИИ собирается заново переизобрести всё то, что вы делаете в Windows». Команда Microsoft Surface также сообщила, что «ИИ изменит то, как мы используем ПК». Всё однозначно указывает на то, что в следующей версии Windows компания собирается очень широко использовать ИИ. Это будет главная новая «фишка» платформы. А чтобы всё это могло работать, Microsoft приходится тесно сотрудничать с партнерами по аппаратному обеспечению, такими как Intel и AMD. Ей нужно, чтобы новые процессоры смогли справляться с высокими рабочими нагрузками ИИ, и без ошибок работали с новой технологией. Это объясняет то, почему Intel, как сообщается, уже упоминает об оптимизации под Windows 12 внутри компании. AMD недавно выпустила свои мобильные процессоры Ryzen 7000, и похвасталась тем, что они являются первыми процессорами x86, содержащими специальный механизм искусственного интеллекта, способный поддерживать эффекты Windows Studio. Эти функции, в том числе удаление фонового шума, размытие фона и автоматическое кадрирование, будут доступны только для техники с ускорителями на чипах ARM, которые работают с помощью специального нейронного процессора (NPU). То есть, AMD и Microsoft сейчас тоже активно совместно работают над внедрением расширенного числа функций на основе ИИ в свои продукты. И такие возможности, как заявляют компании, в дальнейшем будут более широко доступны в традиционных ноутбуках с Windows, работающих на AMD. Возможно, это тоже является подготовкой новых поколений процессоров к запуску с Windows 12. Все эти даты очень хорошо стыкуются. Новые процессоры от Intel и AMD ожидают в конце этого года или в 2024-м. А на разработку новой версии ОС у Microsoft раньше уходило 2-3 года. Windows 11 была выпущена в октябре 2021-го. Так что, если новые утечки верны, следующую основную версию Windows можно ожидать в 2024 году — как раз под ряд новых процессоров. Как она будет выглядеть То, что Windows 12 точно находится в разработке, мы узнали и из утечек декабря 2022 года. Тогда во время мероприятия Ignite Keynote 2022 компания Microsoft (вполне возможно, намеренно) «засветила» интерфейс следующего большого обновления Windows. Потом об этом зудел весь интернет, споря о том, что это могло быть. Что, согласитесь, совсем неплохо для одного скриншота. Изображение, «случайно» показанное на Ignite Keynote, демонстрирует ряд нехарактерных для Windows 11 особенностей: плавающая (а не доходящая до краев экрана) панель задач; плавающее меню поиска; системные значки в верхней (а не привычной нижней) части экрана справа. И интересный виджет погоды слева вверху. Видно, что версия будет заметно отличаться от всех предыдущих. Коллективный разум сейчас считает, что она выйдет, скорее всего, в 2024 году, и будет называться Windows 12 как минимум из-за большого количества разных новшеств — так же, как это было с Windows 11. Скорее всего, так будет выглядеть следующая ОС Следующее крупное обновление Windows пока находится на стадии прототипа, но источники, знакомые с ситуацией, говорят, что показанный во время Ignite интерфейс отображает то, к чему стремится Microsoft в следующей версии своей операционной системы. На мероприятии показали только часть возможных новшеств — в реальности их будет больше. Например, ожидается, что в Windows 12 появится новый экран блокировки, новый центр уведомлений, новый поиск. Ну и ИИ везде, где только можно. В плане интерфейса глобальная цель — сделать его более удобным для сенсорного ввода. У Microsoft пока всё не очень хорошо с балансом по этой части: Windows 8 была чересчур сильно ориентированной именно на «тач-взаимодействие», а Windows 10, наоборот, заточена под работу с мышью и клавиатурой. В Windows 11 компания попыталась найти золотую середину, но не получилось: всё-таки удобство управления при помощи мыши однозначно осталось на первом месте. Туманная перспектива Облачная, точнее. Ходят не очень приятные слухи (впрочем, смотря с какой стороны посмотреть). Что, мол, Windows 12 может стать первой облачной ОС от Microsoft. В последние годы Сатья Наделла переносит всё в сеть, в том числе MS Office. И говорит, что за этим будущее, а «облако изменит всё». Microsoft 365 активно развивается, в том числе за счет Windows 365, облачного компьютера, который можно использовать на любом устройстве, способном поддерживать браузер. Сервис был запущен компанией в 2021 году, работать так можно как с Windows 10, так и с Windows 11. Компания позиционирует это как альтернативу облачным сервисам Google: теперь у неё в облаке тоже можно и хранить данные, и редактировать документы, и даже ходить по меню «Пуск» и папкам в привычном интерфейсе, даже если у вас стоит Ubuntu или вы вообще сидите с планшетом на Android. Если Windows 12 будет доступна только в виде такой облачной платформы (по крайней мере, изначально) — это может серьезно повысить видимость пакета 365 в глазах рядовых пользователей. А там, глядишь, многие и задумаются, не удобнее ли им так будет хранить данные, и продолжать работать в одной ОС с разных устройств. Система может быть во многом похожа на традиционные варианты виртуальных рабочих столов, которые уже много лет развертываются предприятиями для поддержки сценариев BYOD. Но с более простой структурой ценообразования, удобным запуском и ориентацией на более широкую аудиторию. Это также может стать более контролируемым способом для Microsoft проверить механизмы работы своих новых ИИ-помощников. При запуске облачной ОС им не придется отпускать ИИ «гулять» на каждый из компьютеров пользователей, все процедуры будут мониториться на собственных серверах. Будем надеяться, что это один из тех наборов слухов, которые скоро будут опровергнуты. Возможно, команда поэкспериментировала с этой идеей, и поняла, что она не приживается. Но пока что — нужно быть морально готовыми к тому, что со всё большим числом сервисов, в том числе от Microsoft, мы будем взаимодействовать через окно браузера. getmatch Рассказываем о том, как строить карьеру в IT Telegram Сайт ВКонтакте Twitter 160 Карма 198.6 Рейтинг @Arnak Пользователь P.S. Тысячи крутых вакансий в РФ и за рубежом — в телеграм-боте getmatch. Задаете нужную зарплату, и к вам приходят лучшие предложения, а наши эксперты помогают пройти интервью. Не нужно ни резюме, ни портфолио, настройка занимает меньше 30 секунд. А ещё это отличный способ следить за текущим рынком труда. Откликаться не обязательно. Теги:   windows , microsoft , windows 12 , AI , intel , chatgpt Хабы:   Блог компании getmatch , Разработка под Windows , Софт , Искусственный интеллект +11 8 32 Комментарии 32 Публикации ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИНФОРМАЦИЯ Сайт getmatch.ru Дата регистрации 30 июля 2020 Численность 51–100 человек Местоположение Россия Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NameCompany</th>\n",
       "      <th>Description</th>\n",
       "      <th>Raiting</th>\n",
       "      <th>DataPublish</th>\n",
       "      <th>Activity</th>\n",
       "      <th>TextArticle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red_mad_robot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.94</td>\n",
       "      <td>6 часов назад</td>\n",
       "      <td>№1 в разработке цифровых решений для бизнеса</td>\n",
       "      <td>117.94 Рейтинг red_mad_robot №1 в разработке ц...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cloud4Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.07</td>\n",
       "      <td>Автор оригинала: Yashwant Singh</td>\n",
       "      <td>#1 Корпоративный облачный провайдер</td>\n",
       "      <td>71.07 Рейтинг Cloud4Y #1 Корпоративный облачны...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cloud4Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.07</td>\n",
       "      <td>Автор оригинала: Yashwant Singh</td>\n",
       "      <td>#1 Корпоративный облачный провайдер</td>\n",
       "      <td>71.07 Рейтинг Cloud4Y #1 Корпоративный облачны...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southbridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280.79</td>\n",
       "      <td>Автор оригинала: Seifeldin Mahjoub</td>\n",
       "      <td>Обеспечиваем стабильную работу highload-проектов</td>\n",
       "      <td>4.29 Оценка 280.79 Рейтинг Southbridge Обеспеч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Альфа-Банк</td>\n",
       "      <td>NaN</td>\n",
       "      <td>404.32</td>\n",
       "      <td>Подписаться</td>\n",
       "      <td>Лучший мобильный банк по версии Markswebb</td>\n",
       "      <td>404.32 Рейтинг Альфа-Банк Лучший мобильный бан...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RUVDS.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2376.9</td>\n",
       "      <td>Автор оригинала: Lahiru Hewawasam</td>\n",
       "      <td>VDS/VPS-хостинг. Скидка 15% по коду HABR15</td>\n",
       "      <td>2376.9 Рейтинг RUVDS.com VDS/VPS-хостинг. Скид...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Хабр</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353.18</td>\n",
       "      <td>8 февраля — день российской науки. Этот праздн...</td>\n",
       "      <td>Экосистема для развития людей, вовлеченных в IT</td>\n",
       "      <td>4.58 Оценка 353.18 Рейтинг Хабр Экосистема для...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RUVDS.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2394.92</td>\n",
       "      <td>Несмотря на то, что постепенно оптические диск...</td>\n",
       "      <td>VDS/VPS-хостинг. Скидка 15% по коду HABR15</td>\n",
       "      <td>2394.92 Рейтинг RUVDS.com VDS/VPS-хостинг. Ски...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RUVDS.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2394.92</td>\n",
       "      <td>20 мар в 12:00</td>\n",
       "      <td>VDS/VPS-хостинг. Скидка 15% по коду HABR15</td>\n",
       "      <td>2394.92 Рейтинг RUVDS.com VDS/VPS-хостинг. Ски...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>getmatch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>414.35</td>\n",
       "      <td>Весь интернет уже готовится к выходу Windows 1...</td>\n",
       "      <td>Рассказываем о том, как строить карьеру в IT</td>\n",
       "      <td>4.74 Оценка 414.35 Рейтинг getmatch Рассказыва...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     NameCompany  Description  Raiting  \\\n",
       "0  red_mad_robot          NaN   117.94   \n",
       "1        Cloud4Y          NaN    71.07   \n",
       "2        Cloud4Y          NaN    71.07   \n",
       "3    Southbridge          NaN   280.79   \n",
       "4     Альфа-Банк          NaN   404.32   \n",
       "5      RUVDS.com          NaN   2376.9   \n",
       "6           Хабр          NaN   353.18   \n",
       "7      RUVDS.com          NaN  2394.92   \n",
       "8      RUVDS.com          NaN  2394.92   \n",
       "9       getmatch          NaN   414.35   \n",
       "\n",
       "                                         DataPublish  \\\n",
       "0                                      6 часов назад   \n",
       "1                    Автор оригинала: Yashwant Singh   \n",
       "2                    Автор оригинала: Yashwant Singh   \n",
       "3                 Автор оригинала: Seifeldin Mahjoub   \n",
       "4                                        Подписаться   \n",
       "5                  Автор оригинала: Lahiru Hewawasam   \n",
       "6  8 февраля — день российской науки. Этот праздн...   \n",
       "7  Несмотря на то, что постепенно оптические диск...   \n",
       "8                                     20 мар в 12:00   \n",
       "9  Весь интернет уже готовится к выходу Windows 1...   \n",
       "\n",
       "                                           Activity  \\\n",
       "0      №1 в разработке цифровых решений для бизнеса   \n",
       "1               #1 Корпоративный облачный провайдер   \n",
       "2               #1 Корпоративный облачный провайдер   \n",
       "3  Обеспечиваем стабильную работу highload-проектов   \n",
       "4         Лучший мобильный банк по версии Markswebb   \n",
       "5        VDS/VPS-хостинг. Скидка 15% по коду HABR15   \n",
       "6   Экосистема для развития людей, вовлеченных в IT   \n",
       "7        VDS/VPS-хостинг. Скидка 15% по коду HABR15   \n",
       "8        VDS/VPS-хостинг. Скидка 15% по коду HABR15   \n",
       "9      Рассказываем о том, как строить карьеру в IT   \n",
       "\n",
       "                                         TextArticle  \n",
       "0  117.94 Рейтинг red_mad_robot №1 в разработке ц...  \n",
       "1  71.07 Рейтинг Cloud4Y #1 Корпоративный облачны...  \n",
       "2  71.07 Рейтинг Cloud4Y #1 Корпоративный облачны...  \n",
       "3  4.29 Оценка 280.79 Рейтинг Southbridge Обеспеч...  \n",
       "4  404.32 Рейтинг Альфа-Банк Лучший мобильный бан...  \n",
       "5  2376.9 Рейтинг RUVDS.com VDS/VPS-хостинг. Скид...  \n",
       "6  4.58 Оценка 353.18 Рейтинг Хабр Экосистема для...  \n",
       "7  2394.92 Рейтинг RUVDS.com VDS/VPS-хостинг. Ски...  \n",
       "8  2394.92 Рейтинг RUVDS.com VDS/VPS-хостинг. Ски...  \n",
       "9  4.74 Оценка 414.35 Рейтинг getmatch Рассказыва...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56671587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260b379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NameCompany",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Description",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Raiting",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DataPublish",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Activity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TextArticle",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2a034852-10a5-4559-a326-1dff38fa75a7",
       "rows": [
        [
         "0",
         "red_mad_robot",
         null,
         "117.94",
         " 6 часов назад",
         "№1 в разработке цифровых решений для бизнеса",
         "117.94 Рейтинг red_mad_robot №1 в разработке цифровых решений для бизнеса  6 часов назад Средний18 мин redmadrobot Cocoapods, Carthage, SPM: как выбрать менеджер зависимостей в iOS 179 Блог компании red_mad_robot ,  Разработка под iOS* FAQ КАК СТАТЬ АВТОРОМ Старший iOS-разработчик red_mad_robot Аня Кочешкова рассказывает, чем отличаются три менеджера зависимостей, в каких случаях и для каких задач подойдёт тот или иной. Материал будет полезен джун-специалистам, которые только начали погружаться в разработку: специально для них подробно объясняем, что такое семантическое версионирование, как устроены модули кода и в чём разница между динамическими и статическими библиотеками. Что такое менеджер зависимостей и зачем он нужен В современной разработке зависимость — это написанный кем-то другим код, который используется в вашей программе. Добавление зависимости позволяет воспользоваться уже сделанной кем-то хорошей работой и избежать «изобретения велосипеда», а также лишних действий: проектирования, написания, тестирования, отладки и поддержки определённой логики. Такие «куски логики» называют пакетами, библиотеками или модулями. Чем сложнее проект, тем больше сторонних библиотек (правда, некоторые приложения могут вообще отказываться от их использования), а у этих библиотек много разных версий. Они могут ссылаться друг на друга и накладывать ограничения на эти версии. Эту работу можно проиллюстрировать таким бытовым примером. Чтобы телефон показал погоду на улице, не нужно встраивать в него термометр и вытаскивать за окно. В нём уже зашита специальная технология, которая позволяет ему связаться с «библиотекой», где хранятся данные гидрометцентра, которые, как и погода, всё время обновляются. Иногда с новой версией добавляется какая-нибудь новая функциональность. В примере с термометром библиотека с новой версией умеет ещё и выдавать карту дождей, а до этого могла показывать только температуру. Менеджеры зависимостей позволяют избежать долгого и муторного решения такого набора «уравнений» — то есть берут на себя часть работы, которую без них разработчик делал бы руками. Это система, которая позволяет управлять зависимостями в вашем проекте. А ещё у сторонних библиотек часто могут выходить новые версии с исправленными ошибками, и благодаря менеджеру зависимостей вам не нужно следить за их выходом — он всё обновит за вас. Предположим, вы решили подключить в ваш проект Firebase Crashlytics и GoogleUtilities. Обе эти библиотеки ссылаются на библиотеку PromisesObjC. Firebase хочет, чтобы она была любой версии, начинающейся с 2.1, а GoogleUtilities — чтобы она была от 1.2 до 3.0. Такое уравнение, конечно, можно решить вручную, но что делать, когда этих уравнений несколько десятков, а уровней зависимостей — два, три или больше? Для таких задач в распоряжении iOS-разработчика существуют три основных менеджера зависимостей: 1. Cocoapods. Пожалуй, самый распространённый сегодня, он требует лишь указывать список зависимостей в определённом формате в текстовом файле. 2. Carthage. Менее удобен в этом плане: вы также указываете список зависимостей в текстовом файле, но помимо этого нужно ещё немного «самостоятельной работы», о которой расскажем ниже. 3. SPM, или Swift Package Manager. Официальный менеджер зависимостей от Apple. Появился недавно, и многие команды переходят на него с Cocoapods. Позволяет управлять зависимостями как через графический, так и через текстовый интерфейс прямо из Xcode. Случается, что нужная библиотека не поддерживает необходимый менеджер зависимостей, потому что разработчики каждой библиотеки сами выбирают, какой менеджер поддержать. Поддержка каждого менеджера — это работа, которая занимает определённое время, поэтому часто выбирают работать только с самым популярным менеджером, Сocoapods. А если в проекте, например, SPM, а библиотека есть только в Cocoapods или просто нужна в одной конкретной версии, приходится добавлять вручную как framework / xcframework, о чём мы также поговорим позже. Но прежде чем начать говорить об управлении зависимостями, нужно разобрать несколько связанных с ним терминов. Фантастическое Semantic Versioning, и где оно обитает Semantic Versioning (SemVer) — это стандарт, который говорит о том, как работать с версиями пакетов/библиотек/приложений, — как их указывать и накладывать на них ограничения. Зачем накладывать ограничения? Возьмём такой пример. Разработчик создаёт приложение и использует некоторый набор библиотек для упрощения работы — например, библиотек для работы с сетью Alamofire. Допустим, начиная с определённой версии в библиотеке появляется поддержка какой-то новой функциональности, например Modern Concurrency. Эта функциональность доступна только начиная с определенной версии библиотеки. В таком случае от разработчика потребуется всего лишь поднять версию Alamofire в манифесте, где он описывает используемые библиотеки. Если начиная с какой-то версии библиотеки исправляется некий важный баг, но не сильно меняется функциональность самой библиотеки, то обновление до свежей исправленной версии произойдёт без лишнего вмешательства разработчика, в следующий раз при обновлении текущих библиотек проекта. Разберём, что такое SemVer, чтобы понимать формат версий, с которыми придётся работать, а также узнаем, как можно указать версию зависимости и наложить на неё ограничения. Этот набор правил — глобальный стандарт, который используется повсеместно, начиная от пакетов Node.js и заканчивая версиями приложений в App Store. По этому стандарту номер версии — это строка, которая состоит из чисел, разделённых точкой, и имеет формат A.B.C, где A — мажорная версия, B — минорная версия и C — патч-версия. Версии сравниваются слева направо, например: 1.0.0 < 2.0.0 < 2.1.0 < 2.1.1. Но зачастую мажорная версия увеличивается или при выпуске каких- либо масштабных фич, или при редизайне приложения, или просто при желании обозначить принципиальное обновление приложения/библиотеки. Синтаксис SemVer в Cocoapods, Carthage и SPM Cocoapods В Cocoapods можно указать одну из стратегий выбора версии: 1. Если необходима конкретная версия, просто укажите её без дополнительных символов, как есть: '5.0.0'. 2. Если необходима точная версия, но при этом вы также хотите обновляться, если придут исправления багов в патч-версии, используйте операнд ~>: '~> 5.0.0'. 3. Можно указывать неравенства: '> 5.0.0', '≥ 5.0.0' или '< 5.0.0', '≤ 5.0.0'. В этом случае Cocoapods обновит библиотеку до 5.0.1, 5.0.2 и т. д., но не возьмёт обновление 5.1.0, так как в нём уже повышена минорная, а не патч-версия. Если вы также хотите завязаться на минорную версию, достаточно просто указать '~> 5.0'. Carthage В Carthage используется тот же формат, что и в Cocoapods, с тем только исключением, что точная версия указывается через равенство: == 5.0.0. SPM SPM позволяет указать стратегию выбора версии прямо в интерфейсе, доступны следующие: Аналогичный синтаксис и у Package.swift файла. Модули — что к чему: библиотеки, фреймворки, XCFramework и Swift-пакеты Организация кода в iOS построена на понятии модулей. Модуль — это отдельный кусок кода или функциональности, который можно распространять разными способами и переиспользовать. Существует четыре вида модулей. Разберёмся, в чём разница: 1. Библиотека — это исполняемый код, скомпилированный под конкретную архитектуру. 2. Фреймворк — папка, содержащая в себе библиотеку и вспомогательные файлы. 3. XCFramework — набор фреймворков под разные архитектуры. 4. Swift-пакет — фреймворк, который используется для работы со Swift Package Manager. Если говорить на языке бытовых примеров, то код — это комната, а модули — разные предметы в ней. Торшер, например, позволяет включать и выключать свет, но как он работает внутри, мы не знаем. И этим торшером могут пользоваться в разных квартирах и комнатах разные люди. Библиотеки Библиотека — это исполняемый код, скомпилированный под конкретную архитектуру, например arm64. Архитектуры могут различаться для разных типов устройств и симуляторов: • iOS-устройства — архитектуры armv7, armv7s, arm64, • iOS-симулятор — архитектуры i386 и x86_64. Исходный код — текст программы, который написал разработчик. Исполняемый код — код, который получился в результате компиляции. Компилятор переводит исходный код в машинный, который сможет исполнить операционная система, — на выходе получается исполняемый код. Библиотеки бывают статическими и динамическими (об этом позже), и представляют собой один бинарный файл. Если представить, что библиотека — это настоящая библиотека со стеллажами, в которых хранятся книги, то эти книги — справочники о том, как что-то сделать (методы для выполнения разных задач). По сути, это скомпилированный набор объектных файлов, которые программа может использовать для выполнения любой задачи. То есть все файлы, которые библиотека собирается использовать, были скомпилированы до этапа компоновки. Фреймворки Фреймворк (.framework) представляет собой папку с файлами, в которой лежит сама библиотека, хедеры и ресурсы. Эта папка должна иметь определённую структуру, но нет нужды запоминать её, так как фреймворки, как правило, собираются самим Xcode. Поддержка фреймворков добавилась в iOS 8. Фреймворки служат той же цели, что и библиотеки, но могут включать в себя различные ресурсы — картинки, документацию, строковые файлы. Они также могут быть статическими или динамическими. Что использовать: фреймворк или библиотеку В чём же разница между фреймворком и библиотекой? И когда их использовать? Принципиальное различие — в архитектуре. XCFrameworks Для начала стоит рассказать, что такое fat-фреймворки. Это такие фреймворки, которые содержат библиотеки для разных архитектур и платформ, соединённые в одну. С появлением Xcode 11 на замену им пришли XCFrameworks, которые ввели структуру, позволяющую разнести библиотеки для разных платформ и архитектур по папкам. XCFramework — это, по сути, такой усовершенствованный fat- фреймворк. Он также содержит различные библиотеки для разных архитектур и платформ, но теперь нет необходимости трудиться над тем, чтобы объединить их в одну. В одном XCFramework может лежать несколько разных фреймворков — для iOS, macOS, tvOS, watchOS, а также для разных архитектур — armv7, x86_64 и т. д., каждый в своей папке. В каком-то смысле фреймворк — тоже разновидность библиотеки, а XCFramework — разновидность фреймворка, но всё же эти понятия принято разделять. Создатель зависимости сам выбирает, в каком виде её распространять. Нам важно понимать различие между этими видами, выбор можно сделать, только создавая свою библиотеку. Swift Package Swift-пакет используется для распространения только через SPM. Содержит исходный код, а также Package.swift файл, который описывает его конфигурацию. Доступен начиная со Swift 3. Static vs Dynamic Прежде чем начать, наконец, говорить об управлении зависимостями, важно разобраться, в чём различие между статическими и динамическими библиотеками и фреймворками. Первое, что здесь важно понимать: системные iOS- и macOS- библиотеки — динамические. Приложения могут хранить ссылку на библиотеку. Например, системная библиотека Foundation — динамическая. Она лежит на айфоне в единственном экземпляре, а все приложения лишь ссылаются на неё. Фреймворки, как и библиотеки, бывают динамические и статические и представляют собой обёртку библиотеки. Так, динамические библиотеки вне фреймворков не поддерживаются на iOS, watchOS и tvOS, и здесь важно отметить, что во всех дальнейших сравнениях, говоря «динамическая библиотека», мы будем иметь в виду её использование в рамках динамического фреймворка. Фреймворки в Cocoapods и Carthage Зависимости, распространяемые через Cocoapods, называются кокоаподами, или подами. До iOS 8 под представлял собой fat- статическую библиотеку. Есть специальная инструкция — по ней Cocoapods использует фреймворки вместо статичных библиотек, которые не поддерживались Swift до Xcode 9 и CocoaPods 1.5.0. Так что раньше эта директива была обязательна, сейчас — опциональна. Если нужно более быстрое время запуска, можно её убрать (см. предыдущий раздел). Carthage поддерживает только фреймворки. target 'TargetName' do   use_frameworks! end Три менеджера зависимостей: Cocoapods, Carthage, SPM Наконец, то, ради чего мы тут собрались, — менеджеры зависимостей. Посмотрим, какие они бывают, чем отличаются и как выбрать между ними. Cocoapods Это один из самых популярных сегодня менеджеров зависимостей. Позволяет указать список зависимостей в текстовом файле, а затем генерирует на основе его .xcworkspace, который будет содержать два проекта — ваш проект и проект с подами. Как правило, почти все библиотеки поддерживают Cocoapods, хотя с развитием SPM стали появляться и такие, которые поддерживают исключительно SPM (в основном это библиотеки Apple). В таком случае, если ваш менеджер зависимостей Cocoapods, единственный вариант воспользоваться библиотекой — вручную её собрать и прилинковать к проекту. Чтобы понять, поддерживает ли библиотека Cocoapods, достаточно заглянуть в readme, либо обратить внимание на наличие .podspec- файла в корне репозитория. Все библиотеки, опубликованные в Cocoapods, можно посмотреть на их официальном сайте. Cocoapods в вашем проекте Для понимания работы Cocoapods важны два термина — подфайл и спека. Подфайл описывает все зависимости приложения, а спека — библиотеку (её название и набор её зависимостей) для Cocoapods. Спеки хранятся в общем репозитории спек Cocoapods. Ещё их можно хранить в приватных репозиториях. Для описания конфигурации зависимостей используется подфайл. Этот файл написан на языке Ruby. 1. В файле опционально укажите источник спек. Источников может быть несколько, даже приватные. source 'https://github.com/CocoaPods/Specs.git' — официальный источник спек, который указан по умолчанию. source 'https://github.com/Artsy/Specs.gi — можно указать свои источники спек, в том числе приватные. 2. Укажите минимальную версию iOS: platform: ios, '9.0'. 3. Укажите таргет и набор зависимостей для него. Если таргетов несколько, укажите все, так как зависимости для них могут различаться. 4. Декларируйте зависимость. Первым идёт её официальное название (то самое, которое указано в спеке), затем версия. Используйте в названии правила из блока про SemVer. Есть и другие способы указать зависимости. Это может понадобиться в случаях, когда нужно использовать какую-то конкретную версию библиотеки. Например, из определённой ветки репозитория, где она хранится, или даже из определённого коммита. Либо указать путь локально, если есть только её файл.     target 'MyApp' do     pod 'GoogleAnalytics', '~> 3.1'     ...     end 1. Можно указать ресурс репозитория, с которого будем загружать библиотеку: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git' 2. Можно указать нужную ветку в репозитории: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :branch => 'dev' 3. Или тег: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :tag => '0.7.0' 4. Или коммит: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :commit => '082f8310af' 5. Можно указать на локальный под (путь должен вести на папку, где лежит .podspec): pod 'AFNetworking', :path => '~/Documents/AFNetworking' Рядом с подфайлом обычно лежит Podfile.lock. Этот файл описывает конечные версии зависимостей после их установки. Он обычно так же заливается в GitHub, как и сам Podfile, чтобы избежать ошибок и рассинхрона при установке зависимостей у разных разработчиков. Ещё он ускоряет выполнение pod install. Чтобы приступить к работе, запустите pod install. Можно выполнить pod update, если вы хотите обновить все библиотеки в проекте, иначе же будут использоваться фиксированные версии зависимостей из Podfile.lock. Как Cocoapods работает под капотом Pod install генерирует workspace — файл .xcworkspace, состоящий из двух проектов — проекта с вашим приложением и отдельного проекта, в котором хранятся зависимости (поды). Чтобы всё работало корректно, всегда открывайте workspace-файл. Если внимательнее посмотреть, что происходит, можно увидеть, что продукт этого проекта линкуется в основной проект: Также Cocoapods добавляет пару скриптов в Build Phases: Поддержка Cocoapods в вашей библиотеке Чтобы библиотека была доступна через Cocoapods, нужно сначала описать библиотеку или под при помощи спеки, собрать её и отправить спеку в приватный либо глобальный репозиторий Cocoapods. Разберёмся, как это сделать. 1. Создайте спеку и файл лицензии. Лицензию нужно добавлять для любой библиотеки, и чаще всего используется лицензия MIT — самая популярная лицензия для программ с открытым исходным кодом. Её текст можно найти в интернете. Здесь нужно описать вашу библиотеку. Файл принято форматировать так, чтобы вторая часть строки была выровнена.     Pod::Spec.new do |spec|     # Название библиотеки. Именно его будут указывать разраб     spec.name                    = 'NAME'      # Версия     spec.version                 = '1.2.1'      # Описание     spec.summary                 = 'Describe your framework.     # Здесь можно указать путь на вебсайт или просто GitHub-     spec.homepage                = 'https://xxx'      # Путь к лицензии. Да, её тоже нужно создать     spec.license                 = { type: 'MIT', file: 'LIC     # Автор библиотеки     spec.author                  = { \"Your Name\" => 'your-em     # Путь к исходным файлам (чаще всего это ваш репозитоий)     spec.source                  = { :git => 'https://github     # Минимальная поддерживаемая версия iOS и Swift     spec.ios.deployment_target   = '13.0'      spec.swift_version           = '5.0'     # Если библиотека использует локальный фреймворк, его ну     spec.vendored_frameworks     = 'Path/To/Files/NameOfVend 2. Проверьте, что всё корректно. Следующая команда валидирует спеку — проверяет, что она не имеет ошибок, а проект билдится — pod lib lint. 3. Отправьте спеку в репозиторий спек. Как правило, это делается уже на финальном этапе разработки — после того, как прошла проверка и вы готовы публиковать работу, — pod trunk push NAME.podspec. Carthage     # Указываем вайлдкард-пути к файлам, которые необходимо      spec.source_files            = 'Path/To/Files/**/*.{swif     # Путь к ресурсам     spec.resources               = 'Path/To/Files/Resources/     # Укажите здесь все ассеты, которые вам нужны, включая .     spec.resource_bundles        = {'BundleName' => ['Path/T     # Указываем зависимости     spec.dependency           \"Alamofire\", \"~> 5.0.0\"      # И системные зависимости     spec.frameworks           = 'Foundation'      spec.ios.frameworks       = 'UIKit'     end Carthagе — это децентрализованный менеджер зависимостей, которые не нужно «поддерживать» разработчику. Carthage сам выкачивает библиотеку из репозитория, компилирует её и предоставляет в готовом виде. Поэтому, если хотите использовать Carthage, не нужно искать в репозитории с библиотекой определённые инструкции или слова о том, что он поддерживается, — просто попробуйте. Нет у Carthage и централизованного источника, как у Cocoapods. Этим источником, по сути, является сам GitHub. Конечно, этого не всегда бывает достаточно. Некоторые библиотеки Carthage просто не может «переварить» — что-то не собирается, а что-то отваливается. Задача разработчика — проверить, всё ли работает корректно. Наконец, Carthage может автоматически подхватить уже собранный фреймворк, если он прикреплён к соответствующему релизу на GitHub. Carthage в вашем проекте Зависимости в Carthage описываются в текстовом файле Cartfile. Он очень похож на Podfile, только гораздо проще по структуре. Мы указываем источник, имя зависимости и версию. 1. Источник зависимости. Как правило, GitHub, либо git для универсальных репозиториев Git, размещённых в другом месте. Ключевое слово git сопровождается путём к репозиторию, будь то удалённый URL-адрес, используя git://, http://, или ssh://, или локальный — путём к репозиторию git на компьютере разработчика. 2. Название. Через слеш указывается владелец и название репозитория, которые можно подсмотреть в ссылке на репозиторий. 3. Версия. Особенности указания версий мы уже рассмотрели в блоке про семантическое версионирование, но можно также указывать и ветки. Примеры: 1. Версия — github \"Alamofire/Alamofire\" == 2.0 2. Ветка — github \"username/project\" \"branch\" 3. Локальный проект — git \"file:///directory/to/project\" \"branch\" 4. Бинарник (путь указывается в json) — binary \"https://my.domain.com/release/MyFramework.json\" ~> 2.3 После описания зависимостей потребуется ещё несколько дополнительных шагов. 1. Вызовите carthage update. Carthage склонирует репозитории с указанных в Cartfile путей и затем сбилдит для каждой зависимости фреймворк. После окончания работы вы увидите несколько файлов и каталогов: 2. Полученные фреймворки затем вручную перетащите из папки Build в проект в секцию Frameworks, Libraries и Embedded Content. 3. Carthage требует добавления нового скрипта в Build Phases в качестве workaround бага Apple. Это позволяет dSYM- файлам корректно подтягиваться при архивации. 4. Перейдите в Build Phases и добавьте новый Run Script. Добавьте следующую команду: /usr/local/bin/carthage copy- frameworks. 5. Нажмите на плюс под Input Files и добавьте запись для каждой платформы: $(SRCROOT)/Carthage/Build/iOS/Alamofire.framework. Как Carthage работает под капотом Как и в случае с Cocoapods, создаётся лок-файл Cartfile.resolved, который фиксирует конечные версии зависимостей. Каталог Carthage содержит два подкаталога: 1. Build. Содержит собранный фреймворк для каждой зависимости. 2. Checkouts. Содержит клонированные исходники, которые Carthage потом компилирует. Это своего рода кэш, который ускоряет работу. Поддержка Carthage в вашей библиотеке Для поддержки Carthage достаточно сделать релиз на GitHub. Но никогда не помешает проверить, всё ли билдится корректно в вашем случае и, если есть какие-то проблемы, решить их. Для этого, выполните carthage build --no-skip-current. Carthage попытается сбилдить ваш проект и создаст в нём папку Carthage. Также стоит обратить внимание, что он видит только те схемы, которые помечены в проекте как Shared. SPM Или Swift Package Manager — официальный менеджер зависимостей от Apple. Работа с ним происходит прямо в Xcode, а .Xcodeproj файлы больше не нужны (хотя при желании есть возможность сгенерировать файл проекта из SPM-пакета). Если заглянуть в репозиторий библиотеки, то первое, что будет говорить о поддержке SPM, — это наличие Package.swift-файла. SPM в вашем проекте Добавление зависимостей в проект происходит в секции Package Dependencies. 1. Для начала выберите File > Swift Packages > Add package dependecy. 2. Найдите модуль по поиску, либо введите URL репозитория. 3. Укажите версию: Как и во всех предыдущих случаях, SPM создает Package.resolved, назначение которого то же, что и у других менеджеров зависимостей. В меню File > Swift Packages доступно несколько действий: • Reset Package Caches удаляет кэши из Derived Data. • Resolve Package Versions резолвит версии зависимостей. Создаёт Package.resolved, если его не существует, но не меняет его, если он уже есть. Может потребоваться, если Package.resolved был, например, обновлён извне. • Update to Latest Packages Versions обновляет версии пакетов, может модифицировать Package.resolved. Поддержка SPM в вашей библиотеке Если вы разрабатываете библиотеку и хотите распространять её через SPM, нужно будет создать Package.swift файл. Таким образом может быть сконфигурирован и обычный проект, что позволяет отойти от использования project-файлов. Конфигурация через SPM выглядит чище и более проста для понимания и редактирования, чем стандартные project-файлы, которые зачастую нечитаемы. Если вы пишете библиотеку и хотите распространять её через SPM, обязательно убедитесь, что потенциальные потребители библиотеки не используют Cocoapods. Одновременная поддержка Cocoapods и SPM — часто непростая задача. import PackageDescription   let package = Package(     // Название нашего пакета     name: \"Resources\",     // Платформы, которые поддерживаются нашим пакетом     platforms: [         .iOS(.v11),     ],     // То, что будут использовать сторонние приложения     products: [         .library(             name: \"Resources\",             // Динамический или статический продукт             // по дефолту значение nil - SPM сам будет поним             // преференция, скорее всего, будет отдаваться .             type: .dynamic,             targets: [\"Resources\"]),     ],         // Зависимости, необходимые для работы нашего пакета   // здесь они просто загружаются, добавляются они в target     dependencies: [         // Название пакета, путь к нему и источник (ветка, в         .package(             name: \"R.swift.Library\",              url: \"https://github.com/mac-cain13/R.swift.Libr             branch: \"master\"),         .package(             name: \"SVGKit\",             url: \"https://github.com/SVGKit/SVGKit.git\", И как выбрать подходящий менеджер             .upToNextMajor(from: \"3.0.0\")),         .package(             name: \"PhoneNumberKit\",             url: \"https://github.com/marmelroy/PhoneNumberKi             from: \"3.3.4\"),         // Пример подключения локального пакета         .package(path: \"../Core\")     ],     targets: [         // Это то, из чего мы будем складывать наш продукт         .target(             name: \"Resources\",             dependencies: [                 // Здесь мы указываем зависимости, которые м                 .product(name: \"RswiftDynamic\", package: \"R.             ],             resources: [                 // Все ресурсы, которые мы хотим использоват                 // Путь к ним относительный от Sources/имя_п                 .process(\"Resources\")             ])     ] ) Универсального ответа нет — выбор менеджера зависимостей зависит исключительно от нужд и особенностей вашего проекта. SPM чаще всего не будут поддерживать старые библиотеки, в то время как Cocoapods вполне могут не поддерживать новые (хотя это и большая редкость, чем первое). Carthage в этом плане может показаться универсальнее. Хоть он более громоздкий и сложный во взаимодействии, но он единственный разделяет шаги управления зависимостями и добавления их в проект. Это даёт контроль над ситуацией и ускоряет разработку — не будет лишних пересборок или заморозки интерфейса, потому что нужно зарезолвить зависимости. Над материалом работали: • текст — Аня Кочешкова, Ника Черникова, • редактура — Виталик Балашов, • иллюстрации — Юля Ефимова. Делимся железной экспертизой от практик в нашем телеграм-канале red_mad_dev. А полезные видео складываем на одноимённом YouTube-канале. Присоединяйся! Теги:   red_mad_robot , ios , cocoapods , carthage , spm , swift package manager , зависимости , управление зависимостями Хабы:   Блог компании red_mad_robot , Разработка под iOS +1 3 1 red_mad_robot №1 в разработке цифровых решений для бизнеса 57 Карма 13 Рейтинг @redmadrobot Пользователь Комментарии 1 Публикации ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты ИНФОРМАЦИЯ Сайт redmadrobot.ru Дата регистрации 16 августа 2009 Дата основания 16 ноября 2008 Численность 1 001–5 000 человек Местоположение Россия Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "1",
         "Cloud4Y",
         null,
         "71.07",
         "Автор оригинала: Yashwant Singh",
         "#1 Корпоративный облачный провайдер",
         "71.07 Рейтинг Cloud4Y #1 Корпоративный облачный провайдер Автор оригинала: Yashwant Singh Взлом Instagram*‑аккаунта — популярный запрос в поисковиках. Поэтому есть смысл рассказать о том, как это обычно работает. Просто для того, чтобы вы знали, откуда может пойти атака.  Cloud4Y 5 часов назад OSINT & Hacking — как работает фишинг для нельзяграма  Простой  3 мин  1.1K Блог компании Cloud4Y , Информационная безопасность *, Социальные сети и сообщества Обзор  Перевод КАК СТАТЬ АВТОРОМ Чтобы начать попытки заполучить доступ к аккаунту, вы должны знать ник человека, которого вы пытаетесь взломать. Так что небольшая разведка будет очень кстати. Только не увлекайтесь. Существуют различные инструменты для разведки, в первую очередь, поиск пользователя в конкретной соцсети с целью узнать его ник. Я нашёл отличный инструмент под названием «Slash», который можно использовать для поиска любых учётных записей пользователя, если он везде регистрируется под одним ником. Ставим Slash git clone https://github.com/theahmadov/slash  cd slash  pip install -r requirements.txt  python slash.py help Я проверил Slash на себе, и посмотрите на эти результаты. Некоторые из учетных записей, перечисленных здесь, были созданы много лет назад. Slash — это простой консольный инструмент. Но вы также можете использовать такие инструменты, как WhatsMyName Web, который совершенно бесплатен. Вот, посмотрите. Я проверил WhatsMyName на себе. Мой ник «earthtoyash». Теперь, когда мы знаем больше о нашем пользователе, можно использовать эти знания. Например, через отправку фишинговых ссылок. Для этого создадим полезную нагрузку с помощью Zphisher. Ставим Zphisher с GitHub Клонируем репозиторий: git clone --depth=1 https://github.com/htr-tech/zphisher.git Запускаем файл zphisher.sh: cd zphisher && ./zphisher.sh При первом запуске он установит зависимости и на этом всё. Система скажет, что Zphisher установлен. После установки вам нужно будет снова запустить zphisher.sh в каталоге zphisher командой ./zphisher , и тогда вы получите что-то вроде этого: Как вы можете видеть, есть много вариантов и шаблонов, благодаря чему любой может заниматься фишингом. Мы сосредоточимся на нельзяграме. Итак, введите «2» и нажмите Enter. Следующий шаг полностью зависит от вас, выберите любой из них. Затем появится окно с выбором. Я выбрал третий вариант, так как он минималистичный и удобен для того, чтобы показать возможности инструмента. Опять же, чтобы все было просто, я пропущу пользовательский порт, но если вы уже используете порт 8080, то можете изменить его на 8000. Если нет, оставляйте всё как есть. Также важно маскировать URL, ну просто в целях безопасности. Можно использовать что‑то вроде этого: Всё, Zphisher создал фишинговую ссылку, которую можно отправить жертве. Как только она нажмёт на ссылку, вы начнёте получать информацию о ней. Например, IP‑адреса, имена пользователей, пароли и т. д. Ещё можно использовать обратный поиск IP, чтобы определить местоположение вашей цели и многое другое. Итак, вот эти фишинговые ссылки. При нажатии открывается страница, похожая на официальную страницу входа в запрещённую соцсеть. Вот она, нехорошая После ввода учётных данных можно получить много информации на «хакерской» стороне терминала. Вот так, господа и дамы, можно без особого труда взломать учетную запись в нельзяграме. Поэтому в очередной раз напоминаем: нельзя нажимать на ссылки, которым вы не доверяете. Само собой разумеется, не используйте информацию из этой статьи с намерением причинить кому-либо вред. OSINT законен, но фишинг и кража личных данных даже в запрещённой соцсети является уголовным преступлением. И да, * Организация Meta, а также её продукт Instagram, на которые мы ссылаемся в этой статье, признаны экстремистскими и запрещены на территории РФ. Спасибо за внимание! Cloud4Y #1 Корпоративный облачный провайдер Сайт Facebook Twitter ВКонтакте Telegram Что ещё интересного есть в блоге Cloud4Y → Информационная безопасность и глупость: необычные примеры → NAS за шапку сухарей → Взлом Hyundai Tucson, часть 1, часть 2 → Столетний язык программирования — какой он → 50 самых интересных клавиатур из частной коллекции Теги:  взлом, соцсети, osint Хабы:  Блог компании Cloud4Y, Информационная безопасность,  Социальные сети и сообщества  0  19  1 149 Карма 53.5 Рейтинг Cloud4Y @Cloud4Y Корпоративный облачный провайдер Сайт ВКонтакте Telegram  Комментарии 1 Публикации ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  И Н Ф О Р М А Ц И Я Сайт www.cloud4y.ru Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Дата регистрации 29 июля 2011 Дата основания 2009 Численность 51–100 человек Местоположение Россия Представитель Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "2",
         "Cloud4Y",
         null,
         "71.07",
         "Автор оригинала: Yashwant Singh",
         "#1 Корпоративный облачный провайдер",
         "71.07 Рейтинг Cloud4Y #1 Корпоративный облачный провайдер Автор оригинала: Yashwant Singh Взлом Instagram*аккаунта — популярный запрос в поисковиках. Поэтому есть смысл рассказать о том, как это обычно работает. Просто для того, чтобы вы знали, откуда может пойти атака.  5 часов назад Простой3 мин Cloud4Y OSINT & Hacking — как работает фишинг для нельзяграма 1.1K Блог компании Cloud4Y ,  Информационная безопасность* ,  Социальные сети и сообщест Обзор Перевод КАК СТАТЬ АВТОРОМ Чтобы начать попытки заполучить доступ к аккаунту, вы должны знать ник человека, которого вы пытаетесь взломать. Так что небольшая разведка будет очень кстати. Только не увлекайтесь. Существуют различные инструменты для разведки, в первую очередь, поиск пользователя в конкретной соцсети с целью узнать его ник. Я нашёл отличный инструмент под названием «Slash», который можно использовать для поиска любых учётных записей пользователя, если он везде регистрируется под одним ником. Ставим Slash Я проверил Slash на себе, и посмотрите на эти результаты. Некоторые из учетных записей, перечисленных здесь, были созданы много лет назад. git clone https://github.com/theahmadov/slash  cd slash  pip install -r requirements.txt  python slash.py help Slash — это простой консольный инструмент. Но вы также можете использовать такие инструменты, как WhatsMyName Web, который совершенно бесплатен. Вот, посмотрите. Я проверил WhatsMyName на себе. Мой ник «earthtoyash». Теперь, когда мы знаем больше о нашем пользователе, можно использовать эти знания. Например, через отправку фишинговых ссылок. Для этого создадим полезную нагрузку с помощью Zphisher. Ставим Zphisher с GitHub Клонируем репозиторий: Запускаем файл zphisher.sh: При первом запуске он установит зависимости и на этом всё. Система скажет, что Zphisher установлен. После установки вам нужно будет снова запустить zphisher.sh в каталоге zphisher командой ./zphisher , и тогда вы получите что-то вроде этого: git clone --depth=1 https://github.com/htr-tech/zphisher.git cd zphisher && ./zphisher.sh Как вы можете видеть, есть много вариантов и шаблонов, благодаря чему любой может заниматься фишингом. Мы сосредоточимся на нельзяграме. Итак, введите «2» и нажмите Enter. Следующий шаг полностью зависит от вас, выберите любой из них. Затем появится окно с выбором. Я выбрал третий вариант, так как он минималистичный и удобен для того, чтобы показать возможности инструмента. Опять же, чтобы все было просто, я пропущу пользовательский порт, но если вы уже используете порт 8080, то можете изменить его на 8000. Если нет, оставляйте всё как есть. Также важно маскировать URL, ну просто в целях безопасности. Можно использовать чтото вроде этого: Всё, Zphisher создал фишинговую ссылку, которую можно отправить жертве. Как только она нажмёт на ссылку, вы начнёте получать информацию о ней. Например, IPадреса, имена пользователей, пароли и т. д. Ещё можно использовать обратный поиск IP, чтобы определить местоположение вашей цели и многое другое. Итак, вот эти фишинговые ссылки. При нажатии открывается страница, похожая на официальную страницу входа в запрещённую соцсеть. Вот она, нехорошая После ввода учётных данных можно получить много информации на «хакерской» стороне терминала. Вот так, господа и дамы, можно без особого труда взломать учетную запись в нельзяграме. Поэтому в очередной раз напоминаем: нельзя нажимать на ссылки, которым вы не доверяете. Само собой разумеется, не используйте информацию из этой статьи с намерением причинить кому-либо вред. OSINT законен, но фишинг и кража личных данных даже в запрещённой соцсети является уголовным преступлением. И да, * Организация Meta, а также её продукт Instagram, на которые мы ссылаемся в этой статье, признаны экстремистскими и запрещены на территории РФ. Спасибо за внимание! Что ещё интересного есть в блоге Cloud4Y → Информационная безопасность и глупость: необычные примеры → NAS за шапку сухарей → Взлом Hyundai Tucson, часть 1, часть 2 → Столетний язык программирования — какой он → 50 самых интересных клавиатур из частной коллекции Теги:   взлом , соцсети , osint Хабы:   Блог компании Cloud4Y , Информационная безопасность , Социальные сети и сообщества 0 19 1 Cloud4Y #1 Корпоративный облачный провайдер Сайт Facebook Twitter ВКонтакте Telegram 149 Карма 53.5 Рейтинг Cloud4Y @Cloud4Y Корпоративный облачный провайдер Сайт ВКонтакте Telegram Комментарии 1 Публикации ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты ИНФОРМАЦИЯ Сайт www.cloud4y.ru Дата регистрации 29 июля 2011 Дата основания 2009 Численность 51–100 человек Местоположение Россия Представитель Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "3",
         "Southbridge",
         null,
         "280.79",
         "Автор оригинала: Seifeldin Mahjoub",
         "Обеспечиваем стабильную работу highload-проектов",
         "4.29 Оценка 280.79 Рейтинг Southbridge Обеспечиваем стабильную работу highload-проектов Автор оригинала: Seifeldin Mahjoub Перевели статью о создании пайплайна для развертывания статического веб-сайта на AWS S3 Bucket на примере Gitlab CI/CD, чтобы быстро вникнуть в основы технологии и начать применять ее в работе. В статье рассматриваются базовые концепции CI и CD, а также этапы CI/CD-пайплайна.  2 часа назад 5 мин zubarek Быстрое начало работы с Gitlab CI/CD: пайплайн для веб-сайта на AWS S3 Bucket 362 Блог компании Southbridge ,  Тестирование IT-систем* ,  Системное администрирование* ,  Туториал Перевод КАК СТАТЬ АВТОРОМ От автора Мне повезло быть частью некоторых профессиональных команд, каждая из которых применяла несколько DevOps практик. И меня поразило то, как качество кода, скорость разработки и позитивный настрой команды коррелируют с CI/CD-пайплайном. По моему мнению, зрелость пайплайна может служить прекрасным показателем опытности разработчика, качества кода и эффективности всей команды. Во многих случаях, которые я наблюдал, пайплайны были выстроены либо DevOps-инженером, либо отдельной DevOps- командой. Да и последний отчет State of CD 2022 продемонстрировал, что только 22% разработчиков создают пайплайны. Моя цель — увеличить это число: помочь разработчикам взять на себя ответственность за пайплайны, выстраивать непрерывный процесс доставки и создавать качественный код. В статье рассматриваются фундаментальные концепции CI и CD. Что такое CI/CD? Многие бизнесы применяют фреймворки Agile, так как они позволяют менять приоритеты и повышать скорость доставки. Кроме всего прочего, такой подход улучшает атмосферу в команде и помогает увеличить прибыль. Если ваша компания следует по пути Agile, то принятие культуры, философии и практик DevOps станет ее большим преимуществом. Модное словечко последних десятилетий, DevOps сегодня считается настоящим стандартом индустрии. CI/CD — это практика DevOps, которая помогает разработчикам ПО доставлять изменения в коде с высокой частотой и надежностью. «Быстрый билд, быстрый тест, быстрый фейл» При наличии автоматизированных тестов команды тяготеют к общей автоматизации задач и частым, надежным поставкам кода. Создание CI/CD-пайплайна в этом случае может привести к нескольким преимуществам. Бизнес выигрывает от снижения затрат и повышения производительности, ускорения Time to Market и адаптации к изменяющимся требованиям рынка. Команда выигрывает от быстрой обратной связи, улучшения эффективности разработки, уменьшения количества бутылочных горлышек и повышения уровня вовлеченности и удовлетворенности сотрудников. Фазы CI и CD CI — непрерывная интеграция. Непрерывная интеграция позволяет по много раз в день коммитить изменения в основную ветку вашей кодовой базы. Учитывая ограниченные когнитивные способности человека, CI стимулирует разработчиков вносить в код небольшие изменения, которые легче рассмотреть, покрыть автоматическими тестами и часто релизить. Это позволяет избежать напряженных и переполненных merge conflict-ами дней подготовки к релизу с тоннами ручного тестирования. CD — непрерывная доставка. Следующий шаг после CI позволяет гарантировать, что кодовая база постоянно готова к деплою, а задеплоить ее можно одним нажатием кнопки. При этом неважно, с чем вы работаете: с масштабной распределенной системой, сложной производственной средой и т. д. Ключевой момент — автоматизация. CD — непрерывное развертывание. Последний этап зрелого CI/CD-пайплайна, где все изменения в коде автоматически развертываются в продакшн без ручного вмешательства. Само собой, для этого требуется большое количество хорошо продуманных автоматических тестов. State of CD 2022 утверждает, что «47% разработчиков применяют CI или СD, но только один из пяти использует оба подхода для автоматизации сборки, тестирования и развертывания кода». Книга Accelerate подводит итоги многолетнего исследования с использованием отчетов State of DevOps, основанных на 23 000 наборов данных компаний по всему миру. Как видите, высокопроизводительные команды могут деплоить по требованию (или несколько раз в день). Этапы CI/CD-пайплайна Стадия исходного кода — здесь запускается пайплайн. Обычно это происходит после изменений в Git-репозитории, которые проявляются в открытии нового Pull Request-а или в пуше в ветку. Другой способ заключается в настройке инструментария CI/CD для запуска пайплайна через автоматическое расписание или после запуска другого пайплайна. Стадия сборки — этап, в процессе которого происходит проверка и сборка кода. Здесь особенно полезны такие инструменты, как Docker: они обеспечивают однородную среду. Стадия тестирования — CI/CD невозможно представить без автоматизированных тестов. В конце концов, все хотят быть уверены, что изменения в коде не сломают продакшн. Стадия развертывания — на последнем этапе (после успешного прохождения всех предыдущих стадий) код можно развернуть в выбранной среде. Пример с Gitlab В этом примере будет использован Gitlab CI/CD, однако концепции аналогичны и для остальных инструментов, поэтому их можно применить к другим сервисам хостинга репозиториев. Существует несколько инструментов CI/CD, например всемирно известный Jenkins. Этот инструмент требует некоторой настройки и конфигурации, в то время как другие поставляются сервисами хостинга репозиториев (такими как GitHub Actions и Bitbucket Pipelines) с предварительной настройкой. Поэтому если ваш код размещен на Gitlab, то легче всего использовать Gitlab CI/CD, поскольку код и управление CI/CD находятся на одной платформе. Как все это может работать без настроек? Для ответа на этот вопрос стоит немного погрузиться в архитектуру Gitlab, а именно — в инстансы и раннеры. Инстансы хранят код приложения и конфигурации пайплайна. Раннеры выступают в качестве агентов, выполняющих операции в пайплайнах. В Gitlab каждый инстанс может быть подключен к одному или нескольким раннерам. Gitlab.com — это управляемый инстанс с несколькими раннерами, которые сам Gitlab и поддерживает. Следовательно, если вы используете этот инстанс, то получаете все необходимое из коробки. Приступим к работе Gitlab предлагает несколько шаблонов при создании нового проекта. Конфигурация пайплайна Gitlab CI/CD по умолчанию находится в файле .gitlab-ci.yml в корневом каталоге. Предположим, мы хотим создать простой пайплайн, который проверяет: написан, протестирован и развернут ли код. Вот несколько концепций и терминов для ознакомления перед началом работы. Пайплайн (Pipeline) Пайплайн — это набор заданий, разделенных на этапы. Gitlab предлагает различные типы пайплайнов, например parent-child или multi-project. Полный список см. здесь. Этап (Stage) Этап — это шаг в пайплайне, предоставляющий информацию о том, какие задания запускать (сборка, тестирование и т. д.). Один этап может включать одно или несколько заданий. Задание (Job) Задание — основной блок пайплайна (компиляция, линтинг и т. д.). Для каждого задания должны быть определены name и script. После выполнения всех заданий на этапе пайплайн переходит к следующему. Теперь — к коду Выстраиваем пайплайн Gitlab CI/CD, который собирает, тестирует и разворачивает статический веб-сайт в AWS S3 Bucket. Для начала создадим новый .gitlab-ci.yml 1. Определим переменные 2. Определим этапы 3. Определим задания на каждом этапе variables: # variabiles definitions for easier reuse of valu  CI_NODE_IMAGE: \"node:16.13.2\" # Pipeline stages stages:  - install  - build  - test  - deploy #install job definition install:  stage: install  image: \"$CI_NODE_IMAGE\" # variable reference  script: # Shell script that is executed by the runner.    - npm ci  cache: # List of files that should be cached between subseq    key:      files:        - package.json        - package-lock.json    paths: # directories to cache      - node_modules # Build Job definition На этом все, спасибо за внимание. Научиться работать с пайплайнами, билдами и артефактами можно на курсе Gitlab CI/CD в Слёрм. Вы узнаете, из чего состоит Gitlab и какие у него возможности и настройки, а также разберете лучшие практики построения пайплайна, особенности шаблонизации и работы с переменными. build:  stage: build  image: $CI_NODE_IMAGE  script:    - npm run build  artifacts: # list of files and directories that are attache    paths:      - dist/  cache:    key:      files:        - package.json        - package-lock.json    paths:      - node_modules    policy: pull # Test Job definition test:  stage: test  image: $CI_NODE_IMAGE  script:    - npm run test # Deploy Job definition deploy:  stage: deploy  image: registry.gitlab.com/gitlab-org/cloud-deploy/aws-base  script:    - aws s3 cp --recursive dist s3://bucket-name # copies th Southbridge Обеспечиваем стабильную работу highload-проектов Сайт Сайт 14 Карма 16.9 Рейтинг Лиза Зубарькова @zubarek Пользователь Комментировать Публикации Видеокурс доступен всегда. Посмотреть программу: https://slurm.club/3JUKdzT Теги:   ci/cd , gitlab-ci , aws , gitlab , pipeline Хабы:   Блог компании Southbridge , Тестирование IT-систем , Системное администрирование , Программирование , DevOps +7 14 0 ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ Ваш аккаунт Разделы Информация Услуги  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИНФОРМАЦИЯ Сайт southbridge.io Дата регистрации 15 ноября 2012 Дата основания 22 февраля 2008 Численность 51–100 человек Местоположение Россия Представитель Антон Скобин Войти Регистрация Публикации Новости Хабы Компании Авторы Песочница Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
        ],
        [
         "4",
         "Альфа-Банк",
         null,
         "404.32",
         "Подписаться",
         "Лучший мобильный банк по версии Markswebb",
         "404.32 Рейтинг Альфа-Банк Лучший мобильный банк по версии Markswebb Подписаться В Альфа-Банке мы внедрили ИИ-агентов, которые проектируют, разрабатывают и проверяют автотесты. При этом полностью автономно, как QA-инженеры, но в разы быстрее и точнее. Подобных примеров, когда ИИ разрабатывает автотесты от анализа требований до пул-реквеста, в нашей стране, пожалуй, ещё не было. Что умеют агенты? Анализировать контекст из Jira и Confluence, вычленяя суть задачи. Прогнозировать риски, зависимости и даже «пограничные» сценарии. Генерировать DTO для REST API и превращать ручные сценарии в Java-тесты за минуты. Сверять код с бизнес-логикой и техстандартами Альфы, защищая прод от случайных ошибок. Создавать вариативные проверки — от позитивных кейсов до сложных негативных условий. Автоматизировать рутину. В ИИ-команде QA есть несколько агентов, каждый работает над своей частью из перечная выше. Сейчас решение пилотируется в нескольких продуктовых командах, но результаты уже видны: AlfaTeam 2 часа назад ИИ-агенты в Альфа-Банке: нейросети создают автотесты без участия человека 1 мин 717 Блог компании Альфа-Банк, Искусственный интеллект, Тестирование IT-систем* Войти Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп КАК СТАТЬ АВТОРОМ Зарплаты айтишников Миграция за ночь? Легко! Без сна и с адре… Альфа-Банк Лучший мобильный банк по версии Markswebb Сайт Хабр Карьера 14 Карма 0.3 Рейтинг @AlfaTeam Пользователь Сайт Facebook ВКонтакте Другие новости mefdayy 19 минут назад Nvidia и Broadcom тестируют передовой процесс производства чипов Intel 2 мин меньше ошибок в проде, предсказуемые дедлайны и высвобожденные ресурсы для творческих задач. «Одна команда ИИ-агентов экономит десятки часов работы, увеличивает скорость релизов и находит на 30% больше багов» Святослав Соловьев, Директор по генеративному ИИ в ИТ Альфа-Банка. В ближайшее время подробнее опишем как устроены агенты, какие технологии используем и как мы измеряем их эффективность. Теги: автоматизация, искусственный интеллект, тестирование, автотесты Хабы: Блог компании Альфа-Банк, Искусственный интеллект, Тестирование IT-систем Редакторский дайджест Присылаем лучшие статьи раз в месяц Электропочта Комментарии 11 123 Подписаться "
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NameCompany</th>\n",
       "      <th>Description</th>\n",
       "      <th>Raiting</th>\n",
       "      <th>DataPublish</th>\n",
       "      <th>Activity</th>\n",
       "      <th>TextArticle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red_mad_robot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.94</td>\n",
       "      <td>6 часов назад</td>\n",
       "      <td>№1 в разработке цифровых решений для бизнеса</td>\n",
       "      <td>117.94 Рейтинг red_mad_robot №1 в разработке ц...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cloud4Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.07</td>\n",
       "      <td>Автор оригинала: Yashwant Singh</td>\n",
       "      <td>#1 Корпоративный облачный провайдер</td>\n",
       "      <td>71.07 Рейтинг Cloud4Y #1 Корпоративный облачны...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cloud4Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.07</td>\n",
       "      <td>Автор оригинала: Yashwant Singh</td>\n",
       "      <td>#1 Корпоративный облачный провайдер</td>\n",
       "      <td>71.07 Рейтинг Cloud4Y #1 Корпоративный облачны...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southbridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280.79</td>\n",
       "      <td>Автор оригинала: Seifeldin Mahjoub</td>\n",
       "      <td>Обеспечиваем стабильную работу highload-проектов</td>\n",
       "      <td>4.29 Оценка 280.79 Рейтинг Southbridge Обеспеч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Альфа-Банк</td>\n",
       "      <td>NaN</td>\n",
       "      <td>404.32</td>\n",
       "      <td>Подписаться</td>\n",
       "      <td>Лучший мобильный банк по версии Markswebb</td>\n",
       "      <td>404.32 Рейтинг Альфа-Банк Лучший мобильный бан...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     NameCompany  Description  Raiting                         DataPublish  \\\n",
       "0  red_mad_robot          NaN   117.94                       6 часов назад   \n",
       "1        Cloud4Y          NaN    71.07     Автор оригинала: Yashwant Singh   \n",
       "2        Cloud4Y          NaN    71.07     Автор оригинала: Yashwant Singh   \n",
       "3    Southbridge          NaN   280.79  Автор оригинала: Seifeldin Mahjoub   \n",
       "4     Альфа-Банк          NaN   404.32                         Подписаться   \n",
       "\n",
       "                                           Activity  \\\n",
       "0      №1 в разработке цифровых решений для бизнеса   \n",
       "1               #1 Корпоративный облачный провайдер   \n",
       "2               #1 Корпоративный облачный провайдер   \n",
       "3  Обеспечиваем стабильную работу highload-проектов   \n",
       "4         Лучший мобильный банк по версии Markswebb   \n",
       "\n",
       "                                         TextArticle  \n",
       "0  117.94 Рейтинг red_mad_robot №1 в разработке ц...  \n",
       "1  71.07 Рейтинг Cloud4Y #1 Корпоративный облачны...  \n",
       "2  71.07 Рейтинг Cloud4Y #1 Корпоративный облачны...  \n",
       "3  4.29 Оценка 280.79 Рейтинг Southbridge Обеспеч...  \n",
       "4  404.32 Рейтинг Альфа-Банк Лучший мобильный бан...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"dataset.csv\")\n",
    "df2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
